1
00:00:00,000 --> 00:00:15,440
Good afternoon, everyone.

2
00:00:15,440 --> 00:00:17,400
Welcome again to the Heilich Center.

3
00:00:17,400 --> 00:00:21,960
I'm Gerald Her with the Associate Director at Heilich.

4
00:00:21,960 --> 00:00:29,240
I want to invite everyone, rather, to our third roundtable of the day entitled,

5
00:00:29,240 --> 00:00:34,200
Coding Fiction, Metafiction, the Parcelation of What Isn't There.

6
00:00:34,200 --> 00:00:44,040
I'm going to take a moment just to describe the brief bios of our wonderful cast of participants.

7
00:00:44,040 --> 00:00:50,720
So first, Unina Hoffman is an Assistant Professor of English at the U.S. Murchin Marine Academy.

8
00:00:50,720 --> 00:00:56,480
Unita's research applies systems theory and phenomenology to 20th century literature

9
00:00:56,480 --> 00:00:58,680
and the global systems novel.

10
00:00:58,680 --> 00:01:05,280
Unina's first book, The Voices of David Foster Wallace, used concepts from narrative theory,

11
00:01:05,280 --> 00:01:11,360
rhetoric, and phenomenology to examine the experiences of reading through novelistic progression

12
00:01:11,360 --> 00:01:13,000
and narrative voices.

13
00:01:13,000 --> 00:01:18,640
Unina's new book project, Ending the Endless, examines the way that contemporary system

14
00:01:18,640 --> 00:01:22,800
novels understand the globe.

15
00:01:22,800 --> 00:01:28,440
Next is Peter A. Glor, who is a research scientist at the Center for the Collective

16
00:01:28,440 --> 00:01:35,520
Intelligence at MIT's Sloan School of Management, where he leads a 20-year project exploring

17
00:01:35,520 --> 00:01:38,440
collaborative innovation networks.

18
00:01:38,440 --> 00:01:45,840
He is also founder and chief creative officer of software company Galaxy Advisors and honorary

19
00:01:45,840 --> 00:01:52,440
professor at University of Cologne and at Gillian University in Chan-Chun, China.

20
00:01:52,440 --> 00:02:00,800
He also taught at Universidad Catarica in Santiago de Chile, Alta University Helsinki, and a

21
00:02:00,800 --> 00:02:07,240
few other universities I have a difficult time pronouncing.

22
00:02:07,240 --> 00:02:11,080
University of Applied Sciences, Northwestern Switzerland, and the University of Applied

23
00:02:11,080 --> 00:02:12,720
Sciences in Luzern.

24
00:02:12,720 --> 00:02:19,000
Earlier, he was a partner with Deloitte and PwC and a manager at UBS.

25
00:02:19,000 --> 00:02:23,160
He got his PhD in computer science from the University of Zurich and was a post-doc at

26
00:02:23,160 --> 00:02:28,040
the MIT lab for computer science.

27
00:02:28,040 --> 00:02:33,400
Mark Hanson is the David and Heli Gurley Brown professor of journalism and the director

28
00:02:33,400 --> 00:02:37,920
of the Brown Institute for Media Innovation at Columbia University.

29
00:02:37,920 --> 00:02:42,720
He's had over 20 years of collaborations with designers, architects, and artists, helping

30
00:02:42,720 --> 00:02:48,920
make work that has been exhibited in the Museum of Modern Art in New York, the Whitney Museum,

31
00:02:48,920 --> 00:02:55,240
the Centro Deart Rhinosophia, the London Science Museum, the Cartaget Foundation in Paris,

32
00:02:55,240 --> 00:03:00,360
and the lobbies of the New York Times building and the public theater in Manhattan.

33
00:03:00,360 --> 00:03:05,880
Hanson holds a BS in Applied Math in the University of California, Davis, and PhD in MA in Statistics

34
00:03:05,880 --> 00:03:10,200
from the University of California, Berkeley.

35
00:03:10,200 --> 00:03:15,400
Jonathan Kramnik is a Maynard Mack professor of English at Yale University.

36
00:03:15,400 --> 00:03:20,400
His research and teaching is in 18th century literature and philosophy, philosophical approaches

37
00:03:20,400 --> 00:03:24,320
to literature, and cognitive science and the arts.

38
00:03:24,320 --> 00:03:28,560
He's the author of three books, his new book Paper Minds, Literature and the Ecology of

39
00:03:28,560 --> 00:03:34,200
Consciousness, asks what distinctive knowledge that literary disciplines and literary form

40
00:03:34,200 --> 00:03:40,680
can contribute to discussions of perceptual consciousness, created and natural environments,

41
00:03:40,680 --> 00:03:43,000
and skilled engagement with the world.

42
00:03:43,000 --> 00:03:48,000
Questions have appeared in critical inquiry, representations, and elsewhere.

43
00:03:48,000 --> 00:03:55,200
Before that, actions and objects from Hobbes to Richardson's Stanford 2010 considered representations

44
00:03:55,200 --> 00:04:01,800
of mind and material objects along with theories of action during the long 18th century.

45
00:04:01,800 --> 00:04:07,640
Dr. Nikos A. Saligaros is professor of mathematics and architecture at the University of Texas

46
00:04:07,640 --> 00:04:13,640
at San Antonio, and internationally recognized architectural theorist and urbanist.

47
00:04:13,640 --> 00:04:20,280
His publications include the books, Algorithmic Sustainable Design, Anti-Architecture and Deconstruction,

48
00:04:20,280 --> 00:04:25,320
A Theory of Architecture, Principles of Urban Structure and Unified Architectural Theory,

49
00:04:25,320 --> 00:04:27,760
plus numerous scientific articles.

50
00:04:27,760 --> 00:04:32,920
He co-authored the Michael, with Michael Mahafie, the books designed for a living planet and

51
00:04:32,920 --> 00:04:36,280
a new patterned language for growing regions.

52
00:04:36,280 --> 00:04:40,400
Saligaros collaborated with the visionary architect Christopher Alexander over more than

53
00:04:40,400 --> 00:04:47,040
20 years in editing Alexander's monumental, four volume book, The Nature of Order.

54
00:04:47,040 --> 00:04:49,160
I think I'll stop there.

55
00:04:49,160 --> 00:04:53,880
There's more to say about every one of these wonderful participants, and we'll get started

56
00:04:53,880 --> 00:05:01,800
with our talk.

57
00:05:01,800 --> 00:05:13,680
Okay, so this is a fairly broad topic I think, and I am amazed by the wide range of expertise

58
00:05:13,680 --> 00:05:16,160
we've assembled here today.

59
00:05:16,160 --> 00:05:28,840
So who wants to begin the narrative of this talk this evening?

60
00:05:28,840 --> 00:05:29,840
Why are we here?

61
00:05:29,840 --> 00:05:32,680
This is a conference on coding.

62
00:05:32,680 --> 00:05:36,080
Is there someone who can make sense of that relative to it?

63
00:05:36,080 --> 00:05:42,760
Oh, I have no idea what the topic is about, but it's fascinating, and the participants

64
00:05:42,760 --> 00:05:48,360
are fascinating people with fascinating expertise, so I was sort of waiting for a combustion

65
00:05:48,360 --> 00:05:53,560
effect of different ideas coming together.

66
00:05:53,560 --> 00:06:01,800
I was attracted to attend because of the link between coding and memes and literature, since

67
00:06:01,800 --> 00:06:07,480
I find many of the explanations I'm looking for in architectural design in literature,

68
00:06:07,480 --> 00:06:12,440
especially the topic literature, 1984 in a brave new world.

69
00:06:12,440 --> 00:06:14,720
I'll stop there.

70
00:06:14,720 --> 00:06:17,520
I'll add one more point.

71
00:06:17,520 --> 00:06:26,120
I guess my thinking about the subject heading was that there's the idea in coding and writing

72
00:06:26,120 --> 00:06:32,680
programs about narratives that it's fungible, that you can take it from here to there.

73
00:06:32,680 --> 00:06:37,720
It's all based on this code, and you can insert it wherever you want to create whatever effect

74
00:06:37,720 --> 00:06:41,720
you want, and it could be a literary effect, it could be from misinformation, which was

75
00:06:41,720 --> 00:06:44,480
the subject to our last roundtable.

76
00:06:44,480 --> 00:06:51,640
I could talk about coding because I was a coder at MIT 20 years ago, and I used to dream

77
00:06:51,640 --> 00:06:57,560
in hyper-talk, which was a programming language, and I still think today when I look at AI,

78
00:06:57,560 --> 00:07:02,800
that this is sort of the only field where people have their own language to communicate

79
00:07:02,800 --> 00:07:10,840
namely type v-steps for AI, and so I'm not sure whether there is other fields.

80
00:07:10,840 --> 00:07:16,040
I mean, in a sense, you have always liked the language of mathematics and so on, but for

81
00:07:16,040 --> 00:07:23,320
me, in software and in AI, I'm at the center for collective intelligence, and other level

82
00:07:23,320 --> 00:07:28,520
of collective intelligence, which is, you have this language that people use to communicate,

83
00:07:28,520 --> 00:07:33,280
which is called Python or some other programming language, and from my own experience, if you

84
00:07:33,280 --> 00:07:39,400
are totally immersed, now I'm useless as a coder, about 20 years ago, that's what I was hired,

85
00:07:39,400 --> 00:07:42,600
and you start reading and thinking in that programming language.

86
00:07:42,600 --> 00:07:47,000
But you can't do it, we're doing now in that programming language, I assume, this kind

87
00:07:47,000 --> 00:07:48,000
of communication.

88
00:07:48,000 --> 00:07:53,320
The gigs are, I mean, they are very introvert, and we have a hard time looking at each other

89
00:07:53,320 --> 00:07:54,320
into the AI.

90
00:07:54,320 --> 00:07:57,360
That's one of the problems when you are a coder.

91
00:07:57,360 --> 00:08:03,880
But I'm curious about the use of the word language here in the, you know, in the relationship

92
00:08:03,880 --> 00:08:11,320
to narrative, is what we're doing, we're a highly specialized human activity of communicating

93
00:08:11,320 --> 00:08:17,680
thoughts and meaning versus in spoken form, which then can take a kind of narrative shape.

94
00:08:17,680 --> 00:08:25,520
And I'm just wondering, I'm always wondered, like, you know, how much one can extend the

95
00:08:25,520 --> 00:08:30,960
meaning of the word language across those two different uses?

96
00:08:30,960 --> 00:08:37,680
Well, in my small startup, we are actually, you don't really need to have to talk too

97
00:08:37,680 --> 00:08:39,560
much anymore.

98
00:08:39,560 --> 00:08:47,960
But this other level of looking at each other and sharing the code seems to be enough sometimes.

99
00:08:47,960 --> 00:08:55,280
So in that sense, the collective intelligence seems to get to a higher level, that's my

100
00:08:55,280 --> 00:08:57,120
perception.

101
00:08:57,120 --> 00:09:01,200
I think it could be possible to have a conversation in code.

102
00:09:01,200 --> 00:09:06,800
It would just be a different kind of conversation because you would presumably have, it's not

103
00:09:06,800 --> 00:09:12,560
like you have language as this sort of flow, but rather you have the production of something

104
00:09:12,560 --> 00:09:19,320
that is encoded, it's a little bit more entrenched, the coded object, and then you put the code

105
00:09:19,320 --> 00:09:23,040
object out there and then someone else looks at it and then they write their own, which

106
00:09:23,040 --> 00:09:27,560
actually sounds a little bit to me like a historical model of what literature is like as well,

107
00:09:27,560 --> 00:09:32,280
that there is an encoded object that gets placed out there and then it does take a while, it's

108
00:09:32,280 --> 00:09:33,680
not this.

109
00:09:33,680 --> 00:09:38,400
But it seems similar.

110
00:09:38,400 --> 00:09:45,320
I was thinking that the idea of a metaphiction where you will cite one work of literature

111
00:09:45,320 --> 00:09:50,120
within the body of a second of work of literature is you're sort of lifting it, appropriating

112
00:09:50,120 --> 00:09:58,240
it, commodifying it in some way, and I think that coding is just rich for that reason,

113
00:09:58,240 --> 00:10:01,760
whether or not it's rich in a good way or not is a different question, but that sort

114
00:10:01,760 --> 00:10:06,000
of, I'm calling you the fungibility of those codes and the sort of narratives that can

115
00:10:06,000 --> 00:10:10,760
be taken out here and put over there reminds me of a metaphiction.

116
00:10:10,760 --> 00:10:22,920
I'm a little worried with the transition from coding to language because coding is a language

117
00:10:22,920 --> 00:10:29,000
spoken only by people who know how to code, whereas language should be understandable

118
00:10:29,000 --> 00:10:31,960
by other people, otherwise you cannot communicate.

119
00:10:31,960 --> 00:10:36,000
If you write something in a computer language and you give it to the ordinary person, it's

120
00:10:36,000 --> 00:10:37,720
meaningless.

121
00:10:37,720 --> 00:10:48,680
So I say two entirely different concepts here and I don't see how they can get mixed.

122
00:10:48,680 --> 00:10:55,760
For something to be useful to society in literature, I think it has to have a language that's

123
00:10:55,760 --> 00:11:02,160
universally available and a language that is even translatable, say from English, to finish.

124
00:11:02,160 --> 00:11:07,640
But if you write something in Python, it cannot be translated into a minutes for a

125
00:11:07,640 --> 00:11:08,640
spoken language.

126
00:11:08,640 --> 00:11:09,640
That was my point.

127
00:11:09,640 --> 00:11:16,280
I have some of the similar reservations as you do in the codes around some of these questions

128
00:11:16,280 --> 00:11:24,840
about how applicable it is across communities and then also about how helpful it is for

129
00:11:24,840 --> 00:11:31,720
our understanding of human cultures, written and spoken and aesthetic artifacts to think

130
00:11:31,720 --> 00:11:39,200
of them in terms of the language of the sticking from computer science, which has a ton of

131
00:11:39,200 --> 00:11:44,840
money and cultural power behind it, but you might not have as much explanatory force behind

132
00:11:44,840 --> 00:11:45,840
it.

133
00:11:45,840 --> 00:11:48,960
I think part of what Unina was suggesting is maybe thinking about it the other way around,

134
00:11:48,960 --> 00:11:54,800
which is that there's something literary about the computer world, but that would make it

135
00:11:54,800 --> 00:12:02,080
that notion of language and narrative more parasitic on what we're doing here and also

136
00:12:02,080 --> 00:12:11,680
the way in which many other people live their lives and practice their communicative acts

137
00:12:11,680 --> 00:12:17,360
and also spend time with works of art, aesthetic practices and acts as well.

138
00:12:17,360 --> 00:12:24,360
You might have heard of TPT-3, which is the biggest AI system available, which basically

139
00:12:24,360 --> 00:12:28,160
allows you to describe in English what software is supposed to do.

140
00:12:28,160 --> 00:12:35,560
That means you can have your English story and then convert that into executable code,

141
00:12:35,560 --> 00:12:40,920
but what I have been thinking more recently about the singularity, which basically means

142
00:12:40,920 --> 00:12:49,120
AI taking over, that basically means that we would have to somehow communicate and the

143
00:12:49,120 --> 00:12:56,080
question then is who is in the driver sitting in the end?

144
00:12:56,080 --> 00:12:57,080
Is it intelligence?

145
00:12:57,080 --> 00:13:01,040
Yes, the person who can pull the plug is in the driver's seat.

146
00:13:01,040 --> 00:13:04,480
That's the only way we'll save us from catastrophe.

147
00:13:04,480 --> 00:13:11,080
There's something common between code.

148
00:13:11,080 --> 00:13:17,400
I think the idea that coding itself may be a form of literature is not something that

149
00:13:17,400 --> 00:13:25,000
reminds me that musical notation is also something that many people don't know how to read.

150
00:13:25,000 --> 00:13:26,000
Oh, sorry.

151
00:13:26,000 --> 00:13:27,000
Yeah.

152
00:13:27,000 --> 00:13:28,000
Sorry about that.

153
00:13:28,000 --> 00:13:30,320
Musical notation is something that most people don't know how to read, but it does create

154
00:13:30,320 --> 00:13:36,360
a sort of, some people refer to as a sort of universal language, its production, its

155
00:13:36,360 --> 00:13:38,920
execution creates a kind of language.

156
00:13:38,920 --> 00:13:42,840
I think that's the sort of level at which maybe coding might apply.

157
00:13:42,840 --> 00:13:46,720
And again, I guess I would contribute to this.

158
00:13:46,720 --> 00:13:54,160
One is that in reading code, right, in picking up a piece of code and having a look at it,

159
00:13:54,160 --> 00:13:58,960
and I know this is, well, anyway, there can be moments of beauty in it in the same way

160
00:13:58,960 --> 00:14:03,080
that there are moments of beauty in reading a mathematical proof or something like that,

161
00:14:03,080 --> 00:14:07,680
right, that there is an aesthetic sense that someone can look at it and go, wow, what you

162
00:14:07,680 --> 00:14:08,680
did here?

163
00:14:08,680 --> 00:14:11,280
Oh, look, that little trip, that's amazing, right?

164
00:14:11,280 --> 00:14:14,640
So there is an aesthetic sense that comes along with it.

165
00:14:14,640 --> 00:14:19,680
I think the second thing is that in my mind, the previous discussion was a little bit upside

166
00:14:19,680 --> 00:14:20,680
down.

167
00:14:20,680 --> 00:14:30,640
I think that coded systems are creating, organizing into systems of power in our world, and part

168
00:14:30,640 --> 00:14:35,280
of the reason why they've gotten away with, and that they are as powerful as they are,

169
00:14:35,280 --> 00:14:42,440
is that there is this gap between the language that we are using to communicate and code,

170
00:14:42,440 --> 00:14:46,920
right, the fact that people don't understand how computer works is a problem, right, the

171
00:14:46,920 --> 00:14:51,720
fact that they don't understand the basics of meaning a lot of reverb, and that makes

172
00:14:51,720 --> 00:14:53,520
me nervous, like I'm saying, something awful.

173
00:14:53,520 --> 00:15:03,640
But the fact that people don't understand, you know, the suggest then that it's difficult

174
00:15:03,640 --> 00:15:15,560
to be, let's say, a responsible citizen, right, in our contemporary political or cultural setting,

175
00:15:15,560 --> 00:15:22,080
because we need to understand the basics of how a computer works, I think, and something

176
00:15:22,080 --> 00:15:32,960
of how code works, right, so I think that not having that access to that language is

177
00:15:32,960 --> 00:15:36,720
leaving us open to a lot of difficulties that we've seen.

178
00:15:36,720 --> 00:15:39,960
That's an interesting proposition, Mark.

179
00:15:39,960 --> 00:15:43,680
I mean, one year's all the time that we need basic scientific literacy, and I agree with

180
00:15:43,680 --> 00:15:49,560
that just as an article of fate, but it's a problem, and we've seen all too clearly in

181
00:15:49,560 --> 00:15:56,720
the last couple years, that it's a huge problem that when citizens lack kind of fundamental

182
00:15:56,720 --> 00:16:02,880
scientific literacy about the nature of things like what's the interesting climate and whether,

183
00:16:02,880 --> 00:16:06,520
you know, how do the dens work, that sort of stuff.

184
00:16:06,520 --> 00:16:12,320
It seems, however, that like, how it is that code works and the computer works seems to

185
00:16:12,320 --> 00:16:16,560
be a different kind of thing in a way that, I don't know how a plane works, I get in the

186
00:16:16,560 --> 00:16:20,640
plane to fly, just flew, you know, across the country last week.

187
00:16:20,640 --> 00:16:26,240
I have my iPhone and my computer, I feel, are sort of analogous to that in a way.

188
00:16:26,240 --> 00:16:32,760
I'm curious in how the argument is that there is something sort of threatening to democracy

189
00:16:32,760 --> 00:16:38,000
or wrong about not having a citizen, not having a capacity for coding, because it doesn't

190
00:16:38,000 --> 00:16:42,680
seem to me to be intuitive in the same way that say there's a real problem when people

191
00:16:42,680 --> 00:16:47,440
don't have access to science or don't understand the way that science works.

192
00:16:47,440 --> 00:16:56,840
I mean, I feel, first of all, there's an increase in which science itself is computational

193
00:16:56,840 --> 00:17:04,920
and it is inheriting, this is driving me crazy, and it's inheriting, yeah, that's a

194
00:17:04,920 --> 00:17:05,920
problem.

195
00:17:05,920 --> 00:17:14,920
I think what I'm suggesting, and I don't think it's either earth shattering or new,

196
00:17:14,920 --> 00:17:24,440
the founder of Python or the gentleman who created it has a very long essay talking, I

197
00:17:24,440 --> 00:17:31,800
guess it was a darker proposal or something about creating a programming language and

198
00:17:31,800 --> 00:17:35,080
the title of it was programming for everyone.

199
00:17:35,080 --> 00:17:42,160
And the idea is simply that we teach everyone to read and write, even though we don't expect

200
00:17:42,160 --> 00:17:48,920
them to become poets and writers, and by the same token we should teach everyone to code

201
00:17:48,920 --> 00:17:55,080
to understand programming or the act of it, because otherwise we are significantly removed

202
00:17:55,080 --> 00:17:59,800
from the technological systems that we rely on.

203
00:17:59,800 --> 00:18:05,480
And I would say that saying that this is a shiny box that we interact with and I don't

204
00:18:05,480 --> 00:18:10,080
really have to know how it works means that now I'm cut off from all the little things

205
00:18:10,080 --> 00:18:12,560
that it could do in terms of surveillance and other things.

206
00:18:12,560 --> 00:18:16,200
I don't have the capacity to ask questions about what this is doing.

207
00:18:16,200 --> 00:18:20,480
And my capacity, the fact that I don't have capacity to ask questions means that there

208
00:18:20,480 --> 00:18:26,080
are significant systems of power in our world that are just happily going along doing their

209
00:18:26,080 --> 00:18:27,240
thing.

210
00:18:27,240 --> 00:18:31,000
And with my position in journalism school, I feel like the one thing that I can offer

211
00:18:31,000 --> 00:18:38,680
them is access to the way these systems start to work, the way power builds up in a technological

212
00:18:38,680 --> 00:18:45,040
system and some of that can start with something as modest as learning how to code.

213
00:18:45,040 --> 00:18:46,040
It says my opinion.

214
00:18:46,040 --> 00:18:51,880
I totally agree with what you say in the sense that if computers or the phone were perfect

215
00:18:51,880 --> 00:18:56,400
and there would be no bad people on the world, we wouldn't need to know how to code because

216
00:18:56,400 --> 00:19:00,720
it would trust in everything would work but unfortunately there is peace and so on.

217
00:19:00,720 --> 00:19:01,720
And...

218
00:19:01,720 --> 00:19:03,360
Right, no, I get that.

219
00:19:03,360 --> 00:19:13,440
I feel like I have the same suspicion about what my phone is doing and tracking and the

220
00:19:13,440 --> 00:19:19,800
threats to democracy there and also just the general modification of everything that happens

221
00:19:19,800 --> 00:19:20,800
there.

222
00:19:20,800 --> 00:19:24,000
And yet I don't know the infrastructure of how it's encoded.

223
00:19:24,000 --> 00:19:29,400
And I guess I'm wondering about the connection from the state is a bit far away from questions

224
00:19:29,400 --> 00:19:34,240
of narrative perhaps but still seems important to pursue.

225
00:19:34,240 --> 00:19:41,440
How we move from the one level of implementation to another and would in fact knowing something

226
00:19:41,440 --> 00:19:51,360
about coding give me an additional capacity for resisting the ways in which my phone may

227
00:19:51,360 --> 00:19:58,320
or may not be tracking what I'm doing and so forth.

228
00:19:58,320 --> 00:20:01,280
I think you could bring that pretty easily back to literature.

229
00:20:01,280 --> 00:20:02,280
Great.

230
00:20:02,280 --> 00:20:04,120
Because I mean really what are we doing here?

231
00:20:04,120 --> 00:20:10,360
We're asking in any object or in any phenomenon of human life what is encoded and why.

232
00:20:10,360 --> 00:20:14,000
And in the phone there are some things that are encoded that allow us to use the phone

233
00:20:14,000 --> 00:20:17,920
for our purposes but there are some things that are encoded in the phone that allow counter

234
00:20:17,920 --> 00:20:23,200
purposes or silent surveillance type purposes but the same is true for a novel.

235
00:20:23,200 --> 00:20:27,800
I mean there are certain things that are encoded that are for the purposes of enjoyment and

236
00:20:27,800 --> 00:20:32,960
there are other things that are encoded ideologies for the purposes of something perhaps behind

237
00:20:32,960 --> 00:20:34,040
the scenes.

238
00:20:34,040 --> 00:20:39,480
And so I mean yes we teach people to read and to write but do we teach at them very well

239
00:20:39,480 --> 00:20:46,040
to see the sort of broader context of power within what they are reading and writing.

240
00:20:46,040 --> 00:20:49,080
Maybe that is something that they also need to know.

241
00:20:49,080 --> 00:20:52,200
Oh for sure and thank you for putting it that way and you know that's really perfect because

242
00:20:52,200 --> 00:20:58,080
like it extends Mark's initial point about questions of pedagogy.

243
00:20:58,080 --> 00:21:04,680
And the importance of sustaining not just kind of basic level computational pedagogy

244
00:21:04,680 --> 00:21:11,080
as well as teaching and sort of fundamentals of science but also humanistic pedagogy.

245
00:21:11,080 --> 00:21:20,200
And what it means to understand our culture from a more traditional humanistic perspective

246
00:21:20,200 --> 00:21:22,320
for these kinds of reasons.

247
00:21:22,320 --> 00:21:28,560
May I disagree with all that has been said up until now I don't think the solution to

248
00:21:28,560 --> 00:21:35,040
saving our civilization is for people to learn how to code and to actually be able to code

249
00:21:35,040 --> 00:21:37,080
the cell phone.

250
00:21:37,080 --> 00:21:41,880
Because I believe that that actually pushes them in the direction of being embraced by

251
00:21:41,880 --> 00:21:49,600
technology while technology is subjugating the free spirit and taking people away from

252
00:21:49,600 --> 00:21:56,520
reading literature so that literature has decreasing value because people are not reading.

253
00:21:56,520 --> 00:21:59,320
People are looking at their cell phone all the time.

254
00:21:59,320 --> 00:22:05,320
So pushing them to learn the code of the cell phone is not going to steer society away from

255
00:22:05,320 --> 00:22:11,000
a total dependence and a total domination by the big tech companies that are taking over

256
00:22:11,000 --> 00:22:12,720
their lives.

257
00:22:12,720 --> 00:22:20,880
Taking them away from looking at art, listening to music and reading literature.

258
00:22:20,880 --> 00:22:23,760
Today's literature or the classical literature.

259
00:22:23,760 --> 00:22:25,760
It's all going, going, going, going.

260
00:22:25,760 --> 00:22:30,880
So Nick was I then saying we should take away the self-part of everybody and not keep on

261
00:22:30,880 --> 00:22:37,280
anymore and put some well-class literature on their own.

262
00:22:37,280 --> 00:22:41,960
Well there's been an effort to, and I'm not so sure I think it will work.

263
00:22:41,960 --> 00:22:46,720
So I don't want to say I'm advocating this, but there's a movement toward all right well

264
00:22:46,720 --> 00:22:50,800
we're going to take coding and have it write for stories.

265
00:22:50,800 --> 00:22:55,760
I'm not still in the future a little bit but one of our talks tomorrow on the general

266
00:22:55,760 --> 00:22:56,760
GPT-4.

267
00:22:56,760 --> 00:22:57,760
GPT-4 is the next GPT-4.

268
00:22:57,760 --> 00:22:58,760
Yeah, GPT-4 is the next GPT-4.

269
00:22:58,760 --> 00:22:59,760
Yeah, GPT-4 is the next GPT-4.

270
00:22:59,760 --> 00:23:05,840
Yeah, we'll talk about that and the ability of a computer program to write a story, let's

271
00:23:05,840 --> 00:23:06,840
say.

272
00:23:06,840 --> 00:23:10,720
Now again I'm not advocating for it, but it's happening or it's being developed.

273
00:23:10,720 --> 00:23:16,520
There's an interesting I think trope about that which is, and this I think speaks to your

274
00:23:16,520 --> 00:23:20,320
criticism although it doesn't invalidate it at all.

275
00:23:20,320 --> 00:23:23,560
Namely we find that a little creepy.

276
00:23:23,560 --> 00:23:29,440
Like I'm reading a story but there's really like a text beneath that that's the real

277
00:23:29,440 --> 00:23:31,280
story right?

278
00:23:31,280 --> 00:23:38,080
And that's a, look that's in a lot of horror stories about the how 9,000 computer right?

279
00:23:38,080 --> 00:23:39,080
But that's a great point.

280
00:23:39,080 --> 00:23:42,920
Why do you find it's creepy because you don't know what's going on?

281
00:23:42,920 --> 00:23:47,840
If you knew and that's the same as with why I don't want to get the vaccine because I

282
00:23:47,840 --> 00:23:51,560
don't trust the vaccine that it's not messing up my body because I don't know what's going

283
00:23:51,560 --> 00:23:52,560
on.

284
00:23:52,560 --> 00:23:54,360
But then you are back to the argument.

285
00:23:54,360 --> 00:23:58,240
If you know how to code perhaps you know what's going on and then you don't think it's creepy

286
00:23:58,240 --> 00:23:59,240
if you get the phone.

287
00:23:59,240 --> 00:24:04,440
So you can blame literature for what fostering this trope about the weakness of computer

288
00:24:04,440 --> 00:24:05,440
projects.

289
00:24:05,440 --> 00:24:10,760
We have to be ever heard of the, you know, in the world that you need an iron in without,

290
00:24:10,760 --> 00:24:16,600
you know, agreeing with Nico's point, you know, just quite quickly which is, you know, people

291
00:24:16,600 --> 00:24:19,920
should be reading works of literature and encountering works of art.

292
00:24:19,920 --> 00:24:25,240
I think, I mean, I think that's so compatible entirely with more people knowing more about

293
00:24:25,240 --> 00:24:26,240
how to code.

294
00:24:26,240 --> 00:24:30,440
I mean, it's, which is just to say that people should know as much as they can about the

295
00:24:30,440 --> 00:24:32,160
world.

296
00:24:32,160 --> 00:24:39,680
And I think from the perspective of the humanities, we often feel like our sector of the world,

297
00:24:39,680 --> 00:24:45,000
which is as real and as important as anything else, is just not drawing as much attention

298
00:24:45,000 --> 00:24:54,560
and it doesn't have the same kind of purchase within the university or among the young or

299
00:24:54,560 --> 00:25:00,840
getting, have as much support from higher educational establishment as it used to.

300
00:25:00,840 --> 00:25:05,520
So it's, again, it's hard not to feel like these conversations are sometimes weighted

301
00:25:05,520 --> 00:25:14,000
against a more sort of aesthetically oriented view of the world and the kind of capacities

302
00:25:14,000 --> 00:25:20,720
for flourishing and democracy that they can bring.

303
00:25:20,720 --> 00:25:26,440
And this speaks to the point that you just raised about, you know, the future of computers

304
00:25:26,440 --> 00:25:30,600
writing works of literature, which, you know, is an interesting thing to contemplate.

305
00:25:30,600 --> 00:25:34,400
I've read some of the earliest stuff there, just like I've seen some of the computer

306
00:25:34,400 --> 00:25:38,120
art that's been generated in the visual domain and it's all interesting and provocative.

307
00:25:38,120 --> 00:25:40,480
There's nothing wrong with it as such.

308
00:25:40,480 --> 00:25:47,360
What can be sometimes troubling about it is when it's bundled together into a kind of

309
00:25:47,360 --> 00:25:54,320
reform picture or, say, higher education in a democratic society in which it is, you

310
00:25:54,320 --> 00:25:57,880
know, deemed to be like the future.

311
00:25:57,880 --> 00:26:02,680
And then it just becomes, you know, to me at least, and I think, you know, for reasons

312
00:26:02,680 --> 00:26:05,200
that are real and just kind of ugly.

313
00:26:05,200 --> 00:26:06,200
Not ugly as such.

314
00:26:06,200 --> 00:26:07,880
There's nothing wrong with computers writing stories.

315
00:26:07,880 --> 00:26:09,880
That's, again, interesting.

316
00:26:09,880 --> 00:26:16,200
Ugly in some of the directions that that's pushed and some of the kind of like wide-eyed,

317
00:26:16,200 --> 00:26:21,800
sort of like belated, alvin, pufflerism that it's sort of, you know, made to serve.

318
00:26:21,800 --> 00:26:23,040
Like the future is here.

319
00:26:23,040 --> 00:26:25,400
It looks like computers writing stories.

320
00:26:25,400 --> 00:26:29,280
We need to put all of our money there and we need to basically just, you know, starve

321
00:26:29,280 --> 00:26:34,440
the people who want to just sit around a table like this and talk about a book.

322
00:26:34,440 --> 00:26:39,400
And that way lies, I think, you know, kind of disaster.

323
00:26:39,400 --> 00:26:48,840
So obviously the sort of the idea that we teach people to code is not just, all right,

324
00:26:48,840 --> 00:26:53,120
and we'll teach them basic and we'll hear as a variable and we'll assign it the number

325
00:26:53,120 --> 00:26:56,280
five and we can add two to it and then we can do it.

326
00:26:56,280 --> 00:27:02,760
Like, that's when I describe or when I think about or when in my classes we talk about

327
00:27:02,760 --> 00:27:05,320
how computation works.

328
00:27:05,320 --> 00:27:11,680
You know, there's a well-defined notion of what we're looking at or at least what I've

329
00:27:11,680 --> 00:27:17,320
tried to put across in terms of computational literacy, right?

330
00:27:17,320 --> 00:27:23,080
The step one is that coding step, like the functional literacy, how does it work?

331
00:27:23,080 --> 00:27:25,720
Then there's a critical literacy, right?

332
00:27:25,720 --> 00:27:33,680
The idea that you have enough capacity to say why does this look the way it does, right?

333
00:27:33,680 --> 00:27:38,000
I mean, technology relentlessly tries to make itself look invisible, right?

334
00:27:38,000 --> 00:27:43,840
So it's not there and so we want to have the capacity to say there's something here and

335
00:27:43,840 --> 00:27:44,840
why does it look that way?

336
00:27:44,840 --> 00:27:46,440
Could it look some other way?

337
00:27:46,440 --> 00:27:52,080
So there's functional literacy, there is critical literacy and then a rhetorical literacy,

338
00:27:52,080 --> 00:27:53,080
right?

339
00:27:53,080 --> 00:27:58,000
Which is that all of these things are, there is no, you know, optimal iPhone, right?

340
00:27:58,000 --> 00:28:03,640
It's a thing and it comes from human deliberation and it's a settling in some way.

341
00:28:03,640 --> 00:28:08,040
And it itself gives off and kind of has a rhetorical function, right?

342
00:28:08,040 --> 00:28:11,880
Again, it makes us feel modern, it makes us feel connected or something like that.

343
00:28:11,880 --> 00:28:23,160
So you can deepen all of these concepts and they depend in many ways on sort of humanity-style

344
00:28:23,160 --> 00:28:24,720
interpretation, right?

345
00:28:24,720 --> 00:28:29,400
I bring in to my class people like Joanna Drucker or something who, you know, makes the

346
00:28:29,400 --> 00:28:34,560
point about data versus capta, what is given versus what is taken, right?

347
00:28:34,560 --> 00:28:39,840
Like, like, that these things make the teaching of code, right?

348
00:28:39,840 --> 00:28:47,200
A richer, fuller experience so that people do come away or presumably students do come

349
00:28:47,200 --> 00:28:53,800
away better understanding the systems of power that circulate around technologies in one

350
00:28:53,800 --> 00:28:54,800
form or other.

351
00:28:54,800 --> 00:28:59,360
And for someone in a journalism school, that's important because journalists are the experts

352
00:28:59,360 --> 00:29:03,240
and explainers of last resort in our society and if they can't call shenanigans on something

353
00:29:03,240 --> 00:29:06,000
that's happening, then I don't know who's going to do it.

354
00:29:06,000 --> 00:29:13,280
Yeah, I'm afraid journalists have been singularly unsuccessful in alerting the world to all

355
00:29:13,280 --> 00:29:16,600
the bad things and the manipulations that are occurring by-

356
00:29:16,600 --> 00:29:18,360
I would say it's extremely-

357
00:29:18,360 --> 00:29:19,360
The next extreme-

358
00:29:19,360 --> 00:29:20,360
The next extreme-

359
00:29:20,360 --> 00:29:22,360
The harsh, harsh comment.

360
00:29:22,360 --> 00:29:25,360
And extreme, yeah, sure.

361
00:29:25,360 --> 00:29:27,360
Yeah, okay, I said.

362
00:29:27,360 --> 00:29:37,120
I want to answer to the challenge that Peter threw at me as in response to my early comment,

363
00:29:37,120 --> 00:29:46,200
Peter, you said that therefore we should make people appreciate literature and art better,

364
00:29:46,200 --> 00:29:49,200
but nobody can make anybody do anything.

365
00:29:49,200 --> 00:29:54,040
The only way you can make someone do something is by tricking them into thinking that they're

366
00:29:54,040 --> 00:29:56,160
getting an advantage by doing something.

367
00:29:56,160 --> 00:30:02,080
So all the technologies that we have has tricked us by offering some utility for our

368
00:30:02,080 --> 00:30:03,080
lives.

369
00:30:03,080 --> 00:30:04,080
That's why we have adopted it.

370
00:30:04,080 --> 00:30:11,640
So becoming slaves to the technology was a step-by-step process because it gave us the

371
00:30:11,640 --> 00:30:15,080
power, increased power, and we felt that.

372
00:30:15,080 --> 00:30:24,160
So the only way to get someone to study literature today is to make it advantageous in an evolutionary

373
00:30:24,160 --> 00:30:25,680
sense.

374
00:30:25,680 --> 00:30:30,120
If you do this, then you get an advantage, a evolutionary advantage.

375
00:30:30,120 --> 00:30:33,440
And not by the academic.

376
00:30:33,440 --> 00:30:36,720
You have to take this course because it's required.

377
00:30:36,720 --> 00:30:41,160
Yes, but I mean, thank you very much for throwing the ball back to me.

378
00:30:41,160 --> 00:30:47,160
But I think you are neglecting one point and that's beauty because what you raise is

379
00:30:47,160 --> 00:30:53,000
a very cynical utilitarian view and that's unfortunately the main motivation, particularly

380
00:30:53,000 --> 00:30:54,840
in capitalist societies.

381
00:30:54,840 --> 00:30:58,320
Then there's this other aspect that I think it has been raised and if you look at pieces

382
00:30:58,320 --> 00:31:03,560
of world art, and I'm not sure whether a computer will actually ever get there because

383
00:31:03,560 --> 00:31:09,080
TPT-3, it can write something that sounds like a reporter or a lot of string and so on.

384
00:31:09,080 --> 00:31:17,000
But it will never be, or even if you go to some higher level literature on the same level.

385
00:31:17,000 --> 00:31:18,000
On to now.

386
00:31:18,000 --> 00:31:20,680
Perhaps it will come at one point, but I'm not sure.

387
00:31:20,680 --> 00:31:25,560
Well, is there something that you could say or can we itemize a number of things that

388
00:31:25,560 --> 00:31:32,880
we believe really distinguishes literature from anything that might be available through

389
00:31:32,880 --> 00:31:33,880
coding?

390
00:31:33,880 --> 00:31:34,880
Yes.

391
00:31:34,880 --> 00:31:37,080
Is there a way to itemize that?

392
00:31:37,080 --> 00:31:40,080
I mean, I was about to pick up on that.

393
00:31:40,080 --> 00:31:44,400
I wanted to respond to Peter's comment about we don't understand how it works and therefore

394
00:31:44,400 --> 00:31:49,520
we're afraid because I actually think that frequently I don't know how a piece of literature

395
00:31:49,520 --> 00:31:53,560
works, but that is the cause for wonder and delight rather than fear.

396
00:31:53,560 --> 00:31:58,000
And that is because I think the distinguishing fact of a piece of literature from something

397
00:31:58,000 --> 00:32:04,960
that might be encoded for utilitarian purposes is that it's okay for something like literature

398
00:32:04,960 --> 00:32:09,560
to give us something we understand, something that exceeds us and something that is so saturated

399
00:32:09,560 --> 00:32:15,480
with meaning that it can't be reduced to a single, perposive goal.

400
00:32:15,480 --> 00:32:19,040
And that this kind of brings us back to a point that was coming up in earlier conversations,

401
00:32:19,040 --> 00:32:24,920
the question of trust because the unique compact, I think rhetorically between reader and author

402
00:32:24,920 --> 00:32:29,640
is ideally one of trust whereby the question of trust is a very different issue when we

403
00:32:29,640 --> 00:32:34,920
look at code because we know that the person who is encoding something has perhaps more

404
00:32:34,920 --> 00:32:37,840
defined purposes than the literary author.

405
00:32:37,840 --> 00:32:43,760
But just to disagree about one thing for myself, Finnegan's Wake scares the daylights

406
00:32:43,760 --> 00:32:44,760
out of him.

407
00:32:44,760 --> 00:32:48,640
That's because it shows you something about yourself you don't want to admit.

408
00:32:48,640 --> 00:32:49,640
It's a great thing.

409
00:32:49,640 --> 00:32:55,640
This is a rather pedestrian response, but for many people that I know, especially younger

410
00:32:55,640 --> 00:33:03,400
people, and people who have leading disorders for instance, the phone is their lifeline

411
00:33:03,400 --> 00:33:08,400
to literature because they're listening to books on tape, they're listening to audio

412
00:33:08,400 --> 00:33:09,920
books.

413
00:33:09,920 --> 00:33:15,240
And for someone who's been dyslexic all their lives, it's like a whole world opens up to

414
00:33:15,240 --> 00:33:16,240
them.

415
00:33:16,240 --> 00:33:21,360
And for many people who are very busy, I'm in the world of literature all the time.

416
00:33:21,360 --> 00:33:26,000
If I'm walking in Central Park, I'm listening to a novel.

417
00:33:26,000 --> 00:33:30,480
It's the only time I have to go into fiction rather than professional books.

418
00:33:30,480 --> 00:33:34,520
And that's through this little device.

419
00:33:34,520 --> 00:33:43,400
And among the young who spend most of their time on social media, there's a lot of feedback

420
00:33:43,400 --> 00:33:48,000
between online activity and staring at your phone and then also reading, which is a lot

421
00:33:48,000 --> 00:33:53,840
of active book talk on TikTok, for example, which is all about the analog object.

422
00:33:53,840 --> 00:33:55,840
It's not about the small shiny thing.

423
00:33:55,840 --> 00:34:03,600
And in fact features usually have the aesthetics of books, what they look like, how they feel

424
00:34:03,600 --> 00:34:04,600
in your hands.

425
00:34:04,600 --> 00:34:08,120
And they'll show it, I've been reading this, etc.

426
00:34:08,120 --> 00:34:14,920
So in fact, it's not just a dichotomous phenomena.

427
00:34:14,920 --> 00:34:21,960
It's the digital world and the handheld world and the social media world has created a renewed

428
00:34:21,960 --> 00:34:27,400
interest in literary artifacts and objects as it has and painted and sculpted ones as

429
00:34:27,400 --> 00:34:28,400
well.

430
00:34:28,400 --> 00:34:31,240
Things that exist, however, off your phone, not that you're listening to as you're walking

431
00:34:31,240 --> 00:34:36,440
through Central Park, but as you're also just sitting and holding your hand as reading.

432
00:34:36,440 --> 00:34:39,240
And I think that's, first of all, terrific.

433
00:34:39,240 --> 00:34:42,600
I think it's an interesting cultural phenomenon in its own right.

434
00:34:42,600 --> 00:34:50,360
But I also think it shows the viability and the prudence of the aesthetic domain as something

435
00:34:50,360 --> 00:34:58,880
that has both a purchase on an appeal to humanity across the board, but also is worth cultivating

436
00:34:58,880 --> 00:35:01,960
and spending time with and supporting ultimately.

437
00:35:01,960 --> 00:35:06,400
You've got to listen to something and then I will go by the book.

438
00:35:06,400 --> 00:35:12,040
And what I mostly was converted by was Finnegan's Wake.

439
00:35:12,040 --> 00:35:15,120
Listening to Finnegan's Wake is a true adventure.

440
00:35:15,120 --> 00:35:18,480
Well, I should definitely see all of that.

441
00:35:18,480 --> 00:35:22,920
I want to return to you in his response because even though I made fun of the Finnegan's Wake

442
00:35:22,920 --> 00:35:26,160
and fear, I like the idea that you mentioned trust.

443
00:35:26,160 --> 00:35:30,560
I think that it's interesting that when I ask the question, all right, well, can we start

444
00:35:30,560 --> 00:35:35,560
itemized differences between coding generated?

445
00:35:35,560 --> 00:35:40,680
Art, not even just literature and literature.

446
00:35:40,680 --> 00:35:45,040
You said something about trust, which I think is a wonderful sort of, it's an emotion.

447
00:35:45,040 --> 00:35:46,040
It's an emotion.

448
00:35:46,040 --> 00:35:47,600
You're expressing an emotional thought.

449
00:35:47,600 --> 00:35:51,400
So far everyone else has made instrumental arguments about it.

450
00:35:51,400 --> 00:35:55,400
As a psychiatrist, I like hearing, okay, so what about how are you feeling about this

451
00:35:55,400 --> 00:36:01,520
and where does the feeling that is evoked by art, is that matchable?

452
00:36:01,520 --> 00:36:02,520
Can we match that?

453
00:36:02,520 --> 00:36:03,520
Because I think you're right.

454
00:36:03,520 --> 00:36:10,040
You might be honest, something trust, it would be hard to earn it out of a machine.

455
00:36:10,040 --> 00:36:14,720
So perhaps how does it bother an art drive?

456
00:36:14,720 --> 00:36:20,520
Because if you look at Lord of the Ring, the original one, it was huge success.

457
00:36:20,520 --> 00:36:25,800
And that sequel that Disney has been producing, the reception was horrible, because they were

458
00:36:25,800 --> 00:36:30,920
just applying all the formalized things, so that means they didn't get the trust.

459
00:36:30,920 --> 00:36:33,400
Is that what you would say?

460
00:36:33,400 --> 00:36:34,400
Do you mean the movie adaptation?

461
00:36:34,400 --> 00:36:35,400
Yes, exactly.

462
00:36:35,400 --> 00:36:36,400
Oh, I see.

463
00:36:36,400 --> 00:36:40,680
So the movie adaptations to the sequels failed to earn trust because they were too formalized

464
00:36:40,680 --> 00:36:44,320
according to the cinematic industry.

465
00:36:44,320 --> 00:36:46,320
Yes, they were too robotic.

466
00:36:46,320 --> 00:36:47,320
They were too robotic.

467
00:36:47,320 --> 00:36:51,720
But they were just applying some formulas.

468
00:36:51,720 --> 00:36:59,120
And so that seems to me an example of what you said, but I'm not an expert in answer

469
00:36:59,120 --> 00:37:00,120
you.

470
00:37:00,120 --> 00:37:05,960
I would say that probably sounds right, because viewers of media are pretty savvy to tropes.

471
00:37:05,960 --> 00:37:10,520
And yeah, I remember seeing the Lord of the Rings the second movie, and it's just trope

472
00:37:10,520 --> 00:37:16,160
after trope kind of piled on each other, which feels like it falls short of the kind of meaning

473
00:37:16,160 --> 00:37:21,160
saturation that I want from a single other human being.

474
00:37:21,160 --> 00:37:25,320
I don't know if that answers your question.

475
00:37:25,320 --> 00:37:34,080
The good question also was initially, like, would, can we imagine a novel written entirely

476
00:37:34,080 --> 00:37:39,200
by a computer as appealing as a novel written by a human being?

477
00:37:39,200 --> 00:37:47,920
I think if you assume time and exponentially increasing computer power, I have no difficulty

478
00:37:47,920 --> 00:37:51,120
really imagining that.

479
00:37:51,120 --> 00:37:52,960
And no strong feelings either way.

480
00:37:52,960 --> 00:37:58,760
Well, okay, well then, why would it be difficult for, let's say, it would be a challenge.

481
00:37:58,760 --> 00:38:02,080
What are they surmounting to meet that challenge, right?

482
00:38:02,080 --> 00:38:06,880
There's something, anything that requires human intelligence to create.

483
00:38:06,880 --> 00:38:12,040
And what's difficult about that, I think it's probably Peter's domain, or maybe Mark

484
00:38:12,040 --> 00:38:14,080
more than mine.

485
00:38:14,080 --> 00:38:22,080
But I think it's, you know, it would be an analogous problem to anything that requires,

486
00:38:22,080 --> 00:38:26,920
again, human intelligence to do, whether or not that's write a novel or drive a car or

487
00:38:26,920 --> 00:38:30,240
make scrambled eggs for you in the morning.

488
00:38:30,240 --> 00:38:33,560
All of which have been really hard things to do.

489
00:38:33,560 --> 00:38:41,040
I mean, it seems to me the most powerful advances in artificial intelligence, as far as I know,

490
00:38:41,040 --> 00:38:47,080
and that's very little, have been in, you know, kind of narrow but deep domains.

491
00:38:47,080 --> 00:38:48,720
A chance.

492
00:38:48,720 --> 00:38:53,800
A chance, for example, sort of power, but radically so, I suppose.

493
00:38:53,800 --> 00:39:00,360
Anything that involves, like, as many factors as, you know, would go into writing an novel

494
00:39:00,360 --> 00:39:06,520
or again, you know, making you scrambled eggs in the morning, is going to be much harder

495
00:39:06,520 --> 00:39:08,520
to do but not inconceivable.

496
00:39:08,520 --> 00:39:11,720
So, Magnus Nielsen, the reigning Carlson.

497
00:39:11,720 --> 00:39:15,080
Carlson, thank you.

498
00:39:15,080 --> 00:39:23,080
He walked out on this match because I think something about the first several moves and

499
00:39:23,080 --> 00:39:26,640
because of some previous report that his opponent made cheap.

500
00:39:26,640 --> 00:39:30,920
Something about his body, this is opening with, that's it.

501
00:39:30,920 --> 00:39:34,000
A human wouldn't do this, I know you're cheap, right?

502
00:39:34,000 --> 00:39:38,800
So here you have, no, but that actually wasn't that intuitive that a human wouldn't do it.

503
00:39:38,800 --> 00:39:41,800
That human would be cheap.

504
00:39:41,800 --> 00:39:42,800
No, no, I think.

505
00:39:42,800 --> 00:39:44,800
He was using a computer.

506
00:39:44,800 --> 00:39:45,800
Oh, I see that.

507
00:39:45,800 --> 00:39:48,560
He recognized that the movement was cheap in chess.

508
00:39:48,560 --> 00:39:52,160
He recognized the judges being generated by a robot.

509
00:39:52,160 --> 00:39:57,600
And so, it's interesting, so here you have something that's gotten right to the pinnacle

510
00:39:57,600 --> 00:40:01,280
of computer programming and beating humans and it does beat humans.

511
00:40:01,280 --> 00:40:05,520
But it's missing something from the perspective of human observers.

512
00:40:05,520 --> 00:40:07,520
It's missing something, right?

513
00:40:07,520 --> 00:40:10,160
Carlson was able to pick that up, it seems.

514
00:40:10,160 --> 00:40:17,800
But when they'll go to chess for that, that was solved in the game of Go several years

515
00:40:17,800 --> 00:40:18,800
ago.

516
00:40:18,800 --> 00:40:23,800
Because Go became, the deep mind became the Go champion.

517
00:40:23,800 --> 00:40:29,440
Right, but I'm saying there's still things that humans perceive about the difference between

518
00:40:29,440 --> 00:40:34,440
their opponent when it's a human versus a, it's a human versus a computer.

519
00:40:34,440 --> 00:40:38,440
And I'm wondering if that perception has something to do with what we would find deficient in

520
00:40:38,440 --> 00:40:40,440
a computer generated work of the literature.

521
00:40:40,440 --> 00:40:41,440
That's what I'm getting at.

522
00:40:41,440 --> 00:40:42,440
Yeah, for sure.

523
00:40:42,440 --> 00:40:45,800
And I'm out of the screen with you, maybe even in the very far-efficient future, even

524
00:40:45,800 --> 00:40:48,240
that difference would be taken away.

525
00:40:48,240 --> 00:40:56,880
In the visual arts where I have more experience being a painter in my youth, in the visual

526
00:40:56,880 --> 00:41:04,720
arts, AI can produce paintings today that are better than the professional visual artists.

527
00:41:04,720 --> 00:41:05,720
I think we have reached that.

528
00:41:05,720 --> 00:41:10,360
I cannot speak about AI generated literature.

529
00:41:10,360 --> 00:41:20,840
But the AI generated paintings, depending on what text to image AI programs can generate

530
00:41:20,840 --> 00:41:32,480
very beautiful paintings because they sum the opinions of individuals, say, 10 to the

531
00:41:32,480 --> 00:41:37,080
10th number of individuals, which is an extraordinary number.

532
00:41:37,080 --> 00:41:41,200
So they can generate a very beautiful painting and you go to a contemporary art gallery,

533
00:41:41,200 --> 00:41:43,120
it's mostly garbage.

534
00:41:43,120 --> 00:41:47,600
So there's a huge disconnect between what the computers are generating or what are the

535
00:41:47,600 --> 00:41:50,120
so-called artists are generating.

536
00:41:50,120 --> 00:41:55,960
The Detroit Museum of Art has a new exhibition of Van Gogh and they did it because they

537
00:41:55,960 --> 00:41:59,240
were the first American Museum to embrace Van Gogh.

538
00:41:59,240 --> 00:42:05,840
And the reviewers in America, the armory show said this guy's dance is terrible, of course

539
00:42:05,840 --> 00:42:06,840
he's about that.

540
00:42:06,840 --> 00:42:11,280
He's about the most popular artists, visual arts now.

541
00:42:11,280 --> 00:42:17,320
That's a trajectory that some human artists have pursued and we would say, well that's

542
00:42:17,320 --> 00:42:20,320
a form of imagination, innovation.

543
00:42:20,320 --> 00:42:21,320
That's incorrect.

544
00:42:21,320 --> 00:42:22,320
That's incorrect.

545
00:42:22,320 --> 00:42:29,000
That is propaganda in order for a certain click to make enormous amounts of money by

546
00:42:29,000 --> 00:42:33,600
selling garbage and by promoting talentless charlatans.

547
00:42:33,600 --> 00:42:37,840
But somebody...

548
00:42:37,840 --> 00:42:43,600
What you are saying is totally out of date because it happened 100 years ago.

549
00:42:43,600 --> 00:42:47,880
I don't agree with your extrapolation that what happened to Van Gogh, the same thing happened

550
00:42:47,880 --> 00:42:52,880
to the Impressionist in Paris that totally extrapolated to the garbage we see produced

551
00:42:52,880 --> 00:43:01,600
today and put in art galleries and displacing beautiful works of art from our museums.

552
00:43:01,600 --> 00:43:07,840
The Museum's decommissioned wonderful pieces of art and sculpture and fill it with contemporary

553
00:43:07,840 --> 00:43:11,000
art that's supposed to be art but it's really garbage.

554
00:43:11,000 --> 00:43:12,000
And there's a definition.

555
00:43:12,000 --> 00:43:19,280
And here's an AI conjugge where there's garbage for where the bits are because AI has this

556
00:43:19,280 --> 00:43:24,160
attempt to the tense, a piece of data of human reactions.

557
00:43:24,160 --> 00:43:29,920
Does this create a positive emotional reaction in the body?

558
00:43:29,920 --> 00:43:35,200
And we have the sensors and the medical data to see if it creates distress then it's a

559
00:43:35,200 --> 00:43:36,200
piece of garbage.

560
00:43:36,200 --> 00:43:42,120
If it creates a positive healing reaction according to the movie, medical sensors that

561
00:43:42,120 --> 00:43:46,880
can be worn today, is that what we're having to go to the laboratory, wearable sensors

562
00:43:46,880 --> 00:43:52,320
if it's positive then it's a valid piece of art because it helps you to cope with the

563
00:43:52,320 --> 00:43:53,320
stress.

564
00:43:53,320 --> 00:43:54,320
This is...

565
00:43:54,320 --> 00:43:55,320
This is...

566
00:43:55,320 --> 00:43:56,320
Sorry.

567
00:43:56,320 --> 00:44:00,840
Another example I suppose of we're not knowing how a computer functions and not knowing how

568
00:44:00,840 --> 00:44:04,760
something works leads you to a kind of crazy conclusion.

569
00:44:04,760 --> 00:44:10,240
And I'm assuming the way you were saying this meant ironically because the AI...

570
00:44:10,240 --> 00:44:16,240
First of all, Dolly doesn't work by hooking up responses or anything like that.

571
00:44:16,240 --> 00:44:22,680
But suppose it did the idea that a set of measurements, what are we going to measure

572
00:44:22,680 --> 00:44:25,400
to capture our aesthetic reaction?

573
00:44:25,400 --> 00:44:26,400
What are we going to choose?

574
00:44:26,400 --> 00:44:28,400
How are we going to choose to measure that?

575
00:44:28,400 --> 00:44:29,400
How are we going to choose to measure that?

576
00:44:29,400 --> 00:44:30,400
I'm going to put some...

577
00:44:30,400 --> 00:44:31,400
Hold on a second.

578
00:44:31,400 --> 00:44:36,600
So first of all, there are choices in what to choose to represent lived experience and

579
00:44:36,600 --> 00:44:38,400
to even...

580
00:44:38,400 --> 00:44:40,400
To stop and to think that there are choices.

581
00:44:40,400 --> 00:44:43,400
There are different ways of expressing it.

582
00:44:43,400 --> 00:44:47,400
Do you talk to someone before seeing something and then interview them afterwards?

583
00:44:47,400 --> 00:44:49,680
It's a little more qualitative but is that something?

584
00:44:49,680 --> 00:44:53,800
Like why do we reduce it to a kind of physiological response first of all?

585
00:44:53,800 --> 00:44:59,640
And secondly, what Dolly does is it's based on sort of collections of data where captions

586
00:44:59,640 --> 00:45:02,320
are matched with images.

587
00:45:02,320 --> 00:45:11,200
And the obvious question around almost all AI processes now are like what are the data

588
00:45:11,200 --> 00:45:12,280
that they were trained on?

589
00:45:12,280 --> 00:45:15,680
Who are the people who even if they were hooked up in some way?

590
00:45:15,680 --> 00:45:16,680
Who are they?

591
00:45:16,680 --> 00:45:18,720
What cultural references do they have?

592
00:45:18,720 --> 00:45:22,880
What are you showing them that's consistent or not?

593
00:45:22,880 --> 00:45:27,880
And I think that some of this discussion is what we're missing here is it's sort of

594
00:45:27,880 --> 00:45:31,200
not AI versus the artist.

595
00:45:31,200 --> 00:45:33,400
It's sort of AI and the artist, right?

596
00:45:33,400 --> 00:45:39,640
If you've tried Dolly, you can type something out and put some text in and get an image

597
00:45:39,640 --> 00:45:42,360
and go, well, that's not really what I'd like.

598
00:45:42,360 --> 00:45:46,040
And so you alter a little bit and you alter a little bit and you alter a little bit and

599
00:45:46,040 --> 00:45:48,480
you sort of start to work with it, right?

600
00:45:48,480 --> 00:45:53,880
I think to go back to the literacy and then I'll shut up, but to go to the literacy question

601
00:45:53,880 --> 00:45:58,440
one more time, that idea that you're typing something into Dolly to get an image and then

602
00:45:58,440 --> 00:46:03,200
typing something else and to get an image, slowly Dolly is training you to do what it

603
00:46:03,200 --> 00:46:08,160
expects so that you get the answer that you want in the exact same way that when you Google

604
00:46:08,160 --> 00:46:12,520
something, right, and if you don't get back the set of results that you want, you don't

605
00:46:12,520 --> 00:46:16,040
say bad Google, you say, oh, I did something wrong.

606
00:46:16,040 --> 00:46:18,960
And in that moment, Google is training you how to interact with it.

607
00:46:18,960 --> 00:46:24,000
And if you don't recognize those training moments, little by little, right, that's how

608
00:46:24,000 --> 00:46:25,560
the systems of power work.

609
00:46:25,560 --> 00:46:32,120
That's how things start to take to exercise control in a way.

610
00:46:32,120 --> 00:46:33,600
Sorry.

611
00:46:33,600 --> 00:46:36,600
What are you saying Mark?

612
00:46:36,600 --> 00:46:38,160
We are already going there.

613
00:46:38,160 --> 00:46:39,160
Exactly.

614
00:46:39,160 --> 00:46:41,000
There's no question.

615
00:46:41,000 --> 00:46:44,160
Our students don't have that connection, right?

616
00:46:44,160 --> 00:46:47,160
They don't have Google more than my own.

617
00:46:47,160 --> 00:46:54,160
Also the, yeah, then in return to Nikos' point, I mean, here I think I disagree entirely.

618
00:46:54,160 --> 00:46:59,880
I mean, in the sense, but with you.

619
00:46:59,880 --> 00:47:05,640
I think that first of all, I mean, I don't believe that you could actually hook people

620
00:47:05,640 --> 00:47:10,160
up and get a picture of what their responses to works of art are.

621
00:47:10,160 --> 00:47:15,880
But you could get a set of correlations.

622
00:47:15,880 --> 00:47:23,520
What they would tell you is actually an entirely debatable phenomenon or question.

623
00:47:23,520 --> 00:47:30,360
But, yes, of course you can record what's happening at various levels of your somatic

624
00:47:30,360 --> 00:47:34,440
reaction when you were presented with an art object or anything else.

625
00:47:34,440 --> 00:47:38,120
Whether that tells you anything meaningful is another question.

626
00:47:38,120 --> 00:47:42,680
So that's one thing, even if you could, I mean the assumption that for work of art to

627
00:47:42,680 --> 00:47:49,320
be beautiful, what it does is make you feel calm and happier, whatever your phrase was.

628
00:47:49,320 --> 00:47:56,160
That also seems to me to be like just, you know, wrong.

629
00:47:56,160 --> 00:48:01,440
And that, you know, what you call bad art would produce a somatic response of like irritation

630
00:48:01,440 --> 00:48:03,480
or something else.

631
00:48:03,480 --> 00:48:07,720
And that that somehow indicates the poor quality of the work also just seems to be

632
00:48:07,720 --> 00:48:08,720
wrong.

633
00:48:08,720 --> 00:48:13,720
But there's a set of nested assumptions here about what kind of response art works ought

634
00:48:13,720 --> 00:48:18,440
to have, whether they're producing you that I think are debatable about whether or not

635
00:48:18,440 --> 00:48:23,560
actually like, you know, whether the criterion of judgment ought to lie, whether it should

636
00:48:23,560 --> 00:48:28,880
be in, you know, in just an immediate response by someone who has no relationship or education

637
00:48:28,880 --> 00:48:40,880
in how to encounter and understand works of art or should be among those who have actually

638
00:48:40,880 --> 00:48:45,160
spent time with and are educated in art and art history.

639
00:48:45,160 --> 00:48:49,480
All those things are actually to be discussed and debated.

640
00:48:49,480 --> 00:48:56,680
So I mean, and again, provide an argument for, it seems to me actually, to return to our

641
00:48:56,680 --> 00:49:00,680
earlier discussion, you know, fundamental artistic and aesthetic literacy.

642
00:49:00,680 --> 00:49:06,360
Because it's not the case that you can just simply like take someone, you know, and put

643
00:49:06,360 --> 00:49:11,680
them in a museum and they will understand what it means to actually look at, you know, a

644
00:49:11,680 --> 00:49:17,040
van Gogh, for example, to use a painter that was mentioned earlier or the works of the

645
00:49:17,040 --> 00:49:23,680
French impressionist or let alone a more difficult and challenging working contemporary

646
00:49:23,680 --> 00:49:24,680
art.

647
00:49:24,680 --> 00:49:30,040
And all those things actually require, you know, discussion, education, learning, you

648
00:49:30,040 --> 00:49:36,680
know, experience which, you know, ought to be available to as many people as possible.

649
00:49:36,680 --> 00:49:40,680
Well, actually, Charlie, we have exactly done that experiment.

650
00:49:40,680 --> 00:49:45,040
In Cologne, in the Kedekolis Museum, this year is one of the tools we developed at MIT.

651
00:49:45,040 --> 00:49:46,680
It's the happy meter.

652
00:49:46,680 --> 00:49:49,440
It measures my mood based on the habits.

653
00:49:49,440 --> 00:49:52,680
Sorry, there's something so dystopian and frightening about the expression.

654
00:49:52,680 --> 00:49:54,680
There's a lot of people in the

655
00:49:54,680 --> 00:49:55,680
papers about that.

656
00:49:55,680 --> 00:49:56,680
It has been used.

657
00:49:56,680 --> 00:50:02,680
It measures my happiness based on heart rate, acceleration, voice emotion.

658
00:50:02,680 --> 00:50:08,680
And in the Kedekolis Museum in Cologne, we have given that to participants, to people.

659
00:50:08,680 --> 00:50:15,680
And then we have measured which rooms, which paintings and which sort of watching style

660
00:50:15,680 --> 00:50:18,680
actually trigger the strongest emotional reaction.

661
00:50:18,680 --> 00:50:21,680
And what is the result?

662
00:50:21,680 --> 00:50:25,680
The result is that it very much matters on the tool guide.

663
00:50:25,680 --> 00:50:27,680
On the tool guide?

664
00:50:27,680 --> 00:50:32,680
Yeah, which is to say, which actually, which is part of what I was saying earlier, which

665
00:50:32,680 --> 00:50:35,680
actually depends partly on how the work is presented.

666
00:50:35,680 --> 00:50:36,680
Exactly.

667
00:50:36,680 --> 00:50:39,680
So, like, you know, how each piece of art is present.

668
00:50:39,680 --> 00:50:46,040
And, well, and also, you know, how, even in a very quick way, how the person viewing the

669
00:50:46,040 --> 00:50:48,080
work of art, you know, is led into it.

670
00:50:48,080 --> 00:50:50,680
It's led to understand it.

671
00:50:50,680 --> 00:50:53,680
And, you know, how that person has been taught.

672
00:50:53,680 --> 00:50:59,680
So, it's entirely, it seems to me, like, obvious and arguably so, that if someone is actually

673
00:50:59,680 --> 00:51:03,680
shown through a museum, thereby an educated, you know, articulate personable tour guide,

674
00:51:03,680 --> 00:51:07,680
they will have, they'll be happier on your happy meter because they will actually just

675
00:51:07,680 --> 00:51:10,680
be able to understand what they're seeing a bit more.

676
00:51:10,680 --> 00:51:12,680
That makes another point about context.

677
00:51:12,680 --> 00:51:18,680
The context is always, because if you look at the Mona Lisa, the context is, I have this

678
00:51:18,680 --> 00:51:24,680
expectation and I see that sort of vaguely interesting face, but because in East Mona

679
00:51:24,680 --> 00:51:28,680
Lisa, I must feel this is the biggest piece of art on the world.

680
00:51:28,680 --> 00:51:32,680
So, the context has already been said, and I'm sure I have never done that with happy

681
00:51:32,680 --> 00:51:35,680
meter, but I would expect that I would get such a reaction.

682
00:51:35,680 --> 00:51:42,680
I would like to clarify tremendous confusions here that have been thrown at the table.

683
00:51:42,680 --> 00:51:50,680
I was referring to unconscious reaction of the human body based on the happy meter, exactly

684
00:51:50,680 --> 00:51:58,680
those wearable sensors, but unconscious responses of the body without priming and they occur

685
00:51:58,680 --> 00:52:02,680
within the first one second or even few milliseconds.

686
00:52:02,680 --> 00:52:10,680
Whereas, if you explain the art, you are conditioning the individual to like something.

687
00:52:10,680 --> 00:52:15,680
The second point is that when you are looking at the context of 100 people from the Mona

688
00:52:15,680 --> 00:52:17,680
Lisa, of course you have to like it.

689
00:52:17,680 --> 00:52:23,680
So, control and spur, and it has to be carried out with the individual measuring only the

690
00:52:23,680 --> 00:52:27,680
first, the second or less of unconscious reaction.

691
00:52:27,680 --> 00:52:31,680
But this, again, I just think that's a wrong understanding or a misguided understanding about

692
00:52:31,680 --> 00:52:34,680
what's the issue, but that's an opinion.

693
00:52:34,680 --> 00:52:36,680
Wait, but there's a different point I think.

694
00:52:36,680 --> 00:52:37,680
Of course, and we have a different opinion.

695
00:52:37,680 --> 00:52:41,680
I just wanted to explore that difference because I think it's an interesting one, which is

696
00:52:41,680 --> 00:52:48,680
that there is on the one hand a model, which actually has some real history, historical

697
00:52:48,680 --> 00:52:57,680
significance to it, that what aesthetic experience is all about is just like this raw, unschooled,

698
00:52:57,680 --> 00:53:01,680
immediate thing where you just bring someone in and you show them something.

699
00:53:01,680 --> 00:53:04,680
I don't think that's really the way that art tends to work.

700
00:53:04,680 --> 00:53:08,680
We're talking about something very, very recent, these measurements.

701
00:53:08,680 --> 00:53:11,680
Yes, I know we're talking about technology that's recent, but we're also talking about an understanding

702
00:53:11,680 --> 00:53:14,680
about what art is that goes way back.

703
00:53:14,680 --> 00:53:18,680
And actually, we have one of many of them, a millennial understanding what art is, and

704
00:53:18,680 --> 00:53:21,680
we have contradictory opinions during millennia.

705
00:53:21,680 --> 00:53:23,680
So that doesn't settle anything.

706
00:53:23,680 --> 00:53:28,680
But what I'm proposing is that these recent technology actually settles the issue because

707
00:53:28,680 --> 00:53:30,680
it matters positive body responses.

708
00:53:30,680 --> 00:53:36,680
And here's what I disagree with you because you want to validate equally a work of art

709
00:53:36,680 --> 00:53:39,680
that creates negative and stressful body responses.

710
00:53:39,680 --> 00:53:40,680
I disagree with that.

711
00:53:40,680 --> 00:53:45,680
I think that the purpose of art is to help in healing the human body because the human body

712
00:53:45,680 --> 00:53:49,680
experiences daily stress, just from the act of living.

713
00:53:49,680 --> 00:53:56,680
So what about where the phrase, which is that art should disturb the comfortable and comfort

714
00:53:56,680 --> 00:53:57,680
that is disturbed?

715
00:53:57,680 --> 00:53:59,680
Why can't we have both?

716
00:53:59,680 --> 00:54:00,680
I'd like to be disturbed.

717
00:54:00,680 --> 00:54:01,680
Yes.

718
00:54:01,680 --> 00:54:04,680
Well, it should be interesting in a lot of words that are interesting.

719
00:54:04,680 --> 00:54:09,680
But I think what's been lost here, and I was surprised, Niko, that you took the direction

720
00:54:09,680 --> 00:54:16,680
you did, given that the first point you seemed to make was coding and computers are negative,

721
00:54:16,680 --> 00:54:19,680
and we should be immersing ourselves in literature and art.

722
00:54:19,680 --> 00:54:24,680
And I actually was trying to point out that maybe there are things that coding cannot

723
00:54:24,680 --> 00:54:30,680
possibly provide, like for example, whether or not they produce really beautiful paintings,

724
00:54:30,680 --> 00:54:32,680
there may yet be something that's missing.

725
00:54:32,680 --> 00:54:38,680
I think it's wrong to conclude that when people say they don't mind being disturbed by art,

726
00:54:38,680 --> 00:54:44,680
that the whole point of art and the end point of art is to be disturbed, I mentioned Van Gogh

727
00:54:44,680 --> 00:54:49,680
because there was a point in the history when he disturbed some people, and now most people

728
00:54:49,680 --> 00:54:54,680
I'm sure have happy meters going wild when they see him, right?

729
00:54:54,680 --> 00:55:01,680
And that trajectory, that development is a wonderful thing about humanism, right?

730
00:55:01,680 --> 00:55:03,680
And I don't see that in computers, that's the thing.

731
00:55:03,680 --> 00:55:10,680
The computers can take a huge sampling of people's reactions and look at different works of art

732
00:55:10,680 --> 00:55:15,680
that are considered wonderful by experts or not, and it could generate another monoliths, perhaps.

733
00:55:15,680 --> 00:55:22,680
But when it comes to being equal to an artist who can create innovation, that may put us off kilter a little bit,

734
00:55:22,680 --> 00:55:26,680
but then later we learned a lot of that's something I think I believe computers are not going to be able to do

735
00:55:26,680 --> 00:55:28,680
for a long, long time, if at all.

736
00:55:28,680 --> 00:55:29,680
Why?

737
00:55:29,680 --> 00:55:32,680
I don't know, I was asking all of you, you're the expert.

738
00:55:32,680 --> 00:55:34,680
What would I mean, you said you believed, but why?

739
00:55:34,680 --> 00:55:38,680
Well, I got thrown into this loop because he started telling me why they could do that.

740
00:55:38,680 --> 00:55:39,680
Okay, great.

741
00:55:39,680 --> 00:55:41,680
First you were, yeah.

742
00:55:41,680 --> 00:55:49,680
You're hung up on Frank Hock, who according to the 17th, 18th century sensibility of realism,

743
00:55:49,680 --> 00:55:53,680
disturbs some people, but the colors are very beautiful and it's slightly disturbing,

744
00:55:53,680 --> 00:55:58,680
but not as disturbing as the garbage we have seen later, that really creates anxiety,

745
00:55:58,680 --> 00:56:01,680
measurable by the happy meter, okay?

746
00:56:01,680 --> 00:56:08,680
Without an explanation, but our art schools have been teaching students that the goal of contemporary art

747
00:56:08,680 --> 00:56:14,680
for the last several decades is to disturb and create anxiety, which I think is so neuro.

748
00:56:14,680 --> 00:56:19,680
And here is where, here is where I think I would like to put two things together.

749
00:56:19,680 --> 00:56:28,680
I mentioned earlier that healing comes to the body, not by looking at code, but by looking at beauty.

750
00:56:28,680 --> 00:56:36,680
But if we go according to accepted contemporary standards of beauty, we can disturb because it creates anxiety,

751
00:56:36,680 --> 00:56:40,680
it's not beautiful, it has switched the function of beauty.

752
00:56:40,680 --> 00:56:43,680
And now only computers can tell you what's beautiful.

753
00:56:43,680 --> 00:56:49,680
See, I would prefer not to debate that particular point further because it's a great topic.

754
00:56:49,680 --> 00:56:51,680
And I'm not arguing, I don't want to argue about it.

755
00:56:51,680 --> 00:56:57,680
I don't think we should be debating that any longer because the real issue is the one I'm trying to direct us to,

756
00:56:57,680 --> 00:57:05,680
which is, let's imagine that there's this wonderful facsimile and it may be superior to what the garbage that they're putting out of art schools now.

757
00:57:05,680 --> 00:57:07,680
That's okay, I'll accept that.

758
00:57:07,680 --> 00:57:19,680
What is it that, is it still the case that there is some, is there some spirit in human creativity that the computers will or will not be able to produce?

759
00:57:19,680 --> 00:57:23,680
And you think, Jonathan, that they will be able to eventually produce that?

760
00:57:23,680 --> 00:57:26,680
I don't think I have any idea.

761
00:57:26,680 --> 00:57:29,680
I mean, I wouldn't rule it out, I guess what I would say.

762
00:57:29,680 --> 00:57:36,680
I mean, like, doesn't seem to be any, it seems to be par for the course for any complicated human creative activity.

763
00:57:36,680 --> 00:57:42,680
And therefore, it's a problem for, again, like, Peter's world to solve.

764
00:57:42,680 --> 00:57:45,680
But it doesn't seem to me to be, like, intractable.

765
00:57:45,680 --> 00:57:53,680
I think that's the, I think part of the thing, I myself feel a little bit confused with parts of this discussion.

766
00:57:53,680 --> 00:58:03,680
Because we are talking about, on the one hand, what is today, then we are talking about what AI is going to do in the future.

767
00:58:03,680 --> 00:58:22,680
And whether it's going to do that or not, is AI going to be able to produce a work of literature or art that in any way is going to contain all of the emotions that are presently in the work of literature or in the work of art?

768
00:58:22,680 --> 00:58:28,680
And all the personal experiences of the person who wrote it or who painted it is AI.

769
00:58:28,680 --> 00:58:37,680
One day going to be able to do that, you may be have the answer to that, but as far as I know, we don't know yet.

770
00:58:37,680 --> 00:58:47,680
So there are people who write about all the things that AI is going to do in the next 25, 50 years, such as Max Tegmark and others.

771
00:58:47,680 --> 00:58:50,680
But we'll have to wait and see.

772
00:58:50,680 --> 00:58:59,680
But so I don't see how that directly connects with the bigger or more general subject, which is coding.

773
00:58:59,680 --> 00:59:10,680
Coding is not only about creating literature or art, it's also about all the positives that we have from being able to use the computer, the iPhone and so on.

774
00:59:10,680 --> 00:59:24,680
Yes, there are of course plenty of negatives. One of our people who was here in the audience, a neuroscientist from Italy, was telling me how his son doesn't want to read anything because he's looking at his computer.

775
00:59:24,680 --> 00:59:31,680
He said, my grandson isn't the same one. He said, my grandson doesn't want to read his constitution.

776
00:59:31,680 --> 00:59:40,680
Yes, there are negatives, but there are also plenty of positives in terms of people being in contact with each other, being in touch with each other.

777
00:59:40,680 --> 00:59:48,680
So I'm a little bit confused if you will about the direction that we are taking.

778
00:59:48,680 --> 00:59:57,680
And as far as art goes, I think the example you gave of the, was it in cold experiment with the heart rate and so on?

779
00:59:57,680 --> 01:00:00,680
In the cold experiment in the Kedakolis Museum.

780
01:00:00,680 --> 01:00:20,680
Yes, so I think that already gives you an indication that wherever that research is nowhere near reality, if you are going to a museum and there are five paintings, one next to each other, and you have somebody explaining it to you, what you are reacting to is not really that much the painting as what the person is telling you.

781
01:00:20,680 --> 01:00:34,680
Now, and that's why I feel when I go to a museum, a big part of it is a waste of time because I'm seeing so many things one after the other by the time I come out, I don't know what I've really seen.

782
01:00:34,680 --> 01:00:45,680
If I see in front of one art and I spent ten minutes in front of it, I begin to appreciate and feel something that I can never feel walking through a museum.

783
01:00:45,680 --> 01:01:07,680
So to test me just because I think that this is a very exciting color, he put it here especially because he does this and that doesn't tell me anything about the art or the ability of that particular feeling machine to assess anything.

784
01:01:07,680 --> 01:01:09,680
It's too early for it.

785
01:01:09,680 --> 01:01:26,680
Actually, it's not too early and it's just that I didn't get the idea and I have to thank me because for giving me the idea of having people doing exactly exactly experiment you described by exposing them to different pieces of art to compare which one is the best.

786
01:01:26,680 --> 01:01:39,680
But let me just say this because I want to say this clearly. If you're criteria for what is the best is a measurable response of happiness, you're going to get a lot of shit.

787
01:01:39,680 --> 01:01:46,680
Just a lot of fucking bad art and bad literature especially.

788
01:01:46,680 --> 01:01:57,680
So again, I would just disagree with the basic premise.

789
01:01:57,680 --> 01:02:04,680
We will support this premise that you disagree with if they are unconscious responses.

790
01:02:04,680 --> 01:02:07,680
Unconscious or not?

791
01:02:07,680 --> 01:02:20,680
It's just a bit of a ton of shit. If it's unconscious, it's the body using its evolved mechanism that helped in its survival and if it identifies positively with the art, then it's a positive response that's good for the health.

792
01:02:20,680 --> 01:02:24,680
If it has to be explained and then you are conditioning.

793
01:02:24,680 --> 01:02:26,680
No, I'm not talking about it.

794
01:02:26,680 --> 01:02:41,680
In fact, I think probably most art that I would find interesting you have to have training to appreciate and understand and I think most art that produces that kind of evolved health response you're talking about is going to be crap.

795
01:02:41,680 --> 01:02:45,680
We have a opposite opinion and we disagree violently.

796
01:02:45,680 --> 01:02:49,680
I think Mark's happy meter is going to get too low for all of us.

797
01:02:49,680 --> 01:02:58,680
It feels too close to a mood ring.

798
01:02:58,680 --> 01:03:07,680
There is a moment that we can pop this up and talk about narrative for a second.

799
01:03:07,680 --> 01:03:21,680
Think about the ways in which we are taking physiological data, whether we should be doing that or not or instantaneous or not, the idea that the AI optimizes something.

800
01:03:21,680 --> 01:03:27,680
The narrative that comes along with it, there is an efficiency there in applying the AI.

801
01:03:27,680 --> 01:03:35,680
There is an efficiency that comes along with it that we can make better things that are soothing to us or pointed to the progression.

802
01:03:35,680 --> 01:03:44,680
That's a very common AI rationale, a very common narrative that goes along with applying an AI system.

803
01:03:44,680 --> 01:03:49,680
It makes it hard to unpack and say, well, what are the problems here?

804
01:03:49,680 --> 01:03:52,680
Where are the disparities?

805
01:03:52,680 --> 01:04:07,680
To say that it's more efficient to whom and to, you know, are the physiological observations sort of steady across different race and gender characterizations and so on.

806
01:04:07,680 --> 01:04:14,680
You have to kind of resist that narrative that says that AI is optimizing something.

807
01:04:14,680 --> 01:04:24,680
The other thing I would say to the gentleman with his grandson, we say that, pardon?

808
01:04:24,680 --> 01:04:27,680
Hold your son.

809
01:04:27,680 --> 01:04:30,680
It's quite satisfying.

810
01:04:30,680 --> 01:04:37,680
I would say that we say that they kiss her on their phones all the time, right?

811
01:04:37,680 --> 01:04:44,680
We say that means that they're really good at computers because they're not good at computers.

812
01:04:44,680 --> 01:04:46,680
They're good at learning other people's interfaces.

813
01:04:46,680 --> 01:04:49,680
They're good at learning the choices that other people have made.

814
01:04:49,680 --> 01:04:57,680
Because they don't recognize anything of that surface as being a choice, right, that this means zoom in and this means zoom out.

815
01:04:57,680 --> 01:05:01,680
Somebody made that choice and it wasn't the student working at it.

816
01:05:01,680 --> 01:05:16,680
And we have to be able to question those choices, right, and I feel like a citizenry that doesn't have access to the basic ways in which computer design works and computers work can't do that.

817
01:05:16,680 --> 01:05:18,680
Yeah, and I agree entirely.

818
01:05:18,680 --> 01:05:30,680
The conversation that we were just having about, you know, measuring somatic responses and this connection to artificial intelligence reminds me of just like the FAD 10 years ago,

819
01:05:30,680 --> 01:05:35,680
the Vittorio remember very well of just using fMRI to settle all questions.

820
01:05:35,680 --> 01:05:43,680
This lights up, that lights up, you know, therefore, you know, this can explain for us ex or wife phenomena in politics or the world or whatever.

821
01:05:43,680 --> 01:05:48,680
And that had all the problems that you just identified, Mark and that Vittorio knows very well.

822
01:05:48,680 --> 01:05:56,680
FMRI experiments were a good pioneering step in the right direction because they did settle some questions of beauty.

823
01:05:56,680 --> 01:06:03,680
The problem is that FMRI is hugely inconvenient, but we're talking about a new generation, but it's a wearable sensor.

824
01:06:03,680 --> 01:06:06,680
Yeah, because you can have thousands.

825
01:06:06,680 --> 01:06:13,680
The N, FMRI is very small because it's so expensive and here you can, I mean, I have been measuring emotions for the last three years.

826
01:06:13,680 --> 01:06:15,680
The phase emotion, voice emotion.

827
01:06:15,680 --> 01:06:25,680
The last thing is that we have plans which are highly sensitive movement sensors and we take the electrostatic discharge and then we measure emotions of humans from the plant response.

828
01:06:25,680 --> 01:06:28,680
That's totally privacy respecting.

829
01:06:28,680 --> 01:06:38,680
Well, but, okay, so Nikas, you use this idea that the goal of art should be to be soothing or to help.

830
01:06:38,680 --> 01:06:40,680
I didn't say soothing, I say.

831
01:06:40,680 --> 01:06:42,680
Inducive to human health.

832
01:06:42,680 --> 01:06:43,680
Okay.

833
01:06:43,680 --> 01:06:44,680
Physiological condition.

834
01:06:44,680 --> 01:06:45,680
Perfectly good.

835
01:06:45,680 --> 01:06:46,680
Perfectly good.

836
01:06:46,680 --> 01:06:47,680
Okay.

837
01:06:47,680 --> 01:06:53,680
Now, so I feel happier in front of a beautiful painting and I'm sure I do.

838
01:06:53,680 --> 01:06:57,680
Now, however, feeling happy on what time scale.

839
01:06:57,680 --> 01:07:07,680
You know, one of the whole points about mental health, for example, people who are troubled by their own mental health is that they need to oftentimes go into some form of treatment.

840
01:07:07,680 --> 01:07:14,680
They get psychotherapy and they talk about their problems and they may sit with their therapist and say, oh, I love this therapist.

841
01:07:14,680 --> 01:07:15,680
What a great therapist they're going to make.

842
01:07:15,680 --> 01:07:20,680
I feel so happy when I'm with them and then a month later they think this person doesn't like me.

843
01:07:20,680 --> 01:07:21,680
I don't understand.

844
01:07:21,680 --> 01:07:25,680
And they keep going, hopefully, and keep going and talking.

845
01:07:25,680 --> 01:07:39,680
And they work through that and it involves trust to go back to your comment, you know, there's a trust between two human beings and over time the feeling of happiness or contentment really evolves.

846
01:07:39,680 --> 01:07:42,680
So I believe that can happen with works of art as well.

847
01:07:42,680 --> 01:07:43,680
That's the only reason I mentioned Van Gogh.

848
01:07:43,680 --> 01:07:51,680
Are there other examples?

849
01:07:51,680 --> 01:07:58,680
I don't think I would probably disagree with you about whether I like the same one you like or don't like.

850
01:07:58,680 --> 01:08:00,680
I'm not so sure I would disagree in most cases.

851
01:08:00,680 --> 01:08:09,680
But what I'm saying is that there's an evolution of feelings that you may not pick up in just a moment when you're looking at a work of art, even if it's based on unconscious experience because there's a trajectory that has some bearing on what we think about the art and literature in the long term.

852
01:08:09,680 --> 01:08:10,680
You work our way through it.

853
01:08:10,680 --> 01:08:13,680
There is a difference.

854
01:08:13,680 --> 01:08:16,680
You are making an unsupported conjecture which may or may not be true.

855
01:08:16,680 --> 01:08:19,680
It could be true possibly in your field.

856
01:08:19,680 --> 01:08:22,680
But in our there are no measurements so far.

857
01:08:22,680 --> 01:08:27,680
Measurements have to be done by apparatus exactly like me.

858
01:08:27,680 --> 01:08:30,680
The meters those experiments have not yet been done.

859
01:08:30,680 --> 01:08:39,680
I would like to bring it back to the question of coding and point out that when we talk about code, we are talking about mediation.

860
01:08:39,680 --> 01:08:46,680
We are talking about something that is created and that the form of creation is not transparent.

861
01:08:46,680 --> 01:08:58,680
And I think the problem here with this question is partly that we are only talking about visual art and visual art allows us to persist in this very strange assumption that visual art is unmediated.

862
01:08:58,680 --> 01:09:01,680
That visual art is not encoded.

863
01:09:01,680 --> 01:09:12,680
And your assumptions about our responses to visual art rely on an assumption about visual art that it is not encoded and that it lacks historicity.

864
01:09:12,680 --> 01:09:30,680
But if we, so what would you say for instance if we had someone from I don't know 200 years ago or from a different country or from a different social position and they had the happy meter and they looked at a piece of visual art which is entirely encoded

865
01:09:30,680 --> 01:09:33,680
and they had the opposite reaction.

866
01:09:33,680 --> 01:09:52,680
So I mean what you are basically assuming here is that the visual art lacks any artistry as such and that our response to it is a purely one to one relationship between this sort of non-mediated thing that it does to our body.

867
01:09:52,680 --> 01:10:07,680
And let me just add to you in this point the assumption also is that as you were saying earlier it is unconscious but not just unconscious somatically unconscious which leaves out the, you know anything that might be cognitive or intellectual.

868
01:10:07,680 --> 01:10:27,680
The, you know that art actually might teach us things or change the way that we think or that might be actually conscious responses to or processes or you know or dynamics involved in encountering not only visual works of art but especially literary ones.

869
01:10:27,680 --> 01:10:38,680
And I mean again like that's one account of maybe what an artwork does but it's an is an ought to do but it's a, but it seems to me a kind of you know a pretty small one.

870
01:10:38,680 --> 01:10:42,680
I have an answer to this statement.

871
01:10:42,680 --> 01:11:02,680
Based on unconfirmed data there seems to be a majority of invariant response from all people all races all cultures roughly estimated 80% and the other 20% is cultural locally variant.

872
01:11:02,680 --> 01:11:14,680
But those of us who were in the topic we claim based upon unconfirmed data that about 80% is invariant across all people.

873
01:11:14,680 --> 01:11:17,680
And this is the type of responses instantaneous.

874
01:11:17,680 --> 01:11:38,680
I ask you a question in neuroscience they're finding out more and more that the placebo effect is partly based on placebo readiness and receptivity in the individual. Is there any placebo effect in relationship to how people respond to these different codes.

875
01:11:38,680 --> 01:11:54,680
And the expression of the codes and the effect on the body in what you're describing. Well the honest answer is I have not thought about this deeply about I think the answer is yes that's my suspicion because you are conditioned

876
01:11:54,680 --> 01:12:14,680
and this goes back to that we believe Google more than ourselves and so if the happy meters and happy then I think I have to be happy and if I look at painting like one of these are which has been given the reputation of being the most beautiful painting in the world and I have to feel that.

877
01:12:14,680 --> 01:12:25,680
And so the happy me to show me that that sort of seems to me to confirm your. Yes the expectation that this is rewarding and stuff that like yes. Okay I would like to go ahead.

878
01:12:25,680 --> 01:12:51,680
It's just another put another pay in for computation and narrative. Before you said like one of the positives of trying to apply an AI system right is comes from the or a set of them come from the work you have to do to get there.

879
01:12:51,680 --> 01:13:08,680
Right so so that these questions we're asking about is an instantaneous response or not is it something that you can measure from a watch is it might is it a mood ring right taking a little camera of my mood ring right that we that to to go along the path of we're going to optimize

880
01:13:08,680 --> 01:13:25,680
or we're going to make efficient the creation of a work of art or something like that means that we need to step back and break that down into pieces. So first of all how are we going to decide that something is great or not is it is it our our physiological response is it a survey that we give someone six months later

881
01:13:25,680 --> 01:13:40,680
you're still thinking about this has this changed your opinion like like we can now sit and talk about what are the variables that might that might give rise to this and by talking about those set variables we are doing a lot of actual real work to kind of understand

882
01:13:40,680 --> 01:13:49,680
and think about it theorize about the ways in which we appreciate art and that's coming from what's fundamentally a computational question the first place.

883
01:13:49,680 --> 01:14:00,680
Yeah. You remind me I was I'm glad you mentioned that because I was about to again challenge Jonathan about his idea that eventually this will be something that will happen I'll tell you what I mean I want to apply what Mark just said.

884
01:14:00,680 --> 01:14:17,680
I know I was thinking in a way I you may be again likely contradicting yourself so lightly here in this sense but if you say okay well the solution that Niko's has suggested you absolutely reject that couldn't possibly produce art

885
01:14:17,680 --> 01:14:31,680
of you know equal to a human or it's the wrong way to go about performing so the question would be and I maybe Mark is beginning to provide an answer so what do you think other than time what else might get us there.

886
01:14:31,680 --> 01:14:46,680
I was saying it's the wrong way of judging work. Yeah. No I would be a way that would allow to allow me machine learning to generate the sort of thing that we think would be equal to a human so-called genius.

887
01:14:46,680 --> 01:14:56,680
I mean first of all I don't know because that's not what I do but the but I go from my gatherer and again what I didn't say I wasn't predicting anything because I would have no grounds to make any predictions.

888
01:14:56,680 --> 01:15:12,680
I just said I have no problem with the idea that that very that you know very powerful artificial intelligence might be able to write a novel that someone might actually read and not understand that it was written by a machine and not a person but that's because actually that

889
01:15:12,680 --> 01:15:26,680
program has been fed you know a gajillion novels written by humans so it actually seems to me that the achievement there is not as huge and kind of world creating as might be as others might claim.

890
01:15:26,680 --> 01:15:41,680
Which is just a you know it's just a computing step in computing power and the way that computers work with work with linguistic objects that have been you know around and created by humans with

891
01:15:41,680 --> 01:15:50,680
whose computational systems are you know web you still seem to be unsure about what criteria you could apply to train the computer.

892
01:15:50,680 --> 01:15:58,680
I mean the ones you would say I guess you'd say the zonicos is suggestion at least in the middle arts would be too simplistic and it wouldn't.

893
01:15:58,680 --> 01:16:00,680
No you said the my suggestion is moderately wrong.

894
01:16:00,680 --> 01:16:01,680
Okay well it may be.

895
01:16:01,680 --> 01:16:04,680
This is a suggestion of judgment right now.

896
01:16:04,680 --> 01:16:17,680
Yeah you know so I think that the I think the question of like you know judgment is going to be this or the criteria judgment will be the same whether or not we are reading a book that's written by a computer or whether or not you're reading a book by a human being.

897
01:16:17,680 --> 01:16:26,680
I think maybe you know there's a kind of like touring test level in which it might be interesting to see if something could be you know picked up and read and mistaken.

898
01:16:26,680 --> 01:16:43,680
I think that that tells you more again about the powers of the computer than it tells you about the nature of you know of literary fiction or literary art simply because what the computer is doing is trying to actually figure out a way of like you know it's reengineering something by

899
01:16:43,680 --> 01:16:57,680
amassing massive examples and trying to create something that's like them.

900
01:16:57,680 --> 01:17:16,680
That would be the same whether you know the author is machine or person.

901
01:17:16,680 --> 01:17:30,680
Especially with nonfiction writing when computers can actually like write papers for example so like there's you know this plagiarism software which can detect whether or not a student has a copy to paper.

902
01:17:30,680 --> 01:17:45,680
You can track down sometimes you know whether or not another human being has written it for money but whether there will soon be programs available that will can have a machine write a paper for you if you simply input what the topic is and what the sources are.

903
01:17:45,680 --> 01:17:48,680
The answer is it has this has been done.

904
01:17:48,680 --> 01:17:49,680
It can.

905
01:17:49,680 --> 01:18:02,680
So that presents a real question of judgment I think actually like where again like it's where the question is like you know has the human written that has a machine written that has a real you know kind of important practical dimension to it.

906
01:18:02,680 --> 01:18:19,680
And we are actually doing what you just described not for literature but for marketing for simpler and tasks and we are having humans there in our hardware and our tools and then checking whether they respond differently to human designed

907
01:18:19,680 --> 01:18:31,680
and we are I'm at business school so we use marketing concept or from cheap developed from cheap history and we don't know the answer yet but in half a year I can tell you.

908
01:18:31,680 --> 01:18:37,680
No one's raised the possibility that because humans have free will yet I'm just going to throw that out there.

909
01:18:37,680 --> 01:18:54,680
And computers don't wait let me finish that they do the humans are free will and we may have access to let's say an infinite realm including fictional world that don't exist yet and I'm going to have that you create a fictional world that hasn't been in literature so far.

910
01:18:54,680 --> 01:19:08,680
The computer might be able to come close maybe but maybe not but don't know when it seems to have taken that possible slam.

911
01:19:08,680 --> 01:19:30,680
I knew you were going to say that.

912
01:19:30,680 --> 01:19:42,680
Of course there's no free will because free will means that from the stuff in the trash.

913
01:19:42,680 --> 01:19:59,680
I think the problem is free will seems to me maybe the to be leading us down the wrong path I mean because I think I'm not sure it solves the question for us in the way that you might want to but maybe just thinking about this is something about the imagination.

914
01:19:59,680 --> 01:20:17,680
It seems like what you're trying to get at like that humans have a kind of imagined the capacity to create something that has never existed before an entire world or just simply a community of other humans and problems specific to them.

915
01:20:17,680 --> 01:20:41,680
And then is that something that if you give enough examples like feed a computer like you know hundreds and thousands of novels whether the computer could figure out what it means to create a world that has never existed before based on a kind of you know imitation of what it is read.

916
01:20:41,680 --> 01:20:49,680
Without lacking the without having the human imaginative capacity to do it on its own without having done all that reading.

917
01:20:49,680 --> 01:21:07,680
I think that's you know I mean that I guess is a kind of interesting question about like what gets added to these kind of just acts of basic like imitation and slight adjustment and whether or not there's something special that goes into something that's special

918
01:21:07,680 --> 01:21:15,680
that's very detectable by a reader in an artifact that's created by human versus an artifact that's created by computer.

919
01:21:15,680 --> 01:21:19,680
That's why I'm writing in the subtitle the Procellation of what's not there.

920
01:21:19,680 --> 01:21:24,680
Yeah because there are things that are not there yet and computers can.

921
01:21:24,680 --> 01:21:30,680
Right the question is can they get so advanced they can imagine something that humans can imagine.

922
01:21:30,680 --> 01:21:38,680
It's impossible to me but it's an interesting question. I think we keep talking about computers as if they aren't human.

923
01:21:38,680 --> 01:21:42,680
They come from us like we made them we made we're the ones who are programming them.

924
01:21:42,680 --> 01:21:48,680
We are the ones who put the metaphors forward through which certain things are easy to express or hard to express.

925
01:21:48,680 --> 01:21:50,680
We're the ones who are collecting the data.

926
01:21:50,680 --> 01:21:58,680
We're the ones who setting up the individual criteria upon which it's running and which it's trying to evaluate it.

927
01:21:58,680 --> 01:22:01,680
We're the ones putting putting all that together. It's us.

928
01:22:01,680 --> 01:22:09,680
So it is a human thing I think to say that we invented hammers too and to do a hammer everything is in hell.

929
01:22:09,680 --> 01:22:11,680
Isn't that what they say?

930
01:22:11,680 --> 01:22:12,680
Yes.

931
01:22:12,680 --> 01:22:13,680
They say that.

932
01:22:13,680 --> 01:22:19,680
What they mean by that is that someone using a hammer everything appears in hell which I think is part of Mars coin.

933
01:22:19,680 --> 01:22:35,680
There's a more sophisticated hammer. I think that my students I mean I teach computational journalism and we spend some time with natural language processing

934
01:22:35,680 --> 01:22:42,680
which is sort of computers applied to creation of language to the Alice's language.

935
01:22:42,680 --> 01:22:58,680
We play with GPT three off and on and ask them what are the tasks that are what kinds of things could you as a journalist imagine using the score.

936
01:22:58,680 --> 01:23:03,680
There are certain data chain things you can do.

937
01:23:03,680 --> 01:23:14,680
Like when New York State started publishing statistics about monkeypox it was putting it out on the health department website as a sentence.

938
01:23:14,680 --> 01:23:20,680
Last week there were five cases in New York City. There were three cases in something county that were NGB three would do.

939
01:23:20,680 --> 01:23:26,680
You could ask it to do to fleet take that number and put it into a tape those data put it into a table and it would happily do that for you.

940
01:23:26,680 --> 01:23:33,680
You could also ask it write me something about a story about what's happening at the border.

941
01:23:33,680 --> 01:23:40,680
Well problem is GPT three is not is trained up to like 2014 something like that.

942
01:23:40,680 --> 01:23:47,680
So if you mentioned something about Ukraine it's not a contemporary situation it's a past situation.

943
01:23:47,680 --> 01:23:55,680
But I think that what I wanted to say is that the way you interact with GPT three it's looking to complete an idea.

944
01:23:55,680 --> 01:24:03,680
It's trained on a bank of literature as you suggest.

945
01:24:03,680 --> 01:24:12,680
But what the underlying prediction problem is trying to do is to say given them this far in the sentence or this far into the paragraph or whatever it is what comes next.

946
01:24:12,680 --> 01:24:16,680
It's always just like guessing what comes next to generate.

947
01:24:16,680 --> 01:24:25,680
So you can start and you can do what's called prompt programming and start it you know often in a direction so easiest kind of prompt.

948
01:24:25,680 --> 01:24:37,680
You could say English colon hi French colon Bonjour English colon skyscraper French colon and it'll give you what the French translation of skyscraper.

949
01:24:37,680 --> 01:24:57,680
Thank you. And then you could ask it to you could start to ask it to you know here's here's an example of five yellow reviews of a particular thing identified to that have sort of racially biased whatever right so you can start to sort of weave it in

950
01:24:57,680 --> 01:25:16,680
and there are various ways in which you might interact with a GPT three through through divine defining these prompts and you can chain them together and almost start to build programming ideas that I believe you could sort of teach the fundamentals of computational thinking

951
01:25:16,680 --> 01:25:31,680
through a GPT three style interface because it's all just language at that point you're not you know do you know for I and one does something or they do this right I think you are you are pulling apart computational computational questions

952
01:25:31,680 --> 01:25:35,680
by by giving instructions to a machine and seeing what comes back.

953
01:25:35,680 --> 01:25:38,680
And there's always this moment of oh I didn't expect that.

954
01:25:38,680 --> 01:25:55,680
Oh look what happened there right and there's a little bit of delight and whatever that comes from interacting with these systems and I think I think again it comes down to to to to to be open enough and having the exposure to to what these computational systems are all about to be able

955
01:25:55,680 --> 01:26:03,680
to ground that conversation and to say all right this is where it might be useful this is where it might be dangerous and we can start to make choices.

956
01:26:03,680 --> 01:26:13,680
I think thank you I think we should open the floor up to questions and please I'm going to put the microphone up there.

957
01:26:13,680 --> 01:26:20,680
Come on up here.

958
01:26:20,680 --> 01:26:33,680
So thank you so much for this lively discussion I'm here since then and in the heating meter I think you score the highest figures so far so thanks.

959
01:26:33,680 --> 01:26:45,680
I have a comment and a question the comment is being having involved in some empirical research on aesthetics.

960
01:26:45,680 --> 01:27:04,680
I think one is to distinguish the tools from the questions so with the same tools you can ask different questions you can be interested in spotting the place in the brain where beauty sits which is a few millimeters away from where the sublime sits

961
01:27:04,680 --> 01:27:22,680
and legitimate question to be to be asked but it's not my cup of tea or you can be more interested in the experience in front of some cultural artifacts and then starts the difficulty because our experience is always situated.

962
01:27:22,680 --> 01:27:35,680
So you can do it in easy experiment think about your favorite piece of music and listening to it while you're contemplating your favorite landscape with respect to when you're filling your tax form.

963
01:27:35,680 --> 01:27:51,680
So the physiology up to a certain point is clearly the same you you activate the same or theatory pathway blah blah blah but then in the end the experience is completely different.

964
01:27:51,680 --> 01:28:02,680
So one of the greatest difficulties is to contextualize the experience within the individual in different time of this life so it's incredibly difficult.

965
01:28:02,680 --> 01:28:09,680
The question relates to the parallel between narrative and coding.

966
01:28:09,680 --> 01:28:19,680
So if we can draw such a parallel and entertain the idea that narrative is a particular form of coding.

967
01:28:19,680 --> 01:28:39,680
Can we also push it to say to the point to entertain the idea that they are more similar than they might look at first sight because narrative in itself is the outcome of a specific cognitive technology.

968
01:28:39,680 --> 01:28:56,680
There are people who have been suggesting that the first form of narrative stems from the syntax of movement, the Shenoupe Ratua, Le Raguaran for example, was one of the earliest proposals of such theory.

969
01:28:56,680 --> 01:29:03,680
So can this be this idea how does it sound to you? Do you think it is legitimate or?

970
01:29:03,680 --> 01:29:11,680
I don't think that I quite understand the claim that narrative is based on a syntax of movement.

971
01:29:11,680 --> 01:29:16,680
I think at its fundamentals narrative is based on an expression of causation.

972
01:29:16,680 --> 01:29:34,680
And the root can be traced in a time of human evolution when the main activity was utility.

973
01:29:34,680 --> 01:29:46,680
The main object whose main purpose is not to serve a specific need but the only purpose is to say something to someone else.

974
01:29:46,680 --> 01:30:03,680
Can we trace is there a red line, a continuum between the motor syntax that enables the production of tools for example, with the motor syntax, the narrative syntax that enable you to produce

975
01:30:03,680 --> 01:30:10,680
an average, a feature of narrative. Does it sound crazy?

976
01:30:10,680 --> 01:30:13,680
I'll close it.

977
01:30:13,680 --> 01:30:21,680
Yeah, I mean, it's an interesting way of thinking. I'm not really sure how you would explore the phenomena, to be honest.

978
01:30:21,680 --> 01:30:26,680
I mean, like, or it seems like a good prompt for thinking.

979
01:30:26,680 --> 01:30:28,680
Yeah, it's kind of a judgement story.

980
01:30:28,680 --> 01:30:42,680
A little bit. I think that I share a Nina's intuition that there's a kind of deeply causal structure to most narratives if you want to get them to their simplest structure.

981
01:30:42,680 --> 01:31:04,680
And establishing connections between one event and another. The on the side of response, like, why is it that we seem to have a kind of appetite again?

982
01:31:04,680 --> 01:31:23,680
It's just so sort of phrased but for want to have a better term. Like, why we have an appetite for stories and narratives presents, you know, is to come at it from the other side, sort of more, you know, on this sort of level of like, you know, consumption rather than the production of stories

983
01:31:23,680 --> 01:31:33,680
or what stories are. And I could be there that some of the kind of, some of the deep history you're talking about actually plays an important role.

984
01:31:33,680 --> 01:31:37,680
Maybe I'll stop there.

985
01:31:37,680 --> 01:31:48,680
I think it's a very, it's a complicated question. I think we're all having a little difficult time wrapping our minds around. Not because it's crazy because you asked, I wouldn't have used that adjective.

986
01:31:48,680 --> 01:31:59,680
But you asked, but no, no, no, no, I really mean that. I don't know. I think it's interesting to think, you know, the one criticism of some books is that some literature is like, you know, nothing happens in this story, right?

987
01:31:59,680 --> 01:32:10,680
And that doesn't mean that's a valid, that's only one way you can look at it. But more advanced forms of literature, there were, there are more stories where it's more internal that goes on in the story.

988
01:32:10,680 --> 01:32:19,680
And some people don't go for that because obviously Homer, in Homer, a lot of action is happening. Closation and movement are closely related.

989
01:32:19,680 --> 01:32:37,680
So for sure, we know was based on the idea of one billion ball, ball, that's another, that's sort of a paradigm for causation. So, yeah. There's a connection to between your initial comment and your question, which is that, and I agree entirely with your formulation that you gave of the

990
01:32:37,680 --> 01:32:54,680
situatedness of any aesthetic response, which I think raises some real problems for the idea that you could just kind of connect it to something like a happiness meter, which is intrinsically decontextualized, it's just like attached to you.

991
01:32:54,680 --> 01:33:11,680
And I think that that raises questions around how you would, I mean, most of the way that we think and talk about artificial intelligence in the way that it can pose the stories is unsituated, that is the program has no situation.

992
01:33:11,680 --> 01:33:23,680
It's lifted out from the world. And it's only access to the world is through what we feed it, through other stories, and through a writing of code.

993
01:33:23,680 --> 01:33:48,680
That seems to me, I mean, the picture that you were giving, which I know is one that you hold closely, is that of a kind of much of a more embodied interaction with the world, not embodied in the sense of, you know, the kind of unmoving physiology of someone who is just reading or looking at something and then attached to a meter, but actually acting in the world, moving around in it.

994
01:33:48,680 --> 01:33:53,680
And there, I think, you know, our agree the entirely.

995
01:33:53,680 --> 01:34:00,680
Anyone else?

996
01:34:00,680 --> 01:34:06,680
Good to see you. I have a couple of colleagues here. I'm one of Jonathan's friends at Yale.

997
01:34:06,680 --> 01:33:59,680
I was struck by the conversation about code from the beginning in that it didn't seem that we could ever, like there was ever really a consensus about what a code is, that there's a lot of sort of implicit

998
01:33:59,680 --> 01:34:16,680
understanding about who, like, the difference between, you know, code switching and language and computer code. And like, so I, you know, while I was sitting there at Google, a little bit of etymological, I was just thinking about

999
01:34:16,680 --> 01:34:31,680
where these words come from and code comes from codecs. It comes from the book of laws that were established that everyone kind of referenced as a kind of index of cultural authority, right? That was the sort of where we get the word code.

1000
01:34:31,680 --> 01:34:45,680
The idea of code as a cipher or system of signals and the rules which govern their use isn't until 1808. And then, you know, in subsequent decades, you get Morse code and things like that.

1001
01:34:45,680 --> 01:35:11,680
But the idea of it as a system of expressing information and instructions in the form, usable by a computer, is not until 1946 and 47 when it becomes a verb.

1002
01:35:11,680 --> 01:35:25,680
So what I want to ask is, do you understand code as being something that a computer is the sort of repository or archive of, or is it something that the computer does or that we do to a computer?

1003
01:35:25,680 --> 01:35:42,680
And I ask because it strikes me as an interesting question about decoding. Does the computer decode what we give it? And my assumption is no, the computer doesn't understand what's encoded, it just processes information.

1004
01:35:42,680 --> 01:35:58,680
But because it's described as code, there's almost a way in which the machine is sort of like from the origin of the term takes on this sort of humanistic aura that it's this thing that can do what we do, which is decode.

1005
01:35:58,680 --> 01:36:13,680
But in fact, machines don't decode, they just churn. I mean, I guess you could program something to decode something else, but that's different than saying in the way that we would decipher a cipher.

1006
01:36:13,680 --> 01:36:36,680
So tell me about code, why is it, why is, why are we using this term to describe what computers do? Well, I would actually just say the other way around, and I would say computer is capable of repeatably exactly in the same way as many times as I wanted, executing my code,

1007
01:36:36,680 --> 01:36:49,680
whereas if you have a codecs and you have humans interpreting it, then we have all these contexts, and so it's not repeatable. And you don't know because you have the mood of the day and many other influences.

1008
01:36:49,680 --> 01:36:54,680
And so I think in that sense, the computer is from a consistent.

1009
01:36:54,680 --> 01:37:05,680
I think all of the senses, the animal article senses that you gave, I think they're fairly consistent, and they do all come back to the sense of a set of rules.

1010
01:37:05,680 --> 01:37:21,680
And whether that's a set of rules for behavior, in the case of when you give a computer certain lines of code, those are then rules that determine what it will do, what it will produce, or if it's a code, say in narrative,

1011
01:37:21,680 --> 01:37:37,680
the fundamental set of rules that govern what a narrative is, that all of those different senses of code seem to me like compatible with this idea of a set of rules.

1012
01:37:37,680 --> 01:37:51,680
So, I mean, I guess if I were to say what is a code, I'm comfortable with it's a set of fundamental principles or rules that govern the generation of something further.

1013
01:37:51,680 --> 01:37:54,680
Anything else?

1014
01:37:54,680 --> 01:37:56,680
Okay.

1015
01:37:56,680 --> 01:38:06,680
Which is, yes, what you said is true, but the asymmetric conditions of the creation of code is inseparable from the cryptographic history, right?

1016
01:38:06,680 --> 01:38:16,680
So, understanding that, because that's one thing that I'm a magician, and I track asymmetric information conditions, and I interact with a lot of academics.

1017
01:38:16,680 --> 01:38:30,680
And one thing I find fascinating is that in these discussions, the asymmetric information condition associated with codes and information systems in general is not so well tracked or brought up in these conversations.

1018
01:38:30,680 --> 01:38:47,680
So, to his point, I think there's something much more deep under what he's saying that's actually quite relevant these days to track how this information, what's the nature of information, we know unique and interoperable data is incredibly valuable.

1019
01:38:47,680 --> 01:39:06,680
But it really takes a kind of cryptographer, my father-in-law was a naval commander in Holland who worked at NATO Command, and then I have a lot of friends in mathematics at places like the Institute for Defense Analysis, and I think they'd have a lot to say about this conversation.

1020
01:39:06,680 --> 01:39:14,680
So, and I think those things are especially relevant today, so I'm just backing up your point, really.

1021
01:39:14,680 --> 01:39:22,680
Well, thank you.

1022
01:39:22,680 --> 01:39:24,680
Thank you for another wonderful panel.

1023
01:39:24,680 --> 01:39:29,680
I want to make a question that's going to connect us with the previous panel.

1024
01:39:29,680 --> 01:39:46,680
Is the publishing of, let's say, computer-generated novel misinformation if I don't disclose that it's being generated by a computer?

1025
01:39:46,680 --> 01:40:00,680
Well, sure, I mean, in some level, if you define it as information, the question is whether or not...

1026
01:40:00,680 --> 01:40:04,680
Well, I'm sure, because the computer has been generated by humans.

1027
01:40:04,680 --> 01:40:05,680
There you go.

1028
01:40:05,680 --> 01:40:07,680
It's a good answer.

1029
01:40:07,680 --> 01:40:24,680
I think in journalistic applications, when a story has been written has been automated, even if it's human-edited, the bylines you just, it's been done by an AI.

1030
01:40:24,680 --> 01:40:30,680
But we do attribute middle marks to George Eliot, right?

1031
01:40:30,680 --> 01:40:34,680
Even though she wasn't a man, that's my point.

1032
01:40:34,680 --> 01:40:38,680
Yeah, what does it matter?

1033
01:40:38,680 --> 01:40:51,680
Yeah, so, question, I think the, from what I understand about the previous panel, was about misinformation, which has a real kind of social effect, or a political effect.

1034
01:40:51,680 --> 01:41:06,680
And in this case, I mean, it might, that would depend upon the context in which the information mattered.

1035
01:41:06,680 --> 01:41:10,680
As such, it would be either in some literal sense misleading or withholding, but it wouldn't necessarily.

1036
01:41:10,680 --> 01:41:20,680
Whether or not it counts as misinformation in the social meaning of that term today, which is really quite important, would depend entirely on the context that day.

1037
01:41:20,680 --> 01:41:30,680
I think it would be misinformation if you claimed that it was a work of nonfiction, but it would not be misinformation if you gave it the label of fiction.

1038
01:41:30,680 --> 01:41:31,680
That's all right.

1039
01:41:31,680 --> 01:41:35,680
Well, everyone, I want to thank you again for an amazing roundtable.

1040
01:41:35,680 --> 01:41:49,680
I want everyone to know that we're returning here tomorrow morning at 11 for the first roundtable is entitled, Our Natural Language Generators for Real, which is about GPT-3 that we spoke of tonight.

1041
01:41:49,680 --> 01:41:56,680
And then we'll end the conference with, is the universe a metaverse, and that begins at 2pm.

1042
01:41:56,680 --> 01:41:59,680
So thank you again, everyone. It was wonderful.

1043
01:41:59,680 --> 01:42:25,680
Thank you.

1044
01:42:29,680 --> 01:42:36,680
Thank you.

