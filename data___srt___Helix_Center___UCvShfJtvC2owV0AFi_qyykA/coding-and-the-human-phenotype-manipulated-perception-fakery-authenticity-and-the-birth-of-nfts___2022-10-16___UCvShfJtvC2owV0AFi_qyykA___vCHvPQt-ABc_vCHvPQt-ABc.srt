1
00:00:00,000 --> 00:00:24,800
Good afternoon, everyone.

2
00:00:24,800 --> 00:00:31,080
I'm Gerald Horowitz. I'm the Associate Director of Helix Center and I apologize for the delay

3
00:00:31,080 --> 00:00:36,480
in our getting started. One of our participants due to a family emergency is unable to make

4
00:00:36,480 --> 00:00:46,680
this talk on misinformation, coding and misinformation, and the birth of NFTs. And I'll say a word

5
00:00:46,680 --> 00:00:51,120
or two about what we're thinking this topic will be about, but the participants are going

6
00:00:51,120 --> 00:00:55,600
to be the ones who really help decide, and you'll decide among yourselves whether they're

7
00:00:55,600 --> 00:01:01,240
telling you the truth or not. Let me say something first about Laura Edelson, who's not here,

8
00:01:01,240 --> 00:01:07,400
unfortunately, but I'll give a brief description. She is a post-doc researcher at NYU with

9
00:01:07,400 --> 00:01:13,280
the Cybersecurity for Democracy Project, which she co-directs with Damon McCoy. There she

10
00:01:13,280 --> 00:01:19,800
leads the observatory and the ob-observer projects, which aim to increase public transparency

11
00:01:19,800 --> 00:01:26,120
of digital advertising, particularly during elections.

12
00:01:26,120 --> 00:01:32,480
We have here with us today, Susanna Martinez-Khande. Is that the way to say it? Isn't that good?

13
00:01:32,480 --> 00:01:38,600
She is an award-winning neuroscientist, author and a professor at the State University of

14
00:01:38,600 --> 00:01:44,760
New York, Downstate Health Sciences University. She is the founder and executive director

15
00:01:44,760 --> 00:01:51,760
of the annual Best Illusion of the Year contest, which inspired her most recent book, Champions

16
00:01:51,760 --> 00:01:57,160
of Illusion, published by Ferro Strauss and Jerome. Her first book, The International Best

17
00:01:57,160 --> 00:02:02,680
Seller, Sleight of Hands, Sleight of Mind, What the Neuroscience of Magic reveals about

18
00:02:02,680 --> 00:02:09,400
our everyday deceptions was published by Holt and won the Prisma Prize for Best Science

19
00:02:09,400 --> 00:02:14,520
Book of the Year. Martinez-Khande is one of the premier science communicators in the United

20
00:02:14,520 --> 00:02:20,560
States and has made television appearances on the National Geographic's channels, redesigned

21
00:02:20,560 --> 00:02:28,560
my brain, discovery channels, head games, the daily planet, PBS' Nova, Star Talk, CBS,

22
00:02:28,560 --> 00:02:35,600
Sunday Morning and the World According to Jeff Goldblum.

23
00:02:35,600 --> 00:02:41,680
Sidney Gess is an assistant professor of politics and public affairs at Princeton University.

24
00:02:41,680 --> 00:02:46,840
His research and teaching interests lie at the intersection of political communication,

25
00:02:46,840 --> 00:02:52,800
public opinion and political behavior. Via a combination of experimental methods, large

26
00:02:52,800 --> 00:02:58,200
datasets, machine learning and innovative measurement, he studies how people choose,

27
00:02:58,200 --> 00:03:03,880
process, spread and respond to information about politics. Recent work investigates the

28
00:03:03,880 --> 00:03:11,320
extent to which online Americans, news habits are polarized. The popular echo chamber hypothesis

29
00:03:11,320 --> 00:03:16,200
patterns in the consumption and spread of online misinformation and the effectiveness

30
00:03:16,200 --> 00:03:22,560
of efforts to counteract misperceptions encountered on social media.

31
00:03:22,560 --> 00:03:30,280
Yotem up here. Is that, did I get to? Yeah, good. He's saying good enough. Thank you.

32
00:03:30,280 --> 00:03:35,480
He's an assistant professor of communication at the University of Buffalo. His work combines

33
00:03:35,480 --> 00:03:43,080
computational methods for text mining, network analysis, experiments and surveys to study

34
00:03:43,080 --> 00:03:48,720
media content and effects in the areas of political science and health communication.

35
00:03:48,720 --> 00:03:55,080
Dr. O'Pyr authored and co-authored more than 300 peer reviewed academic papers published

36
00:03:55,080 --> 00:04:00,360
in journals such as the American Journal of Public Health, Health Security, Tobacco Regulatory

37
00:04:00,360 --> 00:04:06,720
Science, Risk Analysis, plus one Journal of Communication, Communication Research, Public

38
00:04:06,720 --> 00:04:11,480
Understanding of Science, Journal of Public Health at Health Communication, Communications

39
00:04:11,480 --> 00:04:26,000
Methods and Measures and more. So with that, we're going to get started. I'll after I

40
00:04:26,000 --> 00:04:32,080
take my seat. Okay, so I'm interested in getting things underway and I invite any of

41
00:04:32,080 --> 00:04:37,760
the three of you to jump in and express at least some sort of ideas about what you're

42
00:04:37,760 --> 00:04:43,400
seeing, what everyone of us, I think, is seeing in our culture about misinformation, polarization,

43
00:04:43,400 --> 00:04:51,480
etc. And obviously because the conference is on coding, we're interested in sort of how

44
00:04:51,480 --> 00:04:59,320
digitalization of information may be playing a role in this. So who wants to start?

45
00:04:59,320 --> 00:05:06,040
Okay, I'll offer a sort of starting idea and you'll feel free to sort of revise that

46
00:05:06,040 --> 00:05:10,160
or throw it away later, but just to get things started. I think part of what's happening

47
00:05:10,160 --> 00:05:16,440
or what has been happening is that we have incredible digital communication technologies

48
00:05:16,440 --> 00:05:21,680
that have enabled many people who previously were not able to express themselves at the

49
00:05:21,680 --> 00:05:26,000
sort of ease and scale that they can now. And this created a lot of opportunities for

50
00:05:26,000 --> 00:05:32,240
different people, different perspectives in different groups to express themselves in

51
00:05:32,240 --> 00:05:38,000
society and also to engage with society and participate in society in new ways that I

52
00:05:38,000 --> 00:05:45,600
think have sort of cut against the usual kinds of inequalities. However, what that also means,

53
00:05:45,600 --> 00:05:49,400
another way of saying that is that sort of traditional gatekeepers, including gatekeepers

54
00:05:49,400 --> 00:05:56,480
of information, have lost their kind of received power. And so kind of a flip side of that

55
00:05:56,480 --> 00:06:03,240
potentially is that gatekeepers who exist to kind of maintain hierarchies but also to

56
00:06:03,240 --> 00:06:08,480
vet information for better or for worse, have lost some other additional power. And so one

57
00:06:08,480 --> 00:06:15,880
of the big questions today is sort of who or what, including technologies, is replacing

58
00:06:15,880 --> 00:06:23,720
these kind of standard gatekeepers that have been in place for, let's say, 100 years.

59
00:06:23,720 --> 00:06:27,080
And how do we as society sort of adapt to this new reality?

60
00:06:27,080 --> 00:06:38,560
And I can add to that from a neuroscientific perspective, our brains are not wired to

61
00:06:38,560 --> 00:06:44,080
multitask. It's the opposite. We're wired to pay attention to one thing and one thing

62
00:06:44,080 --> 00:06:50,520
only and suppress everything else. What happens is that when we are pulled in all directions,

63
00:06:50,520 --> 00:06:57,760
we cannot really pay attention to more than one thing at once and arrive to any type of

64
00:06:57,760 --> 00:07:03,720
quality judgment or performance. This is something that happens in magic shows, actually magicians

65
00:07:03,720 --> 00:07:09,200
get us to multitask in a magic show, they split our attention, and that's how they get

66
00:07:09,200 --> 00:07:17,480
away with magical murder. What happens this day is that with all this digital content

67
00:07:17,480 --> 00:07:24,520
and social media, this is really pushing our capabilities to pay attention to the limit.

68
00:07:24,520 --> 00:07:35,280
And so we are, it's very taxing to be able to determine what actual data, what's misinformation,

69
00:07:35,280 --> 00:07:42,440
because everything seems to have the same priority and we are not focusing and analyzing

70
00:07:42,440 --> 00:07:47,720
in any depth for any length of time.

71
00:07:47,720 --> 00:07:54,480
I mean one of the big questions that I think are on my mind recently, is are we really in

72
00:07:54,480 --> 00:07:59,200
a new era of post truth? I mean, because you hear it everywhere, right? That we move to

73
00:07:59,200 --> 00:08:05,440
a different time in human history, but it's not clear to me based on the empirical work

74
00:08:05,440 --> 00:08:14,560
that we've been conducting and based on reviews of previous eras that really crossed a rubric

75
00:08:14,560 --> 00:08:22,360
on to a different time in terms of relationship between humans and truth or humans and information.

76
00:08:22,360 --> 00:08:29,640
I'm wondering in recent years why is everybody so preoccupied with misinformation now? It

77
00:08:29,640 --> 00:08:35,360
wasn't like that 10 years ago, right? When I started, I'm relatively, this guy is new

78
00:08:35,360 --> 00:08:42,120
in the area, but when I started in 2010 to look into misinformation was kind of a niche

79
00:08:42,120 --> 00:08:50,560
topic in social sciences. I wonder if something really changed in the way we communicate with

80
00:08:50,560 --> 00:08:57,480
one another or some external events happen that kind of pushed us to look for explanations.

81
00:08:57,480 --> 00:09:02,320
In other words, are we sitting here because something really changed in how we use media

82
00:09:02,320 --> 00:09:07,520
or are we sitting here because we're still trying to explain to ourselves the 2016 election?

83
00:09:07,520 --> 00:09:12,040
Yeah, what is going on? Well, this reminds me of, I think there's sort of two general

84
00:09:12,040 --> 00:09:19,320
classes of commentators on what's going on with us. So the one class says, oh my God,

85
00:09:19,320 --> 00:09:24,520
this is completely different. We're really in trouble. And then I'm sure you've all seen

86
00:09:24,520 --> 00:09:31,480
other articles that suggested, oh no, back in 1820, there was just as much politicization.

87
00:09:31,480 --> 00:09:38,320
Maybe there was more people were fist fighting in the Capitol building, et cetera. If it's

88
00:09:38,320 --> 00:09:43,960
true that there was an earlier period where there was more of this sort of fermentation

89
00:09:43,960 --> 00:09:50,760
of politicization, can we see anything in common then from now? Is it possible to draw a line

90
00:09:50,760 --> 00:10:02,080
between those two processes? Yeah, I mean, yeah, I know it could please go ahead.

91
00:10:02,080 --> 00:10:07,560
I think they work fundamentally the same people that we were a couple of hundred years ago

92
00:10:07,560 --> 00:10:15,160
and even much earlier. And we are, actually we have the capability and even you could

93
00:10:15,160 --> 00:10:23,480
save the drive to lie and to deceive the brain is the great story better. And we try to make

94
00:10:23,480 --> 00:10:29,800
sense of reality by telling stories about what's happening to others as well as to ourselves

95
00:10:29,800 --> 00:10:36,280
and that we make incorrect connections between cause and effect that has always happened.

96
00:10:36,280 --> 00:10:46,000
And even if something as easy as trying to trick somebody into thinking that we're interested

97
00:10:46,000 --> 00:10:53,240
in something that we're not or the other way around, we are able as primates, we are able

98
00:10:53,240 --> 00:10:59,240
to pay attention to something that we're not looking at for will intend to deceive for

99
00:10:59,240 --> 00:11:04,720
the primates to do this as well. But not every species is able to do it to dissociate what

100
00:11:04,720 --> 00:11:10,920
they're looking at from the place that they're actually paying attention to. So we have these

101
00:11:10,920 --> 00:11:17,400
mechanisms that are inherent to our brains that we have always lied to one another. I

102
00:11:17,400 --> 00:11:25,040
think what's different now is the tools that we have accessible to us. And I think that

103
00:11:25,040 --> 00:11:32,640
it used to be much easier if you don't know is this true or is this a false hood and you

104
00:11:32,640 --> 00:11:39,720
go and you try different sources and you can arrive to the veracity or particular issue,

105
00:11:39,720 --> 00:11:49,080
I think much more in a much more reliable way. I mean, even today you see an image, you see

106
00:11:49,080 --> 00:11:55,800
a video or an audio record. You cannot really tell if this is manipulated or not. This did

107
00:11:55,800 --> 00:11:58,160
not used to be the case.

108
00:11:58,160 --> 00:12:06,800
And I guess misinformation has always been there. It's not a new phenomenon. The only

109
00:12:06,800 --> 00:12:16,560
thing that seems to me is new is more people have access to internet and so on, which allows

110
00:12:16,560 --> 00:12:23,680
them to express their opinions, which before they had no access. So if, let's say, you

111
00:12:23,680 --> 00:12:30,800
were organizing a coup in a foreign country, you gave a story about what was happening

112
00:12:30,800 --> 00:12:36,800
that was in the papers and everybody accepted and nobody said, and if you knew information

113
00:12:36,800 --> 00:12:42,360
that countered the motives that were expressed, you couldn't make it public because there

114
00:12:42,360 --> 00:12:51,600
was no outlet. Now you have questions about, I don't know, whatever political issue, you

115
00:12:51,600 --> 00:12:57,040
immediately put it on the internet and you have an opinion and then other people follow

116
00:12:57,040 --> 00:13:04,440
you. And of course, part of the problem is the more you claim to have some kind of conspiracy

117
00:13:04,440 --> 00:13:11,040
behind it, the more excited you make people get and the more you have tension.

118
00:13:11,040 --> 00:13:15,960
Yeah, I think, I mean, one way of kind of restating a part of what you said is that

119
00:13:15,960 --> 00:13:22,280
propaganda has always been with us. It's traditionally been the domain of elites and

120
00:13:22,280 --> 00:13:27,760
today, the ability to produce propaganda, like a lot of things, has been democratized.

121
00:13:27,760 --> 00:13:32,840
So now anyone can sort of make up their own stories and they can have it disseminated

122
00:13:32,840 --> 00:13:39,000
instantaneously at great and unprecedented scale. So that's one thing that has changed.

123
00:13:39,000 --> 00:13:43,400
Right. And there are positive aspects to it because, like you said, there used to be

124
00:13:43,400 --> 00:13:50,440
like a one version of the truth and there was no questioning and that I'm thinking,

125
00:13:50,440 --> 00:13:55,640
well, something that I'm afraid about is the flu of 1918, what we call the Spanish flu,

126
00:13:55,640 --> 00:14:00,280
when it turns out it did not originate in Spain, actually it's still debated, were it

127
00:14:00,280 --> 00:14:10,760
actually? China, right. Sorry. Start a new rumor. Exactly. So, but the point is that

128
00:14:10,760 --> 00:14:16,400
apparently it is the case that the Spanish newspapers were the ones that were not lying

129
00:14:16,400 --> 00:14:22,280
about the numbers, other places were suppressing that information. So that's how it became

130
00:14:22,280 --> 00:14:28,680
to be known as the Spanish flu. But so that was like, I kind of like a worldwide misinformation

131
00:14:28,680 --> 00:14:35,360
back then, 100 years ago. Right. There may have been this sort of also this movement

132
00:14:35,360 --> 00:14:41,880
going back to their early 19th century where one of the pieces of propaganda was that our

133
00:14:41,880 --> 00:14:48,040
elected officials were dependable, reliable, they were reliable elitists and they were

134
00:14:48,040 --> 00:14:53,520
going to tell you the truth. And that was a useful bit of propaganda to a degree. I mean,

135
00:14:53,520 --> 00:14:57,480
you know, I don't want to sound too cynical about it, but that was a story and narrative

136
00:14:57,480 --> 00:15:02,480
that many people in the United States accepted true or false, but they accepted it, but it

137
00:15:02,480 --> 00:15:07,160
may have done some good because people said, well, I can look to this authority to establish

138
00:15:07,160 --> 00:15:14,000
the truth or falseness. Now we have politicians in this article this morning in New York Times

139
00:15:14,000 --> 00:15:20,040
that shows the number of Republican candidates who deny the results of the 2020 election.

140
00:15:20,040 --> 00:15:29,120
The majority of them deny the results. We're, it's interesting, fake news was is the claim

141
00:15:29,120 --> 00:15:34,200
of the people who've been creating a lot of the fake news, right? So they're not only

142
00:15:34,200 --> 00:15:39,640
producing fake news, but they're undermining everyone's faith in any authority to set

143
00:15:39,640 --> 00:15:44,880
the strength. The term fake news has come to me in something different that it used to

144
00:15:44,880 --> 00:15:51,240
mean some years ago, but it is true. I mean, something that it is not just trusting politicians

145
00:15:51,240 --> 00:15:59,960
and trusting governments, but I believe in trusting in scientists actually, we just published

146
00:15:59,960 --> 00:16:10,840
a paper in which we found in an international sample that the willingness to be vaccinated

147
00:16:10,840 --> 00:16:18,920
for COVID-19, the parameter that was mostly related to with one's willingness to be vaccinated

148
00:16:18,920 --> 00:16:26,920
was trust in scientists. And when this trust in scientists is not there anymore, then you

149
00:16:26,920 --> 00:16:32,680
have people who are thinking, well, why should I be vaccinated? I can do my own research.

150
00:16:32,680 --> 00:16:36,560
And I think that that's what we're seeing also that anybody can do their own research.

151
00:16:36,560 --> 00:16:41,160
And everybody can be an expert. There is no separation between expert opinion and the

152
00:16:41,160 --> 00:16:46,520
opinion that basically anybody could have in any field.

153
00:16:46,520 --> 00:16:53,320
I think if there's one thing, political scientists in my field of political science,

154
00:16:53,320 --> 00:16:59,520
it's the one thing that we have found time and time again is that trust is this core

155
00:16:59,520 --> 00:17:06,040
ingredient in making democracy work. And it's trust in institutions, trust in scientists.

156
00:17:06,040 --> 00:17:14,400
I would add maybe trust in the news media, trust in some basic quality sources of information

157
00:17:14,400 --> 00:17:21,880
about society, just trust is the thing that makes things run more smoothly. And when you

158
00:17:21,880 --> 00:17:29,680
lose that, things can come apart. And so this conversation is making me ask, on the one

159
00:17:29,680 --> 00:17:37,160
hand, trust is good. It has a lot of beneficial properties. But does trust also come with a

160
00:17:37,160 --> 00:17:42,440
cost? So what are we giving up in order to maintain high levels of trust? I think you

161
00:17:42,440 --> 00:17:47,800
suggested one, which is that if we sit on one extreme blind trust, you could blindly

162
00:17:47,800 --> 00:17:53,960
trust the government, you could blindly trust what the news tells you. And we've just seen

163
00:17:53,960 --> 00:18:02,520
through history that that too can have its pitfalls. And so maybe just I don't really

164
00:18:02,520 --> 00:18:07,840
have an answer to this, just to pose it as a dilemma. But I might suggest that there

165
00:18:07,840 --> 00:18:13,320
are perhaps different sort of equilibria that might work. So you could have a high trust

166
00:18:13,320 --> 00:18:21,680
equilibrium where there are perhaps more established gatekeepers, there are more transparent institutions

167
00:18:21,680 --> 00:18:26,920
and people understand how that works. And then we're in the process of moving to another

168
00:18:26,920 --> 00:18:32,280
equilibrium and we haven't quite figured out what that looks like, but it's a destabilizing

169
00:18:32,280 --> 00:18:33,280
period.

170
00:18:33,280 --> 00:18:38,080
And I think that perhaps, I mean, we're trying, I feel like the conversation that we're having

171
00:18:38,080 --> 00:18:46,480
is that it's top down. What can we do to achieve a balance or what should we do that so that

172
00:18:46,480 --> 00:18:54,320
society has a diversity of sources of information, but maybe not too much diversity or maybe

173
00:18:54,320 --> 00:19:00,000
another everybody's opinion should be way the same. I think that perhaps it's a, I don't

174
00:19:00,000 --> 00:19:07,280
have an answer, but maybe we can, there are certainly positives in having accessibility,

175
00:19:07,280 --> 00:19:18,440
having access to all of these kinds of forms of information, even fake news. But what we

176
00:19:18,440 --> 00:19:24,360
are lacking is education. And I think that what we, some of the things that we need to

177
00:19:24,360 --> 00:19:31,080
as a society is to empower individuals and that in the educational systems, I feel like

178
00:19:31,080 --> 00:19:38,120
even today children are not giving the tools to develop critical thinking to be able to

179
00:19:38,120 --> 00:19:44,760
face and evaluate all of these alternative and competing sources of information. So that's

180
00:19:44,760 --> 00:19:47,360
a serious lack.

181
00:19:47,360 --> 00:19:55,000
I want to build on something that Andy said, I think it's a healthy thing to be skeptical.

182
00:19:55,000 --> 00:20:00,080
We are scientists. That's what we do. That's our job, right? I mean, our job is to be skeptical

183
00:20:00,080 --> 00:20:05,720
to a healthy degree to question knowledge even after it's been established and so on. But

184
00:20:05,720 --> 00:20:11,160
what we've, what we're seeing in recent years is kind of a shift that I see as dangerous

185
00:20:11,160 --> 00:20:18,280
from skepticism to cynicism. And I think it has a lot to do with this erosion interest

186
00:20:18,280 --> 00:20:26,160
in institutions of knowledge that's actually predates the internet and social media. I

187
00:20:26,160 --> 00:20:30,240
mean, you can choose a lot of random points to kind of start the discussion of it, but

188
00:20:30,240 --> 00:20:41,640
I'll choose for now the early 90s where conservative talk radio shows like Rush Limbaugh and a

189
00:20:41,640 --> 00:20:50,800
few years later Fox News were launching these campaigns against the institution of knowledge

190
00:20:50,800 --> 00:20:56,960
if it's the mainstream media, if it's the scientific community, basically a populist

191
00:20:56,960 --> 00:21:02,240
argument that elites are lying to you and they're not working for you. I think this

192
00:21:02,240 --> 00:21:09,360
is where you start to see the beginning of the process that we're in the middle of right

193
00:21:09,360 --> 00:21:16,600
now. In the last six years or so, I believe that the Trump presidency pushed it even

194
00:21:16,600 --> 00:21:26,160
farther by not so much spreading specific lies, but challenging the epistemology of

195
00:21:26,160 --> 00:21:30,800
knowledge. I mean, what does it even mean to know something? Who should we trust and

196
00:21:30,800 --> 00:21:35,320
who shouldn't we trust? If you look at Trump's making arguments, for example, and I'm sorry

197
00:21:35,320 --> 00:21:41,000
that I'm making it about Trump a bit too tempting, but all those, all those sayings

198
00:21:41,000 --> 00:21:45,440
that people are saying, right? I mean, Trump says people are saying that the elections

199
00:21:45,440 --> 00:21:51,840
were stolen and now we should accept that as a source of knowledge. The whole perception

200
00:21:51,840 --> 00:21:57,520
that there is no reliable source of knowledge, that everything is subjective and everything

201
00:21:57,520 --> 00:22:05,360
is an attempt to push in the agenda of sorts, is I think kind of the cause of where we are

202
00:22:05,360 --> 00:22:06,360
right now.

203
00:22:06,360 --> 00:22:13,360
In our first panel this morning, Jorgi Bizaki was mentioning how so many of the facts that

204
00:22:13,360 --> 00:22:19,840
we think we know that we ascribe to, we don't know firsthand. We get them from our parents

205
00:22:19,840 --> 00:22:24,600
or from other authorities and we don't have the time to go and validate everyone, every

206
00:22:24,600 --> 00:22:30,080
one of them. If the world's in a state of chaos and it's making everyone anxious and

207
00:22:30,080 --> 00:22:34,960
I think rightfully it's making many people anxious, one thing to say is, well, one thing

208
00:22:34,960 --> 00:22:39,200
to hope for is that you have a great leader who's going to help communicate to you why

209
00:22:39,200 --> 00:22:45,920
you need to stay calm and carry on. And the other thing that can happen is leaders could

210
00:22:45,920 --> 00:22:50,680
say, no, you're right to be anxious, the elites are screwing you and there's a conspiracy

211
00:22:50,680 --> 00:23:00,080
going on. And weirdly enough, that conspiracy thinking, I think puts some of their minds

212
00:23:00,080 --> 00:23:05,280
partially at ease because I think, well, now I have an explanation for why this is going

213
00:23:05,280 --> 00:23:10,360
on rather than having anything more meaningful. I just want to say one other thing, I'm a

214
00:23:10,360 --> 00:23:15,880
physician and I treat a lot of, I'm a psychiatrist and many patients will come to me and complain

215
00:23:15,880 --> 00:23:20,360
about their doctors, their non psychiatric doctors, their primary care doctor or their

216
00:23:20,360 --> 00:23:25,000
cardiologist, whatever. Can you believe he didn't do this or say this or do this test

217
00:23:25,000 --> 00:23:32,080
or she didn't do this or? And I'm skeptical about the quality that some of these patients

218
00:23:32,080 --> 00:23:36,520
may get from time to time. Some of it's excellent, but some of it's not so good. But I always

219
00:23:36,520 --> 00:23:41,480
say to these patients, like, listen, if you want to go with the numbers, go toward the

220
00:23:41,480 --> 00:23:46,680
doctor and do what they tell you, okay, because, you know, it's not perfect, but it's better

221
00:23:46,680 --> 00:23:50,600
than you staying away and thinking you're going to figure this out on your own, right?

222
00:23:50,600 --> 00:23:56,920
So maybe that's a weak argument in favor of trusting authority. You say, look, you're

223
00:23:56,920 --> 00:24:02,560
better off of just imagining them to be honest and fair to you. It's pretty difficult with

224
00:24:02,560 --> 00:24:05,040
what's being advertised these days, right?

225
00:24:05,040 --> 00:24:09,720
And it's true that we need to develop some sort of algorithms for what you should trust

226
00:24:09,720 --> 00:24:15,640
and what you should. Because nobody has the time or the resources to question everything.

227
00:24:15,640 --> 00:24:24,680
And it seems that from regular skepticism about the particular sense of claims, we have a

228
00:24:24,680 --> 00:24:31,560
right-to-situation in which everything is set for questioning. It is different to question.

229
00:24:31,560 --> 00:24:39,560
I believe two vaccines cause autism, right? That has been debunked many times. But from

230
00:24:39,560 --> 00:24:45,480
there, we shouldn't, you know, go on and question whether the Earth maybe is flat and that there's

231
00:24:45,480 --> 00:24:53,080
actually a flat Earth society. I don't know how serious they are. But yeah, I think that

232
00:24:53,080 --> 00:25:02,120
goes again, I think, to the issue of like the expertise. Do we even rely or do we even trust

233
00:25:02,120 --> 00:25:08,120
to recognize that there is an actual thing as an expertise and who gets to be an expert

234
00:25:08,120 --> 00:25:14,080
and who gets to have an opinion that should be valued?

235
00:25:14,080 --> 00:25:20,000
There was this movement that's going on now for many years now. I think it's a high-frequency

236
00:25:20,000 --> 00:25:26,280
trading in the stock market, right? Where these transactions are occurring way faster than

237
00:25:26,280 --> 00:25:31,200
human beings can involve. And I mention that because apparently it does have an impact

238
00:25:31,200 --> 00:25:38,160
that the speed of these transactions has an impact on the dynamics of the market. So I'm

239
00:25:38,160 --> 00:25:43,440
wondering about, because this is a talk after all about coding and digitalization and such,

240
00:25:43,440 --> 00:25:49,760
we are getting a much faster feedback by the social media outlets that are tailoring the

241
00:25:49,760 --> 00:25:56,200
news items we get in a much higher rate. I wonder what you all think about that as a

242
00:25:56,200 --> 00:25:59,200
phenomenon.

243
00:25:59,200 --> 00:26:05,280
I mean, I think you're absolutely right that the velocity, the sheer velocity of information

244
00:26:05,280 --> 00:26:12,320
is increasing. And, you know, I think the most common response to this that I've seen

245
00:26:12,320 --> 00:26:19,040
is that people become overwhelmed and just tune out. So news avoidance is now like a

246
00:26:19,040 --> 00:26:23,120
term for it. There's a phenomenon of news avoidance. People just turn it off. They pay

247
00:26:23,120 --> 00:26:29,040
less attention. The flip side of that is people who actually do want to inject the information

248
00:26:29,040 --> 00:26:35,040
into their veins can do so at the most extreme rate that was never before possible. And so

249
00:26:35,040 --> 00:26:42,240
you have super engaged people who want to consume every little bit of content and are

250
00:26:42,240 --> 00:26:49,080
sort of mainlining like Twitter or TikTok or whatever it is. And then you have everyone

251
00:26:49,080 --> 00:26:54,000
else who's sort of not as engaged. And, you know, that creates a whole set of new problems

252
00:26:54,000 --> 00:26:59,280
for itself, because then you have, imagine the feedback, you have, you know, you have

253
00:26:59,280 --> 00:27:04,040
content producers, you have information sources that are now catering, whether they're fully

254
00:27:04,040 --> 00:27:09,000
aware of it or not to these like most hyper engaged consumers whose interests may or may

255
00:27:09,000 --> 00:27:14,880
not reflect those of the rest of society. And we don't consume all information equally

256
00:27:14,880 --> 00:27:22,040
because we are all subjective to cognitive biases, you know, confirmation bias. We hear

257
00:27:22,040 --> 00:27:27,560
what we want to hear and that this piece of information that is supporting my prior view,

258
00:27:27,560 --> 00:27:37,160
I'm going to really go deep into it and I'm going to process it in high detail with high

259
00:27:37,160 --> 00:27:41,920
focus and that this other piece of information that goes against my views, I'm just going

260
00:27:41,920 --> 00:27:49,520
to ignore, keep scrolling. So, and I think that the fact that we have so many source

261
00:27:49,520 --> 00:27:56,400
information coming at us with high speed and high amount, this only serves to exacerbate

262
00:27:56,400 --> 00:28:01,920
the cognitive biases that we already have because we're overwhelmed all the time and

263
00:28:01,920 --> 00:28:08,560
just defaulting to these ingrained ways of thinking. So, I think it is harder to change

264
00:28:08,560 --> 00:28:11,320
minds today than it used to be.

265
00:28:11,320 --> 00:28:16,320
I know in magic, one of the things they'll do is they'll get you to look here and you

266
00:28:16,320 --> 00:28:22,080
look there and they're getting you to confirm some false belief quickly, right? One, two,

267
00:28:22,080 --> 00:28:26,480
three, and then the trick goes, that takes place, right? And that's a little bit of what's

268
00:28:26,480 --> 00:28:32,800
going on with this, I think, right? There's all this sort of false and it's at least emotionally

269
00:28:32,800 --> 00:28:34,800
stirs you up.

270
00:28:34,800 --> 00:28:40,160
Yeah, absolutely. There's probably a paper about what the psychology and neuroscience

271
00:28:40,160 --> 00:28:45,400
of magic tell us about misinformation because there are so many parallels.

272
00:28:45,400 --> 00:28:52,440
I think the word emotional is key here because when you talk about polarization and the way

273
00:28:52,440 --> 00:28:57,280
the algorithms are maybe pushing us towards our own views, it's important to remember

274
00:28:57,280 --> 00:29:03,360
that most people don't care about politics or don't understand politics at all. I feel

275
00:29:03,360 --> 00:29:09,200
like what we do see right now is a lot of emotionality, a lot of affective polarization,

276
00:29:09,200 --> 00:29:14,040
right? I mean, I hate conservatives or I hate liberals. I mean, I just can't stand them

277
00:29:14,040 --> 00:29:24,000
and what they represent, this kind of sentiment is more driving people than actual understanding

278
00:29:24,000 --> 00:29:30,840
of policy or, you know, deep ideological views. Most people don't care about politics at all

279
00:29:30,840 --> 00:29:37,400
or I understand very little about it. But it seems like more and more we are occupied

280
00:29:37,400 --> 00:29:44,360
with feeling about the other side. That's what I see. I don't think we are more politically

281
00:29:44,360 --> 00:29:46,520
sophisticated now than 10 years ago.

282
00:29:46,520 --> 00:29:48,360
More and more we are.

283
00:29:48,360 --> 00:29:49,360
Say again, sorry?

284
00:29:49,360 --> 00:29:55,800
You said it's not so much that we are actually interested but we are more and more kind of

285
00:29:55,800 --> 00:29:58,000
preoccupied by it.

286
00:29:58,000 --> 00:30:07,360
I think we are more and more identifying emotionally with one side of the cultural divide that

287
00:30:07,360 --> 00:30:13,000
we have in this country and the more we see ourselves as part of one group and not the

288
00:30:13,000 --> 00:30:19,840
other, the more we seem to hate the other group more than we care about the actual policies

289
00:30:19,840 --> 00:30:22,400
that they are trying to promote.

290
00:30:22,400 --> 00:30:26,800
I think one example of how it's not always just about politics either. I mean, the way

291
00:30:26,800 --> 00:30:33,600
we might or might not be manipulated. One is I'm an infrequent user of Instagram and

292
00:30:33,600 --> 00:30:38,200
I don't use TikTok but I understand Instagram says hey, we have to get more of what TikTok

293
00:30:38,200 --> 00:30:40,400
is doing. So we are going to show little videos.

294
00:30:40,400 --> 00:30:45,280
So they show those ridiculously appealing videos of dogs and cats.

295
00:30:45,280 --> 00:30:50,760
And the next thing I'm going with, before I'm able to break away and that's just plucking

296
00:30:50,760 --> 00:30:55,840
in my emotions. There's nothing typically political. It was the one with the bulldog

297
00:30:55,840 --> 00:31:01,880
with the Trump hat on but aside from that it was just emotionally engaging and they are

298
00:31:01,880 --> 00:31:06,760
very good at it. I was like Doritos. You want to have another one and another one and you

299
00:31:06,760 --> 00:31:08,040
keep going.

300
00:31:08,040 --> 00:31:12,720
But the other topic which is again not really political, maybe it is a little bit, but it's

301
00:31:12,720 --> 00:31:18,320
this article today in the paper about this new finding about Chaucer. I mentioned this

302
00:31:18,320 --> 00:31:27,920
earlier today. So Chaucer has been accused by certain literary experts of having raped

303
00:31:27,920 --> 00:31:34,240
somebody. And in any way being a misogynist and a lot of feminist literary scholars have

304
00:31:34,240 --> 00:31:41,680
been applying feminist analysis to Chaucer as the father of English literature so to speak.

305
00:31:41,680 --> 00:31:45,920
Anyway, someone came up with this wonderful, it seemed to be wonderful and it may not be

306
00:31:45,920 --> 00:31:51,400
because we have to be checked. But the interpretation of these legal documents may mean that it

307
00:31:51,400 --> 00:32:00,600
wasn't the word, it wasn't rape. Anyways, this is a special term they use. They seem

308
00:32:00,600 --> 00:32:09,080
to demonstrate that this was basically, the woman was basically defending herself against

309
00:32:09,080 --> 00:32:15,280
the accusations she left her previous employer too quickly and was taking to Chaucer's and

310
00:32:15,280 --> 00:32:19,720
both of them were in defense of this lawsuit. Anyway, I'm sorry if this is confusing to

311
00:32:19,720 --> 00:32:25,520
her. My point is it may have changed the worldview of many of these people. So I'm thinking,

312
00:32:25,520 --> 00:32:30,640
gee, I wonder how some of these feminist scholars are going to react. And I was really pleased

313
00:32:30,640 --> 00:32:35,480
to see that the ones that were quoted in the New York Times anyway had a really good and

314
00:32:35,480 --> 00:32:39,800
measured response to it. You know, like, well, this doesn't mean that everything we do about

315
00:32:39,800 --> 00:32:45,160
feminist scholarship is now invalidated whether or not this is, whatever this, however this

316
00:32:45,160 --> 00:32:50,280
gets determined. But I knew I was thinking to myself, people are invested in a particular

317
00:32:50,280 --> 00:32:55,880
view, you know, and they almost resist the idea of changing. Like, they will disbelieve

318
00:32:55,880 --> 00:32:59,520
they're a little bit more skeptical of the new finding, let's say, because they want

319
00:32:59,520 --> 00:33:04,480
to hold on to their view of the way things are. And I think that's fascinating because

320
00:33:04,480 --> 00:33:09,680
that's, again, it's not exactly a political case.

321
00:33:09,680 --> 00:33:11,640
Isn't that rational that what you just described?

322
00:33:11,640 --> 00:33:16,480
How do you mean? I mean, like, I've lived, you know, I've lived a number of years, like,

323
00:33:16,480 --> 00:33:20,400
I've educated myself about a topic and I've like developed what I consider to be informed

324
00:33:20,400 --> 00:33:25,120
views about things. Find counter a piece of information that like, maybe challenges some

325
00:33:25,120 --> 00:33:29,440
part of my existing belief structure, like, sure, like, you know, I might be skeptical

326
00:33:29,440 --> 00:33:34,280
of it because it contradicts, like, it contradicts my worldview, which I've rationally, you know,

327
00:33:34,280 --> 00:33:39,880
constructed, of course. But like, it would be very strange from a rational perspective

328
00:33:39,880 --> 00:33:43,800
I were doing counter one piece of information that contradicts what I think and then so totally

329
00:33:43,800 --> 00:33:45,800
changed my opinion, right?

330
00:33:45,800 --> 00:33:51,240
But wouldn't you take pride in thinking that you're open-minded to changing your point

331
00:33:51,240 --> 00:33:56,800
of view? And whether I've changed it, you might say, oh, good.

332
00:33:56,800 --> 00:34:01,720
Yeah, yeah, yeah, but I think open-mindedness doesn't necessarily mean that you are just

333
00:34:01,720 --> 00:34:05,200
like, your opinions are being swayed by every piece of information that you encounter.

334
00:34:05,200 --> 00:34:10,280
And I think sometimes that's the idea that people have when the term sort of open-minded

335
00:34:10,280 --> 00:34:14,520
is used. And I think I actually think that would be irrational and totally unworkable

336
00:34:14,520 --> 00:34:18,960
if people went around the world just totally changing their views based on the last piece

337
00:34:18,960 --> 00:34:24,000
of information that they encounter. I'm caricaturing what you just said, but we have people here

338
00:34:24,000 --> 00:34:28,240
with more expertise who could probably speak to what I'm trying to say. But I guess, you

339
00:34:28,240 --> 00:34:31,080
know, often these kinds of patterns where people say, like, oh, you're resisting this

340
00:34:31,080 --> 00:34:35,840
information. Well, yeah, I mean, of course, people are resisting information that like

341
00:34:35,840 --> 00:34:40,240
challenges their worldview. Like, why would we expect anything otherwise? Scientists do

342
00:34:40,240 --> 00:34:41,800
that all the time.

343
00:34:41,800 --> 00:34:52,400
I guess I'm thinking there's a value in believing in your intellectual indifference to the outcome.

344
00:34:52,400 --> 00:34:57,320
I'm not saying you are going to do that. You're a human being and that's natural, it is natural.

345
00:34:57,320 --> 00:35:02,600
But if you say about people, oh, it's only natural, you're going to be in a camp. That's

346
00:35:02,600 --> 00:35:09,960
the camp you're in. And if you say, well, there's a real value in ascribing to some impartiality.

347
00:35:09,960 --> 00:35:14,840
And I think that we've lost a little bit of that, that there's... Trump made some comments

348
00:35:14,840 --> 00:35:19,000
some time ago that, oh, of course, someone made that decision because they're a democratic

349
00:35:19,000 --> 00:35:26,440
judge. And Justice Roberts said, nothing like democratic judges and Republican judges.

350
00:35:26,440 --> 00:35:33,080
Well, I think this is fascinating because I have to say, I don't know anyone in my circle

351
00:35:33,080 --> 00:35:38,400
who, most of those people do not like Trump whatsoever. I think they all go, yeah, he's

352
00:35:38,400 --> 00:35:46,040
right. Okay. Now, see, to me, that comment of Roberts was prescriptive, not so much descriptive.

353
00:35:46,040 --> 00:35:51,960
Like, it's sort of, yes, we should be able to believe in the indifference of our jurists

354
00:35:51,960 --> 00:35:57,520
because that's really the ideal. You say, well, in a real world, it doesn't work that way.

355
00:35:57,520 --> 00:36:00,760
You're being a fool. You're being naive. But you see, instead of saying, oh, good for

356
00:36:00,760 --> 00:36:05,240
you for supporting the idea of impartiality, you're sort of looked at like you're some

357
00:36:05,240 --> 00:36:12,800
kind of knife, right? I think that's unfortunate. I mean, why not believe impartiality as an

358
00:36:12,800 --> 00:36:13,800
ideal?

359
00:36:13,800 --> 00:36:18,640
Well, I think as long as it's clear that this is aspirational rather than descriptive.

360
00:36:18,640 --> 00:36:25,640
I think impartiality is good, but it depends on what it is about. There are certain things

361
00:36:25,640 --> 00:36:32,400
that you can't be impartial about. So if they told you the best treatment for depression

362
00:36:32,400 --> 00:36:42,560
is to hang yourself, you can't be impartial about such a thing. But I think part of the

363
00:36:42,560 --> 00:36:52,000
issue is to be somewhat somewhat, it's said that skeptical and have a certain more balanced

364
00:36:52,000 --> 00:36:58,560
view is that it helps with anxiety. You can see that in your office, if you have a couple

365
00:36:58,560 --> 00:37:04,200
and one of them doesn't trust the other one, the lack of trust becomes a major preoccupation

366
00:37:04,200 --> 00:37:08,560
going around and around. Next thing you know, they're checking each other's cell phones,

367
00:37:08,560 --> 00:37:14,680
they're doing this. So trust is very important and I think what's happening with all this

368
00:37:14,680 --> 00:37:21,680
misinformation is that you end up diminishing people's trust and therefore you get the kind

369
00:37:21,680 --> 00:37:26,840
of agitation that we've seen, especially in this country a couple years ago where everybody

370
00:37:26,840 --> 00:37:31,880
is so agitated, you have patients who are waking up at two o'clock, three o'clock, four o'clock

371
00:37:31,880 --> 00:37:36,440
in the morning. I say patience because that's my sorts of information, but I'm sure some

372
00:37:36,440 --> 00:37:46,200
of my friends were doing that to check the news. And so that kind of thing is psychologically

373
00:37:46,200 --> 00:37:56,480
damaging. So there is a limit to how much misinformation can actually affect people's

374
00:37:56,480 --> 00:38:03,680
mental health and I think it does and to develop skepticism about all the information is a

375
00:38:03,680 --> 00:38:09,360
positive look. I think you're both getting on something important in terms of you mentioned

376
00:38:09,360 --> 00:38:15,280
something about presenting a balanced view and what is a balanced view because hanging

377
00:38:15,280 --> 00:38:20,240
yourself is not a good treatment for depression and you shouldn't be given the same way as

378
00:38:20,240 --> 00:38:31,600
some actual forms of therapy. But this is something that the news media has struggled

379
00:38:31,600 --> 00:38:39,440
with and that leading to the Trump election used to be in a newspaper recording that a

380
00:38:39,440 --> 00:38:45,840
new person two sides of an issue and that's supposed to be balanced. But I think that

381
00:38:45,840 --> 00:38:51,280
reports today they have come to still struggling with this but they're coming to the realization

382
00:38:51,280 --> 00:38:58,320
that I just giving equal time and equal work come to two issues. That's not necessarily

383
00:38:58,320 --> 00:39:08,480
a balanced view or if you're talking about having a TV show with interviews about a panel

384
00:39:08,480 --> 00:39:16,200
about climate change and you have a climate scientist and a climate denier that's not

385
00:39:16,200 --> 00:39:22,840
a balanced view because in one case you have the exception view of a specific issue and

386
00:39:22,840 --> 00:39:30,080
the other case you have somebody who represents thousands of climate scientists so it's not

387
00:39:30,080 --> 00:39:36,840
that's not balanced that would be in balance but I do not think that we have arrived to

388
00:39:36,840 --> 00:39:45,160
the right formula or 100% different size of an issue with appropriate weight.

389
00:39:45,160 --> 00:39:54,760
Well you and Tim you mentioned the cynicism before and you know for me the issue of cynicism

390
00:39:54,760 --> 00:40:04,160
as I was saying in my thesis just a moment ago that if people are considered to be somehow

391
00:40:04,160 --> 00:40:12,200
stupid or naive to be open to some you know consensus opinion or some authority and that

392
00:40:12,200 --> 00:40:18,880
or if they're made fun of for thinking or believing in impartiality that doesn't breed

393
00:40:18,880 --> 00:40:23,560
that doesn't breed trust it just won't you know you're wrong for being for having any

394
00:40:23,560 --> 00:40:29,400
trust in an impartial decision you're just a fool and I'm afraid that's a meme in our

395
00:40:29,400 --> 00:40:36,480
society right now it's keeping us away from forming establishing some kind of legitimate

396
00:40:36,480 --> 00:40:43,160
sources of authority for many of these topics and there seem to be a change in how we react

397
00:40:43,160 --> 00:40:50,280
to evidence I think evidence is maybe a key word here on one hand we have more and more

398
00:40:50,280 --> 00:40:57,120
segments of the population who just reject evidence as irrelevant and on the other let

399
00:40:57,120 --> 00:41:02,920
me take for example you know what the the January 6 hearings right now there is a I

400
00:41:02,920 --> 00:41:08,560
think kind of a feeling that no matter how much evidence the committee will accumulate

401
00:41:08,560 --> 00:41:13,200
some people will not be persuaded by it right maybe because they distrust the source of

402
00:41:13,200 --> 00:41:18,680
the evidence or maybe because they just don't it doesn't change our mind at the same time

403
00:41:18,680 --> 00:41:26,160
where evidence is losing its meaning for many people you now have a new kind of brand of

404
00:41:26,160 --> 00:41:32,720
conspiracies and that doesn't rely on any of it and it's at all I mean I take the they

405
00:41:32,720 --> 00:41:39,440
QAnon conspiracy for example people are willing to believe in in outrageous arguments based

406
00:41:39,440 --> 00:41:47,400
on very little it's it's different even from the conspiracy theories of let's say 2001

407
00:41:47,400 --> 00:41:53,560
where truther is around the 9 11 attacks were trying to collect every bead of information

408
00:41:53,560 --> 00:41:59,120
they could you know they would try to I mean it was of course inaccurate and wrong but

409
00:41:59,120 --> 00:42:03,760
at least it wasn't attempt to kind of find some information that can support their side

410
00:42:03,760 --> 00:42:09,680
I remember all the you know attempts to calculate the heat levels of the metal and what not

411
00:42:09,680 --> 00:42:16,360
these days the new conspiracies are just arguments being thrown around people are willing to

412
00:42:16,360 --> 00:42:23,160
at least spread them without any support whatsoever and at the same time they're willing to reject

413
00:42:23,160 --> 00:42:30,640
the scientific community that that is working around around facts and evidence right and

414
00:42:30,640 --> 00:42:34,840
I think that part of the problem and one of the reasons not necessarily the main reason

415
00:42:34,840 --> 00:42:41,120
but one of the reasons that we don't have trust and that misinformation continues and

416
00:42:41,120 --> 00:42:46,560
impeded is that we have a lack of accountability I don't think that we can have trust without

417
00:42:46,560 --> 00:42:55,160
accountability and I think that perhaps there are some there's a there's a sea change or

418
00:42:55,160 --> 00:43:02,720
the beginnings of a sea change I mean we just had the Alex Jones trial for spreading this

419
00:43:02,720 --> 00:43:11,120
misinformation with the with the with the families of the Sunday Sunday hook victims but

420
00:43:11,120 --> 00:43:18,480
for for a very long time for over a decade he was able to spread this misinformation and

421
00:43:18,480 --> 00:43:25,120
making a lot of money out of it without without any consequence.

422
00:43:25,120 --> 00:43:32,160
I mean just to throw out one other ingredient in this yeah I'm I thought your take on the

423
00:43:32,160 --> 00:43:37,040
sort of objectivity and you know the extent to which sort of whether something is truly

424
00:43:37,040 --> 00:43:43,760
objective say in journalism has been challenged you know I think the you know what's going

425
00:43:43,760 --> 00:43:49,120
up part of what's going on here is that we have claims by kind of established players

426
00:43:49,120 --> 00:43:55,200
so like journalists like judges who are you know they're claiming some sort of objectivity

427
00:43:55,200 --> 00:44:00,800
or neutrality but people you know have they're taking some other evidence like whether it's

428
00:44:00,800 --> 00:44:04,800
their experience or they're feeling that there's no accountability or being able to point to

429
00:44:04,800 --> 00:44:11,120
outcomes that contradict this this assertion of objectivity and they're saying what you're

430
00:44:11,120 --> 00:44:14,800
saying isn't true or you know I don't believe what you're telling us because we have this

431
00:44:14,800 --> 00:44:20,040
other evidence that you know you're obviously biased towards this outcome and it's easier

432
00:44:20,040 --> 00:44:26,520
than ever to obtain that independent evidence that at least challenges these traditional

433
00:44:26,520 --> 00:44:31,920
claims to objectivity and lack of bias and you know one thing that's making that easier

434
00:44:31,920 --> 00:44:37,440
is transparency or just you know the ability to actually collect your own your own data

435
00:44:37,440 --> 00:44:42,200
and make that available for other people traditionally you know most information that people would

436
00:44:42,200 --> 00:44:47,280
use to make these kinds of assessments was channeled through these kinds of gatekeepers

437
00:44:47,280 --> 00:44:53,280
but but today you know whether it's through literally laws that allow people to just request

438
00:44:53,280 --> 00:44:57,920
documents from from government authorities or social media which can take very obscure

439
00:44:57,920 --> 00:45:03,680
pieces of information and suddenly put them in before millions of people suddenly you

440
00:45:03,680 --> 00:45:07,920
know there is other evidence that people can use to challenge these claims to authority

441
00:45:07,920 --> 00:45:15,720
which are based on you know neutrality or objectivity and that can't withstand that kind of scrutiny

442
00:45:15,720 --> 00:45:21,480
you know if people are willing to believe that that the claims are not fully fully based

443
00:45:21,480 --> 00:45:26,560
on yeah like on a solid foundation.

444
00:45:26,560 --> 00:45:33,160
Even the efforts have been to try to police or regulate the messages in social media

445
00:45:33,160 --> 00:45:37,760
and then that that's a fraud to some degree fraud right.

446
00:45:37,760 --> 00:45:45,120
I think people don't understand how unregulated social media is part of it is intentional

447
00:45:45,120 --> 00:45:50,800
I mean we decided to put social media under section 230 which means that we read them

448
00:45:50,800 --> 00:45:55,680
as the technology industry and not a media industry right.

449
00:45:55,680 --> 00:46:01,800
I don't think people understand how arbitrary the process of moderating information on social

450
00:46:01,800 --> 00:46:12,200
media is it's basically still you know based on on those companies making decisions based

451
00:46:12,200 --> 00:46:20,800
on their on their financial kind of benefit they are not legally bound to remove misinformation

452
00:46:20,800 --> 00:46:26,360
there are no clear rules about what's considered misinformation or not this is the wild west

453
00:46:26,360 --> 00:46:27,360
of information.

454
00:46:27,360 --> 00:46:33,840
Why is misinformation bothering us more than that because we have more access to it.

455
00:46:33,840 --> 00:46:34,920
Because what sorry.

456
00:46:34,920 --> 00:46:39,680
Because we have more of it or more access to misinformation why are we so bothered about

457
00:46:39,680 --> 00:46:40,680
it.

458
00:46:40,680 --> 00:46:42,120
I mean we knew it was always there.

459
00:46:42,120 --> 00:46:50,240
I think that what's new about misinformation in the digital era is that we are communicating

460
00:46:50,240 --> 00:46:56,240
now in an environment that prioritized the algorithms that you talked about earlier.

461
00:46:56,240 --> 00:47:02,040
I think for a long time we were worried that the algorithms are pushing us toward information

462
00:47:02,040 --> 00:47:03,560
we already believe in.

463
00:47:03,560 --> 00:47:08,880
So there was a lot of discussion of filter bubbles and echo chambers and all those ideas

464
00:47:08,880 --> 00:47:15,560
but I think these days it seems more evident that the environment that we communicate in

465
00:47:15,560 --> 00:47:21,440
prioritizes engagement I mean at the end of the day it's a financial decision the social

466
00:47:21,440 --> 00:47:26,120
media companies like Facebook and TikTok and YouTube are free meaning they need to make

467
00:47:26,120 --> 00:47:31,560
money somehow when they do that by keeping us engaged for as long as possible collecting

468
00:47:31,560 --> 00:47:35,920
our data and tailoring advertisements to us right.

469
00:47:35,920 --> 00:47:40,880
In order to do that they need to give us information that that's going to keep us interested.

470
00:47:40,880 --> 00:47:45,280
This information just fits the bill better than facts.

471
00:47:45,280 --> 00:47:50,520
I think the information environment that we are working in and almost living in these

472
00:47:50,520 --> 00:47:57,120
days just prioritizes misinformation more than before more than the mass media or mainstream

473
00:47:57,120 --> 00:48:03,760
media era that was more bound to some objective norms of objectivity.

474
00:48:03,760 --> 00:48:09,800
It's something that I think should bother us is about the access and vulnerability that

475
00:48:09,800 --> 00:48:13,200
young people have to misinformation.

476
00:48:13,200 --> 00:48:20,320
This is no and I have three children there 11, 12 and 15 and I noticed especially during

477
00:48:20,320 --> 00:48:27,560
the pandemic during Zoom school and my two all this kids were a couple of years older

478
00:48:27,560 --> 00:48:35,560
at the time I guess 13 and 11 and I learned a little bit too late and that we've better

479
00:48:35,560 --> 00:48:41,480
best to remedy it but they have been subjected to a vast amount of misinformation online

480
00:48:41,480 --> 00:48:48,400
and we have a number of conversations and I have not even been aware of this and that

481
00:48:48,400 --> 00:48:56,040
even with misinformation being kind of like my field in a sense but this was basically

482
00:48:56,040 --> 00:49:02,360
happening in front of me and I wasn't seeing it and young people they have access but they

483
00:49:02,360 --> 00:49:07,080
don't have the tools it's difficult to have the tools as even as informing as informed

484
00:49:07,080 --> 00:49:14,720
adults but in some of these misinformation is specifically targeting children and young

485
00:49:14,720 --> 00:49:20,080
people and yeah I think it's a big problem and it's a no problem.

486
00:49:20,080 --> 00:49:27,480
Well if it's true that it's way less regulated and never in the past would that mean how

487
00:49:27,480 --> 00:49:33,520
would it be regulated now is that something you're in favor of?

488
00:49:33,520 --> 00:49:39,600
I mean it's I mean if you're talking about things that are targeted to children I mean

489
00:49:39,600 --> 00:49:44,400
that seems like a pretty ripe area for regulation that seems like yeah that seems like that's

490
00:49:44,400 --> 00:49:51,040
a pretty pretty uncontroversial statement so let's start there.

491
00:49:51,040 --> 00:49:56,080
Yeah are we skipping a right are we going to regulation now?

492
00:49:56,080 --> 00:50:01,240
I mean I was interested in actually peaking off something else that you said so I mean

493
00:50:01,240 --> 00:50:06,120
you've been talking a little bit about sort of learning basically like training and teaching

494
00:50:06,120 --> 00:50:14,400
people how to basically sift through the information that they encounter and there's been a lot

495
00:50:14,400 --> 00:50:21,120
of talk about you know trying to improve educational efforts both you know in children and even

496
00:50:21,120 --> 00:50:26,040
later in life so I've been very interested for example in things like digital data.

497
00:50:26,040 --> 00:50:31,040
Media literacy you know trying to come up with ways of improving people's digital literacy

498
00:50:31,040 --> 00:50:39,920
skills as a potential solution for people's susceptibility to misinformation and so I'm

499
00:50:39,920 --> 00:50:44,360
curious if you've like you know ever kind of thought about that or if you've encountered

500
00:50:44,360 --> 00:50:49,400
these kinds of issues if you are optimistic about these kinds of approaches for helping

501
00:50:49,400 --> 00:50:51,680
to solve this problem.

502
00:50:51,680 --> 00:50:58,160
Well optimistic I don't think this is the only solution but I think it's an integral

503
00:50:58,160 --> 00:51:10,640
part of the solution and because I mean to be able to sift through all of this information

504
00:51:10,640 --> 00:51:17,680
and decide what to keep what to discard you first need to be aware of your own vulnerability

505
00:51:17,680 --> 00:51:21,360
so you have to be aware that there are all of these various sources of information.

506
00:51:21,360 --> 00:51:28,960
I think also there's lack of awareness of our own just neurological susceptibility

507
00:51:28,960 --> 00:51:36,960
such as human beings with the nervous systems that we have how we can folkry to all sorts

508
00:51:36,960 --> 00:51:38,760
of perceptual cognitive biases.

509
00:51:38,760 --> 00:51:44,920
I don't think that this is really well understood or accepted you think okay I'm going to be

510
00:51:44,920 --> 00:51:51,080
able to I know that there is misinformation out there but I know myself I know that I

511
00:51:51,080 --> 00:51:56,080
can make a good decision and you can convince yourself that you can be a good judge and

512
00:51:56,080 --> 00:52:02,000
but fundamentally you have to realize that you have to keep questioning yourself because

513
00:52:02,000 --> 00:52:07,600
you are vulnerable in ways that are not going to be apparent to you.

514
00:52:07,600 --> 00:52:12,320
I want to go back to regulation for a second it's a really tough topic and all of us are

515
00:52:12,320 --> 00:52:14,840
struggling to answer those questions.

516
00:52:14,840 --> 00:52:19,160
I think there are two parts or two components that we need to consider.

517
00:52:19,160 --> 00:52:25,640
First is what do we regulate and the second is who is supposed to do that.

518
00:52:25,640 --> 00:52:29,640
So the first one is hard I mean both of them are hard but the first one is really really

519
00:52:29,640 --> 00:52:34,720
hard because the amount of misinformation right now is so big that we cannot regulate

520
00:52:34,720 --> 00:52:39,720
all of it right so we need to make a decision on where to draw the line in the sand.

521
00:52:39,720 --> 00:52:46,720
For me for example I mean it's still vague but my kind of border line will be where people

522
00:52:46,720 --> 00:52:55,560
live or well being are put at risk right so maybe we wouldn't regulate any misstated

523
00:52:55,560 --> 00:53:00,040
argument on the internet but if something is for example the conspiracy is putting a

524
00:53:00,040 --> 00:53:06,600
complete full racial group at risk for example maybe that's the place where you want to intervene.

525
00:53:06,600 --> 00:53:10,760
So first of all we need to decide what to regulate and what not I mean what kind of

526
00:53:10,760 --> 00:53:13,200
lies to take down and what not.

527
00:53:13,200 --> 00:53:19,880
The second problem of course is who's going to do it and both options are pretty bad.

528
00:53:19,880 --> 00:53:24,360
One option is to let the government do that and we know that in the past governments

529
00:53:24,360 --> 00:53:27,200
had abused this power.

530
00:53:27,200 --> 00:53:33,360
The Chinese government for example right now is regulating TikTok in a way that removes

531
00:53:33,360 --> 00:53:42,040
let's say information that's favorable to the Hong Kong and Taiwanese side right.

532
00:53:42,040 --> 00:53:46,560
We can think of other examples from the history when governments use their power to regulate

533
00:53:46,560 --> 00:53:50,200
information to detrimental effects.

534
00:53:50,200 --> 00:53:55,640
The other option is to let the companies themselves do that and that once again is problematic.

535
00:53:55,640 --> 00:53:59,440
Do we trust Twitter to decide what's truth and what's false.

536
00:53:59,440 --> 00:54:03,600
Do we trust YouTube to be the arbitrators of truth.

537
00:54:03,600 --> 00:54:05,000
The answer is of course not.

538
00:54:05,000 --> 00:54:08,960
They are first and foremost business people.

539
00:54:08,960 --> 00:54:17,380
Their concern is with the bottom line of their companies and they don't have a motivation

540
00:54:17,380 --> 00:54:20,680
really to keep the information environment clean.

541
00:54:20,680 --> 00:54:25,480
Now between these two if I had to choose I would still go with the government because

542
00:54:25,480 --> 00:54:28,520
at least the government is being elected by the people.

543
00:54:28,520 --> 00:54:34,160
I mean nobody elected Mark Zuckerberg to have so much power over information.

544
00:54:34,160 --> 00:54:35,480
Politicians are not perfect.

545
00:54:35,480 --> 00:54:49,320
But at least as citizens we have some authority over selecting them or monitoring their behaviors.

546
00:54:49,320 --> 00:54:56,240
So no solution is perfect but I think we need to start talking about government regulation.

547
00:54:56,240 --> 00:54:58,040
It happens in other countries.

548
00:54:58,040 --> 00:55:03,080
I mean in the United States it sounds like a big deal to ask the government to interfere

549
00:55:03,080 --> 00:55:09,560
with information but take Germany or Austria for example where it is illegal to deny the

550
00:55:09,560 --> 00:55:10,560
Holocaust.

551
00:55:10,560 --> 00:55:16,360
I mean there are laws in place and again how do you decide what to make into a law where

552
00:55:16,360 --> 00:55:23,520
something is becoming an existential threat to people's safety.

553
00:55:23,520 --> 00:55:26,920
So maybe it's time to start seriously thinking about regulation.

554
00:55:26,920 --> 00:55:29,040
I wasn't actually necessary.

555
00:55:29,040 --> 00:55:32,720
I didn't want you to get the idea that I was in favor of regulation although I know there

556
00:55:32,720 --> 00:55:35,840
aren't that many obvious solutions to it.

557
00:55:35,840 --> 00:55:43,160
I also agree with the idea of educating people more to try to become more skeptical.

558
00:55:43,160 --> 00:55:48,840
I'm a little bit pessimistic about educating people basically because of what your original

559
00:55:48,840 --> 00:55:54,400
premises which is we're really prone to these sorts of trickery.

560
00:55:54,400 --> 00:55:58,040
We just are as human beings.

561
00:55:58,040 --> 00:56:02,120
And then there's this sort of general idea that there's a, you know it's interesting

562
00:56:02,120 --> 00:56:07,560
that there's so much skepticism about truth started on the left.

563
00:56:07,560 --> 00:56:15,640
There's a lot of critics of skepticism and the relativism of truth started on the left

564
00:56:15,640 --> 00:56:20,120
and then it's sort of really been appropriated by the right now.

565
00:56:20,120 --> 00:56:25,240
And so we have both camps have become relativist which is difficult.

566
00:56:25,240 --> 00:56:31,320
Right, as people say well I don't think the government on the one hand should be the ones

567
00:56:31,320 --> 00:56:34,680
who decide what's true or what's not true.

568
00:56:34,680 --> 00:56:38,720
And then the other people say we should be free to say whatever we want.

569
00:56:38,720 --> 00:56:40,280
We want the First Amendment rights.

570
00:56:40,280 --> 00:56:43,200
Everything's turned upside down.

571
00:56:43,200 --> 00:56:48,120
But maybe the idea might be also, you know, first of all I think when folks say well for

572
00:56:48,120 --> 00:56:57,080
example Holocaust denying is illegal in Germany I don't think about their legal system around

573
00:56:57,080 --> 00:57:01,880
that but I imagine if you've been cited for claiming the Holocaust wasn't real you might

574
00:57:01,880 --> 00:57:07,440
have some ways to address that legally to try to, no I didn't do it or was it work of

575
00:57:07,440 --> 00:57:10,040
art or whatever, you know something of that nature.

576
00:57:10,040 --> 00:57:12,400
I would think maybe that would be something that would happen here.

577
00:57:12,400 --> 00:57:16,040
The other is you can get together these businesses and say look you have to come together with

578
00:57:16,040 --> 00:57:18,760
to some consensus of what's legitimate.

579
00:57:18,760 --> 00:57:24,360
Let the companies do it as a group and let them know that otherwise it's going to be

580
00:57:24,360 --> 00:57:27,280
the government's going to step in.

581
00:57:27,280 --> 00:57:32,200
It's interesting that a lot of, I mean you were mentioning where the groups being oppressed

582
00:57:32,200 --> 00:57:37,320
because of false information I think Alex Jones was a good example but one of the things

583
00:57:37,320 --> 00:57:42,280
that came up with Alex Jones and I know we've all heard about this in other context is that

584
00:57:42,280 --> 00:57:46,000
there are people getting threatening phone calls and you can imagine that that's something

585
00:57:46,000 --> 00:57:53,200
you could keep a record of and it could be something used as a trigger point for there's

586
00:57:53,200 --> 00:57:54,680
something has to be done about this.

587
00:57:54,680 --> 00:57:56,240
This is leading to a lot of threats.

588
00:57:56,240 --> 00:57:59,440
I don't know what you all think of that as a possible.

589
00:57:59,440 --> 00:58:03,200
Yeah I mean that's something that would probably be illegal you know offline with a telephone

590
00:58:03,200 --> 00:58:08,440
so why would that be legal if you're doing it through Twitter or through Facebook.

591
00:58:08,440 --> 00:58:13,080
I will say I thought the way you framed that was very good.

592
00:58:13,080 --> 00:58:16,640
You know personally in terms of regulation I feel like we actually had a pretty good workable

593
00:58:16,640 --> 00:58:18,600
solution for a while.

594
00:58:18,600 --> 00:58:23,040
This section 230 idea which is we're going to let there be a flourishing of different

595
00:58:23,040 --> 00:58:24,040
platforms.

596
00:58:24,040 --> 00:58:27,240
They can all figure out their rules and people can sort of go to the platform whose rules

597
00:58:27,240 --> 00:58:28,680
that they like.

598
00:58:28,680 --> 00:58:34,480
Problem is that was not designed for a world in which we have like two or three huge platforms

599
00:58:34,480 --> 00:58:39,320
that have come to somehow dominate our kind of public sphere or that's what it seems

600
00:58:39,320 --> 00:58:40,640
like anyway.

601
00:58:40,640 --> 00:58:44,080
It was for a world in which there was like AOL and copy serve and all these little dial

602
00:58:44,080 --> 00:58:47,920
up services and the stakes just don't seem so high.

603
00:58:47,920 --> 00:58:51,960
And so you know the question to me is is there a way to sort of adapt this kind of like middle

604
00:58:51,960 --> 00:58:58,320
path to our present day reality which just does not reflect the reality in which the

605
00:58:58,320 --> 00:59:00,720
original regulations were drawn.

606
00:59:00,720 --> 00:59:04,400
So I did included.

607
00:59:04,400 --> 00:59:09,920
I'll just say the idea bearing section 230 was that if you're a technology company that

608
00:59:09,920 --> 00:59:15,760
produces let's say a phone you can't be held responsible for what people say on the phone.

609
00:59:15,760 --> 00:59:18,400
But with Facebook and Twitter that's not the case anymore.

610
00:59:18,400 --> 00:59:24,200
Their algorithms are determining which information will be more permanent which information will

611
00:59:24,200 --> 00:59:26,280
be more obscure.

612
00:59:26,280 --> 00:59:31,600
Meaning they are beginning to hold not beginning they are holding editorial role of what's

613
00:59:31,600 --> 00:59:35,560
being said on their platforms and maybe they're not a technology company anymore.

614
00:59:35,560 --> 00:59:39,720
Maybe they are media companies at this point and if they are we need to regulate them just

615
00:59:39,720 --> 00:59:45,160
like we did for cable TV and newspapers and all the media that came before.

616
00:59:45,160 --> 00:59:50,240
Yeah I think the question really does come down to the extent to which there is some

617
00:59:50,240 --> 00:59:52,080
sort of editorial discretion.

618
00:59:52,080 --> 00:59:56,720
It's like literally this is literally what some of the court cases now are hinging on.

619
00:59:56,720 --> 00:59:59,280
So it actually isn't the case that there aren't regulations or laws.

620
00:59:59,280 --> 01:00:01,160
There actually are regulations or laws.

621
01:00:01,160 --> 01:00:06,920
They're going to be made by one actor or another whether it's a court right or some sort of

622
01:00:06,920 --> 01:00:12,320
de facto rule by a company and so I think what we should try to figure out is whether

623
01:00:12,320 --> 01:00:18,520
the public can sort of play a role in determining what the rules are going to look like because

624
01:00:18,520 --> 01:00:22,400
the rules are going to be set one way or another.

625
01:00:22,400 --> 01:00:29,000
I included the reference NFTs which are a little bit have receded a little bit in the

626
01:00:29,000 --> 01:00:33,760
media coverage of LA but I saw it as a being an effort.

627
01:00:33,760 --> 01:00:39,400
I mentioned this to you earlier, Rotem, it's like an effort to reestablish authenticity

628
01:00:39,400 --> 01:00:46,400
in some way because these are ways in which these things are supposedly indelible and

629
01:00:46,400 --> 01:00:54,400
not open to dispute and similarly Bitcoin which have a relationship in the cryptocurrencies

630
01:00:54,400 --> 01:00:58,160
the idea that okay we don't want to trust the Fed anymore with the way our money is being

631
01:00:58,160 --> 01:01:03,480
managed we're going to find this sort of democratized way of making sure our money is

632
01:01:03,480 --> 01:01:09,080
transactions are I don't want to ledger that's indelible how quickly that sort of what do

633
01:01:09,080 --> 01:01:13,080
you think about the fact that how quickly this is sort of it was like a little bit of

634
01:01:13,080 --> 01:01:16,840
a fat and it seems to be I don't know if it's going to receive forever but it has receded

635
01:01:16,840 --> 01:01:21,760
a little bit.

636
01:01:21,760 --> 01:01:27,200
One thing that interests me about NFTs is that we perceive them as less authentic as

637
01:01:27,200 --> 01:01:33,440
you said than money but for me it's kind of ironic because money was always.

638
01:01:33,440 --> 01:01:34,440
Fictional.

639
01:01:34,440 --> 01:01:36,160
I mean right, I mean what is money?

640
01:01:36,160 --> 01:01:41,640
What does it mean that I give you a piece of grain paper that has no value at all you

641
01:01:41,640 --> 01:01:44,880
cannot wear it you cannot turn it into a tent or anything you know.

642
01:01:44,880 --> 01:01:46,160
It depends on trust.

643
01:01:46,160 --> 01:01:50,840
It depends on trust right I mean the only reason money works is that I can give you this piece

644
01:01:50,840 --> 01:01:55,560
of paper and both of us know that you can take this piece of paper to a genet by the

645
01:01:55,560 --> 01:01:58,520
share with it right.

646
01:01:58,520 --> 01:02:04,200
I think the NFTs are just another iteration of the same old idea of imagining a financial

647
01:02:04,200 --> 01:02:10,440
system and making up rules to play by but yeah it seems like these days it's kind of

648
01:02:10,440 --> 01:02:12,200
collapsing I don't know.

649
01:02:12,200 --> 01:02:16,240
I'm not an expert on NFT but it seems that the.

650
01:02:16,240 --> 01:02:25,480
In the NFTs there is no country behind the NFT it's between the buyer and the seller

651
01:02:25,480 --> 01:02:30,960
but with the let's say the owner you have the United States treasured guaranteed that

652
01:02:30,960 --> 01:02:36,400
what's guaranteed the NFTs was.

653
01:02:36,400 --> 01:02:44,480
Just trust in the other or in the market.

654
01:02:44,480 --> 01:02:50,280
In a way it connects to the previous point you started talking a little bit about kind

655
01:02:50,280 --> 01:02:56,400
of libertarian models of communication what if we just let people be and hope that they

656
01:02:56,400 --> 01:02:59,600
are going to do the best with it.

657
01:02:59,600 --> 01:03:04,480
We seem to put a lot of hopes into that and it's true for NFTs but it's also true for

658
01:03:04,480 --> 01:03:07,480
social media and discussion.

659
01:03:07,480 --> 01:03:14,400
We built this utopian perception of media environments like the internet where if we

660
01:03:14,400 --> 01:03:19,120
just let people be themselves if we don't tell them what to you know what to read or

661
01:03:19,120 --> 01:03:24,560
what to think or what to say the best content will always prevail and it doesn't seem to

662
01:03:24,560 --> 01:03:25,560
hold water.

663
01:03:25,560 --> 01:03:30,760
When push comes to shove it seems like we're not always looking for the best content.

664
01:03:30,760 --> 01:03:36,080
We're not always trying to spread the most accurate information.

665
01:03:36,080 --> 01:03:41,760
It's nice to kind of dream about the libertarian world where you can just leave people through

666
01:03:41,760 --> 01:03:44,880
their own devices and they're going to do the best.

667
01:03:44,880 --> 01:03:47,040
It doesn't seem to work.

668
01:03:47,040 --> 01:03:52,560
Do you think this concern about misinformation is going to diminish over the years since

669
01:03:52,560 --> 01:03:57,200
there's so much of it and people will then start thinking well I'm reading all this but

670
01:03:57,200 --> 01:04:07,600
it's misinformation so that they develop a sense of skepticism of a degree that unless

671
01:04:07,600 --> 01:04:13,360
they find real evidence in something they just dismiss it.

672
01:04:13,360 --> 01:04:15,240
I think arguably it's already diminishing.

673
01:04:15,240 --> 01:04:19,880
I think it's being replaced with other concerns.

674
01:04:19,880 --> 01:04:29,560
For example you hear a lot about other kinds of consequences of social media like harassment,

675
01:04:29,560 --> 01:04:34,720
hate speech, even things like incidility, things that just also make things unpleasant,

676
01:04:34,720 --> 01:04:38,480
have a lot of harmful effects but don't necessarily hinge on the question of whether something

677
01:04:38,480 --> 01:04:42,200
is factually accurate or not.

678
01:04:42,200 --> 01:04:47,120
I guess my prediction would be the next time that there's a new big technological shift

679
01:04:47,120 --> 01:04:54,120
in mass communication that raises worries about whether people will be manipulated in some

680
01:04:54,120 --> 01:04:58,000
way we'll hear again about misinformation.

681
01:04:58,000 --> 01:05:04,000
Now I think the conversation has sort of moved on a little bit and misinformation is I think

682
01:05:04,000 --> 01:05:10,080
a potential harmful effect of social media but there are lots of others and I think it's

683
01:05:10,080 --> 01:05:14,240
perhaps healthy that we're kind of making some room for some of these other things as

684
01:05:14,240 --> 01:05:15,240
well.

685
01:05:15,240 --> 01:05:21,760
And I think it will be unlikely that people will just, anything that I read, anything

686
01:05:21,760 --> 01:05:26,800
that I see on the web I'm going to discharge.

687
01:05:26,800 --> 01:05:33,600
I think it's going to keep, it's being curated currently it's going to be even more so that

688
01:05:33,600 --> 01:05:40,040
it links to the polarization issue but I think that people will always have some particular

689
01:05:40,040 --> 01:05:46,720
sources of information that they trust implicitly and they'll be seeking that kind of content

690
01:05:46,720 --> 01:05:49,760
that they already predisposed to trust.

691
01:05:49,760 --> 01:05:56,120
So I don't think that there will be like a widespread skepticism about any sort of

692
01:05:56,120 --> 01:06:00,640
information but perhaps more polarization, heaven.

693
01:06:00,640 --> 01:06:10,000
I think the idea behind NFTs and cryptocurrencies was an effort to sort of codify a form of

694
01:06:10,000 --> 01:06:12,800
authenticity and it fails.

695
01:06:12,800 --> 01:06:16,040
It does fails because what people need is more trust.

696
01:06:16,040 --> 01:06:22,280
And you know you like to think well if I codify it I make it absolutely unassailable yes

697
01:06:22,280 --> 01:06:26,120
or no that that's not replacing trust between human beings.

698
01:06:26,120 --> 01:06:28,280
It's really an interesting, right?

699
01:06:28,280 --> 01:06:31,280
That isn't a good point.

700
01:06:31,280 --> 01:06:37,480
I agree that that's potentially a cautionary tale but I'm not sure, I guess I don't know

701
01:06:37,480 --> 01:06:39,080
if that's universally the case.

702
01:06:39,080 --> 01:06:44,440
So here we have a situation in kind of financial markets, right?

703
01:06:44,440 --> 01:06:52,360
So maybe like we really need trust to sustain a healthy market.

704
01:06:52,360 --> 01:06:56,960
But I see some more things going on with our previous discussion about how do you know

705
01:06:56,960 --> 01:06:59,880
which sources of information to trust.

706
01:06:59,880 --> 01:07:02,800
Can we replace that kind of trust with authenticity?

707
01:07:02,800 --> 01:07:05,640
Well I think that's happening too, right?

708
01:07:05,640 --> 01:07:10,200
If you look at where, I mean when I just talk to my students, you know, they don't even

709
01:07:10,200 --> 01:07:12,840
think about information in terms of trust.

710
01:07:12,840 --> 01:07:16,760
They think about influences that they follow.

711
01:07:16,760 --> 01:07:23,280
They think about broadcasters who they find authentic or real.

712
01:07:23,280 --> 01:07:29,040
So people are already using authenticity as a sort of a metric for where they should

713
01:07:29,040 --> 01:07:31,480
get information.

714
01:07:31,480 --> 01:07:36,360
And so it is a question to me as to whether that sort of replacing trust with authenticity.

715
01:07:36,360 --> 01:07:43,120
But is that cause or effect like do you trust some influencer because they're authentic

716
01:07:43,120 --> 01:07:44,760
or they seem authentic, will you trust?

717
01:07:44,760 --> 01:07:46,360
Really, I know it's a good question.

718
01:07:46,360 --> 01:07:48,160
It's a good question.

719
01:07:48,160 --> 01:07:49,600
Yeah, yeah.

720
01:07:49,600 --> 01:07:52,960
But I guess like one way that you could think about this, I don't know if this is accurate

721
01:07:52,960 --> 01:07:58,120
but you know as trust is sort of declined across the board in a number of spaces.

722
01:07:58,120 --> 01:08:04,520
People still need something that can just simply solve the problem of like where do

723
01:08:04,520 --> 01:08:05,520
I turn to?

724
01:08:05,520 --> 01:08:06,520
People will always find shortcuts.

725
01:08:06,520 --> 01:08:07,520
Yeah.

726
01:08:07,520 --> 01:08:09,000
What shortcuts do I use?

727
01:08:09,000 --> 01:08:14,560
And maybe I'm just using these words interchangeably or something but it's like there is something

728
01:08:14,560 --> 01:08:19,520
about the authenticity of the messenger now that seems different than this kind of traditional

729
01:08:19,520 --> 01:08:20,520
notion of the problem.

730
01:08:20,520 --> 01:08:21,520
But I don't...

731
01:08:21,520 --> 01:08:23,520
The apparent of this.

732
01:08:23,520 --> 01:08:25,160
The apparent, yes, exactly.

733
01:08:25,160 --> 01:08:27,640
I'm all in favor of authenticity and I agree with everything you said.

734
01:08:27,640 --> 01:08:32,040
And I'm just saying that thinking you're going to find it by having it digitalized that

735
01:08:32,040 --> 01:08:37,560
the other thing, this is going to be vouch safe by the digital, the code is not going

736
01:08:37,560 --> 01:08:38,560
to happen.

737
01:08:38,560 --> 01:08:42,080
If Bitcoin and other cryptocurrencies ever take off, it's not going to be because they

738
01:08:42,080 --> 01:08:43,080
have...

739
01:08:43,080 --> 01:08:47,880
Because of the tech, their blockchain technology, it's going to be because people start to put

740
01:08:47,880 --> 01:08:49,880
trust into it.

741
01:08:49,880 --> 01:08:53,000
It's not going to be because of the technology is what I'm saying.

742
01:08:53,000 --> 01:08:56,000
Well, it's like something that inherently can't be hard coded.

743
01:08:56,000 --> 01:08:59,320
Like, well, at the point where you have to sign a contract with someone, it's like,

744
01:08:59,320 --> 01:09:00,800
well, you're not relying on trust anymore.

745
01:09:00,800 --> 01:09:02,240
Like, you know, right?

746
01:09:02,240 --> 01:09:03,240
So...

747
01:09:03,240 --> 01:09:08,400
Well, should we open up the floor to some questions?

748
01:09:08,400 --> 01:09:09,400
Does anyone...

749
01:09:09,400 --> 01:09:10,400
Does anyone here have any questions?

750
01:09:10,400 --> 01:09:11,400
Why don't you come over here?

751
01:09:11,400 --> 01:09:14,400
No, please, because one of them...

752
01:09:14,400 --> 01:09:21,680
Pick up a demike.

753
01:09:21,680 --> 01:09:27,560
So I'm curious, with so many different views of reality and everything you guys have been

754
01:09:27,560 --> 01:09:33,360
talking about, how will history be written about our time?

755
01:09:33,360 --> 01:09:35,440
Will it be in parallel tracks?

756
01:09:35,440 --> 01:09:40,240
Will there be a history that is true history of what was real?

757
01:09:40,240 --> 01:09:42,240
What was fact-based?

758
01:09:42,240 --> 01:09:44,640
We never had true history.

759
01:09:44,640 --> 01:09:45,640
Yeah.

760
01:09:45,640 --> 01:09:51,640
Well, maybe, but it seems like it's more at risk now than possibly it's been.

761
01:09:51,640 --> 01:10:02,200
I mean, there's a book by Vargas Losa that came out last year about the events in Guatemala

762
01:10:02,200 --> 01:10:07,560
and the U.S. role in whatever it was that happened in Guatemala.

763
01:10:07,560 --> 01:10:18,280
Well, that only is something that came up in a more consistent and coherent way with Vargas

764
01:10:18,280 --> 01:10:19,800
Losa's book.

765
01:10:19,800 --> 01:10:26,080
So what is the accurate history of what happened in Guatemala until that point?

766
01:10:26,080 --> 01:10:30,080
No, I know what's the problem.

767
01:10:30,080 --> 01:10:35,720
History's written through the eyes of the historians.

768
01:10:35,720 --> 01:10:40,160
But this seems even more fraught with conflict about what reason did that.

769
01:10:40,160 --> 01:10:42,160
I think I'm not that I'm an historian, but I have a little bit more faith.

770
01:10:42,160 --> 01:10:46,760
There's going to be a consensus that will be built over time.

771
01:10:46,760 --> 01:10:52,480
Right now, historians just agree about the way things went and they reinterpret parts

772
01:10:52,480 --> 01:10:57,600
of history still to this day and not always in a highly polarized Democrat versus Republican

773
01:10:57,600 --> 01:11:02,560
way, but just because they're honest disagreements among historians about how things go.

774
01:11:02,560 --> 01:11:05,440
I mentioned a story about Chaucer today is a good example.

775
01:11:05,440 --> 01:11:07,560
That may be debated.

776
01:11:07,560 --> 01:11:15,520
I think among academic historians, there will be enough of a consensus about what's...

777
01:11:15,520 --> 01:11:23,600
Actually, I am more optimistic about the future historians than I am, but what's happening

778
01:11:23,600 --> 01:11:24,600
today.

779
01:11:24,600 --> 01:11:34,000
Because I do think that the access to information and to spreading information, there are so

780
01:11:34,000 --> 01:11:37,040
many people today, like anybody has a voice.

781
01:11:37,040 --> 01:11:43,760
And so future historians will have access not just to establish experts, governments,

782
01:11:43,760 --> 01:11:51,680
borders, but we don't know how the everyday person used to think about in the Middle Ages.

783
01:11:51,680 --> 01:11:55,080
They didn't have a voice that we have been able to recover today.

784
01:11:55,080 --> 01:12:01,760
So I think that future historians will have to shift through content, but they will have

785
01:12:01,760 --> 01:12:08,400
a lot more to work with than we have today about the arrest passed.

786
01:12:08,400 --> 01:12:10,000
But that itself is going to pose...

787
01:12:10,000 --> 01:12:15,000
It's going to be this challenge of abundance versus...

788
01:12:15,000 --> 01:12:18,000
What is the scientist that I will say?

789
01:12:18,000 --> 01:12:20,000
Do you have too much data?

790
01:12:20,000 --> 01:12:21,000
I agree.

791
01:12:21,000 --> 01:12:23,000
Which is like a practical matter.

792
01:12:23,000 --> 01:12:27,000
I've tried to reconstruct what people have said on Twitter last week.

793
01:12:27,000 --> 01:12:28,000
And you should try it sometime.

794
01:12:28,000 --> 01:12:34,640
It's really hard because people are responding to other people directly and indirectly.

795
01:12:34,640 --> 01:12:43,200
People are making vague references to things that are not always obvious after the fact.

796
01:12:43,200 --> 01:12:48,520
Reconstructing the context, I think is going to be increasingly difficult because it's

797
01:12:48,520 --> 01:12:54,720
told through memes and references that are constantly evolving.

798
01:12:54,720 --> 01:12:58,320
And it's going to be trying to decipher hieroglyphics in a way.

799
01:12:58,320 --> 01:12:59,320
Perhaps so.

800
01:12:59,320 --> 01:13:03,280
And will that information be preserved in forms that are accessible?

801
01:13:03,280 --> 01:13:06,280
Well, yeah, just because it's digital doesn't mean it doesn't decay.

802
01:13:06,280 --> 01:13:07,280
Exactly.

803
01:13:07,280 --> 01:13:08,280
Yeah.

804
01:13:08,280 --> 01:13:11,680
Well, hopefully there will be people who are academics who are motivated to do that work

805
01:13:11,680 --> 01:13:13,280
on the hieroglyphics.

806
01:13:13,280 --> 01:13:14,920
I like to think there is.

807
01:13:14,920 --> 01:13:22,720
A lot of people who have written about history, books about historical developments are not

808
01:13:22,720 --> 01:13:24,760
always exactly historians either.

809
01:13:24,760 --> 01:13:30,240
And we can think of economists who have written pretty divergent views, Hayek and Keynes,

810
01:13:30,240 --> 01:13:35,200
and pretty divergent views about how history is going to economic history, but also general

811
01:13:35,200 --> 01:13:36,200
history.

812
01:13:36,200 --> 01:13:37,400
And we've survived with that.

813
01:13:37,400 --> 01:13:42,640
So I'm reasonably hopeful about that.

814
01:13:42,640 --> 01:13:43,640
Anybody else?

815
01:13:43,640 --> 01:13:47,600
Oh, please step up to the mic.

816
01:13:47,600 --> 01:13:49,680
Okay.

817
01:13:49,680 --> 01:14:00,200
So since we have some political scientists here, the first thing I want to ask this concept

818
01:14:00,200 --> 01:14:04,440
is a little tangential, but it comes back political science.

819
01:14:04,440 --> 01:14:10,720
My understanding is science is you observe the world, you make theories, you test those

820
01:14:10,720 --> 01:14:15,280
theories through experiment, and then you see if your theories were correct.

821
01:14:15,280 --> 01:14:25,640
Now, if we want to guide this in the context of today's topic, misinformation and algorithms,

822
01:14:25,640 --> 01:14:32,920
would a political scientist be able to conduct an experiment on this topic and make predictions

823
01:14:32,920 --> 01:14:34,000
in theories?

824
01:14:34,000 --> 01:14:40,440
And is that something they would do, or even more generally would do, political scientists

825
01:14:40,440 --> 01:14:43,720
just make predictions but don't run experiments?

826
01:14:43,720 --> 01:14:46,240
Can you comment on that a little bit?

827
01:14:46,240 --> 01:14:48,080
That's a great question.

828
01:14:48,080 --> 01:14:49,080
Yeah, you're right.

829
01:14:49,080 --> 01:14:56,320
And as sort of social scientists, we want to be able to make and test hypotheses, which

830
01:14:56,320 --> 01:14:59,600
often means you're testing predictions.

831
01:14:59,600 --> 01:15:05,280
So certainly, speaking for myself and my own research, everything that I try to do fits

832
01:15:05,280 --> 01:15:07,320
the framework that you just described.

833
01:15:07,320 --> 01:15:10,960
When you're talking about misinformation, things become difficult because there are ethical

834
01:15:10,960 --> 01:15:11,960
issues involved.

835
01:15:11,960 --> 01:15:17,520
So if I could think of the ideal experiment that I would like to run, it would involve

836
01:15:17,520 --> 01:15:21,600
exposing people to a ton of misinformation and see what happens.

837
01:15:21,600 --> 01:15:23,440
Well, that doesn't seem ethical to me.

838
01:15:23,440 --> 01:15:25,240
And so I can't run that.

839
01:15:25,240 --> 01:15:26,240
You can't rat.

840
01:15:26,240 --> 01:15:27,240
Sorry?

841
01:15:27,240 --> 01:15:28,240
You can rat.

842
01:15:28,240 --> 01:15:29,240
Exactly.

843
01:15:29,240 --> 01:15:29,920
So you can do it in rats.

844
01:15:29,920 --> 01:15:38,080
And so part of the creative challenge of doing this kind of research is trying to explore

845
01:15:38,080 --> 01:15:43,480
ways of studying these questions in ways that don't require you to be a mad scientist,

846
01:15:43,480 --> 01:15:44,480
basically.

847
01:15:44,480 --> 01:15:49,560
It involves natural experiments, so have platforms changed the way that they've operated.

848
01:15:49,560 --> 01:15:55,320
And so does this speak to this particular hypothesis about, say, the effects of misinformation?

849
01:15:55,320 --> 01:15:58,120
The other thing is that a lot of the most interesting research that's happening right

850
01:15:58,120 --> 01:16:03,880
now, both by political scientists but also people in psychology and other fields, and

851
01:16:03,880 --> 01:16:11,920
in communication, is trying to test the effectiveness of different kinds of solutions for misinformation.

852
01:16:11,920 --> 01:16:13,560
So take it as a given.

853
01:16:13,560 --> 01:16:19,040
How can we do to reduce people's belief in misinformation in ways that don't infringe

854
01:16:19,040 --> 01:16:21,960
on other kinds of values, like free expression?

855
01:16:21,960 --> 01:16:25,240
So I think this is all a very active area of research.

856
01:16:25,240 --> 01:16:30,520
But I agree that we want to take this sort of scientific empiricist path to understanding

857
01:16:30,520 --> 01:16:31,520
these questions.

858
01:16:31,520 --> 01:16:41,600
Can you give a performance for us to give some sort of experience with, well, not on a sometimes

859
01:16:41,600 --> 01:16:48,040
sort of misinformation, we have the number of experiments with misdirection, as you might

860
01:16:48,040 --> 01:16:50,800
have, in a magic show.

861
01:16:50,800 --> 01:16:58,120
We actually study magic tricks directly to investigate various kinds of cognitive biases.

862
01:16:58,120 --> 01:17:02,360
So you can do these kinds of things.

863
01:17:02,360 --> 01:17:06,440
It's a bit less of an ethical concern where you're investigating magic tricks, but you

864
01:17:06,440 --> 01:17:09,680
can get similar kinds of issues.

865
01:17:09,680 --> 01:17:15,200
I'll just add, oh, I'm so sorry.

866
01:17:15,200 --> 01:17:21,720
I'll just add that, first of all, there ain't one approach to scientific inquiry of political

867
01:17:21,720 --> 01:17:22,720
science.

868
01:17:22,720 --> 01:17:29,040
Both Andy and me are more quantitative and, I guess, post-postivistic in nature, but some

869
01:17:29,040 --> 01:17:34,600
people don't accept that as the only way to understand the world.

870
01:17:34,600 --> 01:17:39,960
In the area of misinformation, specifically, I think that in both your field of political

871
01:17:39,960 --> 01:17:45,880
science and in my field of communication, we are getting very good at building microtheories

872
01:17:45,880 --> 01:17:53,360
of misinformation, which can be tested experimentally in the lab or in controlled environments.

873
01:17:53,360 --> 01:17:59,680
What we are not able to do is to use scientific methods to answer those big questions.

874
01:17:59,680 --> 01:18:03,160
Like, how does misinformation influence our society?

875
01:18:03,160 --> 01:18:07,280
That's something you cannot put into a lab or you cannot create two societies that are

876
01:18:07,280 --> 01:18:12,160
equal on everything except for the presence of misinformation.

877
01:18:12,160 --> 01:18:16,200
So I think we're spending a lot of our time on the micro level of just understanding the

878
01:18:16,200 --> 01:18:18,840
psychological processes.

879
01:18:18,840 --> 01:18:24,200
And in their regard, we are taking a very, an approach that's very similar to the hard

880
01:18:24,200 --> 01:18:28,440
science as much as possible within the creation of movements.

881
01:18:28,440 --> 01:18:29,440
That's such.

882
01:18:29,440 --> 01:18:33,920
Well, first of all, thank you for this wonderful discussion.

883
01:18:33,920 --> 01:18:40,800
And a few years ago, a study made at MIT Media Lab.

884
01:18:40,800 --> 01:18:50,200
The question was, do fake news spread faster and more than real news or truthful news?

885
01:18:50,200 --> 01:18:53,040
And the answer was yes.

886
01:18:53,040 --> 01:19:01,280
So the problem is why and one may think well because of bots and new technology that can

887
01:19:01,280 --> 01:19:03,560
multiply the diffusion.

888
01:19:03,560 --> 01:19:08,400
But the result of the study tells something different.

889
01:19:08,400 --> 01:19:17,280
Apparently, fake news spread faster because of human beings, because of the emotional

890
01:19:17,280 --> 01:19:21,600
reaction they evoke.

891
01:19:21,600 --> 01:19:27,560
So my question is, when we are discussing the impact of new technologies, the temptation

892
01:19:27,560 --> 01:19:31,240
to be apocalyptic is already at hand.

893
01:19:31,240 --> 01:19:34,480
But shouldn't we go back to the human factor?

894
01:19:34,480 --> 01:19:37,480
Because as you said, we are basically the same.

895
01:19:37,480 --> 01:19:48,000
So confirmation bias, you said that reassurance, conformism, the feeling of being part of a

896
01:19:48,000 --> 01:19:52,360
community of people who share the same belief are always the same.

897
01:19:52,360 --> 01:19:57,120
So I think the human factor probably has to be more investigative than interface.

898
01:19:57,120 --> 01:19:58,120
There's too much emphasis.

899
01:19:58,120 --> 01:20:01,000
Don't you think there's too much emphasis on the technology?

900
01:20:01,000 --> 01:20:02,000
Thank you.

901
01:20:02,000 --> 01:20:07,280
I do think that emotions are a critical component.

902
01:20:07,280 --> 01:20:13,840
And we know from a neuroscientific perspective and that in studies of attention, that emotions

903
01:20:13,840 --> 01:20:15,480
prioritize attention.

904
01:20:15,480 --> 01:20:20,280
That's actually a lot of the reasons that magicians take advantage of this.

905
01:20:20,280 --> 01:20:27,600
Magicians actually make us laugh at critical points where they need to misdirect us.

906
01:20:27,600 --> 01:20:37,120
So you can use emotion almost surgically in a magic show, also in social media and in

907
01:20:37,120 --> 01:20:38,560
the spread of fake news.

908
01:20:38,560 --> 01:20:46,880
And we used to think, I mean, many years ago, the thinking in science communication was

909
01:20:46,880 --> 01:20:51,840
that, well, you need to put out the correct information out there and you need to give

910
01:20:51,840 --> 01:20:55,440
more detail and you need to communicate it more.

911
01:20:55,440 --> 01:20:59,080
And so people are going to extract the right conclusion.

912
01:20:59,080 --> 01:21:03,680
It's just they are arriving to the wrong conclusion because they don't have all the information.

913
01:21:03,680 --> 01:21:05,040
So let's give more information.

914
01:21:05,040 --> 01:21:06,800
Let's make it more accessible.

915
01:21:06,800 --> 01:21:12,480
That doesn't fix the problem because you're ignoring the emotional components.

916
01:21:12,480 --> 01:21:15,400
Can I add something?

917
01:21:15,400 --> 01:21:17,680
I thought that was a great question too.

918
01:21:17,680 --> 01:21:21,920
I also, as a social scientist too, I think humans are always going to be at the center

919
01:21:21,920 --> 01:21:24,160
of what I think is important.

920
01:21:24,160 --> 01:21:29,400
But as a political scientist, I think human behavior is structured by institutions.

921
01:21:29,400 --> 01:21:35,600
And so kind of another way of saying that, how that applies to this topic is that I think

922
01:21:35,600 --> 01:21:41,040
social media platforms have different affordances or different features which can bring out or

923
01:21:41,040 --> 01:21:43,840
suppress tendencies that exist in humans.

924
01:21:43,840 --> 01:21:45,560
And so that's the thing that's really important.

925
01:21:45,560 --> 01:21:51,880
There's nothing inherent to, I think there's nothing inherent to social media in the abstract

926
01:21:51,880 --> 01:21:58,240
that means that it's going to incentivize engagement with misinformation.

927
01:21:58,240 --> 01:22:02,600
But it is baked into some platforms today and that the way that they set up the incentives

928
01:22:02,600 --> 01:22:03,920
within the platform.

929
01:22:03,920 --> 01:22:10,360
And so I think that redesigning social media, how they're designed, the kinds of behaviors

930
01:22:10,360 --> 01:22:15,200
that they facilitate for people, the kinds of behavior that they encourage from people

931
01:22:15,200 --> 01:22:17,680
is something that we should be thinking very seriously about.

932
01:22:17,680 --> 01:22:18,680
Because yeah, it's true.

933
01:22:18,680 --> 01:22:22,520
Humans have a tendency to think of ourselves as part of a group.

934
01:22:22,520 --> 01:22:27,280
But that can be emphasized or de-emphasized.

935
01:22:27,280 --> 01:22:30,880
And what the group is, right, that's not hardwired either.

936
01:22:30,880 --> 01:22:35,520
And so these are things that can be brought out through features of social media, including

937
01:22:35,520 --> 01:22:39,200
things like algorithms or not.

938
01:22:39,200 --> 01:22:44,120
There's been a movement I know and I'll know much about it, but I do know that some groups,

939
01:22:44,120 --> 01:22:49,920
organizations will have people go out together and live camp out or solve problems.

940
01:22:49,920 --> 01:22:53,680
And the whole idea is here they're identifying with each other, yeah, they're cooperating

941
01:22:53,680 --> 01:22:58,000
with each other towards certain goals which is different just saying, well, I'm a liberal

942
01:22:58,000 --> 01:22:59,600
or I'm a conservative, right?

943
01:22:59,600 --> 01:23:03,960
They're actually targeting some goals and that seems to foster a little bit more of

944
01:23:03,960 --> 01:23:05,600
a sense of community.

945
01:23:05,600 --> 01:23:06,600
Yes.

946
01:23:06,600 --> 01:23:11,120
Thank you very much for a very stimulating discussion.

947
01:23:11,120 --> 01:23:19,800
I cannot help but remember that it goes to time in human history where the heliocentric

948
01:23:19,800 --> 01:23:22,880
system was considered misinformation.

949
01:23:22,880 --> 01:23:30,680
And the geocentric system was considered human to be the truth.

950
01:23:30,680 --> 01:23:37,760
And people were actually tried, they were put on the fire for challenging the so-called

951
01:23:37,760 --> 01:23:40,040
truth by spreading misinformation.

952
01:23:40,040 --> 01:23:46,520
Okay, so my question is, is there such a thing as information?

953
01:23:46,520 --> 01:23:50,480
Because if there is misinformation, it means that the information is what's true and the

954
01:23:50,480 --> 01:23:52,480
misinformation is what's wrong.

955
01:23:52,480 --> 01:23:55,440
Now is there such a thing as information?

956
01:23:55,440 --> 01:24:00,360
And even if there is, what's wrong with challenging it?

957
01:24:00,360 --> 01:24:12,120
And so the last thing I'm going to say is science is very limited in what objective,

958
01:24:12,120 --> 01:24:28,680
let's say, is like physics, I mean, more than 95% of medical, biomedical published research,

959
01:24:28,680 --> 01:24:36,120
which we consider science, cannot be replicated.

960
01:24:36,120 --> 01:24:43,800
That tells you how hard establishing what information is at least in the scientific context where

961
01:24:43,800 --> 01:24:49,320
you can have objective methodology let alone in the social sciences.

962
01:24:49,320 --> 01:24:50,800
That's what the problem is.

963
01:24:50,800 --> 01:24:59,040
So should we err on the side of allowing misinformation because that makes the dialectic process which

964
01:24:59,040 --> 01:25:07,280
allows us to filter and synthesize and refuse and accept that should we allow misinformation?

965
01:25:07,280 --> 01:25:09,760
Should we err on the side of allowing misinformation?

966
01:25:09,760 --> 01:25:16,200
Or should we err on the side of imposing quote-unquote information which actually may turn out in

967
01:25:16,200 --> 01:25:20,280
maybe the next decade or the next century to be false?

968
01:25:20,280 --> 01:25:22,640
Thank you.

969
01:25:22,640 --> 01:25:29,800
My feeling, I think that these are different issues I've played here.

970
01:25:29,800 --> 01:25:37,120
And I think one issue pertains to the science enterprise itself and that it is true that

971
01:25:37,120 --> 01:25:46,520
we do have a replicability crisis or problem more in some areas than in others.

972
01:25:46,520 --> 01:25:51,960
But and I think that there's a lot of fundamental problems with the structure of science, how

973
01:25:51,960 --> 01:25:59,200
it's done this day, the kinds of science that gets incentivized, the pressure to publish.

974
01:25:59,200 --> 01:26:07,080
There are a lot of problematic issues and certainly sometimes we're playing it a bit

975
01:26:07,080 --> 01:26:15,800
too safe in the sense of the research that gets resources to be performed.

976
01:26:15,800 --> 01:26:25,880
I think that's a different problem from misinformation itself and in the spirit of misinformation,

977
01:26:25,880 --> 01:26:35,680
I think that by individuals, not necessarily scientists, I think that's a different domain

978
01:26:35,680 --> 01:26:42,600
from a science than the best way it could be done.

979
01:26:42,600 --> 01:26:51,760
So to me, those are different issues and yes, in terms of the science, the other thing

980
01:26:51,760 --> 01:26:57,200
I think that's important to keep in mind in terms of the science because it was mentioned

981
01:26:57,200 --> 01:26:59,080
something about truth.

982
01:26:59,080 --> 01:27:05,720
And I would say that scientists don't try to get at the truth or they should not try

983
01:27:05,720 --> 01:27:06,720
to get at the truth.

984
01:27:06,720 --> 01:27:11,840
The truth in science is always aspirational and that's why we talk about validating hypothesis,

985
01:27:11,840 --> 01:27:18,120
not verifying a hypothesis and the hypothesis is correct until it's proven otherwise.

986
01:27:18,120 --> 01:27:24,440
So as scientists, at least the way that I was taught to the science and the way that

987
01:27:24,440 --> 01:27:33,680
I teach my trainees is that you try to eliminate hypothesis that are incorrect and hopefully

988
01:27:33,680 --> 01:27:37,200
you get to narrow down more and more the actual answer.

989
01:27:37,200 --> 01:27:44,200
But as a scientist, you have to be always in the mind frame that you need to be questioning

990
01:27:44,200 --> 01:27:49,920
your own results and there's also others of course, but all the time you're supposed

991
01:27:49,920 --> 01:27:55,560
to be revising your framework in light of no evidence.

992
01:27:55,560 --> 01:27:58,000
Yeah I absolutely love this question.

993
01:27:58,000 --> 01:28:02,560
We can spend two more hours just answering this, right?

994
01:28:02,560 --> 01:28:07,000
You asked what is information, forget about misinformation, what is information and that

995
01:28:07,000 --> 01:28:12,680
bothered every thinker since we learned how to put our mind into words, right?

996
01:28:12,680 --> 01:28:17,040
I mean that's Greek philosophy that's played out thinking about the people in the cave

997
01:28:17,040 --> 01:28:22,760
trying to figure out if they are really seeing the world or not.

998
01:28:22,760 --> 01:28:28,800
I'll kind of get inspiration from all the statues of someone Freud in this building and

999
01:28:28,800 --> 01:28:33,440
go to another Austrian philosopher of science which is Karl Popper, right?

1000
01:28:33,440 --> 01:28:41,640
And I'm a very strong, a very strong, a properian person and I think he reminded us that science

1001
01:28:41,640 --> 01:28:45,960
is about, as you said, science is not the search for information.

1002
01:28:45,960 --> 01:28:50,160
It's the systematic rejection of misinformation.

1003
01:28:50,160 --> 01:28:56,640
We would never be able to prove with certainty that anything that we see in studies is true,

1004
01:28:56,640 --> 01:28:57,640
right?

1005
01:28:57,640 --> 01:29:02,720
The best we can do is to reject hypotheses that are wrong.

1006
01:29:02,720 --> 01:29:08,160
We are able logically to reject some ideas and then at any given moment we need to take

1007
01:29:08,160 --> 01:29:13,960
the actions that are most suitable to the current state of knowledge that we have at

1008
01:29:13,960 --> 01:29:15,260
that point.

1009
01:29:15,260 --> 01:29:19,480
Take vaccines, for example, that you talked about before.

1010
01:29:19,480 --> 01:29:24,200
Can we say for certain that vaccines have no negative impact?

1011
01:29:24,200 --> 01:29:25,320
Of course not.

1012
01:29:25,320 --> 01:29:26,320
We can't.

1013
01:29:26,320 --> 01:29:27,320
Certainly we can't.

1014
01:29:27,320 --> 01:29:34,120
We can just reject the hypothesis that people who take the vaccines are more likely than

1015
01:29:34,120 --> 01:29:36,280
others to have autism, for example.

1016
01:29:36,280 --> 01:29:41,960
We can reject this hypothesis but we cannot say for sure that vaccines will never have

1017
01:29:41,960 --> 01:29:44,800
a negative influence.

1018
01:29:44,800 --> 01:29:53,280
So yes, science is kind of, as if it's working in this regard, we will never get to the truth

1019
01:29:53,280 --> 01:30:00,800
but I think we are able to gather enough consensus on what's wrong to direct policy and behavior

1020
01:30:00,800 --> 01:30:03,160
that's more educated.

1021
01:30:03,160 --> 01:30:09,320
I wanted to make a real quick response to this too.

1022
01:30:09,320 --> 01:30:15,680
There are different sorts of misinformation and when the consequence of misinformation

1023
01:30:15,680 --> 01:30:20,960
could be dangerous, of course, we want to figure out a way to get around that.

1024
01:30:20,960 --> 01:30:26,520
A famous director now, sort of infamous director had this wonderful little bit where he was

1025
01:30:26,520 --> 01:30:32,280
his neurotic child version of himself was being reprimanded by his mother who said,

1026
01:30:32,280 --> 01:30:36,400
what's at your business if the universe is expanding?

1027
01:30:36,400 --> 01:30:40,800
So the idea is, okay, you know what, if the universe is expanding or not, we don't need

1028
01:30:40,800 --> 01:30:41,800
to get to the bottom of that.

1029
01:30:41,800 --> 01:30:44,880
It's okay with us if you don't accept authority on that.

1030
01:30:44,880 --> 01:30:50,920
But if you go to your doctor and you have a, God forbid you have a cancer and the treatment

1031
01:30:50,920 --> 01:30:53,880
that's going to be offered for you and oftentimes if they're experimental treatments, they'll

1032
01:30:53,880 --> 01:30:55,720
even tell you in advance.

1033
01:30:55,720 --> 01:31:01,160
I can't even say this is going to be good or safe for you but you have to make a decision.

1034
01:31:01,160 --> 01:31:03,480
You know, that's where the rubber meets the road.

1035
01:31:03,480 --> 01:31:07,920
You then, I think, you're best off putting your trust into the doctor and in return,

1036
01:31:07,920 --> 01:31:13,320
the doctor has to have in his or her mind the idea that I don't want to let my patient

1037
01:31:13,320 --> 01:31:18,120
down because this is almost like a sacred trust that's being placed on me.

1038
01:31:18,120 --> 01:31:25,320
So that means we have to go back and make sure that this treatment is good and valid.

1039
01:31:25,320 --> 01:31:28,760
Sometimes you don't know in any particular point in time but trust is another, again,

1040
01:31:28,760 --> 01:31:34,000
comes up as another important feature separating information from misinformation.

1041
01:31:34,000 --> 01:31:37,400
And not always making a clear demarcation, right?

1042
01:31:37,400 --> 01:31:41,480
Then there cannot be, which is part of, I think, what your question entails.

1043
01:31:41,480 --> 01:31:43,240
Yeah, thank you.

1044
01:31:43,240 --> 01:31:45,240
Yes, please.

1045
01:31:45,240 --> 01:31:54,480
Actually, that response was very closely related to the question I wanted to ask because, so,

1046
01:31:54,480 --> 01:31:58,880
there's an implication to certain kinds of misinformation.

1047
01:31:58,880 --> 01:32:02,920
And so, we touched on regulation before.

1048
01:32:02,920 --> 01:32:08,720
My question's about are there other kinds of regulatory capacity in sort of societies

1049
01:32:08,720 --> 01:32:11,960
that exist today or that we can build into our societies?

1050
01:32:11,960 --> 01:32:16,560
Because if certain kinds of misinformation might imply certain kinds of harm to specific

1051
01:32:16,560 --> 01:32:18,840
people and they're endowed with certain kinds of voices.

1052
01:32:18,840 --> 01:32:26,080
So my question is, is it possible to think of how can that create the potential harm

1053
01:32:26,080 --> 01:32:31,640
of misinformation, how can that create feedback loops to like a repair capacity?

1054
01:32:31,640 --> 01:32:34,120
Is it possible?

1055
01:32:34,120 --> 01:32:41,720
I mean, I think this raises, I mean, I think this is an interesting question that raises

1056
01:32:41,720 --> 01:32:52,280
the challenge as to how we sort of build sort of responsive mechanisms of accountability

1057
01:32:52,280 --> 01:32:56,560
for any kind of regulatory regime that we end up wanting to have.

1058
01:32:56,560 --> 01:33:02,280
So what are the rules of the road for content moderation and platform governance?

1059
01:33:02,280 --> 01:33:06,200
And who gets the decide, who gets a voice and how these rules are determined?

1060
01:33:06,200 --> 01:33:11,240
I think there's a strong case to be made that whether this is being channeled through

1061
01:33:11,240 --> 01:33:16,200
like legislatures or some other citizens' bodies, those bodies should be representative

1062
01:33:16,200 --> 01:33:18,920
of the diversity of interest in society.

1063
01:33:18,920 --> 01:33:25,920
Very much including people who are the victims of, I mean, not only misinformation, but harassment

1064
01:33:25,920 --> 01:33:32,320
and hateful speech that can deal legitimized people, deal with legitimized groups and make

1065
01:33:32,320 --> 01:33:36,440
it more difficult to participate as citizens.

1066
01:33:36,440 --> 01:33:40,160
When we say regulation, we usually think of content moderation, right?

1067
01:33:40,160 --> 01:33:44,040
Having information that is wrong or adding some disclaimers and so on.

1068
01:33:44,040 --> 01:33:50,080
But recently some scholars raised some more creative approaches to regulation.

1069
01:33:50,080 --> 01:33:57,000
And one that I found really interesting is the one suggested by Victor Picard from the

1070
01:33:57,000 --> 01:33:58,160
University of Pennsylvania.

1071
01:33:58,160 --> 01:34:06,800
He says, instead of focusing on moderation of content, let's take taxes from social media

1072
01:34:06,800 --> 01:34:07,800
companies.

1073
01:34:07,800 --> 01:34:10,800
There's a lot of money from that misinformation, right?

1074
01:34:10,800 --> 01:34:16,600
Let's take some taxes back and use it to fund high quality journalism.

1075
01:34:16,600 --> 01:34:23,840
You can create a fund that kind of builds on the money that being made from our misinformation

1076
01:34:23,840 --> 01:34:26,880
and build more reliable institutions of knowledge with it.

1077
01:34:26,880 --> 01:34:32,680
So maybe there are more solutions that we're not thinking about, but they're more creative

1078
01:34:32,680 --> 01:34:38,240
than just running after the next mistake and trying to correct it all the time.

1079
01:34:38,240 --> 01:34:39,240
I think that's wonderful.

1080
01:34:39,240 --> 01:34:41,480
It's a great point to high point to end on.

1081
01:34:41,480 --> 01:34:42,480
It's not my idea.

1082
01:34:42,480 --> 01:34:44,280
But it's a great comment.

1083
01:34:44,280 --> 01:34:50,600
And I'm really pleased to hear all the different efforts to sort of come up with some, you

1084
01:34:50,600 --> 01:34:54,040
know, handle some solution on this issue.

1085
01:34:54,040 --> 01:34:58,520
And I want to thank you all three of you for contributing the way you did.

1086
01:34:58,520 --> 01:34:59,520
Thank you.

1087
01:34:59,520 --> 01:35:00,520
Great.

1088
01:35:00,520 --> 01:35:03,880
I appreciate that.

1089
01:35:03,880 --> 01:35:04,900
Congratulations.

1090
01:35:04,900 --> 01:35:13,320
A lot of talk sharing.

1091
01:35:13,320 --> 01:35:16,100
Thank you.

1092
01:35:16,100 --> 01:35:17,600
That's all we're going to do, right?

