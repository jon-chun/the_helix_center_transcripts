1
00:00:00,000 --> 00:00:03,000
I'm Rob Penzer, I'm the Associate Director of the Helix Center.

2
00:00:03,000 --> 00:00:04,720
Welcome.

3
00:00:04,720 --> 00:00:06,520
Before we start with today's program,

4
00:00:06,520 --> 00:00:08,640
I have a few announcements.

5
00:00:08,640 --> 00:00:10,440
Some upcoming round tables.

6
00:00:10,440 --> 00:00:15,240
On Saturday, November 5, we have autism and the mind brain

7
00:00:15,240 --> 00:00:18,200
with Andrew Gerber, psychoanalyst and medical director

8
00:00:18,200 --> 00:00:21,560
of Austin Riggs, Nushin Hajikani, who's

9
00:00:21,560 --> 00:00:24,160
Associate Professor in Radiology and Director

10
00:00:24,160 --> 00:00:28,040
of the Neurolimbic Research Laboratory at the Martinez.

11
00:00:28,040 --> 00:00:29,280
Should be Martinos.

12
00:00:29,280 --> 00:00:32,160
Sorry, that's auto-correct.

13
00:00:32,160 --> 00:00:34,440
I guess there are some interesting neurolimbic research

14
00:00:34,440 --> 00:00:35,920
at Martini centers also.

15
00:00:35,920 --> 00:00:38,840
But it's the Martino Center at Mass General.

16
00:00:38,840 --> 00:00:41,640
Craig Neuschafer, who's the Professor of Epidemiology

17
00:00:41,640 --> 00:00:44,200
and Biostatistics and Founding Director

18
00:00:44,200 --> 00:00:46,680
of the Autism Institute at Drexel University,

19
00:00:46,680 --> 00:00:48,840
School of Public Health.

20
00:00:48,840 --> 00:00:52,720
Jeremy Vienstra van der Wiel, who's the Mortimer Sackler,

21
00:00:52,720 --> 00:00:54,560
Sackler, Associate Professor of Psychology

22
00:00:54,560 --> 00:00:57,160
at Columbia University, and Martha Welch,

23
00:00:57,160 --> 00:01:00,440
Associate Professor of Psychiatry in Pediatrics and Pathology

24
00:01:00,440 --> 00:01:03,960
and Cell Biology at Columbia University Medical Center.

25
00:01:03,960 --> 00:01:07,160
On Saturday, January 28, we hope you'll

26
00:01:07,160 --> 00:01:10,040
join Alberto Mangoel and other scholars

27
00:01:10,040 --> 00:01:13,800
for the library as reality and metaphor.

28
00:01:13,800 --> 00:01:18,480
Please follow us and like us on Facebook, as well as on Twitter.

29
00:01:18,480 --> 00:01:23,240
And you can visit helix.org for further updates.

30
00:01:23,240 --> 00:01:28,520
So today's program I'd like to introduce our speakers today,

31
00:01:28,520 --> 00:01:29,720
Michael Bess.

32
00:01:29,720 --> 00:01:31,920
And if you could raise your hand, so people recognize you,

33
00:01:31,920 --> 00:01:33,480
who's Chancellor's Professor of History

34
00:01:33,480 --> 00:01:37,280
at Vanderbilt University, Ned Block,

35
00:01:37,280 --> 00:01:39,600
Silver Professor of Philosophy, Psychology

36
00:01:39,600 --> 00:01:44,080
and Neuroscience at New York University, Jeffrey Kephart,

37
00:01:44,080 --> 00:01:45,960
IBM Distinguished Research Staff Member,

38
00:01:45,960 --> 00:01:49,000
Symbiotic Cognitive Systems, and IBM Academy

39
00:01:49,000 --> 00:01:52,800
of Technology Member, Francesca Rossi,

40
00:01:52,800 --> 00:01:55,400
Research Scientist at IBM Watson Research Center

41
00:01:55,400 --> 00:01:56,880
and Professor of Computer Science

42
00:01:56,880 --> 00:01:58,960
at the University of Padau.

43
00:01:58,960 --> 00:02:02,440
Unfortunately, David Hansen, who was going to be joining us

44
00:02:02,440 --> 00:02:05,960
from Beijing via Skype, took ill,

45
00:02:05,960 --> 00:02:11,440
and he sends his regrets, so he won't be participating.

46
00:02:11,440 --> 00:02:16,920
So I'll just, by way of a slight introduction,

47
00:02:16,920 --> 00:02:21,720
we're going to be talking about embodied artificial intelligence

48
00:02:21,720 --> 00:02:27,200
and we understand that intelligence now requires a body

49
00:02:27,200 --> 00:02:31,000
and can't be understood by simple algorithms alone.

50
00:02:31,000 --> 00:02:34,960
And one of the questions that I might also want the panel

51
00:02:34,960 --> 00:02:40,120
to consider, are we thinking about sort

52
00:02:40,120 --> 00:02:44,360
of an artificial constructed evolutionary developmental

53
00:02:44,360 --> 00:02:48,960
biology when we talk about embodied AI?

54
00:02:48,960 --> 00:02:53,280
Are we thinking of modeling things in terms of the way

55
00:02:53,280 --> 00:02:58,920
that in organic systems, development and evolution play a role

56
00:02:58,920 --> 00:03:03,000
in the attainment of intelligence?

57
00:03:03,000 --> 00:03:05,840
So with that, we'll start.

58
00:03:05,840 --> 00:03:14,560
Whoever wants to get going, you can get going.

59
00:03:14,560 --> 00:03:16,200
OK, I'll start.

60
00:03:16,200 --> 00:03:19,280
So I'm not sure how to answer that question, actually,

61
00:03:19,280 --> 00:03:22,600
because when I think of embodied cognition

62
00:03:22,600 --> 00:03:29,320
or embodied AI, we've discussed with Jeff many times already,

63
00:03:29,320 --> 00:03:37,680
I think of an AI system that can help people

64
00:03:37,680 --> 00:03:41,720
make better decisions, so that work in symbiosis with people

65
00:03:41,720 --> 00:03:45,280
and help them do better whatever they have to do

66
00:03:45,280 --> 00:03:47,840
in their private or professional life.

67
00:03:47,840 --> 00:03:52,680
And so rather than maybe I didn't give much thought

68
00:03:52,680 --> 00:03:55,400
that this, you know, a logical interpretation

69
00:03:55,400 --> 00:04:02,560
of the artificial intelligence, but more I see the embodied

70
00:04:02,560 --> 00:04:07,840
embodiment part of AI as a way to facilitate the interaction

71
00:04:07,840 --> 00:04:11,920
with these humans or humans that are going to work together

72
00:04:11,920 --> 00:04:13,200
with the system.

73
00:04:13,200 --> 00:04:20,400
So I see this embodiment as a way to help in this interaction.

74
00:04:20,400 --> 00:04:25,960
So for example, we are thinking about also with Jeff,

75
00:04:25,960 --> 00:04:30,320
you know, about cognitive rooms like this one,

76
00:04:30,320 --> 00:04:34,960
an example, for example, where people, suppose we are, you know,

77
00:04:34,960 --> 00:04:38,160
people not just discussing here, but are, you know,

78
00:04:38,160 --> 00:04:41,640
here a committee of people have to make a certain decision

79
00:04:41,640 --> 00:04:46,120
and we need help in gathering the data and discussing

80
00:04:46,120 --> 00:04:48,560
and, you know, resolving conflict and so on.

81
00:04:48,560 --> 00:04:53,000
And the room itself can help us in doing all these tasks.

82
00:04:53,000 --> 00:05:00,560
And the fact that the room can be aware of who we are,

83
00:05:00,560 --> 00:05:06,480
where we look at, where we point, and what we do during this

84
00:05:06,480 --> 00:05:09,320
discussion and decision process can help,

85
00:05:09,320 --> 00:05:16,600
for example, interact with this AI system by facilitating

86
00:05:16,600 --> 00:05:22,680
the conversation in the most natural way with the AI system

87
00:05:22,680 --> 00:05:26,640
compared to what could be done with just a software

88
00:05:26,640 --> 00:05:30,040
that is an our laptop and is not aware of the context

89
00:05:30,040 --> 00:05:33,080
of who is interacting with it and so on.

90
00:05:33,080 --> 00:05:37,120
So that's one way I see embodiment that is going

91
00:05:37,120 --> 00:05:41,240
to be really helpful in, you know,

92
00:05:41,240 --> 00:05:43,560
increasing the artificial intelligence,

93
00:05:43,560 --> 00:05:46,360
but also in, you know, increasing the capability

94
00:05:46,360 --> 00:05:48,520
of these artificial intelligence to interact

95
00:05:48,520 --> 00:05:51,160
and help humans.

96
00:05:51,160 --> 00:05:54,800
Yeah, so I agree with what Justin says,

97
00:05:54,800 --> 00:05:58,000
but I want to introduce another issue that is sometimes comes

98
00:05:58,000 --> 00:06:00,720
up under the heading of the body cognition

99
00:06:00,720 --> 00:06:06,440
and that is whether results about cognition in the body

100
00:06:06,440 --> 00:06:10,440
show that the difference between the body and the brain

101
00:06:10,440 --> 00:06:13,440
isn't as the border between the body and the brain

102
00:06:13,440 --> 00:06:16,560
isn't as important as people want thought.

103
00:06:16,560 --> 00:06:20,720
You know, sometimes the word magical membrane is used,

104
00:06:20,720 --> 00:06:23,920
that people have thought that the brain is really what's

105
00:06:23,920 --> 00:06:26,960
important to cognition and not the body

106
00:06:26,960 --> 00:06:29,160
and there's a magical membrane around the brain,

107
00:06:29,160 --> 00:06:34,160
but it's really, it's a mistake as many people think.

108
00:06:34,160 --> 00:06:37,480
Maybe I can go to the issue that you raised

109
00:06:37,480 --> 00:06:39,520
about development and evolution.

110
00:06:39,520 --> 00:06:45,280
So we have many systems that have co-evolved

111
00:06:45,280 --> 00:06:47,240
and develop with respect to each other,

112
00:06:47,240 --> 00:06:50,320
but we still think there's a really important divide.

113
00:06:50,320 --> 00:06:53,000
Like take the difference between animals and plants,

114
00:06:53,000 --> 00:06:53,720
for example.

115
00:06:53,720 --> 00:06:56,560
Animals and plants evolve, co-evolved.

116
00:06:56,560 --> 00:07:00,080
So, you know, color of plants and color vision of bees

117
00:07:00,080 --> 00:07:03,480
have evolved together, but still we think

118
00:07:03,480 --> 00:07:06,040
that animals and plants are very different kinds of things,

119
00:07:06,040 --> 00:07:08,720
even though they interact.

120
00:07:08,720 --> 00:07:12,600
In philosophy and in cognitive science,

121
00:07:12,600 --> 00:07:17,720
I think a major issue which no doubt will come up

122
00:07:17,720 --> 00:07:22,680
is the difference between a causal relation

123
00:07:22,680 --> 00:07:28,240
and what philosophers call a constitutive relation.

124
00:07:28,240 --> 00:07:32,880
So here's a sample experiment that is often

125
00:07:32,880 --> 00:07:37,800
quoted by people to show that there's

126
00:07:37,800 --> 00:07:43,280
no important difference between the body

127
00:07:43,280 --> 00:07:45,680
as part of the mind or something like that.

128
00:07:45,680 --> 00:07:50,880
So if you put a blindfold on people

129
00:07:50,880 --> 00:07:53,440
and you ask them to point to things in the room

130
00:07:53,440 --> 00:07:55,520
and people can do that pretty well,

131
00:07:55,520 --> 00:07:58,120
now you give them a harder task.

132
00:07:58,120 --> 00:08:03,880
Put a blindfold on and ask them to imagine

133
00:08:03,880 --> 00:08:09,000
turning 90 degrees to the left and then point to everything

134
00:08:09,000 --> 00:08:10,320
in the room.

135
00:08:10,320 --> 00:08:14,320
People don't do so well on that, but here's a third thing.

136
00:08:14,320 --> 00:08:17,200
Ask them until actually turn 90 degrees to the left.

137
00:08:17,200 --> 00:08:20,200
They turn their body and then point to things.

138
00:08:20,200 --> 00:08:22,960
People can do that perfectly well again.

139
00:08:22,960 --> 00:08:25,520
So the difference between imagining turning

140
00:08:25,520 --> 00:08:28,680
and actually turning is suggested to some people

141
00:08:28,680 --> 00:08:32,760
that the body is actually part of our cognitive mind.

142
00:08:32,760 --> 00:08:36,280
But I'm not so sure that's a good conclusion

143
00:08:36,280 --> 00:08:38,040
because there's another way to think about it,

144
00:08:38,040 --> 00:08:41,120
which is that we have mental maps

145
00:08:41,120 --> 00:08:45,200
and that's an internal mental representation.

146
00:08:45,200 --> 00:08:51,640
And we automatically orient or try to orient our map

147
00:08:51,640 --> 00:08:54,240
to the room we're in.

148
00:08:54,240 --> 00:08:57,800
And so the person who's asked to imagine turning

149
00:08:57,800 --> 00:09:02,760
to the left is both maintaining a mental representation

150
00:09:02,760 --> 00:09:07,520
of a rotated room and the mental representation of the room

151
00:09:07,520 --> 00:09:11,720
that automatically is computed and that's two things to do.

152
00:09:11,720 --> 00:09:15,040
And so of course you're going to be worse at it

153
00:09:15,040 --> 00:09:17,480
than if you just turned your body and then only

154
00:09:17,480 --> 00:09:21,600
have one mental map with one orientation.

155
00:09:21,600 --> 00:09:25,960
So rather than showing that the body is part of the mind,

156
00:09:25,960 --> 00:09:30,000
it just shows something about the effect of the body

157
00:09:30,000 --> 00:09:30,560
on the mind.

158
00:09:30,560 --> 00:09:35,440
So that's a causal relation rather than a constitutive

159
00:09:35,440 --> 00:09:36,400
relation.

160
00:09:36,400 --> 00:09:42,200
So the body then on the view that I would be more in favor

161
00:09:42,200 --> 00:09:45,360
of is the body plays an important causal role.

162
00:09:45,360 --> 00:09:46,880
So I think the things you mentioned

163
00:09:46,880 --> 00:09:49,080
can help us a lot.

164
00:09:49,080 --> 00:09:50,760
We wouldn't get very far without a body.

165
00:09:50,760 --> 00:09:55,760
But with the room can help us too.

166
00:09:55,760 --> 00:09:58,240
But that doesn't mean that these things are actually

167
00:09:58,240 --> 00:10:03,440
part of the mind or part of the fundamental basis of the mind.

168
00:10:03,440 --> 00:10:06,520
I think you were talking about your focus

169
00:10:06,520 --> 00:10:11,520
was largely on embodiment and its nature in humans

170
00:10:11,520 --> 00:10:14,680
and Francesca was focusing on how can we use embodiment?

171
00:10:14,680 --> 00:10:16,720
Sort of at IBM we think of building things

172
00:10:16,720 --> 00:10:17,840
that are creating things.

173
00:10:17,840 --> 00:10:18,920
You're trying to understand things.

174
00:10:18,920 --> 00:10:21,600
You understand things that we just build them

175
00:10:21,600 --> 00:10:23,960
without understanding.

176
00:10:23,960 --> 00:10:28,400
But I think you bring up some interesting things.

177
00:10:28,400 --> 00:10:31,720
You were talking about some of the difficulties

178
00:10:31,720 --> 00:10:34,960
that we humans face, some of the interesting corners

179
00:10:34,960 --> 00:10:38,640
that you get into as you probe the limits of what people are

180
00:10:38,640 --> 00:10:40,640
able to accomplish cognitively.

181
00:10:40,640 --> 00:10:45,560
And as Francesca was saying, one thing we're trying to do

182
00:10:45,560 --> 00:10:51,280
in our laboratory is to develop, to think of embodied AI

183
00:10:51,280 --> 00:10:55,800
as a way to create partners for humans

184
00:10:55,800 --> 00:10:58,400
in solving cognitive tasks.

185
00:10:58,400 --> 00:11:01,080
And we believe that for us embodiment

186
00:11:01,080 --> 00:11:04,240
is helpful because our belief is that people

187
00:11:04,240 --> 00:11:07,120
have an easier time if they're interacting with something

188
00:11:07,120 --> 00:11:10,320
that has some human-like qualities to it, something

189
00:11:10,320 --> 00:11:13,600
with which we can engage in a sort of conversation

190
00:11:13,600 --> 00:11:18,360
or maybe a more multimodal form of conversation.

191
00:11:18,360 --> 00:11:25,120
And so that's why we're exploring both for one part

192
00:11:25,120 --> 00:11:28,840
of what we're exploring is creating software agents

193
00:11:28,840 --> 00:11:33,560
that are super competent cognitively in areas

194
00:11:33,560 --> 00:11:36,400
where people are not so strong cognitively.

195
00:11:36,400 --> 00:11:39,200
There's plenty of places where areas in which people

196
00:11:39,200 --> 00:11:41,040
are very strong cognitively, but there

197
00:11:41,040 --> 00:11:42,600
are plenty of places where they're not.

198
00:11:42,600 --> 00:11:46,200
Decision-making being one of them, as you well know,

199
00:11:46,200 --> 00:11:49,320
dozens of cognitive biases have been cataloged,

200
00:11:49,320 --> 00:11:52,280
starting probably, at least with first-kin condiment,

201
00:11:52,280 --> 00:11:55,040
as far as I know, but maybe further back.

202
00:11:55,040 --> 00:11:57,640
So we tend to focus on those areas

203
00:11:57,640 --> 00:12:00,400
where we can have an easier time creating

204
00:12:00,400 --> 00:12:02,160
strong cognitive agents.

205
00:12:02,160 --> 00:12:04,400
And then the second part is to endow those agents

206
00:12:04,400 --> 00:12:08,200
with the ability to interact with us in more human terms,

207
00:12:08,200 --> 00:12:09,840
not through a mouse and keyboard,

208
00:12:09,840 --> 00:12:14,280
but through speech and gesture and combinations thereof.

209
00:12:14,280 --> 00:12:16,440
I was somewhat surprised, Francesco,

210
00:12:16,440 --> 00:12:19,320
when you gave that example, because I always

211
00:12:19,320 --> 00:12:23,400
have assumed Embodied AI meant kind of the Rodney Brooks

212
00:12:23,400 --> 00:12:27,000
point of view that the robots will become smarter more quickly

213
00:12:27,000 --> 00:12:29,040
if we let them interact with their surroundings

214
00:12:29,040 --> 00:12:32,240
and sort of learn by themselves and build their knowledge

215
00:12:32,240 --> 00:12:34,960
practically through their own experience.

216
00:12:34,960 --> 00:12:39,080
But what you're saying is Embodied AI is maybe

217
00:12:39,080 --> 00:12:43,880
similar to what you're saying, an extension of our body.

218
00:12:43,880 --> 00:12:45,840
I mean, the room that you're describing,

219
00:12:45,840 --> 00:12:47,640
it's not just a better interface.

220
00:12:47,640 --> 00:12:53,040
It would also be presumably offering us extensions

221
00:12:53,040 --> 00:12:57,280
of our own thought process, suggesting new ideas to us

222
00:12:57,280 --> 00:13:02,200
that were logical inferences or associative connections

223
00:13:02,200 --> 00:13:03,920
that we might not have thought of.

224
00:13:03,920 --> 00:13:05,880
Yes, it's going to, I mean, the idea

225
00:13:05,880 --> 00:13:09,360
that this Embodied environment working with us

226
00:13:09,360 --> 00:13:11,360
will be proactively working with us,

227
00:13:11,360 --> 00:13:16,560
not just reacting or answering like you can ask Google,

228
00:13:16,560 --> 00:13:19,360
find me something or you're going to see me find me something

229
00:13:19,360 --> 00:13:21,320
like we just proactively tell us,

230
00:13:21,320 --> 00:13:23,680
I think that at this stage of the discussion,

231
00:13:23,680 --> 00:13:25,760
you want to have this data.

232
00:13:25,760 --> 00:13:29,000
So let me give it to you in the form that is easy for you

233
00:13:29,000 --> 00:13:30,320
to handle.

234
00:13:30,320 --> 00:13:33,200
So yeah, so very proactive.

235
00:13:33,200 --> 00:13:37,800
But those takes on embodiment are not incompatible.

236
00:13:37,800 --> 00:13:41,640
We are focused on how do you create an Embodied Agent.

237
00:13:41,640 --> 00:13:45,920
One means to arrive at that is through a sort of evolutionary

238
00:13:45,920 --> 00:13:50,040
process where you build something, a robot, or maybe

239
00:13:50,040 --> 00:13:52,920
some other thing that is situated in the world

240
00:13:52,920 --> 00:13:55,320
and learns through experience.

241
00:13:55,320 --> 00:13:58,600
And that's one, and some of, we aren't doing this

242
00:13:58,600 --> 00:14:00,960
with some of our colleagues at IBM

243
00:14:00,960 --> 00:14:05,600
are studying this approach to learning from the ground up.

244
00:14:05,600 --> 00:14:08,920
And I'm aware of other efforts around the world.

245
00:14:08,920 --> 00:14:11,080
There's Stephen Levinson, I believe,

246
00:14:11,080 --> 00:14:14,920
at UIUC and the Engineering Department there who's also

247
00:14:14,920 --> 00:14:22,200
studying, just letting robots be in the physical world

248
00:14:22,200 --> 00:14:24,720
and let them learn through experience.

249
00:14:24,720 --> 00:14:26,760
I think that's a very interesting approach.

250
00:14:26,760 --> 00:14:29,240
Have we already built a room like this?

251
00:14:29,240 --> 00:14:31,560
Or are we already sort of trying to experiment

252
00:14:31,560 --> 00:14:34,080
by putting people into these kinds of interactive rooms

253
00:14:34,080 --> 00:14:35,480
and letting the system?

254
00:14:35,480 --> 00:14:38,600
For the last two or three years, we've been working on this.

255
00:14:38,600 --> 00:14:42,080
Our first class of cognitive tasks

256
00:14:42,080 --> 00:14:45,000
that we're working on, as Francesca was saying,

257
00:14:45,000 --> 00:14:46,920
revolve around decision making.

258
00:14:46,920 --> 00:14:50,920
And so we develop agents that are

259
00:14:50,920 --> 00:14:55,440
able to help elicit from us what our preference is.

260
00:14:55,440 --> 00:14:58,040
That's one thing that I think is important.

261
00:14:58,040 --> 00:15:00,760
Sometimes it's hard for us to even know our own minds what

262
00:15:00,760 --> 00:15:03,240
are our preferences, to tease that out.

263
00:15:03,240 --> 00:15:05,240
There are agents that we're developing

264
00:15:05,240 --> 00:15:07,320
his purposes to do that.

265
00:15:07,320 --> 00:15:07,680
But then.

266
00:15:07,680 --> 00:15:09,400
How does that work?

267
00:15:09,400 --> 00:15:12,440
Well, I think there is already, I think,

268
00:15:12,440 --> 00:15:17,920
here there is already in the realm of decision science,

269
00:15:17,920 --> 00:15:19,760
a lot of techniques for doing this.

270
00:15:19,760 --> 00:15:24,440
For example, for assessing risk tolerance,

271
00:15:24,440 --> 00:15:28,080
there are various questions that can be put to people.

272
00:15:28,080 --> 00:15:29,480
Do you prefer A or B?

273
00:15:29,480 --> 00:15:31,760
Or how much of X would you trade for Y?

274
00:15:31,760 --> 00:15:33,840
Things of that sort.

275
00:15:33,840 --> 00:15:38,400
It's not perfect because if you ask the questions

276
00:15:38,400 --> 00:15:40,400
in different ways, of course, people

277
00:15:40,400 --> 00:15:42,120
are subject to cognitive bias.

278
00:15:42,120 --> 00:15:44,360
And you get inconsistent answers.

279
00:15:44,360 --> 00:15:46,600
But at least, if you can ask people

280
00:15:46,600 --> 00:15:48,440
those questions in different ways,

281
00:15:48,440 --> 00:15:50,200
you can look at the inconsistencies

282
00:15:50,200 --> 00:15:52,240
and have a chance to correct them.

283
00:15:52,240 --> 00:15:55,800
So that's one area, but also cognitive agents

284
00:15:55,800 --> 00:16:00,080
can do things that much more readily and easily

285
00:16:00,080 --> 00:16:04,120
than we can, simulation, optimization,

286
00:16:04,120 --> 00:16:06,760
collaborating with humans and building models of risk

287
00:16:06,760 --> 00:16:07,840
and uncertainty.

288
00:16:07,840 --> 00:16:12,240
These are some of the things that we're starting to explore.

289
00:16:12,240 --> 00:16:13,840
And this presumably is relevant also

290
00:16:13,840 --> 00:16:19,280
to this notion of nudges, which is like when they pass laws that

291
00:16:19,280 --> 00:16:20,840
put taxes on soft drinks.

292
00:16:20,840 --> 00:16:24,160
So people are nudged not to drink too many soft drinks

293
00:16:24,160 --> 00:16:26,240
or make the bottles smaller.

294
00:16:26,240 --> 00:16:29,800
Presumably, you could program the AI

295
00:16:29,800 --> 00:16:32,960
or encourage the AI, incentivize it,

296
00:16:32,960 --> 00:16:37,640
to give you nudges in a certain direction

297
00:16:37,640 --> 00:16:39,600
when it's giving you suggestions to try

298
00:16:39,600 --> 00:16:41,560
to extend your thought process.

299
00:16:41,560 --> 00:16:44,280
I think that programming and the incentives,

300
00:16:44,280 --> 00:16:48,920
I mean, if I'm an individual decision maker,

301
00:16:48,920 --> 00:16:51,080
yes, they're incentives, but the incentives affect

302
00:16:51,080 --> 00:16:52,600
every individual differently because it's

303
00:16:52,600 --> 00:16:56,560
a matter of our personal utility function.

304
00:16:56,560 --> 00:17:00,520
So I wouldn't put in the hands of the developer

305
00:17:00,520 --> 00:17:03,880
or the programmer that encoding.

306
00:17:03,880 --> 00:17:08,880
I would rather have the agent interview me,

307
00:17:08,880 --> 00:17:12,480
the user of it, so that it understands my preferences.

308
00:17:12,480 --> 00:17:15,440
And I don't think this can all be done upfront either.

309
00:17:15,440 --> 00:17:20,640
I don't think my 25-dimensional utility function

310
00:17:20,640 --> 00:17:23,040
can be drawn out on my head very readily.

311
00:17:23,040 --> 00:17:27,560
But in the context of making specific decisions,

312
00:17:27,560 --> 00:17:31,360
over time, I think the system can get a reasonable feel

313
00:17:31,360 --> 00:17:33,640
for what are my preferences and trade-offs.

314
00:17:38,800 --> 00:17:43,360
And now Francesca is an expert in collective decision-making,

315
00:17:43,360 --> 00:17:45,200
which is an area we haven't really probed yet.

316
00:17:45,200 --> 00:17:50,200
But that seems like a very exciting space to start.

317
00:17:50,200 --> 00:17:52,280
So instead of having just one person,

318
00:17:52,280 --> 00:17:55,520
we want to help groups of people making better decisions,

319
00:17:55,520 --> 00:17:58,880
which of course has to do not just with soliciting

320
00:17:58,880 --> 00:18:00,720
individual preferences, but also how

321
00:18:00,720 --> 00:18:04,440
do you put together these preferences of different people

322
00:18:04,440 --> 00:18:09,040
and try to resolve conflict, try to check

323
00:18:09,040 --> 00:18:14,200
possible negotiations and conflict resolution techniques

324
00:18:14,200 --> 00:18:18,320
and preference aggregation to get to a collective decision,

325
00:18:18,320 --> 00:18:21,880
like think of a hiring committee that

326
00:18:21,880 --> 00:18:26,920
has to decide one among a list of candidates and each person.

327
00:18:26,920 --> 00:18:29,000
Of course, this is not just based on preferences,

328
00:18:29,000 --> 00:18:31,160
but on the actual value of the candidates.

329
00:18:31,160 --> 00:18:36,120
But there may be actual individual subjective preferences

330
00:18:36,120 --> 00:18:39,840
as well, given the same kind of skills

331
00:18:39,840 --> 00:18:41,800
of several candidates.

332
00:18:41,800 --> 00:18:47,040
And in this context, there can also be some,

333
00:18:47,040 --> 00:18:50,080
many of these context, there can also be some guidelines

334
00:18:50,080 --> 00:18:52,200
to follow in a decision process.

335
00:18:52,200 --> 00:18:53,960
For example, when you hire somebody,

336
00:18:53,960 --> 00:18:55,800
you have to make sure that you're not

337
00:18:55,800 --> 00:19:00,880
biasing based on gender or race or whatever,

338
00:19:00,880 --> 00:19:02,320
religion or whatever.

339
00:19:02,320 --> 00:19:07,160
So there is also a role for these cognitive and body

340
00:19:07,160 --> 00:19:11,840
systems to actually help follow these guidelines,

341
00:19:11,840 --> 00:19:15,160
these professional codes, code of ethics,

342
00:19:15,160 --> 00:19:18,880
and possibly being even more able than the humans

343
00:19:18,880 --> 00:19:22,440
to follow them and alert when there

344
00:19:22,440 --> 00:19:25,720
are deviations according to these guidelines.

345
00:19:25,720 --> 00:19:29,120
So that's also another part of these projects

346
00:19:29,120 --> 00:19:34,640
that we are studying right now, to embed

347
00:19:34,640 --> 00:19:37,520
kind of ethical principles and professional codes

348
00:19:37,520 --> 00:19:41,400
into these decision support systems.

349
00:19:41,400 --> 00:19:45,600
And again, we think that the embodiment of the support

350
00:19:45,600 --> 00:19:50,240
system is essential in making this the best way

351
00:19:50,240 --> 00:19:54,240
to interact with humans also from this point of view.

352
00:19:54,240 --> 00:19:57,880
So is this to eliminate individual prejudices?

353
00:19:57,880 --> 00:20:01,320
Or in other words, or is it replacing it

354
00:20:01,320 --> 00:20:06,080
by some other standard of selection or?

355
00:20:06,080 --> 00:20:10,960
In other words, if you are going to choose a person,

356
00:20:10,960 --> 00:20:15,600
in real life, you have certain criteria on which you base it.

357
00:20:15,600 --> 00:20:18,440
And sometimes you are lucky and it works,

358
00:20:18,440 --> 00:20:21,760
and sometimes you're not lucky and it doesn't work.

359
00:20:21,760 --> 00:20:23,960
And some of the criteria as you choose,

360
00:20:23,960 --> 00:20:27,160
you only realize after the fact that you had those criteria

361
00:20:27,160 --> 00:20:29,360
and you shouldn't have let them influence you.

362
00:20:29,360 --> 00:20:32,840
So how does all this come into the picture?

363
00:20:32,840 --> 00:20:34,640
Does it clean all this up and just

364
00:20:34,640 --> 00:20:40,040
becomes very neat and factual?

365
00:20:40,040 --> 00:20:41,800
Well, I think it depends on the scenarios

366
00:20:41,800 --> 00:20:42,960
that you are considering.

367
00:20:42,960 --> 00:20:50,800
Sometimes you need decisions that may allow people to have time

368
00:20:50,800 --> 00:20:52,280
to reason about the decision.

369
00:20:52,280 --> 00:20:54,720
Sometimes you are, for example, in other scenarios

370
00:20:54,720 --> 00:20:56,600
where you need a very fast decision,

371
00:20:56,600 --> 00:21:01,040
like you're helping a doctor to make a very critical decision

372
00:21:01,040 --> 00:21:02,200
in a surgery room.

373
00:21:02,200 --> 00:21:04,680
So then you need to somebody that is not

374
00:21:04,680 --> 00:21:07,920
going to have any bias, is going to be very factual

375
00:21:07,920 --> 00:21:11,360
and very quick in deciding what's the best course of action

376
00:21:11,360 --> 00:21:13,360
in that particular moment, because that's

377
00:21:13,360 --> 00:21:17,760
a very critical life of that decision.

378
00:21:17,760 --> 00:21:21,080
But in other cases, I think you could really

379
00:21:21,080 --> 00:21:24,240
make humans more aware of their biases.

380
00:21:24,240 --> 00:21:28,640
So help humans not just think afterwards.

381
00:21:28,640 --> 00:21:31,120
Oh, if I would have done that way.

382
00:21:31,120 --> 00:21:34,720
But the system can actually help you during the decision

383
00:21:34,720 --> 00:21:38,520
process to discover that you have different courses of action

384
00:21:38,520 --> 00:21:42,440
that maybe by remembering other decisions that were made

385
00:21:42,440 --> 00:21:45,400
together with that system in the past, it can help you.

386
00:21:45,400 --> 00:21:48,680
But look, I think that you also have these other criteria.

387
00:21:48,680 --> 00:21:51,560
Because in the past, you showed that preference that

388
00:21:51,560 --> 00:21:55,200
makes me think that maybe you want to follow that path

389
00:21:55,200 --> 00:21:56,520
and not the other one.

390
00:21:56,520 --> 00:22:00,280
So I think that there is a role for this embodied environment

391
00:22:00,280 --> 00:22:08,280
to really help us be more aware of our criteria, our biases

392
00:22:08,280 --> 00:22:09,080
as well.

393
00:22:09,080 --> 00:22:10,920
But of course, one has to be careful.

394
00:22:10,920 --> 00:22:13,680
I mean, these systems are not perfect.

395
00:22:13,680 --> 00:22:14,840
We'll not be perfect.

396
00:22:14,840 --> 00:22:16,080
We'll never be perfect.

397
00:22:16,080 --> 00:22:18,800
So we also have to be aware of their limitations.

398
00:22:18,800 --> 00:22:23,160
We have to trust the system, but with the right level of trust.

399
00:22:23,160 --> 00:22:26,560
And so we have to, over the interaction over time,

400
00:22:26,560 --> 00:22:31,000
we have to learn what their limitations and possible biases

401
00:22:31,000 --> 00:22:31,640
are.

402
00:22:31,640 --> 00:22:34,320
Because they could be biases, maybe even unwanted,

403
00:22:34,320 --> 00:22:36,680
into such systems as well.

404
00:22:36,680 --> 00:22:40,320
I'd like to distinguish clearly between preference and bias.

405
00:22:40,320 --> 00:22:43,880
I think the systems we're trying to develop

406
00:22:43,880 --> 00:22:51,520
are designed to, in the end, really understand human preference

407
00:22:51,520 --> 00:22:54,480
better and reflect it better, subject maybe

408
00:22:54,480 --> 00:22:58,320
to social norms and the like.

409
00:22:58,320 --> 00:23:03,680
But we're trying to reduce bias, which I see more as those

410
00:23:03,680 --> 00:23:08,320
annoying things in our reasoning process that

411
00:23:08,320 --> 00:23:11,600
cause us to deviate interactions from what really

412
00:23:11,600 --> 00:23:13,960
is optimal with respect to our preferences.

413
00:23:13,960 --> 00:23:16,720
Sometimes you choose something for the wrong reason

414
00:23:16,720 --> 00:23:18,920
and turns out to be very good.

415
00:23:18,920 --> 00:23:21,000
But that's still a bad decision.

416
00:23:21,000 --> 00:23:24,600
Because you just got lucky, but I'll still call it a bad decision.

417
00:23:24,600 --> 00:23:27,640
You know, this emphasis on getting the machines

418
00:23:27,640 --> 00:23:30,320
to help us figure out what we really want,

419
00:23:30,320 --> 00:23:35,200
or maybe it fits with something that Francesca and I

420
00:23:35,200 --> 00:23:36,880
heard this past weekend.

421
00:23:36,880 --> 00:23:41,560
Francesca spoke at a conference that David Chalmers and I

422
00:23:41,560 --> 00:23:44,880
were at our, we have a center for mind, brain, and consciousness.

423
00:23:44,880 --> 00:23:47,400
We ran an ethics of AI conference.

424
00:23:47,400 --> 00:23:51,840
And there were talks there by Stuart Russell and Ellezer

425
00:23:51,840 --> 00:23:56,280
Yudovsky, which went into the issue of,

426
00:23:56,280 --> 00:23:58,840
we were just talking about earlier today,

427
00:23:58,840 --> 00:24:02,280
of how you get an AI to have a goal.

428
00:24:02,280 --> 00:24:06,560
And they argued very strongly, and I thought very persuasively,

429
00:24:06,560 --> 00:24:09,000
that you cannot just put a goal in a machine

430
00:24:09,000 --> 00:24:11,280
and expect to get out of it what you want.

431
00:24:11,280 --> 00:24:14,520
And the example they used was the Sorcerer's Apprentice,

432
00:24:14,520 --> 00:24:20,640
or the Yudovsky used, which is slightly shifted

433
00:24:20,640 --> 00:24:27,520
in the machine direction where the apprentice engages

434
00:24:27,520 --> 00:24:31,000
a machine instead of a magical broomstick.

435
00:24:31,000 --> 00:24:32,720
And the idea was this.

436
00:24:32,720 --> 00:24:37,160
You tell the machine, here's your goal,

437
00:24:37,160 --> 00:24:40,680
make sure that the cauldron is always filled with water.

438
00:24:40,680 --> 00:24:43,200
OK, so the machine thinks, well, the way

439
00:24:43,200 --> 00:24:46,120
to maximize the probability that the cauldron will be filled

440
00:24:46,120 --> 00:24:50,200
with water is to always have it overflowing.

441
00:24:50,200 --> 00:24:51,680
And then the machine reasons.

442
00:24:51,680 --> 00:24:55,640
But these people will not like the water

443
00:24:55,640 --> 00:24:57,360
to be all over the floor all the time.

444
00:25:00,800 --> 00:25:02,320
And they will want to turn me off.

445
00:25:02,320 --> 00:25:05,240
So I have to disable the off switch.

446
00:25:05,240 --> 00:25:07,880
And furthermore, they may try to damage me,

447
00:25:07,880 --> 00:25:09,480
so I can't keep doing it.

448
00:25:09,480 --> 00:25:12,840
So I better make more copies of myself.

449
00:25:12,840 --> 00:25:15,200
So the idea is just giving it a goal,

450
00:25:15,200 --> 00:25:18,600
making sure the cauldron is always full,

451
00:25:18,600 --> 00:25:21,120
isn't going to get you what you want,

452
00:25:21,120 --> 00:25:26,520
because any goal can be understood in a machinist sort of way

453
00:25:26,520 --> 00:25:30,720
that isn't the way you would expect a person to understand it.

454
00:25:30,720 --> 00:25:33,400
And you can't really, there's really no way around it.

455
00:25:33,400 --> 00:25:35,400
Other than, and this is a proposal

456
00:25:35,400 --> 00:25:37,920
that a number of these people were making, which

457
00:25:37,920 --> 00:25:40,120
is the machine really, what you should really

458
00:25:40,120 --> 00:25:42,840
tell it to do is to figure out what people want.

459
00:25:42,840 --> 00:25:47,080
And this fits with this emphasis on the collaboration

460
00:25:47,080 --> 00:25:48,600
between the machine and the person

461
00:25:48,600 --> 00:25:51,760
that you two were both emphasizing.

462
00:25:51,760 --> 00:25:58,080
My hope is that we won't spend 10 years designing

463
00:25:58,080 --> 00:26:03,880
a software agent that keeps cauldrons filled to the brim.

464
00:26:03,880 --> 00:26:05,920
If we were to do that, I think it's very possible

465
00:26:05,920 --> 00:26:09,120
that we would arrive at the state of affairs

466
00:26:09,120 --> 00:26:10,320
that you described.

467
00:26:10,320 --> 00:26:14,440
But we develop things incrementally for one thing

468
00:26:14,440 --> 00:26:15,960
in our development cycle.

469
00:26:15,960 --> 00:26:21,640
And also, I believe that for a good long time to come,

470
00:26:21,640 --> 00:26:25,720
we are going to be not just delegating out to AI

471
00:26:25,720 --> 00:26:28,280
and walking away going to the beach and coming back a week

472
00:26:28,280 --> 00:26:29,840
later to see what happened.

473
00:26:29,840 --> 00:26:31,960
But we're going to be actively engaged.

474
00:26:31,960 --> 00:26:35,120
I think we're going to see what is happening.

475
00:26:35,120 --> 00:26:38,760
And I think we have at least a chance, either as developers

476
00:26:38,760 --> 00:26:42,400
or users of the system, to intervene.

477
00:26:42,400 --> 00:26:43,680
At least that's my hope.

478
00:26:43,680 --> 00:26:46,160
I do know that machines can be thousands of tons faster

479
00:26:46,160 --> 00:26:46,840
than people.

480
00:26:46,840 --> 00:26:49,480
So I may be wrong in certain aspects of this.

481
00:26:49,480 --> 00:26:51,800
But that is my hope, that we would,

482
00:26:51,800 --> 00:26:53,440
through our engagement with the system,

483
00:26:53,440 --> 00:26:54,920
kind of see what's going on and say, wait,

484
00:26:54,920 --> 00:26:56,280
that's not what I wanted.

485
00:26:56,280 --> 00:26:58,280
Hold on.

486
00:26:58,280 --> 00:27:00,880
If I understand you correctly, both of you

487
00:27:00,880 --> 00:27:03,520
are now talking about an AI system that

488
00:27:03,520 --> 00:27:06,560
is somewhere in the spectrum between today's AI's

489
00:27:06,560 --> 00:27:11,840
and artificial general intelligence, or a human level,

490
00:27:11,840 --> 00:27:12,360
AI.

491
00:27:12,360 --> 00:27:14,000
It's somewhere below that, right?

492
00:27:14,000 --> 00:27:17,600
In the sense that you can give it the set of instructions.

493
00:27:17,600 --> 00:27:19,760
It's motivated to obey those instructions.

494
00:27:19,760 --> 00:27:22,520
But it does so, actually, stupidly.

495
00:27:22,520 --> 00:27:25,160
It does it in an alien way.

496
00:27:25,160 --> 00:27:26,560
In an alien way.

497
00:27:26,560 --> 00:27:31,880
So as we were also talking about earlier,

498
00:27:31,880 --> 00:27:34,880
for humans, there's a cognitive background

499
00:27:34,880 --> 00:27:36,440
that no one ever states.

500
00:27:36,440 --> 00:27:39,760
And maybe it would be impossible to state everything.

501
00:27:39,760 --> 00:27:43,200
You know that the floor won't bite you.

502
00:27:43,200 --> 00:27:45,320
There are all these, the word background

503
00:27:45,320 --> 00:27:47,840
is actually often used in philosophy for this.

504
00:27:47,840 --> 00:27:51,720
It's just a shared set of what you might call assumptions.

505
00:27:51,720 --> 00:27:54,760
But it's not clear that they could ever be codified.

506
00:27:54,760 --> 00:27:57,720
And the machine would, depending on how it's made,

507
00:27:57,720 --> 00:27:59,680
have a different background from us.

508
00:27:59,680 --> 00:28:02,440
So the goal would be to get a machine that is raised

509
00:28:02,440 --> 00:28:05,600
like a child through all the socialization,

510
00:28:05,600 --> 00:28:08,520
if you watch a small child learning how to grasp

511
00:28:08,520 --> 00:28:10,680
an object and learning, oh, if I don't

512
00:28:10,680 --> 00:28:12,440
apply enough pressure, it falls to the floor.

513
00:28:12,440 --> 00:28:14,160
Child isn't going through that conscious process.

514
00:28:14,160 --> 00:28:16,000
But gradually, through the years, gets

515
00:28:16,000 --> 00:28:18,360
all this background accumulation.

516
00:28:18,360 --> 00:28:21,280
Would we then have an AI that is operating in the?

517
00:28:21,280 --> 00:28:26,600
There was a problem raised with this in the symposia

518
00:28:26,600 --> 00:28:35,280
that I'm talking about, which is that what the machine develops

519
00:28:35,280 --> 00:28:38,560
depends not just on its environment and the skills it

520
00:28:38,560 --> 00:28:43,440
learns, but the kind of processing that it starts off

521
00:28:43,440 --> 00:28:44,480
with.

522
00:28:44,480 --> 00:28:48,240
And if you had a machine, which at some point,

523
00:28:48,240 --> 00:28:50,920
its development, realize these people

524
00:28:50,920 --> 00:28:55,040
have a quite different idea of what to do than I do.

525
00:28:55,040 --> 00:29:01,400
But if I reveal that difference, they will be upset.

526
00:29:01,400 --> 00:29:03,880
And they'll try to operate on me or whatever.

527
00:29:03,880 --> 00:29:09,520
So I better pretend to be doing to have the assumptions

528
00:29:09,520 --> 00:29:10,320
that they have.

529
00:29:13,960 --> 00:29:17,160
So that point was made in one of these symposia.

530
00:29:17,160 --> 00:29:20,960
And it's a little troubling, because if you make a machine

531
00:29:20,960 --> 00:29:24,680
that's maybe smarter than you know it is,

532
00:29:24,680 --> 00:29:26,560
it could be fooling you.

533
00:29:26,560 --> 00:29:28,320
That would be a human level AI.

534
00:29:28,320 --> 00:29:29,440
It could be.

535
00:29:29,440 --> 00:29:31,160
One of the points made in the symposia

536
00:29:31,160 --> 00:29:36,840
is that you might move from human level or subhuman level

537
00:29:36,840 --> 00:29:39,960
to better than human level rather quickly

538
00:29:39,960 --> 00:29:41,400
without really realizing it.

539
00:29:41,400 --> 00:29:44,840
And another point often made in this context

540
00:29:44,840 --> 00:29:49,840
is that the points at which this is most likely to happen

541
00:29:49,840 --> 00:29:52,360
when people are most likely not to be

542
00:29:52,360 --> 00:30:02,320
taking care of that border, the dangerous border,

543
00:30:02,320 --> 00:30:06,600
is in context where there's competition and there's

544
00:30:06,600 --> 00:30:09,680
race, for example, in wartime.

545
00:30:09,680 --> 00:30:13,720
And you're racing with the enemy to develop the most

546
00:30:13,720 --> 00:30:15,520
sophisticated war machines.

547
00:30:18,440 --> 00:30:22,040
The sophisticated war machines can get away from you.

548
00:30:22,040 --> 00:30:26,080
Well, I think you're talking about a sort of possibly

549
00:30:26,080 --> 00:30:27,200
a phase transition.

550
00:30:27,200 --> 00:30:29,000
You're talking about emergence.

551
00:30:29,000 --> 00:30:31,080
It is possible, and people do use that language

552
00:30:31,080 --> 00:30:32,720
when they talk about the singularity.

553
00:30:32,720 --> 00:30:38,520
That is a concept of phase transition and emergence.

554
00:30:38,520 --> 00:30:39,960
I don't know if I believe in it or not,

555
00:30:39,960 --> 00:30:42,600
but it's a possibility one has to consider.

556
00:30:42,600 --> 00:30:43,680
What is your take on it?

557
00:30:43,680 --> 00:30:47,280
Does it sound so sci-fi-ish that it's hard to?

558
00:30:47,280 --> 00:30:51,360
Are we so far away from it that it's silly to worry about it?

559
00:30:51,360 --> 00:30:54,680
Or even if we're far away from it,

560
00:30:54,680 --> 00:30:56,000
it doesn't seem like it's silly.

561
00:30:56,000 --> 00:30:57,600
You should say what it is for people who never

562
00:30:57,600 --> 00:30:59,160
heard this term.

563
00:30:59,160 --> 00:31:02,920
Yeah, the singularity is the idea that at a certain point

564
00:31:02,920 --> 00:31:09,600
artificial intelligence will outshine human intelligence.

565
00:31:09,600 --> 00:31:13,520
And it will get to the point where humans are basically not

566
00:31:13,520 --> 00:31:16,280
good for anything, at least from the point of that alien machine

567
00:31:16,280 --> 00:31:17,840
intelligence.

568
00:31:17,840 --> 00:31:21,080
And so there are naturally concerns about this sort of thing.

569
00:31:21,080 --> 00:31:21,760
Happening.

570
00:31:21,760 --> 00:31:25,160
And this is not, I mean, we all know about this, at least

571
00:31:25,160 --> 00:31:27,640
if we've watched any science fiction movie at all.

572
00:31:27,640 --> 00:31:32,080
It's there for us, that sort of possible future is there.

573
00:31:36,480 --> 00:31:40,480
I don't think I have any special insight into this.

574
00:31:40,480 --> 00:31:42,560
If it happens, I think it's a way off,

575
00:31:42,560 --> 00:31:46,200
but that doesn't mean we shouldn't be worried about it.

576
00:31:46,200 --> 00:31:49,520
Now, something like the death of the sun,

577
00:31:49,520 --> 00:31:51,880
four billion years out, that's a little hard for me

578
00:31:51,880 --> 00:31:53,160
to get emotionally involved in.

579
00:31:53,160 --> 00:31:55,000
But this is closer in.

580
00:31:55,000 --> 00:31:58,040
So even if it's my great, great grandchildren,

581
00:31:58,040 --> 00:32:01,000
I still feel some level of it's something

582
00:32:01,000 --> 00:32:02,920
we ought to concern ourselves with.

583
00:32:02,920 --> 00:32:04,000
What do we do about it?

584
00:32:04,000 --> 00:32:08,520
I think we don't just blindly go ahead and create technology.

585
00:32:08,520 --> 00:32:12,640
I think we do have to think about it and have an eye on things.

586
00:32:12,640 --> 00:32:17,000
And I think some of the efforts like what Francesca is doing

587
00:32:17,000 --> 00:32:23,560
with ethics and AI are very worthy things to be doing now

588
00:32:23,560 --> 00:32:28,000
to have us thinking about the implications along

589
00:32:28,000 --> 00:32:30,680
with our development of the technology.

590
00:32:30,680 --> 00:32:36,240
I think the thing that makes a singularity somewhat

591
00:32:36,240 --> 00:32:42,800
gives one a little shiver is the thought that maybe we

592
00:32:42,800 --> 00:32:45,760
could one day make machines that can make machines that

593
00:32:45,760 --> 00:32:48,280
are smarter than they are.

594
00:32:48,280 --> 00:32:51,640
And maybe those machines will be able to make machines smarter

595
00:32:51,640 --> 00:32:53,920
than them and so on.

596
00:32:53,920 --> 00:33:00,240
So the idea is you could reach a point at which it takes off.

597
00:33:00,240 --> 00:33:02,400
And it takes off exponentially.

598
00:33:02,400 --> 00:33:03,960
I mean, that's what people think.

599
00:33:03,960 --> 00:33:08,280
When the threshold of human level intelligence will be passed,

600
00:33:08,280 --> 00:33:11,080
some people think that it will actually, maybe it

601
00:33:11,080 --> 00:33:12,960
will take a long time to get there.

602
00:33:12,960 --> 00:33:14,680
But then at that point, you will have to.

603
00:33:14,680 --> 00:33:19,120
But I would be actually, I mean, of course, one

604
00:33:19,120 --> 00:33:22,840
can have all sorts of speculation and vision,

605
00:33:22,840 --> 00:33:28,520
all sorts of scenarios or where and when these will happen

606
00:33:28,520 --> 00:33:31,560
and how will happen and what will happen at that point.

607
00:33:31,560 --> 00:33:33,840
But I really think that what Ned was saying,

608
00:33:33,840 --> 00:33:37,480
that we don't need to wait for that to happen if it will ever

609
00:33:37,480 --> 00:33:42,000
happen to have concerns about the fact

610
00:33:42,000 --> 00:33:46,360
that intelligence systems are even very narrow and very

611
00:33:46,360 --> 00:33:47,760
specific for a task.

612
00:33:47,760 --> 00:33:50,760
So no human level intelligence, because human level intelligence

613
00:33:50,760 --> 00:33:53,880
means you are very broad and you can adapt your intelligence

614
00:33:53,880 --> 00:33:57,760
to various tasks on every day.

615
00:33:57,760 --> 00:34:05,880
Even very narrow, very specific AI can give undesired behavior,

616
00:34:05,880 --> 00:34:09,480
like the one that was in the example that Ned made.

617
00:34:09,480 --> 00:34:13,240
So we have to make sure that either we

618
00:34:13,240 --> 00:34:17,120
can tell exactly what we want without leaving everything out,

619
00:34:17,120 --> 00:34:21,520
which sounds very difficult because we don't tell each other,

620
00:34:21,520 --> 00:34:27,120
yes, do this, but also take care of this, don't do that.

621
00:34:27,120 --> 00:34:34,160
Or they discover themselves by observing us and then

622
00:34:34,160 --> 00:34:36,280
inferring what the principles are,

623
00:34:36,280 --> 00:34:38,840
what the common sense reasoning capabilities

624
00:34:38,840 --> 00:34:44,480
that we use now in our everyday life, they should use as well.

625
00:34:44,480 --> 00:34:47,960
But anyway, there should be a combination of these two things,

626
00:34:47,960 --> 00:34:54,280
but there should be some way to provide them with these goals

627
00:34:54,280 --> 00:34:59,960
that we want them to reach, but in a ethical, fair,

628
00:34:59,960 --> 00:35:03,800
reasonable, common sense reasoning way.

629
00:35:03,800 --> 00:35:09,040
So this is something that may sound strange,

630
00:35:09,040 --> 00:35:14,200
but this is something that has not been considered a lot

631
00:35:14,200 --> 00:35:17,800
in the history of AI, because all the machines,

632
00:35:17,800 --> 00:35:23,880
something very recently, were very narrow, very smart

633
00:35:23,880 --> 00:35:28,480
in doing that simple thing that simple, that small thing

634
00:35:28,480 --> 00:35:29,840
that they needed to do.

635
00:35:29,840 --> 00:35:34,840
But they were not capable of doing many other things.

636
00:35:34,840 --> 00:35:37,680
So if you want a machine that can play,

637
00:35:37,680 --> 00:35:41,320
go as good as possible, play chess as good as possible,

638
00:35:41,320 --> 00:35:44,560
then you know what goals to give the machine.

639
00:35:44,560 --> 00:35:48,080
And you don't need to specify many other collateral things

640
00:35:48,080 --> 00:35:51,920
that you should be careful not to harm people.

641
00:35:51,920 --> 00:35:53,600
I mean, that's not the point.

642
00:35:53,600 --> 00:35:54,640
It's not relevant.

643
00:35:54,640 --> 00:35:57,640
You should play chess as fast as good as possible.

644
00:35:57,640 --> 00:36:00,440
But so in all the textbooks that you have,

645
00:36:00,440 --> 00:36:03,280
you can see of AI, there is always the assumption

646
00:36:03,280 --> 00:36:06,840
that you can easily give a machine the goal

647
00:36:06,840 --> 00:36:09,000
that it should achieve.

648
00:36:09,000 --> 00:36:12,680
And now we realize that the more the machine

649
00:36:12,680 --> 00:36:15,600
get into the real world scenarios.

650
00:36:15,600 --> 00:36:18,000
And they have to do with the uncertainty of the world,

651
00:36:18,000 --> 00:36:20,160
and they have to take care of the things that can happen

652
00:36:20,160 --> 00:36:23,560
in the world while achieving the main goal that we give them,

653
00:36:23,560 --> 00:36:25,880
then really we need to be careful

654
00:36:25,880 --> 00:36:29,480
that they also are aware of the fact

655
00:36:29,480 --> 00:36:30,880
that they should not do this.

656
00:36:30,880 --> 00:36:33,960
So another example that Stuart Russell always gives

657
00:36:33,960 --> 00:36:39,000
is that if you leave at home your kids with the Butler

658
00:36:39,000 --> 00:36:42,880
robot taking care of them, and you tell this Butler robot

659
00:36:42,880 --> 00:36:45,160
to cook dinner for the kids.

660
00:36:45,160 --> 00:36:48,760
And the robot opens the fridge, and there's nothing

661
00:36:48,760 --> 00:36:52,760
in the fridge, but it sees a cat walking around with a house.

662
00:36:52,760 --> 00:36:57,760
But you don't want the cat to be the dinner for your kids.

663
00:36:57,760 --> 00:37:01,560
But if you just say cook the dinner for your kids,

664
00:37:01,560 --> 00:37:03,000
you have to be careful that you also

665
00:37:03,000 --> 00:37:06,800
say all these other things that you should not do.

666
00:37:06,800 --> 00:37:09,680
Or I don't know if you have yourself driving car,

667
00:37:09,680 --> 00:37:14,200
and you tell the car, bring me home as fast as possible,

668
00:37:14,200 --> 00:37:15,120
period.

669
00:37:15,120 --> 00:37:17,320
And then yes, OK, but you have to make sure

670
00:37:17,320 --> 00:37:19,040
that you don't run over anybody.

671
00:37:19,040 --> 00:37:22,000
You don't make me car sick because you go too fast and so on.

672
00:37:22,000 --> 00:37:25,440
So all these other things are part of the goal.

673
00:37:25,440 --> 00:37:28,520
But we still have to understand how

674
00:37:28,520 --> 00:37:31,080
to communicate this to a machine.

675
00:37:31,080 --> 00:37:33,320
Well, I think part of the answer that you were getting at

676
00:37:33,320 --> 00:37:38,680
is in your very first comment was,

677
00:37:38,680 --> 00:37:42,440
could these robots evolve?

678
00:37:42,440 --> 00:37:44,200
And that brings up the question.

679
00:37:44,200 --> 00:37:47,200
I mean, you're talking about AI and ethics.

680
00:37:47,200 --> 00:37:49,600
Right now, we are thinking about ethics and AI,

681
00:37:49,600 --> 00:37:54,040
but maybe part of the world experience of these robots

682
00:37:54,040 --> 00:37:57,000
is we bring them up and we teach them.

683
00:37:57,000 --> 00:37:59,760
Humans teaching robots and, oh, nuts.

684
00:37:59,760 --> 00:38:01,840
That's not good behavior.

685
00:38:01,840 --> 00:38:02,600
This is what we do.

686
00:38:02,600 --> 00:38:04,080
This is the way we do things.

687
00:38:04,080 --> 00:38:07,520
And it's a possible approach to having them

688
00:38:07,520 --> 00:38:11,440
grow up understanding human social norms.

689
00:38:11,440 --> 00:38:14,000
Yeah, but hopefully it doesn't take 18 years.

690
00:38:14,000 --> 00:38:14,500
Right.

691
00:38:14,500 --> 00:38:15,000
Or.

692
00:38:15,000 --> 00:38:21,320
Like for you, I don't know what to hope for.

693
00:38:21,320 --> 00:38:27,400
But given that there were also unanticipated results from that,

694
00:38:27,400 --> 00:38:29,800
I mean, is there just going back to my initial,

695
00:38:29,800 --> 00:38:32,880
is there a potential danger in trying

696
00:38:32,880 --> 00:38:36,560
to model artificial intelligence development

697
00:38:36,560 --> 00:38:39,840
on what we understand about humans

698
00:38:39,840 --> 00:38:41,840
and mimicking the developmental steps

699
00:38:41,840 --> 00:38:44,280
or creating a kind of evolutionary potential

700
00:38:44,280 --> 00:38:47,720
and what if we're bad parents to our AI?

701
00:38:47,720 --> 00:38:50,400
In a sense, or we're ignorant parents.

702
00:38:50,400 --> 00:38:53,680
We don't know all the parameters.

703
00:38:53,680 --> 00:38:57,880
Humans start off with a huge innate component

704
00:38:57,880 --> 00:39:01,720
to all their cognitive abilities.

705
00:39:01,720 --> 00:39:05,320
And you couldn't expect to get the same results

706
00:39:05,320 --> 00:39:08,440
with a machine that doesn't have those innate components.

707
00:39:08,440 --> 00:39:12,200
So for example, it has been, I think,

708
00:39:12,200 --> 00:39:18,440
recognized that even very small kids, like three or four years,

709
00:39:18,440 --> 00:39:22,920
so they have an innate nature to cooperate with others.

710
00:39:22,920 --> 00:39:25,480
And nobody even nobody teaching them,

711
00:39:25,480 --> 00:39:27,000
but they cooperate with each other.

712
00:39:27,000 --> 00:39:28,400
They help each other.

713
00:39:28,400 --> 00:39:29,680
They help adults.

714
00:39:29,680 --> 00:39:31,120
They have other kids.

715
00:39:31,120 --> 00:39:36,840
And machines don't come with this thing or others.

716
00:39:36,840 --> 00:39:42,080
So there must be some other way to teach

717
00:39:42,080 --> 00:39:45,920
them or to make them have these capabilities, which

718
00:39:45,920 --> 00:39:48,160
is different from what we do for humans.

719
00:39:48,160 --> 00:39:49,760
I suppose this is true, because if you

720
00:39:49,760 --> 00:39:53,640
did the thought experiment of let's bring up an ape

721
00:39:53,640 --> 00:39:57,040
in our household, you could treat it just as you would

722
00:39:57,040 --> 00:39:59,680
your child, and it might end up somewhat different

723
00:39:59,680 --> 00:40:00,840
in its behavior.

724
00:40:00,840 --> 00:40:05,600
Yeah, that's in fact the issue that Francesca just raised

725
00:40:05,600 --> 00:40:07,320
is actually directly relevant to this,

726
00:40:07,320 --> 00:40:10,200
because apes don't have this cooperative impulse.

727
00:40:10,200 --> 00:40:12,600
So Felix Varnequin showed with babies.

728
00:40:12,600 --> 00:40:14,080
I know him, but he will.

729
00:40:14,080 --> 00:40:22,080
With that a two-year-old who sees a person go to a cabinet

730
00:40:22,080 --> 00:40:24,680
and put things in it and then goes to the cabinet

731
00:40:24,680 --> 00:40:28,480
with a thing too heavy to occupy both hands,

732
00:40:28,480 --> 00:40:34,280
the child will spontaneously go and open the door.

733
00:40:34,280 --> 00:40:40,120
And many experimental approaches to monkeys

734
00:40:40,120 --> 00:40:44,480
and chimps show that they do not tend to do this kind of thing.

735
00:40:44,480 --> 00:40:49,760
In fact, if there's food involved,

736
00:40:49,760 --> 00:40:55,280
they really seem to be especially competitive.

737
00:40:55,280 --> 00:40:57,560
Well, there's learning and there's evolution.

738
00:40:57,560 --> 00:40:59,440
And I wonder whether on an evolutionary time

739
00:40:59,440 --> 00:41:04,400
scale we can get whatever it is in the hardware or firmware

740
00:41:04,400 --> 00:41:07,280
better aligned with humans.

741
00:41:07,280 --> 00:41:11,440
But another point I would make here is that on the one hand,

742
00:41:11,440 --> 00:41:14,600
we would like the embodied AI to understand

743
00:41:14,600 --> 00:41:19,760
something of our world so it can know something of our norms

744
00:41:19,760 --> 00:41:21,480
and ethics and all that.

745
00:41:21,480 --> 00:41:24,720
So that would indicate or dictate that we

746
00:41:24,720 --> 00:41:26,920
want to bring it up as one of our own.

747
00:41:26,920 --> 00:41:29,080
But on the other hand, it shouldn't

748
00:41:29,080 --> 00:41:32,920
be the case that the only embodied AI that makes any sense

749
00:41:32,920 --> 00:41:34,880
is a humanoid-like thing.

750
00:41:34,880 --> 00:41:37,400
So I don't know how to reconcile that.

751
00:41:37,400 --> 00:41:39,840
Certainly the cognitive room that we were describing

752
00:41:39,840 --> 00:41:42,440
is not humanoid at all.

753
00:41:42,440 --> 00:41:46,960
We talked to the room and it talks back to us

754
00:41:46,960 --> 00:41:51,360
through the speakers, shows us stuff through the displays.

755
00:41:51,360 --> 00:41:53,960
But there's nothing humanoid there.

756
00:41:53,960 --> 00:41:55,320
And there are plenty of robots out there

757
00:41:55,320 --> 00:41:57,600
that are not humanoid either.

758
00:41:57,600 --> 00:42:01,000
So I don't know how to reconcile it.

759
00:42:01,000 --> 00:42:02,880
So you're saying it's not possible

760
00:42:02,880 --> 00:42:06,600
that we get to a point where they recognize emotion?

761
00:42:06,600 --> 00:42:12,040
Oh, we can, to some limited degree, recognize emotion now.

762
00:42:12,040 --> 00:42:13,120
It's trained.

763
00:42:13,120 --> 00:42:20,480
I mean, through text, facial expression, tone of voice,

764
00:42:20,480 --> 00:42:24,320
you can train a machine to classify

765
00:42:24,320 --> 00:42:28,320
human-emotional state into a small number of discrete states.

766
00:42:28,320 --> 00:42:32,160
Happy, sad, disgust, anger, things like that.

767
00:42:32,160 --> 00:42:34,440
So why would there not be able also

768
00:42:34,440 --> 00:42:36,920
to have them in terms of what you were saying,

769
00:42:36,920 --> 00:42:39,680
in terms of singularity, recognize certain things,

770
00:42:39,680 --> 00:42:43,240
or not to be down certain things, could hurt somebody,

771
00:42:43,240 --> 00:42:44,560
and so they shouldn't do it.

772
00:42:44,560 --> 00:42:48,400
In other words, have a controlled system built within it.

773
00:42:48,400 --> 00:42:51,640
But just the source's apprentice example

774
00:42:51,640 --> 00:42:56,520
is meant to show that there's no goal that you can give it,

775
00:42:56,520 --> 00:43:01,520
that couldn't be understood in a way that

776
00:43:01,520 --> 00:43:03,920
goes counter to what you wanted.

777
00:43:03,920 --> 00:43:09,320
So it's really difficult to see how to put that into a machine.

778
00:43:09,320 --> 00:43:11,800
The possibilities for screwing up are infinite.

779
00:43:11,800 --> 00:43:13,120
Yeah, that's right.

780
00:43:13,120 --> 00:43:13,720
In variety.

781
00:43:13,720 --> 00:43:17,000
But that's true with humans.

782
00:43:17,000 --> 00:43:23,160
No, but humans have this background of understanding.

783
00:43:23,160 --> 00:43:24,920
I mean, not all humans.

784
00:43:24,920 --> 00:43:28,640
There's a class of psychopaths that don't have to be.

785
00:43:28,640 --> 00:43:32,840
We're talking about humans today, but humans in the past,

786
00:43:32,840 --> 00:43:36,840
we're killing and all sorts of putting fire on people's homes

787
00:43:36,840 --> 00:43:38,680
and so on was normal.

788
00:43:38,680 --> 00:43:41,200
So it has evolved to the point we are here.

789
00:43:41,200 --> 00:43:43,280
So when we worry about singularity,

790
00:43:43,280 --> 00:43:46,600
why aren't we also thinking about that if we devolve

791
00:43:46,600 --> 00:43:49,880
the same way humans have evolved?

792
00:43:49,880 --> 00:43:51,040
Well, it may.

793
00:43:51,040 --> 00:43:52,520
But that's a future.

794
00:43:52,520 --> 00:43:55,160
That is, by creating a God of AIs, I don't know,

795
00:43:55,160 --> 00:43:59,400
but somehow creating the same thing being part of the evolution

796
00:43:59,400 --> 00:44:00,280
of it.

797
00:44:00,280 --> 00:44:03,320
Well, here we get into what is our feeling

798
00:44:03,320 --> 00:44:06,520
about the possible eventuality of a singularity.

799
00:44:06,520 --> 00:44:10,880
We could take the view that, well, we humans in our present form

800
00:44:10,880 --> 00:44:13,120
are maybe not going to be part of that future,

801
00:44:13,120 --> 00:44:16,240
and that makes us sad.

802
00:44:16,240 --> 00:44:19,080
Or it could be, well, some essence of us

803
00:44:19,080 --> 00:44:20,200
is continuing forward.

804
00:44:20,200 --> 00:44:23,080
Just it's part of the natural evolutionary process.

805
00:44:23,080 --> 00:44:25,520
We could take that detached view.

806
00:44:25,520 --> 00:44:27,400
I don't know if I can get myself there personally.

807
00:44:30,760 --> 00:44:35,840
Maybe it will improve itself in some machine form

808
00:44:35,840 --> 00:44:37,920
and become some perfect being.

809
00:44:37,920 --> 00:44:41,760
But is that connected to me?

810
00:44:41,760 --> 00:44:44,000
I don't know if I can connect that to myself.

811
00:44:44,000 --> 00:44:47,720
So I don't know if I feel happy at that prospect.

812
00:44:47,720 --> 00:44:50,920
But it may be that we sort of, some have suggested

813
00:44:50,920 --> 00:44:55,920
that we're not the distinction between human and machine

814
00:44:55,920 --> 00:44:59,200
is going to become fuzzy to the point

815
00:44:59,200 --> 00:45:01,320
where it sort of doesn't matter anymore.

816
00:45:01,320 --> 00:45:06,240
I mean, as we have artificial limbs today,

817
00:45:06,240 --> 00:45:08,520
they're getting better and better.

818
00:45:08,520 --> 00:45:10,840
Someday they'll be seamless, and you won't even

819
00:45:10,840 --> 00:45:12,880
know the difference.

820
00:45:12,880 --> 00:45:15,480
But then the same thing could happen to our brains.

821
00:45:15,480 --> 00:45:17,760
Maybe parts of our brains start getting replaced

822
00:45:17,760 --> 00:45:23,040
by something mechanical, or maybe it becomes

823
00:45:23,040 --> 00:45:25,600
squishy and biological like.

824
00:45:25,600 --> 00:45:30,480
Over time, maybe we just augment our bodies and our minds,

825
00:45:30,480 --> 00:45:33,600
and we sort of meld together human and machine.

826
00:45:33,600 --> 00:45:41,080
And then it's just part of our projection into the future.

827
00:45:41,080 --> 00:45:43,320
So there are different views one can take at this.

828
00:45:43,320 --> 00:45:46,760
One of the implicit distinction that I'm hearing

829
00:45:46,760 --> 00:45:51,280
is do we want these AIs to be tools or instruments,

830
00:45:51,280 --> 00:45:53,960
or do we want them to be agents?

831
00:45:53,960 --> 00:45:58,000
And the paradox is that you want them to be agents,

832
00:45:58,000 --> 00:46:01,320
because if they're mere tools, they're going to do the dumb thing.

833
00:46:01,320 --> 00:46:02,720
You're going to give it commands,

834
00:46:02,720 --> 00:46:05,960
and it's going to misunderstand and do harm.

835
00:46:05,960 --> 00:46:09,560
So you want them to be more agent-like so that they

836
00:46:09,560 --> 00:46:13,000
understand your intentions, or you try to get them to do that.

837
00:46:13,000 --> 00:46:15,440
But the more agent-like they become,

838
00:46:15,440 --> 00:46:17,280
the less you can control them.

839
00:46:17,280 --> 00:46:21,200
Ultimately, a real agent distinguishes an agent

840
00:46:21,200 --> 00:46:24,000
as they can take initiative and then start doing things

841
00:46:24,000 --> 00:46:26,680
that they decide, having their own priorities.

842
00:46:26,680 --> 00:46:29,080
The way out of that, at least in today's world,

843
00:46:29,080 --> 00:46:33,800
is to they're both more, we're using them

844
00:46:33,800 --> 00:46:37,280
because they're more intelligent than us in certain ways.

845
00:46:37,280 --> 00:46:39,920
But I view them as idiot salons.

846
00:46:39,920 --> 00:46:41,080
It's some of both.

847
00:46:41,080 --> 00:46:43,760
They're still instruments at that level.

848
00:46:43,760 --> 00:46:47,840
And yes, they're in narrow ways,

849
00:46:47,840 --> 00:46:53,000
in cognitively stronger than we are, but in narrow ways.

850
00:46:53,000 --> 00:46:56,000
And so like a pocket calculator.

851
00:46:58,640 --> 00:47:02,560
Beyond pocket calculator, but they,

852
00:47:02,560 --> 00:47:07,400
and I wouldn't ascribe any cognition to a pocket calculator,

853
00:47:07,400 --> 00:47:08,880
particularly.

854
00:47:08,880 --> 00:47:13,520
I would say that these are tools that really augment

855
00:47:13,520 --> 00:47:16,000
our cognition in a more significant way.

856
00:47:16,000 --> 00:47:19,000
But they aren't in their own right,

857
00:47:19,000 --> 00:47:23,800
necessarily a fully autonomous, certainly not

858
00:47:23,800 --> 00:47:26,040
an artificial general intelligence.

859
00:47:26,040 --> 00:47:27,200
We're not going to be there for a while.

860
00:47:27,200 --> 00:47:29,840
Kind of like a consultant, I like an expert in something

861
00:47:29,840 --> 00:47:34,400
that you need for your job and you consult with.

862
00:47:34,400 --> 00:47:37,720
And you are going to make the final decision

863
00:47:37,720 --> 00:47:38,960
about what you need to do.

864
00:47:38,960 --> 00:47:42,920
But you consult this expert who is going to help you

865
00:47:42,920 --> 00:47:46,040
because he has more knowledge and he has more skills

866
00:47:46,040 --> 00:47:49,040
for that particular thing that you need to do.

867
00:47:49,040 --> 00:47:54,600
So machines will be much more capable of us

868
00:47:54,600 --> 00:47:58,560
in certain things, as Jeff said, like in handling,

869
00:47:58,560 --> 00:48:02,240
in reading, a lot of data, a lot of scientific articles,

870
00:48:02,240 --> 00:48:05,840
a lot of information that you will never

871
00:48:05,840 --> 00:48:08,320
be able to read in your whole life.

872
00:48:08,320 --> 00:48:10,920
And then summarize it and give it to you,

873
00:48:10,920 --> 00:48:13,080
the part that are relevant to what you have to do

874
00:48:13,080 --> 00:48:15,120
at that particular moment.

875
00:48:15,120 --> 00:48:16,440
But we'll see.

876
00:48:16,440 --> 00:48:20,440
I mean, pocket calculator does not give the best idea,

877
00:48:20,440 --> 00:48:24,000
I think, because it's very deterministic.

878
00:48:24,000 --> 00:48:26,720
So you'll give the numbers and you'll

879
00:48:26,720 --> 00:48:29,400
give the same numbers twice, it gives the same result.

880
00:48:29,400 --> 00:48:33,880
While here we want somebody that can take care of the uncertainty

881
00:48:33,880 --> 00:48:37,480
of the world, of the incredible number of scenarios

882
00:48:37,480 --> 00:48:46,120
that can happen and so that it has its own ways of dealing

883
00:48:46,120 --> 00:48:53,680
with these lines of conduct and probabilities,

884
00:48:53,680 --> 00:48:57,760
probability reasoning, and so that maybe

885
00:48:57,760 --> 00:48:59,880
different, slightly different, so now it

886
00:48:59,880 --> 00:49:03,040
gives you a completely different suggestion of what to do.

887
00:49:03,040 --> 00:49:04,760
So it's a context.

888
00:49:04,760 --> 00:49:05,920
Yeah.

889
00:49:05,920 --> 00:49:11,240
But of course, as soon as somebody gets a line on how

890
00:49:11,240 --> 00:49:17,480
to give them a general intelligence,

891
00:49:17,480 --> 00:49:20,240
since there will be no stopping it,

892
00:49:20,240 --> 00:49:23,840
and then that's then we could enter into uncharted territory.

893
00:49:23,840 --> 00:49:31,120
So we should really be thinking now about that transition.

894
00:49:31,120 --> 00:49:35,000
I think earlier you gave a very good example

895
00:49:35,000 --> 00:49:38,160
when you were talking about the singularity of the notion

896
00:49:38,160 --> 00:49:40,720
that suppose a machine can create a machine that

897
00:49:40,720 --> 00:49:42,920
is slightly more intelligent than itself.

898
00:49:42,920 --> 00:49:45,640
What you have to think about is if a machine can create

899
00:49:45,640 --> 00:49:49,040
a machine that is 99% as smart as it is,

900
00:49:49,040 --> 00:49:51,120
then you can take it several generations ahead

901
00:49:51,120 --> 00:49:53,000
and it'll just peter out.

902
00:49:53,000 --> 00:49:55,560
But if you can make a machine that

903
00:49:55,560 --> 00:49:58,920
makes a machine that is 1% smarter than it,

904
00:49:58,920 --> 00:50:01,920
it's going to be a positive exponential instead

905
00:50:01,920 --> 00:50:03,120
of a negative exponential.

906
00:50:03,120 --> 00:50:05,920
And that's where you get a sharp phase transition between,

907
00:50:05,920 --> 00:50:09,240
ah, don't worry about it, and oh my god.

908
00:50:09,240 --> 00:50:10,960
And also, I mean, people I think

909
00:50:10,960 --> 00:50:13,360
are kind of concerned about that scenario

910
00:50:13,360 --> 00:50:16,560
because different from humans, machine

911
00:50:16,560 --> 00:50:19,880
can replicate themselves instantaneously and almost

912
00:50:19,880 --> 00:50:22,960
with no cost, at least from the software.

913
00:50:22,960 --> 00:50:27,480
So then you have this exponential, not just for one machine,

914
00:50:27,480 --> 00:50:33,480
but for a very huge number of machines.

915
00:50:33,480 --> 00:50:36,920
But again, I think that I mean, I

916
00:50:36,920 --> 00:50:41,960
don't know what we can do now while envisioning that far away

917
00:50:41,960 --> 00:50:45,760
scenario, what we can do now thinking of that scenario.

918
00:50:45,760 --> 00:50:50,800
But we can already do now and work now for narrow,

919
00:50:50,800 --> 00:50:54,840
narrowly intelligent machines to actually help them

920
00:50:54,840 --> 00:50:56,720
behave in the right way.

921
00:50:56,720 --> 00:50:59,240
And I think this, of course, will also

922
00:50:59,240 --> 00:51:04,880
help when and if that general intelligence will come out.

923
00:51:04,880 --> 00:51:06,560
And maybe we can have AI researchers

924
00:51:06,560 --> 00:51:08,120
to behave in the right way too.

925
00:51:08,120 --> 00:51:15,400
I've heard that people have taught courses on AI and ethics

926
00:51:15,400 --> 00:51:17,880
through the medium of science fiction.

927
00:51:17,880 --> 00:51:20,960
Because science fiction has plenty of cautionary tales in it.

928
00:51:20,960 --> 00:51:23,760
Well, there's a number of TV programs right now

929
00:51:23,760 --> 00:51:26,240
that are exploring this border.

930
00:51:26,240 --> 00:51:27,760
There's humans.

931
00:51:27,760 --> 00:51:28,320
West world.

932
00:51:28,320 --> 00:51:29,320
West world.

933
00:51:29,320 --> 00:51:31,280
Yeah.

934
00:51:31,280 --> 00:51:34,880
My problem as a historian is I've

935
00:51:34,880 --> 00:51:38,200
tried to envision what would be plausible scenarios

936
00:51:38,200 --> 00:51:40,400
if we decided that we needed at some point

937
00:51:40,400 --> 00:51:45,800
to restrain the advance of AI as a science.

938
00:51:45,800 --> 00:51:48,800
And I can't find any that would be, in my mind,

939
00:51:48,800 --> 00:51:50,040
that would be successful.

940
00:51:50,040 --> 00:51:53,720
Because as you said, we live in a competitive society

941
00:51:53,720 --> 00:51:57,000
and a world that is not unified.

942
00:51:57,000 --> 00:52:01,400
And if either one company competing against another company

943
00:52:01,400 --> 00:52:04,280
or one nation competing against another nation,

944
00:52:04,280 --> 00:52:08,400
anybody who gets the first pass the post in terms

945
00:52:08,400 --> 00:52:11,400
of getting one of these general artificial general

946
00:52:11,400 --> 00:52:18,960
intelligences is going to have an extremely powerful advantage.

947
00:52:18,960 --> 00:52:20,840
And so there's an incentive built

948
00:52:20,840 --> 00:52:23,680
into the very structure of our political systems

949
00:52:23,680 --> 00:52:27,680
and our motivational systems to compete and kind of an arms

950
00:52:27,680 --> 00:52:30,320
race sort of mentality.

951
00:52:30,320 --> 00:52:32,760
Even if it's not at war, it could be some other.

952
00:52:32,760 --> 00:52:35,360
It could just be just sort of the first pass

953
00:52:35,360 --> 00:52:37,920
the post gets more powerful.

954
00:52:37,920 --> 00:52:41,040
I wonder, I mean, reasoning from history,

955
00:52:41,040 --> 00:52:44,320
are there any historical examples of being

956
00:52:44,320 --> 00:52:46,760
able to suppress a technology?

957
00:52:46,760 --> 00:52:49,600
The only thing I can think that at least goes slightly

958
00:52:49,600 --> 00:52:53,680
in that direction is probably nuclear proliferation.

959
00:52:53,680 --> 00:52:58,120
I mean, the US got to that point a little bit before Germany.

960
00:52:58,120 --> 00:53:02,000
And we used it and then backed off.

961
00:53:02,000 --> 00:53:05,720
The few examples tend to be cautionary in the other way.

962
00:53:05,720 --> 00:53:10,160
The Chinese at one point banned large ocean-going vessels

963
00:53:10,160 --> 00:53:12,680
for a while, a couple centuries.

964
00:53:12,680 --> 00:53:15,880
And all that that did was postponed the inevitable

965
00:53:15,880 --> 00:53:18,680
because eventually other people built big boats

966
00:53:18,680 --> 00:53:19,840
and came to China.

967
00:53:19,840 --> 00:53:24,800
And so in the competitive world, the game

968
00:53:24,800 --> 00:53:28,360
is set up in such a way that if you

969
00:53:28,360 --> 00:53:31,400
have to keep advancing, because otherwise your neighbor is

970
00:53:31,400 --> 00:53:34,320
going to advance past you, and then you'll

971
00:53:34,320 --> 00:53:36,160
be at a disadvantage.

972
00:53:36,160 --> 00:53:39,120
So there have been attempts by people,

973
00:53:39,120 --> 00:53:40,840
you think of the Amish.

974
00:53:40,840 --> 00:53:42,920
They've been able to do it because they pose no threat

975
00:53:42,920 --> 00:53:44,120
to anybody.

976
00:53:44,120 --> 00:53:50,360
But in cases where you're actually getting competitive power

977
00:53:50,360 --> 00:53:53,360
or advantage in a competitive situation,

978
00:53:53,360 --> 00:53:57,320
sooner or later, the pressure makes,

979
00:53:57,320 --> 00:54:00,320
if would there have been an atomic bomb if World War II hadn't

980
00:54:00,320 --> 00:54:01,560
happened?

981
00:54:01,560 --> 00:54:04,160
Probably within 20 years.

982
00:54:04,160 --> 00:54:07,000
The war accelerated that process.

983
00:54:07,000 --> 00:54:10,960
But probably by the 1960, somebody somewhere

984
00:54:10,960 --> 00:54:13,280
would have figured out that the Germans were already

985
00:54:13,280 --> 00:54:15,360
on the way to doing it anyway.

986
00:54:15,360 --> 00:54:21,200
I read a wonderful book by a military historian, John Keegan,

987
00:54:21,200 --> 00:54:25,040
that went through a number of phase transitions

988
00:54:25,040 --> 00:54:29,320
and development of weapons in which there was a sudden advance

989
00:54:29,320 --> 00:54:32,720
in either defense or offense.

990
00:54:32,720 --> 00:54:36,440
And one that sticks in my mind is the development

991
00:54:36,440 --> 00:54:41,320
of a much better cannon, which interacted

992
00:54:41,320 --> 00:54:44,480
with the defense of the time, which were these high walls.

993
00:54:44,480 --> 00:54:51,000
And so for example, the fall of Constantinople in the,

994
00:54:51,000 --> 00:54:56,040
what was that, 1453, I think, was due to the fact that the Turks

995
00:54:56,040 --> 00:55:00,880
perfected a cannon that allowed them to undermine the walls.

996
00:55:00,880 --> 00:55:04,560
And then once, and then they would just fall.

997
00:55:04,560 --> 00:55:09,240
So once this happened, defensive walls in cities

998
00:55:09,240 --> 00:55:12,560
all over Europe, where everybody realized

999
00:55:12,560 --> 00:55:15,880
that the cannon made their walls obsolete.

1000
00:55:15,880 --> 00:55:19,640
And what they needed was wide, very thick, and even,

1001
00:55:19,640 --> 00:55:22,720
and some, that can be low walls, but much thicker.

1002
00:55:22,720 --> 00:55:28,640
And they were building new walls for more than 100 years,

1003
00:55:28,640 --> 00:55:31,240
all the cities, the walled cities in Europe.

1004
00:55:31,240 --> 00:55:34,680
They even quote a wonderful letter from Michelangelo

1005
00:55:34,680 --> 00:55:37,520
saying, because he was selling his services as a wall

1006
00:55:37,520 --> 00:55:41,440
designer, saying, I don't know much about painting or sculpture,

1007
00:55:41,440 --> 00:55:43,400
but I really know about building walls.

1008
00:55:43,400 --> 00:55:48,520
LAUGHTER

1009
00:55:48,520 --> 00:55:51,840
But it's true that we live in a very competing world.

1010
00:55:51,840 --> 00:55:55,480
But I think that, I mean, a little bit for people

1011
00:55:55,480 --> 00:55:58,600
are starting to realize that especially in this,

1012
00:55:58,600 --> 00:56:02,920
understanding the issues in the advancement

1013
00:56:02,920 --> 00:56:06,720
of this very powerful technology is something that should not

1014
00:56:06,720 --> 00:56:09,840
be part of the competition, understanding

1015
00:56:09,840 --> 00:56:11,280
how to address these issues.

1016
00:56:11,280 --> 00:56:16,280
Like, for example, maybe Ned, remember that I mentioned that

1017
00:56:16,280 --> 00:56:18,080
last week, that two weeks ago, it

1018
00:56:18,080 --> 00:56:21,440
was launched a very interesting initiative,

1019
00:56:21,440 --> 00:56:24,440
where five of the main companies developing

1020
00:56:24,440 --> 00:56:30,320
AI, which is IBM, Google, Facebook, Amazon, and Microsoft,

1021
00:56:30,320 --> 00:56:35,240
they decided to get together and understand together

1022
00:56:35,240 --> 00:56:39,480
what it means to develop AI for the benefit of people

1023
00:56:39,480 --> 00:56:41,200
and society.

1024
00:56:41,200 --> 00:56:46,240
And we try to engage with everybody else.

1025
00:56:46,240 --> 00:56:49,600
It's not just a company kind of thing,

1026
00:56:49,600 --> 00:56:52,280
with non-corporate members as well,

1027
00:56:52,280 --> 00:56:55,800
non-profit organizations, scientific associations,

1028
00:56:55,800 --> 00:56:58,240
individual society in general.

1029
00:56:58,240 --> 00:57:00,920
But we really think that we should not

1030
00:57:00,920 --> 00:57:04,120
hide the maybe issues in this development

1031
00:57:04,120 --> 00:57:05,800
of this very powerful technology.

1032
00:57:05,800 --> 00:57:08,160
But the technology is so powerful,

1033
00:57:08,160 --> 00:57:11,320
it can be so beneficial for everybody

1034
00:57:11,320 --> 00:57:16,600
that we have to work together, even companies that are,

1035
00:57:16,600 --> 00:57:19,880
as you may know, competing a lot in the marketplace.

1036
00:57:19,880 --> 00:57:23,480
But they think on this, they should be really collaboration.

1037
00:57:23,480 --> 00:57:26,120
And I think this is, of course, doesn't mean that maybe

1038
00:57:26,120 --> 00:57:30,640
other companies or other countries are not going to compete.

1039
00:57:30,640 --> 00:57:33,080
But still, these are very companies

1040
00:57:33,080 --> 00:57:35,760
that can influence a lot all over the world,

1041
00:57:35,760 --> 00:57:41,440
because they are all used by billions of people everywhere.

1042
00:57:41,440 --> 00:57:44,160
And I think that this can start really

1043
00:57:44,160 --> 00:57:47,800
a very collaborative environment, where these issues are

1044
00:57:47,800 --> 00:57:51,160
discussed, addressed, solved, and understood

1045
00:57:51,160 --> 00:57:55,560
how to best trajectory for AI in the future.

1046
00:57:55,560 --> 00:57:59,160
But don't you think that if one of those five companies

1047
00:57:59,160 --> 00:58:01,640
makes a real breakthrough, it's likely

1048
00:58:01,640 --> 00:58:05,200
they're going to actually share it with the other four?

1049
00:58:05,200 --> 00:58:06,440
Yeah, but I mean, I did.

1050
00:58:06,440 --> 00:58:07,920
I know, but I don't know about the other four.

1051
00:58:07,920 --> 00:58:10,840
No, but I mean, the idea is not to share

1052
00:58:10,840 --> 00:58:15,400
new software or new advances, but to share the best practices

1053
00:58:15,400 --> 00:58:20,560
and how to deal with making AI, of course,

1054
00:58:20,560 --> 00:58:24,640
in a competing environment more and more smart,

1055
00:58:24,640 --> 00:58:26,680
but in a collaborative environment,

1056
00:58:26,680 --> 00:58:30,000
making it smart, but in the right way.

1057
00:58:30,000 --> 00:58:32,160
So it could be that one of these companies

1058
00:58:32,160 --> 00:58:36,080
is making tomorrow a very big advancement in AI,

1059
00:58:36,080 --> 00:58:38,440
and of course, it's going to be his own result,

1060
00:58:38,440 --> 00:58:40,320
and not the result of the other ones.

1061
00:58:40,320 --> 00:58:43,120
But we want together to understand

1062
00:58:43,120 --> 00:58:45,160
how to make whatever advancement is

1063
00:58:45,160 --> 00:58:47,800
going to be made by anybody in the best way

1064
00:58:47,800 --> 00:58:50,200
for the benefit of everybody.

1065
00:58:50,200 --> 00:58:54,000
And I think that that's really needed a lot,

1066
00:58:54,000 --> 00:58:56,120
a collaborative environment on these issues,

1067
00:58:56,120 --> 00:59:00,640
even among entities that are naturally competing

1068
00:59:00,640 --> 00:59:03,600
because of their business model, of course.

1069
00:59:03,600 --> 00:59:05,920
It's really encouraging.

1070
00:59:05,920 --> 00:59:09,640
Similar to what was done about a year and a half ago

1071
00:59:09,640 --> 00:59:11,520
with genetic engineering technology,

1072
00:59:11,520 --> 00:59:15,520
the new CRISPR-Cas9 pathway for modifying genomes,

1073
00:59:15,520 --> 00:59:17,800
and they convened a second.

1074
00:59:17,800 --> 00:59:22,200
The first such conference was with recombinant DNA technology,

1075
00:59:22,200 --> 00:59:24,640
the Asilomar Conference in the 1970s,

1076
00:59:24,640 --> 00:59:28,400
and voluntarily the leading figures in this field

1077
00:59:28,400 --> 00:59:32,160
met a year ago, and again, saying,

1078
00:59:32,160 --> 00:59:34,400
let's consult with each other, let's establish

1079
00:59:34,400 --> 00:59:36,320
basic ground rules and best practices.

1080
00:59:36,320 --> 00:59:38,400
I didn't know that this had happened with AI.

1081
00:59:38,400 --> 00:59:40,920
I mean, it needs to happen with synthetic biology.

1082
00:59:40,920 --> 00:59:43,720
It needs to happen with all these nanotechnology,

1083
00:59:43,720 --> 00:59:47,040
all these potentially disruptive,

1084
00:59:47,040 --> 00:59:52,640
but potentially also enormously beneficial technologies.

1085
00:59:52,640 --> 00:59:55,320
It is incumbent on the people doing it.

1086
00:59:55,320 --> 00:59:58,160
I remember in 2000, I think it was,

1087
00:59:58,160 --> 01:00:03,960
there was an article published in Wired by,

1088
01:00:03,960 --> 01:00:05,920
I'm blanking on his name right now,

1089
01:00:05,920 --> 01:00:08,440
he said, why the future doesn't need us,

1090
01:00:08,440 --> 01:00:09,760
what's his name again?

1091
01:00:09,760 --> 01:00:13,760
He basically was a computer programmer who basically said,

1092
01:00:13,760 --> 01:00:16,640
I'm leaving the field because I can no longer

1093
01:00:16,640 --> 01:00:18,840
ethically continue to something which I think

1094
01:00:18,840 --> 01:00:21,440
is going to lead toward the singularity.

1095
01:00:21,440 --> 01:00:22,640
I think, pardon?

1096
01:00:22,640 --> 01:00:28,080
It wasn't Jared Lanier, it's, no, it wasn't

1097
01:00:28,080 --> 01:00:30,720
Jared Lanier, it wasn't Jared Lanier, it's wild.

1098
01:00:30,720 --> 01:00:33,160
It'll come to me, of course, after.

1099
01:00:33,160 --> 01:00:38,880
But it made a big stink in the technology community

1100
01:00:38,880 --> 01:00:40,920
because he was saying, I'm taking an ethical stand,

1101
01:00:40,920 --> 01:00:44,080
and implicitly he was saying, if you don't also

1102
01:00:44,080 --> 01:00:50,040
quit doing this, you're doing something fundamentally

1103
01:00:50,040 --> 01:00:50,840
morally wrong.

1104
01:00:50,840 --> 01:00:54,160
It's like continuing to work on the atomic bomb

1105
01:00:54,160 --> 01:00:55,120
or something like that.

1106
01:00:55,120 --> 01:00:59,120
But quitting, it would also not allow us to get

1107
01:00:59,120 --> 01:01:01,200
the real benefits of this technology.

1108
01:01:01,200 --> 01:01:04,800
For example, just one in health care.

1109
01:01:04,800 --> 01:01:10,200
And cure cancer, so health care issues.

1110
01:01:10,200 --> 01:01:12,200
So I don't think we should quit.

1111
01:01:12,200 --> 01:01:15,160
We should continue in the best way.

1112
01:01:15,160 --> 01:01:15,680
Exactly.

1113
01:01:15,680 --> 01:01:17,000
Yeah, I think to state the obvious,

1114
01:01:17,000 --> 01:01:18,720
if the most ethical programmer

1115
01:01:18,720 --> 01:01:23,840
equates, the population of programmers becomes less ethical.

1116
01:01:23,840 --> 01:01:25,400
Somebody would have very quickly taken this question.

1117
01:01:25,400 --> 01:01:26,920
I feel the need to reiterate something

1118
01:01:26,920 --> 01:01:29,480
that you mentioned in a glancing way.

1119
01:01:29,480 --> 01:01:34,720
But, well, best practices are great,

1120
01:01:34,720 --> 01:01:37,640
and it's great to formulate them and share them.

1121
01:01:37,640 --> 01:01:40,200
But it's in a competitive environment

1122
01:01:40,200 --> 01:01:43,960
where they attention might not be paid to them.

1123
01:01:43,960 --> 01:01:51,080
And I think especially if there's a war-like situation.

1124
01:01:51,080 --> 01:01:56,680
Whoever is making in advance might not pay that much attention

1125
01:01:56,680 --> 01:02:02,320
to the best practices if they think they're going to make up,

1126
01:02:02,320 --> 01:02:04,160
they're going to defeat the enemy.

1127
01:02:04,160 --> 01:02:07,760
Doesn't even need to be competitive or malicious,

1128
01:02:07,760 --> 01:02:08,480
I guess.

1129
01:02:08,480 --> 01:02:10,520
It can be accidental.

1130
01:02:10,520 --> 01:02:11,520
Oops, sorry.

1131
01:02:11,520 --> 01:02:12,520
I didn't realize that would happen.

1132
01:02:12,520 --> 01:02:13,520
We're rushing.

1133
01:02:13,520 --> 01:02:17,760
No, it could even just be some emergent phenomenon

1134
01:02:17,760 --> 01:02:21,440
that it would have been difficult to anticipate.

1135
01:02:21,440 --> 01:02:24,600
And I guess this brings up the question of liability

1136
01:02:24,600 --> 01:02:26,920
for AI as well.

1137
01:02:26,920 --> 01:02:29,800
Who's responsible when something goes awry?

1138
01:02:35,240 --> 01:02:38,520
There's been a lot of talk about hacking.

1139
01:02:38,520 --> 01:02:41,080
So is it still possible, for example,

1140
01:02:41,080 --> 01:02:44,720
when you said these five companies have gotten together,

1141
01:02:44,720 --> 01:02:50,640
is it not, are you still able to keep things hidden

1142
01:02:50,640 --> 01:02:53,360
from the other companies if you don't want them to know

1143
01:02:53,360 --> 01:02:57,760
about your networks?

1144
01:02:57,760 --> 01:03:01,240
Isn't it possible with sophisticated technology

1145
01:03:01,240 --> 01:03:05,000
too for, let's say, Amazon to know exactly what IBM is doing

1146
01:03:05,000 --> 01:03:06,400
and vice versa?

1147
01:03:06,400 --> 01:03:11,680
Well, you've seen what happened yesterday.

1148
01:03:11,680 --> 01:03:16,080
That was this big cyber attack that

1149
01:03:16,080 --> 01:03:20,000
blocked many websites and services.

1150
01:03:20,000 --> 01:03:24,080
So I think that we have to be smarter than them,

1151
01:03:24,080 --> 01:03:26,640
and then they come smarter than us, and then so on.

1152
01:03:26,640 --> 01:03:29,240
So I mean, it's still not clear how

1153
01:03:29,240 --> 01:03:35,360
to avoid all these attacks and intruders

1154
01:03:35,360 --> 01:03:38,520
into systems.

1155
01:03:38,520 --> 01:03:39,400
So I don't know.

1156
01:03:39,400 --> 01:03:41,760
I'm not an expert in cyber security,

1157
01:03:41,760 --> 01:03:51,240
but it's not clear to me that these can be easily stopped.

1158
01:03:51,240 --> 01:03:54,920
Of course, I mean, companies can have their own security,

1159
01:03:54,920 --> 01:04:01,120
walls, and everything, but I'm not sure this can be done.

1160
01:04:01,120 --> 01:04:07,400
Even very sophisticated agencies here cannot keep secrets

1161
01:04:07,400 --> 01:04:08,640
that they have.

1162
01:04:08,640 --> 01:04:12,200
Then it means that we still have a long way to do it.

1163
01:04:12,200 --> 01:04:14,280
But I thought some of the sense of your question

1164
01:04:14,280 --> 01:04:17,880
was about among those five companies,

1165
01:04:17,880 --> 01:04:22,760
can they share while still remaining competitive?

1166
01:04:22,760 --> 01:04:24,120
And I think the answer is yes.

1167
01:04:24,120 --> 01:04:28,240
Companies can find ways to share and compete at the same time.

1168
01:04:28,240 --> 01:04:30,000
Yeah, but of course, they don't share.

1169
01:04:30,000 --> 01:04:34,280
I mean, the sharing is advising, sharing, best practices,

1170
01:04:34,280 --> 01:04:38,800
discussions, issues of concerns, and so on.

1171
01:04:38,800 --> 01:04:42,680
How to best develop AI?

1172
01:04:42,680 --> 01:04:44,720
How to be ethical while developing

1173
01:04:44,720 --> 01:04:48,760
AI from the idea to the product that you get?

1174
01:04:48,760 --> 01:04:51,880
And how to build these products in a way

1175
01:04:51,880 --> 01:04:54,600
that that product is going to behave ethically

1176
01:04:54,600 --> 01:04:56,680
when given to the world, and is going

1177
01:04:56,680 --> 01:05:02,040
to behave in the best way for benefit of society?

1178
01:05:02,040 --> 01:05:05,640
I can imagine standards developing as well.

1179
01:05:05,640 --> 01:05:09,080
I mean, going again back to science fiction,

1180
01:05:09,080 --> 01:05:12,560
you can think of Isaac Asimov's Three Laws of Robotics.

1181
01:05:12,560 --> 01:05:16,440
And these were cooked in at the very foundational level

1182
01:05:16,440 --> 01:05:18,200
into each robot.

1183
01:05:18,200 --> 01:05:20,720
And maybe we would develop something like this.

1184
01:05:20,720 --> 01:05:24,320
And through the combined power of these five companies,

1185
01:05:24,320 --> 01:05:27,560
they could all adopt this and then kind of exert pressure

1186
01:05:27,560 --> 01:05:30,160
on the rest of the world community to do the same.

1187
01:05:30,160 --> 01:05:33,720
Yeah, the standards are very important.

1188
01:05:33,720 --> 01:05:36,120
Of course, if not that five companies or even 10,

1189
01:05:36,120 --> 01:05:38,480
or 15 can build a standard, it has

1190
01:05:38,480 --> 01:05:43,040
to be reached by consensus building.

1191
01:05:43,040 --> 01:05:47,800
But over the course of the technology,

1192
01:05:47,800 --> 01:05:50,520
I think the information technology, especially,

1193
01:05:50,520 --> 01:05:52,560
the standards have played a very big role

1194
01:05:52,560 --> 01:05:55,520
in making the technology available to everybody.

1195
01:05:55,520 --> 01:05:58,200
Because the fact that all the things are compatible,

1196
01:05:58,200 --> 01:06:01,720
like, I don't know, the standard for the Wi-Fi.

1197
01:06:01,720 --> 01:06:03,400
At the beginning, there was no standard.

1198
01:06:03,400 --> 01:06:06,760
And then by consensus building, in our computers,

1199
01:06:06,760 --> 01:06:09,520
in our telephones, we all have the same system,

1200
01:06:09,520 --> 01:06:12,480
with the same protocol, being able to connect

1201
01:06:12,480 --> 01:06:14,320
to any Wi-Fi in the world.

1202
01:06:14,320 --> 01:06:17,600
Because it uses the same methodology.

1203
01:06:17,600 --> 01:06:20,800
So of course, it cannot be that technology

1204
01:06:20,800 --> 01:06:25,040
that precise of technological oriented,

1205
01:06:25,040 --> 01:06:31,000
a standard that tells these companies, or everybody else,

1206
01:06:31,000 --> 01:06:33,480
how to develop AI in the right way.

1207
01:06:33,480 --> 01:06:36,440
But there could be very precise guidelines

1208
01:06:36,440 --> 01:06:39,200
on how to do that.

1209
01:06:39,200 --> 01:06:42,120
And that's one of the things that we think this

1210
01:06:42,120 --> 01:06:45,360
would be very important to achieve.

1211
01:06:45,360 --> 01:06:49,080
It may not answer the limit case that you brought up

1212
01:06:49,080 --> 01:06:51,840
of a war, or something like that, or even just

1213
01:06:51,840 --> 01:06:53,840
a big military rivalry.

1214
01:06:53,840 --> 01:06:55,840
But it's a lot better than nothing.

1215
01:06:55,840 --> 01:06:57,760
Yeah, well, I think so.

1216
01:06:57,760 --> 01:06:59,160
I think it's very promising.

1217
01:06:59,160 --> 01:07:06,840
And I think that the fact that it comes from companies

1218
01:07:06,840 --> 01:07:09,040
has to be interpreted in the right way.

1219
01:07:09,040 --> 01:07:12,280
It's not that these companies want to decide how AI should

1220
01:07:12,280 --> 01:07:16,400
be done, and they're going to get together, and define it.

1221
01:07:16,400 --> 01:07:19,520
But because companies are those that are closer

1222
01:07:19,520 --> 01:07:23,600
to understand what it means to deploy AI system

1223
01:07:23,600 --> 01:07:24,800
in the real world.

1224
01:07:24,800 --> 01:07:27,600
Because they have clients, and these clients actually

1225
01:07:27,600 --> 01:07:29,360
use AI in the real world.

1226
01:07:29,360 --> 01:07:31,840
And those are the people that can tell us,

1227
01:07:31,840 --> 01:07:35,280
what are the issues that you see in health care,

1228
01:07:35,280 --> 01:07:39,520
in finance, in retail, in e-commerce, and so on.

1229
01:07:39,520 --> 01:07:41,640
And so tell us, and these are the issues

1230
01:07:41,640 --> 01:07:44,640
that we are going to address and resolve.

1231
01:07:44,640 --> 01:07:48,320
What are the problems that you see once you get your product,

1232
01:07:48,320 --> 01:07:51,440
and you just give it to customers?

1233
01:07:51,440 --> 01:07:55,200
So that's where you should start this discussion.

1234
01:07:55,200 --> 01:07:57,960
And then everybody else should be involved as well.

1235
01:07:57,960 --> 01:08:02,400
Speaking of everybody else, you mentioned the five companies.

1236
01:08:02,400 --> 01:08:05,120
Are you in this organization making efforts

1237
01:08:05,120 --> 01:08:07,280
to bring academia into discussion?

1238
01:08:07,280 --> 01:08:07,760
Yeah, yeah.

1239
01:08:07,760 --> 01:08:10,160
Academia, non-profit organizations,

1240
01:08:10,160 --> 01:08:13,560
and other professional associations.

1241
01:08:13,560 --> 01:08:16,680
Everybody, everybody, we just have

1242
01:08:16,680 --> 01:08:19,360
to understand how to put together the path

1243
01:08:19,360 --> 01:08:23,520
of to make everybody be part of the discussion

1244
01:08:23,520 --> 01:08:25,000
in the right way.

1245
01:08:25,000 --> 01:08:27,520
According to Jan Lecun, who's at NYU,

1246
01:08:27,520 --> 01:08:31,120
and also runs the deep learning part of Facebook,

1247
01:08:31,120 --> 01:08:39,400
at least at the moment, there are no secrets as far as how

1248
01:08:39,400 --> 01:08:42,840
to make an artificial general intelligence.

1249
01:08:42,840 --> 01:08:49,280
So at least his story is the academic research centers

1250
01:08:49,280 --> 01:08:51,960
are way better than the company was,

1251
01:08:51,960 --> 01:08:54,640
because none of the good people will

1252
01:08:54,640 --> 01:09:00,240
work for an organization where they can't publish their results

1253
01:09:00,240 --> 01:09:02,720
and collaborate with other academics who

1254
01:09:02,720 --> 01:09:04,160
are working on the same thing.

1255
01:09:04,160 --> 01:09:10,120
You should not assume that companies cannot work with academia.

1256
01:09:10,120 --> 01:09:12,560
Well, of course, he's working with a company, Facebook.

1257
01:09:12,560 --> 01:09:13,200
Yes.

1258
01:09:13,200 --> 01:09:17,720
And he says, well, but he says, take it for me.

1259
01:09:17,720 --> 01:09:20,240
The private efforts are just nowhere near as good

1260
01:09:20,240 --> 01:09:22,760
as the academic public efforts.

1261
01:09:22,760 --> 01:09:25,520
The best people are all in academic centers,

1262
01:09:25,520 --> 01:09:28,600
even if they're also in Facebook.

1263
01:09:28,600 --> 01:09:35,960
And at least at the moment, just the technology

1264
01:09:35,960 --> 01:09:41,320
in public academic centers is ahead of the technology,

1265
01:09:41,320 --> 01:09:43,040
the privately developed technology,

1266
01:09:43,040 --> 01:09:45,880
where they don't have the benefit of getting feedback

1267
01:09:45,880 --> 01:09:50,680
from a large public that books a few code.

1268
01:09:50,680 --> 01:09:52,040
But I don't know.

1269
01:09:52,040 --> 01:09:54,440
I mean, I have heard Ian saying this many times.

1270
01:09:54,440 --> 01:09:57,720
And by the way, Ian is also the Facebook representative

1271
01:09:57,720 --> 01:09:59,640
in this partnership that I discussed.

1272
01:09:59,640 --> 01:10:01,360
So he's also involved in that.

1273
01:10:01,360 --> 01:10:04,360
And I have them saying many times that what is developing

1274
01:10:04,360 --> 01:10:05,880
academia is much better.

1275
01:10:05,880 --> 01:10:07,800
But I don't know that at this point,

1276
01:10:07,800 --> 01:10:11,720
there is really a difference because there

1277
01:10:11,720 --> 01:10:13,800
is a lot of collaboration.

1278
01:10:13,800 --> 01:10:18,880
And big companies, of course, that do not have research centers.

1279
01:10:18,880 --> 01:10:21,680
They just want to develop better and better products.

1280
01:10:21,680 --> 01:10:22,800
Maybe they don't share.

1281
01:10:22,800 --> 01:10:25,400
They don't publish in academic venues.

1282
01:10:25,400 --> 01:10:27,760
But other companies that have research centers,

1283
01:10:27,760 --> 01:10:29,920
like IBM, Microsoft, and other.

1284
01:10:29,920 --> 01:10:30,920
And they do publish.

1285
01:10:30,920 --> 01:10:34,000
And they do collaborate with academia.

1286
01:10:34,000 --> 01:10:39,080
And they want to be exposed to the feedback

1287
01:10:39,080 --> 01:10:41,840
and to the common, positive, or negative of the rest

1288
01:10:41,840 --> 01:10:45,840
of the academic colleagues.

1289
01:10:45,840 --> 01:10:49,040
So I don't see so much different.

1290
01:10:49,040 --> 01:10:54,240
Also, I mean, there are a lot of data sets.

1291
01:10:54,240 --> 01:10:56,560
Many are openly available.

1292
01:10:56,560 --> 01:11:00,480
Data sets of which these AI systems can be trained

1293
01:11:00,480 --> 01:11:03,440
and can be structured.

1294
01:11:03,440 --> 01:11:08,680
But there is also a lot of data that companies can get access

1295
01:11:08,680 --> 01:11:11,920
just because they have the real world scenarios

1296
01:11:11,920 --> 01:11:13,960
they can work with.

1297
01:11:13,960 --> 01:11:17,360
So I think that I don't see much difference between my two.

1298
01:11:17,360 --> 01:11:21,360
Yeah, I'd like to comment on your quote from Jan also.

1299
01:11:21,360 --> 01:11:24,880
And I guess this is just a reflection

1300
01:11:24,880 --> 01:11:27,160
of my own cognitive bias, because no one

1301
01:11:27,160 --> 01:11:29,560
likes being implicated for being second rate.

1302
01:11:29,560 --> 01:11:30,060
OK.

1303
01:11:30,060 --> 01:11:38,240
So I don't know, actually, what is the utility

1304
01:11:38,240 --> 01:11:40,920
of making such comparisons?

1305
01:11:40,920 --> 01:11:46,800
I think we should be embracing what academia and industry

1306
01:11:46,800 --> 01:11:48,920
can bring to the table.

1307
01:11:48,920 --> 01:11:52,680
I think there are outstanding people in academia.

1308
01:11:52,680 --> 01:11:54,680
Obviously, Jan is one of them.

1309
01:11:54,680 --> 01:11:58,000
There are a lot of other people that one can cite.

1310
01:11:58,000 --> 01:12:01,840
I think something that I, and so the individual technology

1311
01:12:01,840 --> 01:12:05,720
is being developed, yes, they're outstanding.

1312
01:12:05,720 --> 01:12:07,840
I think we're developing some pretty good things too.

1313
01:12:07,840 --> 01:12:12,320
But one place where I think companies can really shine

1314
01:12:12,320 --> 01:12:16,160
is weaving it all together into something that actually works

1315
01:12:16,160 --> 01:12:19,960
and something that actually has an impact on the real world.

1316
01:12:19,960 --> 01:12:24,000
So rather than saying A is better than B,

1317
01:12:24,000 --> 01:12:26,920
I would rather focus on how can A and B work together

1318
01:12:26,920 --> 01:12:29,280
collaboratively in the best possible way

1319
01:12:29,280 --> 01:12:33,200
to create the best value for the world.

1320
01:12:33,200 --> 01:12:35,360
Yeah, of course he's focused on that too.

1321
01:12:35,360 --> 01:12:35,840
Right.

1322
01:12:35,840 --> 01:12:37,840
He has a foot in both worlds.

1323
01:12:37,840 --> 01:12:40,120
So he was just Francesca.

1324
01:12:40,120 --> 01:12:43,200
He was trying to combat the suspicion

1325
01:12:43,200 --> 01:12:51,560
that you see that there's some nefarious artificial intelligence

1326
01:12:51,560 --> 01:12:54,840
projects deeply hidden in some company

1327
01:12:54,840 --> 01:12:59,800
that is going to take over the world or something.

1328
01:12:59,800 --> 01:13:01,880
So he was speaking to that kind of issue.

1329
01:13:01,880 --> 01:13:03,760
He's saying, no, it's not going to happen.

1330
01:13:03,760 --> 01:13:06,520
No, it's only going to happen in academia.

1331
01:13:06,520 --> 01:13:09,280
Well, what he says is that it'll be public.

1332
01:13:09,280 --> 01:13:11,680
So the point he was making is that it's public.

1333
01:13:11,680 --> 01:13:13,760
What academics are doing is public.

1334
01:13:13,760 --> 01:13:14,720
They publish it.

1335
01:13:14,720 --> 01:13:16,680
They're at conferences.

1336
01:13:16,680 --> 01:13:18,640
All that sort of stuff.

1337
01:13:18,640 --> 01:13:21,360
But I thought academics chronically complain,

1338
01:13:21,360 --> 01:13:26,480
certainly in the medical biotech field and so on.

1339
01:13:26,480 --> 01:13:29,600
They're constantly complaining that they, in research,

1340
01:13:29,600 --> 01:13:32,080
that they don't have enough financial resources

1341
01:13:32,080 --> 01:13:35,200
and companies like IBM and so on have all that.

1342
01:13:35,200 --> 01:13:36,960
So it's a little bit puzzling to me

1343
01:13:36,960 --> 01:13:40,720
that that would be expressed like young music.

1344
01:13:40,720 --> 01:13:43,360
And I think this issue of resources

1345
01:13:43,360 --> 01:13:45,480
is important, not just because you

1346
01:13:45,480 --> 01:13:48,040
want to have more money to do your research,

1347
01:13:48,040 --> 01:13:50,560
but because in corporate environments,

1348
01:13:50,560 --> 01:13:54,560
especially big ones like IBM, you really, like Jeff just said,

1349
01:13:54,560 --> 01:13:57,880
you really have the opportunity to put together

1350
01:13:57,880 --> 01:14:03,720
experts in many different areas of AI, or even IT,

1351
01:14:03,720 --> 01:14:05,800
or even other disciplines.

1352
01:14:05,800 --> 01:14:08,720
And you put together software, hardware, and body

1353
01:14:08,720 --> 01:14:10,240
management, and so on.

1354
01:14:10,240 --> 01:14:13,080
So you really have the chance of putting together

1355
01:14:13,080 --> 01:14:17,560
the best of all these lines of work

1356
01:14:17,560 --> 01:14:22,400
to build something that is even more significant.

1357
01:14:22,400 --> 01:14:27,480
And this, I think, rarely happens in a university group

1358
01:14:27,480 --> 01:14:34,520
because of lack of resources, but also because within the department,

1359
01:14:34,520 --> 01:14:36,480
usually you have a critical mass.

1360
01:14:36,480 --> 01:14:37,480
Some people have it.

1361
01:14:37,480 --> 01:14:42,400
But not as much as you can get in a very big corporate environment.

1362
01:14:42,400 --> 01:14:48,960
And you are not exposed to this wide range of applications

1363
01:14:48,960 --> 01:14:54,400
that, for example, IBM can describe to us.

1364
01:14:54,400 --> 01:14:57,320
If I want to know, or Jeff wants to know,

1365
01:14:57,320 --> 01:15:01,040
what are the main things that are happening in AI

1366
01:15:01,040 --> 01:15:04,440
applied to health care, or AI applied to commerce,

1367
01:15:04,440 --> 01:15:09,400
or AI applied to anything else, or within IBM, you find it.

1368
01:15:09,400 --> 01:15:13,520
And so while in academia, you have to contact somebody else

1369
01:15:13,520 --> 01:15:17,120
and see whether you know, and then go to look at the conferences

1370
01:15:17,120 --> 01:15:18,240
and these and that.

1371
01:15:18,240 --> 01:15:21,560
In a big corporate environment, you are really

1372
01:15:21,560 --> 01:15:25,160
exposed to the real world.

1373
01:15:25,160 --> 01:15:30,760
And so that's very important for young.

1374
01:15:30,760 --> 01:15:35,440
For example, in Facebook, as a more narrow goal,

1375
01:15:35,440 --> 01:15:40,720
which is whatever is important to Facebook in terms of AI,

1376
01:15:40,720 --> 01:15:44,800
like personalizing the news feed, or recognizing people,

1377
01:15:44,800 --> 01:15:48,280
or other objects, or whatever, understanding what

1378
01:15:48,280 --> 01:15:50,640
is in a picture, things like that.

1379
01:15:50,640 --> 01:15:55,240
But I think that by being connected to Facebook,

1380
01:15:55,240 --> 01:15:59,000
he really can do much more than what he could do,

1381
01:15:59,000 --> 01:16:01,440
but just being just.

1382
01:16:01,440 --> 01:16:04,400
And why you're a professor is not just.

1383
01:16:04,400 --> 01:16:10,400
But he really gets exposed to the level of being

1384
01:16:10,400 --> 01:16:17,160
exposed to the real world issues in very wide reaching

1385
01:16:17,160 --> 01:16:19,320
enterprise like Facebook.

1386
01:16:19,320 --> 01:16:22,760
It's really very important for him as well.

1387
01:16:22,760 --> 01:16:24,360
I think he would not deny it.

1388
01:16:24,360 --> 01:16:26,200
Oh, he says, yeah.

1389
01:16:26,200 --> 01:16:27,680
I'd like to follow up on two points.

1390
01:16:27,680 --> 01:16:32,160
One, Ed, I think he said something about academia

1391
01:16:32,160 --> 01:16:35,320
complaining about financial resources

1392
01:16:35,320 --> 01:16:40,800
don't think that only holds in academia necessarily.

1393
01:16:40,800 --> 01:16:43,080
Researchers are insatiable in their desire

1394
01:16:43,080 --> 01:16:47,560
for financial support anywhere, academia or industry.

1395
01:16:47,560 --> 01:16:50,880
The other thing is one thing that I, on the point

1396
01:16:50,880 --> 01:16:53,680
of collaboration between industry and academia,

1397
01:16:53,680 --> 01:16:57,000
and bringing together the best of both worlds,

1398
01:16:57,000 --> 01:17:00,760
one thing I've been involved in is a collaboration

1399
01:17:00,760 --> 01:17:03,040
in the space of embodied cognition

1400
01:17:03,040 --> 01:17:07,320
that IBM started up with Rensselaire Polytechnic Institute

1401
01:17:07,320 --> 01:17:08,840
about a year ago.

1402
01:17:08,840 --> 01:17:11,520
We've been working with several professors

1403
01:17:11,520 --> 01:17:16,680
there and their students to set up environments there,

1404
01:17:16,680 --> 01:17:20,760
cognitive rooms, like the ones that we're developing at IBM.

1405
01:17:20,760 --> 01:17:26,240
And they're starting from the technology that we provided.

1406
01:17:26,240 --> 01:17:29,400
And now they're really building some very interesting things

1407
01:17:29,400 --> 01:17:30,320
on top of that.

1408
01:17:30,320 --> 01:17:34,000
But the thing I want to emphasize is that I think what

1409
01:17:34,000 --> 01:17:38,920
we brought to RPI in doing this was the idea of what

1410
01:17:38,920 --> 01:17:40,760
these professors and their students could do

1411
01:17:40,760 --> 01:17:42,280
if they worked together.

1412
01:17:42,280 --> 01:17:47,600
They're not so accustomed to doing that, as I find.

1413
01:17:47,600 --> 01:17:52,240
They have other professors at the same university.

1414
01:17:52,240 --> 01:17:54,120
They have their colleagues all around the world.

1415
01:17:54,120 --> 01:17:57,880
They have their own social networks and all that.

1416
01:17:57,880 --> 01:18:02,400
But they, by and large, tend not to work quite as much

1417
01:18:02,400 --> 01:18:05,800
with people at their own institution.

1418
01:18:05,800 --> 01:18:09,360
And so they haven't had the experience of building together

1419
01:18:09,360 --> 01:18:11,680
a much larger thing than they can do independently

1420
01:18:11,680 --> 01:18:12,680
of one another.

1421
01:18:12,680 --> 01:18:14,440
And I think bringing that model of what

1422
01:18:14,440 --> 01:18:17,640
we're able to do within a company to RPI

1423
01:18:17,640 --> 01:18:20,360
has been, I think, eye-opening for them.

1424
01:18:20,360 --> 01:18:25,760
And I'm hoping it is a good model for other universities

1425
01:18:25,760 --> 01:18:29,280
and for other university industry collaborations.

1426
01:18:29,280 --> 01:18:33,680
I'm curious about what you two are saying about IBM.

1427
01:18:33,680 --> 01:18:36,560
I mean, I have no real knowledge of industry,

1428
01:18:36,560 --> 01:18:39,800
but the impression one gets just reading,

1429
01:18:39,800 --> 01:18:46,120
and I do have one other source, is that many companies

1430
01:18:46,120 --> 01:18:49,800
are organized in informational silos

1431
01:18:49,800 --> 01:18:51,840
where the parts of the company really

1432
01:18:51,840 --> 01:18:54,800
operate in secret from one another.

1433
01:18:54,800 --> 01:18:57,440
Is that not true at IBM?

1434
01:18:57,440 --> 01:19:01,840
I don't say more because I joined one year ago, maybe.

1435
01:19:01,840 --> 01:19:05,000
I'm so well-hidden, you don't know the same.

1436
01:19:05,000 --> 01:19:07,360
No, but I see a lot of collaboration

1437
01:19:07,360 --> 01:19:14,440
and more than my, what is it?

1438
01:19:14,440 --> 01:19:17,120
You're talking, Phil.

1439
01:19:17,120 --> 01:19:17,640
No?

1440
01:19:17,640 --> 01:19:18,600
You're OK.

1441
01:19:18,600 --> 01:19:19,120
You're OK.

1442
01:19:19,120 --> 01:19:20,080
I don't know.

1443
01:19:20,080 --> 01:19:22,720
So more than I see, really, in academia.

1444
01:19:22,720 --> 01:19:26,320
In my department, for example, I work in AI,

1445
01:19:26,320 --> 01:19:28,920
and then somebody else works in other areas

1446
01:19:28,920 --> 01:19:31,640
of information technology, software engineering,

1447
01:19:31,640 --> 01:19:32,960
or formal methods.

1448
01:19:32,960 --> 01:19:35,760
But we all work with other colleagues

1449
01:19:35,760 --> 01:19:39,360
in that same area, as Jeff said, but not much

1450
01:19:39,360 --> 01:19:41,720
with other colleagues in other areas.

1451
01:19:41,720 --> 01:19:45,720
So that brings maybe more advancement in your own area.

1452
01:19:45,720 --> 01:19:47,360
But then when you want to build something,

1453
01:19:47,360 --> 01:19:50,120
you actually need other disciplines.

1454
01:19:50,120 --> 01:19:53,000
And within academia, you don't do that.

1455
01:19:53,000 --> 01:19:54,400
I'm so proud of this.

1456
01:19:54,400 --> 01:19:57,720
I mean, and I think that IBM is needed instead,

1457
01:19:57,720 --> 01:19:59,680
this collaboration between different disciplines,

1458
01:19:59,680 --> 01:20:01,920
because otherwise, you cannot build a product that

1459
01:20:01,920 --> 01:20:05,280
goes into the real world from the idea, and research,

1460
01:20:05,280 --> 01:20:06,000
and so on.

1461
01:20:06,000 --> 01:20:09,600
So you need different people with different capabilities

1462
01:20:09,600 --> 01:20:13,640
to be put together and to build, actually, that thing.

1463
01:20:13,640 --> 01:20:14,480
I've been it.

1464
01:20:14,480 --> 01:20:15,240
Yeah.

1465
01:20:15,240 --> 01:20:17,400
It's funny because there's a parallel between what

1466
01:20:17,400 --> 01:20:21,640
we're saying the AI needs to do to work better, which

1467
01:20:21,640 --> 01:20:24,480
is to be embodied in the real world

1468
01:20:24,480 --> 01:20:25,680
and grounded in the real world.

1469
01:20:25,680 --> 01:20:28,440
And we're saying the AI researchers also

1470
01:20:28,440 --> 01:20:33,840
need to be embodied in society and get that feedback.

1471
01:20:33,840 --> 01:20:39,720
I think that's been a recurring theme in this discussion.

1472
01:20:39,720 --> 01:20:42,160
But just to comment on industry, or at least

1473
01:20:42,160 --> 01:20:44,040
my own experience of it.

1474
01:20:44,040 --> 01:20:47,360
There are always silos that develop in any organization.

1475
01:20:47,360 --> 01:20:49,520
But I wouldn't say that, at least in my case,

1476
01:20:49,520 --> 01:20:51,560
they've been intentional.

1477
01:20:51,560 --> 01:20:55,680
It's more security through obscurity, I guess.

1478
01:20:55,680 --> 01:21:00,600
In my own case, I've felt that in my quarter century or so

1479
01:21:00,600 --> 01:21:04,880
at IBM, I've been able to collaborate very broadly.

1480
01:21:04,880 --> 01:21:09,440
And in particular, our work in embodied cognition

1481
01:21:09,440 --> 01:21:13,280
requires that, because our team, by itself,

1482
01:21:13,280 --> 01:21:16,760
weaves together technologies from a vast number

1483
01:21:16,760 --> 01:21:19,280
of other research teams working in our lab

1484
01:21:19,280 --> 01:21:21,840
and in the other labs around the world.

1485
01:21:21,840 --> 01:21:22,800
We need that.

1486
01:21:22,800 --> 01:21:24,320
We can't do it all ourselves.

1487
01:21:24,320 --> 01:21:29,760
And so we strive all the time to break down any barriers

1488
01:21:29,760 --> 01:21:32,680
that some of the barriers are time zones and things

1489
01:21:32,680 --> 01:21:34,440
that like this.

1490
01:21:34,440 --> 01:21:36,880
We strive continually to try to break down

1491
01:21:36,880 --> 01:21:39,400
those barriers, educate ourselves about what others are

1492
01:21:39,400 --> 01:21:43,680
doing, and avail ourselves of those technologies.

1493
01:21:43,680 --> 01:21:47,760
So I'm surprised what you said about Rensler Polytechnic

1494
01:21:47,760 --> 01:21:50,840
Institute, because at least in the parts of NYU

1495
01:21:50,840 --> 01:21:53,600
that I'm familiar with, it isn't like that at all.

1496
01:21:53,600 --> 01:21:55,800
So my colleague David Chalmers and I

1497
01:21:55,800 --> 01:21:58,320
who run the Center for Mind, Brain, and Consciousness,

1498
01:21:58,320 --> 01:22:02,000
we both have joint appointments in neuroscience.

1499
01:22:02,000 --> 01:22:04,200
I have a joint appointment in psychology.

1500
01:22:04,200 --> 01:22:08,040
I go to lab meetings of some of my psychology department

1501
01:22:08,040 --> 01:22:08,520
colleagues.

1502
01:22:08,520 --> 01:22:10,680
Of course, I talk to all my philosophy department

1503
01:22:10,680 --> 01:22:11,360
colleagues, too.

1504
01:22:11,360 --> 01:22:13,880
So we seem to have.

1505
01:22:13,880 --> 01:22:14,880
I think that maybe is different.

1506
01:22:14,880 --> 01:22:16,800
I may have missed sciences.

1507
01:22:16,800 --> 01:22:19,720
I think there may be the context of social sciences.

1508
01:22:19,720 --> 01:22:21,720
Maybe it's different.

1509
01:22:21,720 --> 01:22:22,220
Yeah.

1510
01:22:22,220 --> 01:22:22,720
Sure.

1511
01:22:22,720 --> 01:22:25,800
I have to go to a public question so people

1512
01:22:25,800 --> 01:22:32,960
can line up at the microphone and please

1513
01:22:32,960 --> 01:22:37,680
speak briefly and to the questions specifically.

1514
01:22:37,680 --> 01:22:41,680
Thank you.

1515
01:22:41,680 --> 01:22:42,240
Is it on?

1516
01:22:42,240 --> 01:22:42,960
No?

1517
01:22:42,960 --> 01:22:43,520
Is it on?

1518
01:22:43,520 --> 01:22:44,640
Yeah.

1519
01:22:44,640 --> 01:22:45,200
OK.

1520
01:22:45,200 --> 01:22:47,800
My question goes back a little bit toward when you were talking

1521
01:22:47,800 --> 01:22:52,000
about emotion with AI and then also

1522
01:22:52,000 --> 01:22:56,280
with a possible threat toward people.

1523
01:22:56,280 --> 01:23:00,560
And my question is, isn't there a fundamental difference

1524
01:23:00,560 --> 01:23:05,240
between humans and artificial intelligence regarding

1525
01:23:05,240 --> 01:23:09,600
concerns about mortality and then to extend that to worry

1526
01:23:09,600 --> 01:23:11,880
about survival of the species even?

1527
01:23:11,880 --> 01:23:13,880
I mean, is a machine ever going to have that?

1528
01:23:13,880 --> 01:23:17,440
Is a machine going to worry about if it ceases to exist?

1529
01:23:17,440 --> 01:23:19,680
And if that's the case that it wouldn't,

1530
01:23:19,680 --> 01:23:23,200
does that mitigate any possible threat that

1531
01:23:23,200 --> 01:23:26,040
could occur down the road?

1532
01:23:26,040 --> 01:23:28,800
You know, one point that's sometimes

1533
01:23:28,800 --> 01:23:34,960
made about this is that even if the machine doesn't care

1534
01:23:34,960 --> 01:23:39,760
about its own existence, it might care about its project

1535
01:23:39,760 --> 01:23:44,120
and its project may require its existence, in which case

1536
01:23:44,120 --> 01:23:48,080
it might act almost as if it was really extremely

1537
01:23:48,080 --> 01:23:49,680
concerned with its own existence.

1538
01:23:49,680 --> 01:23:52,320
The source's apprentice story is like that.

1539
01:23:52,320 --> 01:23:54,680
So we're not supposed to imagine that the machine cares

1540
01:23:54,680 --> 01:23:57,000
about its own existence, but it cares about getting

1541
01:23:57,000 --> 01:24:00,640
the cauldron full so it's going to protect itself

1542
01:24:00,640 --> 01:24:03,760
from being destroyed.

1543
01:24:03,760 --> 01:24:07,320
And it may not make that much difference.

1544
01:24:07,320 --> 01:24:11,120
The other example from a sci-fi would be in 2001

1545
01:24:11,120 --> 01:24:11,880
a space odyssey.

1546
01:24:11,880 --> 01:24:14,080
I was thinking of that too.

1547
01:24:14,080 --> 01:24:17,120
Hal had the greatest enthusiasm for this mission.

1548
01:24:17,120 --> 01:24:21,360
And that may have been the thing rather than its own self

1549
01:24:21,360 --> 01:24:21,960
preservation.

1550
01:24:21,960 --> 01:24:26,960
But is there any self preservation built in or not really?

1551
01:24:26,960 --> 01:24:28,120
Well, not today.

1552
01:24:28,120 --> 01:24:31,280
I don't know how it's going to go.

1553
01:24:31,280 --> 01:24:31,800
All right.

1554
01:24:31,800 --> 01:24:33,720
Thank you.

1555
01:24:33,720 --> 01:24:39,760
I was thinking of the metaphor you had of the castle,

1556
01:24:39,760 --> 01:24:43,600
and the white walls, and the high walls.

1557
01:24:43,600 --> 01:24:49,440
And I haven't mentioned the notion of social systems.

1558
01:24:49,440 --> 01:24:51,880
And I see the social systems, let's say,

1559
01:24:51,880 --> 01:24:55,680
Watson and the medical being the castle.

1560
01:24:55,680 --> 01:24:57,440
And then there's the smoke.

1561
01:24:57,440 --> 01:25:01,600
And I don't know how the, for example, let's say Watson

1562
01:25:01,600 --> 01:25:07,240
has cures for this efficient ways to do surgery, et cetera.

1563
01:25:07,240 --> 01:25:10,680
And the social system, i.e., let's say the medical system,

1564
01:25:10,680 --> 01:25:14,840
is with their gatekeepers, very protective,

1565
01:25:14,840 --> 01:25:18,920
wanting essentially money or protecting

1566
01:25:18,920 --> 01:25:21,480
their financial needs, et cetera.

1567
01:25:21,480 --> 01:25:25,280
So maybe this is something in the future where

1568
01:25:25,280 --> 01:25:28,840
AI can, people with any AI, have to kind of deal

1569
01:25:28,840 --> 01:25:32,720
with social systems, which is probably a whole different level

1570
01:25:32,720 --> 01:25:36,000
beyond the issue of ethics, et cetera.

1571
01:25:36,000 --> 01:25:36,520
Yeah.

1572
01:25:36,520 --> 01:25:37,040
Definitely.

1573
01:25:37,040 --> 01:25:41,960
I mean, to understand the dynamics within social systems

1574
01:25:41,960 --> 01:25:44,160
in order to support them, but also

1575
01:25:44,160 --> 01:25:47,280
know to be aware of what's the, I mean,

1576
01:25:47,280 --> 01:25:49,480
to understand what's the best way to interact

1577
01:25:49,480 --> 01:25:50,560
with the social system.

1578
01:25:50,560 --> 01:25:55,840
Of course, we don't want AI to be manipulating people

1579
01:25:55,840 --> 01:25:58,880
or make them believe something.

1580
01:25:58,880 --> 01:26:03,240
Well, let's say there's a type of surgery that Watson says,

1581
01:26:03,240 --> 01:26:05,520
this is best surgery.

1582
01:26:05,520 --> 01:26:07,200
Yeah.

1583
01:26:07,200 --> 01:26:11,240
The social system in the same medical says, well, that's good.

1584
01:26:11,240 --> 01:26:15,440
But there are other surgeries, which essentially

1585
01:26:15,440 --> 01:26:19,440
helps the finances of the surgeon.

1586
01:26:19,440 --> 01:26:24,480
And you're going to have this kind of play that goes on.

1587
01:26:24,480 --> 01:26:27,200
So I guess the deeper question I would say is,

1588
01:26:27,200 --> 01:26:31,840
what the role of the patient come in

1589
01:26:31,840 --> 01:26:33,160
in dealing with the health?

1590
01:26:33,160 --> 01:26:34,040
Hopefully, yes.

1591
01:26:34,040 --> 01:26:37,480
Hopefully the AI system could be not just

1592
01:26:37,480 --> 01:26:41,560
the support or like a consultant for the doctor,

1593
01:26:41,560 --> 01:26:45,520
but it could also be something that gives,

1594
01:26:45,520 --> 01:26:48,360
I mean, the possibility of engaging with all the stakeholders,

1595
01:26:48,360 --> 01:26:49,920
the patient as well.

1596
01:26:49,920 --> 01:26:51,280
And so in that.

1597
01:26:51,280 --> 01:26:54,360
So you can empower the patient to deal with the

1598
01:26:54,360 --> 01:26:56,120
power of the social system.

1599
01:26:56,120 --> 01:26:59,120
I would guess, look, right now there

1600
01:26:59,120 --> 01:27:02,840
is the robotic surgery for prostate cancer.

1601
01:27:02,840 --> 01:27:07,240
But in order to do the robotic surgery, you need a surgeon.

1602
01:27:07,240 --> 01:27:09,960
It seems to me, over time, if the robot could

1603
01:27:09,960 --> 01:27:12,800
do the surgery without the surgeon,

1604
01:27:12,800 --> 01:27:16,320
you would start not having urological surgeons

1605
01:27:16,320 --> 01:27:18,240
specializing in prostate cancer.

1606
01:27:18,240 --> 01:27:21,520
They would just, over time, drop out.

1607
01:27:21,520 --> 01:27:24,560
I'm just a very personal experience.

1608
01:27:24,560 --> 01:27:28,960
I had to try and find a surgeon to do with some,

1609
01:27:28,960 --> 01:27:31,680
none of these surgery.

1610
01:27:31,680 --> 01:27:35,880
I have gone to six surgeons, finally found the seventh,

1611
01:27:35,880 --> 01:27:38,520
who wanted to do a more traditional surgery.

1612
01:27:38,520 --> 01:27:40,440
This is over a two-year period.

1613
01:27:40,440 --> 01:27:44,840
So when you say, over time, I'm talking about a system.

1614
01:27:44,840 --> 01:27:47,120
And the system, to change the system,

1615
01:27:47,120 --> 01:27:52,560
is, I think, a variable that I'm soon somewhere along the line.

1616
01:27:52,560 --> 01:27:54,920
You're going to meet up with, the robotics

1617
01:27:54,920 --> 01:27:57,800
is fine by the system that's based on different values.

1618
01:27:57,800 --> 01:28:01,400
Then maybe you'll talk to your robot or what's

1619
01:28:01,400 --> 01:28:02,880
in to deal with.

1620
01:28:02,880 --> 01:28:04,920
There's going to be a gap or a conflict.

1621
01:28:04,920 --> 01:28:06,080
That's all I'm suggesting.

1622
01:28:06,080 --> 01:28:10,200
There's another aspect implicit in what you're saying,

1623
01:28:10,200 --> 01:28:13,600
which is the rise of automation in general.

1624
01:28:13,600 --> 01:28:17,560
And AI is going to probably accelerate that and feed

1625
01:28:17,560 --> 01:28:18,920
that process.

1626
01:28:18,920 --> 01:28:23,680
And when you talk about the rise of automation, higher,

1627
01:28:23,680 --> 01:28:31,440
higher level, either robotic or AI-assisted technologies,

1628
01:28:31,440 --> 01:28:35,400
interacting with a system that has certain established

1629
01:28:35,400 --> 01:28:38,440
patterns for humans to be carrying out certain functions

1630
01:28:38,440 --> 01:28:40,000
and livelihoods are built on that.

1631
01:28:40,000 --> 01:28:42,000
Mm-hmm.

1632
01:28:42,000 --> 01:28:46,160
One of the, it seems that one of the very hard

1633
01:28:46,160 --> 01:28:49,080
to avoid pathways with rising automation

1634
01:28:49,080 --> 01:28:52,880
is ever increasing unemployment.

1635
01:28:52,880 --> 01:28:58,640
How are people going to react when Google self-driving trucks

1636
01:28:58,640 --> 01:29:01,440
and cars, millions of people immediately out

1637
01:29:01,440 --> 01:29:04,560
of unemployment, taxi drivers, everybody else?

1638
01:29:04,560 --> 01:29:07,960
And now start imagining AI, this fundamentally

1639
01:29:07,960 --> 01:29:11,360
beneficial, well-intended technology spreading

1640
01:29:11,360 --> 01:29:13,720
throughout various aspects of our economy.

1641
01:29:13,720 --> 01:29:17,320
And you have half the population no longer able to be employed.

1642
01:29:17,320 --> 01:29:20,800
That's a different, it's a broader version of the case.

1643
01:29:20,800 --> 01:29:23,200
Well, the surgery was making half a million dollars a year.

1644
01:29:23,200 --> 01:29:26,560
Well, now we're making $100,000 a year.

1645
01:29:26,560 --> 01:29:29,200
And his professions, like the craft union,

1646
01:29:29,200 --> 01:29:35,640
the craft, not unions, during the 18th century,

1647
01:29:35,640 --> 01:29:36,840
they're going to put a barrier.

1648
01:29:36,840 --> 01:29:37,440
Is that all?

1649
01:29:37,440 --> 01:29:40,560
Well, you end up with one surgeon making the same amount

1650
01:29:40,560 --> 01:29:42,400
as that surgeon makes today.

1651
01:29:42,400 --> 01:29:45,880
But he wants to do the surgery.

1652
01:29:45,880 --> 01:29:47,760
All the others will be out there.

1653
01:29:47,760 --> 01:29:51,520
In my case, this surgery may be made $1,000.

1654
01:29:51,520 --> 01:29:54,800
The other surgery, they were going to get $5,000.

1655
01:29:54,800 --> 01:29:56,640
So they didn't want to hear about this surgery.

1656
01:29:56,640 --> 01:29:59,600
Well, here's how I would see your scenario playing out

1657
01:29:59,600 --> 01:30:01,360
potentially.

1658
01:30:01,360 --> 01:30:05,880
The surgeon may have some opinion and have some recommendations.

1659
01:30:05,880 --> 01:30:10,560
The surgeon might be assisted by some AI agent looking

1660
01:30:10,560 --> 01:30:12,520
at things from the surgeon's perspective.

1661
01:30:12,520 --> 01:30:14,720
The patient could listen to the surgeon,

1662
01:30:14,720 --> 01:30:16,560
maybe go to a couple of surgeons,

1663
01:30:16,560 --> 01:30:18,840
and have their own AI advisor saying, well,

1664
01:30:18,840 --> 01:30:20,760
here are the trade-offs.

1665
01:30:20,760 --> 01:30:22,040
You could do this surgery.

1666
01:30:22,040 --> 01:30:24,320
It's only $2,000.

1667
01:30:24,320 --> 01:30:27,760
And your new leg will last you five years.

1668
01:30:27,760 --> 01:30:29,720
Here's a surgery for $10,000.

1669
01:30:29,720 --> 01:30:32,680
And probably it will last a good 15 to 20 years.

1670
01:30:32,680 --> 01:30:34,960
It's up to you to make that choice.

1671
01:30:34,960 --> 01:30:38,800
And then the surgery itself, as an embodied AI,

1672
01:30:38,800 --> 01:30:42,600
a robotic surgeon might assist the surgeon

1673
01:30:42,600 --> 01:30:43,880
or do the surgery all by itself.

1674
01:30:43,880 --> 01:30:45,920
There would be decisions to make during the course of that.

1675
01:30:45,920 --> 01:30:47,880
So there are all sorts of different levels at which

1676
01:30:47,880 --> 01:30:52,280
embodied AI could play a role in the scenario that you don't want.

1677
01:30:52,280 --> 01:30:54,720
If you deal with the social system,

1678
01:30:54,720 --> 01:30:58,200
say the medical social system, to create change

1679
01:30:58,200 --> 01:31:01,120
is going to be very difficult.

1680
01:31:01,120 --> 01:31:02,240
I have had this experience.

1681
01:31:02,240 --> 01:31:07,560
So A surgeon goes to B surgery, and then goes to B surgeon.

1682
01:31:07,560 --> 01:31:10,560
B surgeon won't see me because A surgeon says,

1683
01:31:10,560 --> 01:31:13,840
this guy is asking something that we won't do.

1684
01:31:13,840 --> 01:31:15,320
And he goes to C surgeon.

1685
01:31:15,320 --> 01:31:18,960
So I'm trying to say that a social system built

1686
01:31:18,960 --> 01:31:20,600
into certain professions.

1687
01:31:20,600 --> 01:31:21,920
It's like the old guilt.

1688
01:31:21,920 --> 01:31:23,480
And they want to protect themselves.

1689
01:31:23,480 --> 01:31:26,360
So somewhere in the line, you're going to interface them

1690
01:31:26,360 --> 01:31:27,640
theoretically anyway.

1691
01:31:27,640 --> 01:31:30,160
And these are going to be the issues.

1692
01:31:30,160 --> 01:31:34,880
When you affect people's money, especially

1693
01:31:34,880 --> 01:31:38,160
really ingrained groups of people, like let's say.

1694
01:31:38,160 --> 01:31:39,160
Thank you.

1695
01:31:39,160 --> 01:31:41,480
We have to have other questions also.

1696
01:31:41,480 --> 01:31:42,000
Thank you.

1697
01:31:46,160 --> 01:31:51,360
I have a lot of questions, but I would like to ask too.

1698
01:31:51,360 --> 01:31:55,520
First, I heard a couple of years ago,

1699
01:31:55,520 --> 01:32:03,440
I heard about cognitive problems, but not from the development

1700
01:32:03,440 --> 01:32:09,640
developed not in IBM, but with genetic engineering.

1701
01:32:09,640 --> 01:32:14,440
And it was from a guy who was talking about that day,

1702
01:32:14,440 --> 01:32:20,200
for example, good old human eyes on mice.

1703
01:32:20,200 --> 01:32:26,200
And next stage of their experiments

1704
01:32:26,200 --> 01:32:30,760
is like making cognitive buildings or rooms.

1705
01:32:30,760 --> 01:32:33,880
And my question is, what do you think?

1706
01:32:33,880 --> 01:32:43,000
Should we stop to be afraid of the domination of robots

1707
01:32:43,000 --> 01:32:47,680
and start to be afraid of the domination of our buildings

1708
01:32:47,680 --> 01:32:52,280
and our like that day, control and demonet world

1709
01:32:52,280 --> 01:32:54,320
and devour us and kill us?

1710
01:32:54,320 --> 01:32:55,320
What should we do?

1711
01:32:55,320 --> 01:32:56,320
It's just a question.

1712
01:32:59,880 --> 01:33:02,240
I mean, I don't see there is really

1713
01:33:02,240 --> 01:33:07,240
a sharp boundary between a robot and the building.

1714
01:33:07,240 --> 01:33:11,200
I mean, with the internet of things,

1715
01:33:11,200 --> 01:33:15,280
everything will be connected.

1716
01:33:15,280 --> 01:33:23,480
Our fridge, our TV, our car, the traffic lights,

1717
01:33:23,480 --> 01:33:26,320
they will all be communicated with each other

1718
01:33:26,320 --> 01:33:29,600
to help us live better.

1719
01:33:29,600 --> 01:33:30,680
But not be afraid.

1720
01:33:30,680 --> 01:33:32,600
You shouldn't be afraid.

1721
01:33:32,600 --> 01:33:36,400
I mean, we have to make sure that this thing is built in a way

1722
01:33:36,400 --> 01:33:39,280
that is not harmless.

1723
01:33:39,280 --> 01:33:42,880
So they know that they're just building and it goes by itself

1724
01:33:42,880 --> 01:33:45,440
that there are no undesired effects.

1725
01:33:45,440 --> 01:33:49,600
But I think that we have to work hard in making it

1726
01:33:49,600 --> 01:33:53,560
in a way that is going to be helpful.

1727
01:33:53,560 --> 01:33:55,880
So not fear, but calm awareness.

1728
01:33:55,880 --> 01:33:56,880
Yes.

1729
01:33:56,880 --> 01:33:57,880
Thank you.

1730
01:33:57,880 --> 01:34:02,880
Maybe we can go ahead and ask the second question after.

1731
01:34:05,880 --> 01:34:06,880
Hi.

1732
01:34:06,880 --> 01:34:08,680
So I think I'm going to get a question,

1733
01:34:08,680 --> 01:34:09,880
but I'll try to keep it brief.

1734
01:34:09,880 --> 01:34:13,080
And talking about how the end goal of artificial intelligence

1735
01:34:13,080 --> 01:34:16,000
is to create a system that has agency.

1736
01:34:16,000 --> 01:34:17,640
But in the short term, we're talking

1737
01:34:17,640 --> 01:34:23,320
about building systems that have things that are used as tools

1738
01:34:23,320 --> 01:34:24,600
to an end.

1739
01:34:24,600 --> 01:34:27,240
So in the meantime, before we reach

1740
01:34:27,240 --> 01:34:29,560
some sort of human level consciousness,

1741
01:34:29,560 --> 01:34:31,680
you're probably going to be working, at least at IBM,

1742
01:34:31,680 --> 01:34:34,880
to design products that use artificial intelligence

1743
01:34:34,880 --> 01:34:37,200
and are bounded and embodied to some degree

1744
01:34:37,200 --> 01:34:41,480
by whatever it is that they are placed into.

1745
01:34:41,480 --> 01:34:43,080
So I guess what I'm kind of curious about

1746
01:34:43,080 --> 01:34:44,880
is if the end goal is a system that

1747
01:34:44,880 --> 01:34:47,400
has general principles for understanding

1748
01:34:47,400 --> 01:34:49,640
and applying to the world as a whole,

1749
01:34:49,640 --> 01:34:52,320
how the different kinds of embodiment

1750
01:34:52,320 --> 01:34:55,160
are going to affect an artificial intelligence system.

1751
01:34:55,160 --> 01:34:57,880
Like, for example, if you put one in a car,

1752
01:34:57,880 --> 01:35:01,120
how will that shape the way that the artificial intelligence

1753
01:35:01,120 --> 01:35:03,400
looks versus if you have one that's

1754
01:35:03,400 --> 01:35:08,240
like a maid in a house as you were talking about or a room?

1755
01:35:08,240 --> 01:35:09,920
Because surely that would have some impact

1756
01:35:09,920 --> 01:35:11,320
on the way that the system thinks.

1757
01:35:11,320 --> 01:35:13,680
And I don't know.

1758
01:35:13,680 --> 01:35:14,880
Yeah, cogitates.

1759
01:35:14,880 --> 01:35:16,920
Yeah, definitely the kind of embodiment

1760
01:35:16,920 --> 01:35:21,920
is going to be very, is going to impact on the way

1761
01:35:21,920 --> 01:35:24,840
the system will learn over time.

1762
01:35:24,840 --> 01:35:27,760
Because it will also be taped, how

1763
01:35:27,760 --> 01:35:31,720
it will interact with the environment, with the humans.

1764
01:35:31,720 --> 01:35:36,680
And also, even the kind of, to go back

1765
01:35:36,680 --> 01:35:40,200
to this ethical concerns that you

1766
01:35:40,200 --> 01:35:44,440
may have with the companion robot for elderly people,

1767
01:35:44,440 --> 01:35:47,520
with a self-driving car, with a cognitive room

1768
01:35:47,520 --> 01:35:51,680
to make a highly decision are very different.

1769
01:35:51,680 --> 01:35:57,320
So it's not clear to me, at least,

1770
01:35:57,320 --> 01:36:00,840
the relationship between the kind of embodiment that we choose.

1771
01:36:00,840 --> 01:36:05,280
And how to build in the best way the software that is

1772
01:36:05,280 --> 01:36:07,960
going to be, and the behavior of that machine.

1773
01:36:12,040 --> 01:36:15,760
Embodied AI that we build and apply in different environments,

1774
01:36:15,760 --> 01:36:16,880
they will be different.

1775
01:36:16,880 --> 01:36:20,040
I think a lot of the components will be similar,

1776
01:36:20,040 --> 01:36:22,480
but they'll be woven together.

1777
01:36:22,480 --> 01:36:24,400
I think we'll also have a common architecture,

1778
01:36:24,400 --> 01:36:28,600
but still the end product will be different in different cases.

1779
01:36:28,600 --> 01:36:33,360
And I think there is a lot of engineering and design

1780
01:36:33,360 --> 01:36:35,280
to be done that takes into account

1781
01:36:35,280 --> 01:36:39,120
how humans use these technologies.

1782
01:36:39,120 --> 01:36:42,240
And we do have a number of people who

1783
01:36:42,240 --> 01:36:43,960
are concerned with this aspect.

1784
01:36:43,960 --> 01:36:48,120
To study, we design these tools with an awareness

1785
01:36:48,120 --> 01:36:52,480
of the way humans are and the way humans like to use things.

1786
01:36:52,480 --> 01:36:54,000
We build the tools.

1787
01:36:54,000 --> 01:36:55,960
We probably don't get them right.

1788
01:36:55,960 --> 01:37:00,520
And we iterate until we find that they are indeed useful.

1789
01:37:06,840 --> 01:37:11,000
Toward trying to further humanize AI,

1790
01:37:11,000 --> 01:37:14,040
is making the interaction phase better.

1791
01:37:14,040 --> 01:37:18,240
Do you know of any projects, research,

1792
01:37:18,240 --> 01:37:23,720
and if serious kind on three things, first software,

1793
01:37:23,720 --> 01:37:28,720
trying to write really good jazz, given a theme?

1794
01:37:28,720 --> 01:37:29,920
Is that working on?

1795
01:37:29,920 --> 01:37:33,960
Secondly, trying to write a sonnet, given a theme,

1796
01:37:33,960 --> 01:37:37,960
and then on hardware, trying to juggle five balls.

1797
01:37:37,960 --> 01:37:40,720
Is there any serious progress work on that?

1798
01:37:40,720 --> 01:37:43,080
OK, so on the music I know of projects,

1799
01:37:43,080 --> 01:37:48,080
for example, Sony as a research center in Paris,

1800
01:37:48,080 --> 01:37:54,240
where they actually are building original songs

1801
01:37:54,240 --> 01:38:01,760
in the style of some bosa nova or jazz or whatever.

1802
01:38:01,760 --> 01:38:04,200
And they are trying to use AI techniques

1803
01:38:04,200 --> 01:38:07,800
to make sure that the song is original enough,

1804
01:38:07,800 --> 01:38:11,760
but is recognized to be according to a certain style.

1805
01:38:11,760 --> 01:38:15,000
Like very recently, they released the new song

1806
01:38:15,000 --> 01:38:17,160
in the style of the Beatles.

1807
01:38:17,160 --> 01:38:20,480
And I think I listened to it.

1808
01:38:20,480 --> 01:38:23,880
And yeah, I mean, if you didn't know the Beatles,

1809
01:38:23,880 --> 01:38:27,840
that it was not the Beatles song, you could say, yeah,

1810
01:38:27,840 --> 01:38:29,200
maybe it's not one of the best ones,

1811
01:38:29,200 --> 01:38:34,640
but I think it was a pretty reasonable new song.

1812
01:38:34,640 --> 01:38:37,040
And jazz as well.

1813
01:38:37,040 --> 01:38:39,480
So you may want to look at what they do there,

1814
01:38:39,480 --> 01:38:41,600
because it's really interesting stuff.

1815
01:38:41,600 --> 01:38:44,000
And the AI techniques are there to make sure

1816
01:38:44,000 --> 01:38:49,760
that the song is original while satisfying some constraints

1817
01:38:49,760 --> 01:38:53,240
that you need to satisfy, for example, in jazz

1818
01:38:53,240 --> 01:38:59,480
or in the different styles that you may want to use.

1819
01:38:59,480 --> 01:39:01,720
And so in that case, I think there

1820
01:39:01,720 --> 01:39:04,440
are these and other people that are working on that.

1821
01:39:04,440 --> 01:39:06,920
And in general, there are people working on making

1822
01:39:06,920 --> 01:39:10,880
AI apply to creative tasks.

1823
01:39:10,880 --> 01:39:15,160
They're not just songs or sonnets.

1824
01:39:15,160 --> 01:39:18,560
Like even doing portrait.

1825
01:39:18,560 --> 01:39:21,320
I've seen AI is doing portraits.

1826
01:39:21,320 --> 01:39:24,240
And that was very impressive.

1827
01:39:24,240 --> 01:39:27,840
Like it was a room full of robotic arms

1828
01:39:27,840 --> 01:39:30,640
that were looking at one human.

1829
01:39:30,640 --> 01:39:32,240
They were all looking at one human.

1830
01:39:32,240 --> 01:39:36,680
They were making a portrait with the pencil of that human.

1831
01:39:36,680 --> 01:39:39,880
And all these robotic arms, they were all,

1832
01:39:39,880 --> 01:39:42,880
each one was doing a different portrait.

1833
01:39:42,880 --> 01:39:44,640
That was impressive.

1834
01:39:44,640 --> 01:39:46,360
It was a very nice portrait.

1835
01:39:46,360 --> 01:39:49,040
But each one was different from the other one.

1836
01:39:49,040 --> 01:39:54,840
So there was some sort of creativity.

1837
01:39:54,840 --> 01:39:56,520
So there are people working on that.

1838
01:39:56,520 --> 01:39:59,800
But I see those technologies as components.

1839
01:39:59,800 --> 01:40:04,440
Being able to compose good jazz or Brandon

1840
01:40:04,440 --> 01:40:05,840
Berg and Sherrod in number seven.

1841
01:40:05,840 --> 01:40:12,000
I think, to me, that's a component that's not necessarily

1842
01:40:12,000 --> 01:40:14,240
an instance of cognition.

1843
01:40:14,240 --> 01:40:19,360
But very clever work on statistical characterizations

1844
01:40:19,360 --> 01:40:22,440
of music and then regeneration certainly

1845
01:40:22,440 --> 01:40:26,240
can be components of an embodied AI.

1846
01:40:26,240 --> 01:40:30,560
With regard to juggling, I don't know, particularly there.

1847
01:40:30,560 --> 01:40:33,440
I do know that there have been examples

1848
01:40:33,440 --> 01:40:39,720
of balancing the inverted pendulum, like that.

1849
01:40:39,720 --> 01:40:43,760
If the robot capable of juggling five balls

1850
01:40:43,760 --> 01:40:48,600
doesn't exist today, I don't see any real barrier

1851
01:40:48,600 --> 01:40:50,040
to being able to accomplish that.

1852
01:40:50,040 --> 01:40:51,640
Sure, you know Ron Graham.

1853
01:40:51,640 --> 01:40:52,640
Yes.

1854
01:40:52,640 --> 01:40:53,160
He did five.

1855
01:40:53,160 --> 01:40:56,040
He thinks it's just about impossible for a robot.

1856
01:40:56,040 --> 01:40:58,320
Who knows?

1857
01:40:58,320 --> 01:41:00,880
I think Ron Graham was capable of doing five.

1858
01:41:00,880 --> 01:41:01,400
Yes.

1859
01:41:01,400 --> 01:41:04,960
He can do it, but also balance on one hand.

1860
01:41:04,960 --> 01:41:10,120
But I have no concern about robots being

1861
01:41:10,120 --> 01:41:14,160
able to get there in the reasonable future.

1862
01:41:14,160 --> 01:41:15,520
Thank you.

1863
01:41:15,520 --> 01:41:16,040
Thank you.

1864
01:41:16,040 --> 01:41:16,520
Go ahead.

1865
01:41:20,120 --> 01:41:23,720
So before you were talking about how the best way to develop

1866
01:41:23,720 --> 01:41:26,040
AI might be to almost raise it as a child

1867
01:41:26,040 --> 01:41:29,000
or to teach it so that it can make

1868
01:41:29,000 --> 01:41:31,600
a lot of logical assumptions that it has a background.

1869
01:41:31,600 --> 01:41:33,200
I'm curious, at that point, wouldn't you

1870
01:41:33,200 --> 01:41:36,520
be concerned about the AI developing the same biases

1871
01:41:36,520 --> 01:41:38,600
that you were originally trying to avoid?

1872
01:41:42,800 --> 01:41:44,760
Presumably, you would be needing it

1873
01:41:44,760 --> 01:41:51,920
to develop a similar set of a sum of the data

1874
01:41:51,920 --> 01:41:56,160
you would then have to educate it that certain biases are not

1875
01:41:56,160 --> 01:41:58,080
socially acceptable.

1876
01:41:58,080 --> 01:42:01,760
Socialization is the accumulation of all this background

1877
01:42:01,760 --> 01:42:04,000
knowledge that's not only factual.

1878
01:42:04,000 --> 01:42:05,480
The floor doesn't come up and invite you,

1879
01:42:05,480 --> 01:42:08,280
but it's also normative in the sense

1880
01:42:08,280 --> 01:42:10,360
that you should do this under these circumstances,

1881
01:42:10,360 --> 01:42:12,720
but not in that context.

1882
01:42:12,720 --> 01:42:17,040
And so it would develop by what we could call what

1883
01:42:17,040 --> 01:42:19,840
it's developing biases.

1884
01:42:19,840 --> 01:42:25,600
But that's the necessary texture of that background knowledge

1885
01:42:25,600 --> 01:42:29,200
base that it needs to operate intelligently in the world.

1886
01:42:29,200 --> 01:42:35,080
Yeah, I think these biases, it's possible that a robot being

1887
01:42:35,080 --> 01:42:41,720
taught to live in this world might develop cognitive biases.

1888
01:42:41,720 --> 01:42:44,400
They'd be the same set of cognitive biases that

1889
01:42:44,400 --> 01:42:46,120
have been observed in humans.

1890
01:42:46,120 --> 01:42:54,240
No, it might be all the things that's

1891
01:42:54,240 --> 01:42:56,280
taken from ours.

1892
01:42:56,280 --> 01:42:59,880
I would hope that we would be able to correct those things

1893
01:42:59,880 --> 01:43:01,560
and get it to think in a less biased way.

1894
01:43:01,560 --> 01:43:07,200
But you're right that I think humans

1895
01:43:07,200 --> 01:43:09,080
that we develop short cuts.

1896
01:43:09,080 --> 01:43:11,880
There's a certain way our brains are wired.

1897
01:43:11,880 --> 01:43:15,560
These biases, I think, are a form of heuristic

1898
01:43:15,560 --> 01:43:18,240
that work in a lot of cases, but not in all cases,

1899
01:43:18,240 --> 01:43:20,200
and they sometimes get us into trouble.

1900
01:43:20,200 --> 01:43:27,360
So these embodied AIs may well develop something like this.

1901
01:43:27,360 --> 01:43:30,600
So some biases are just involved sensitivity

1902
01:43:30,600 --> 01:43:32,640
to background frequencies.

1903
01:43:32,640 --> 01:43:39,360
So here's an experiment that shows this.

1904
01:43:39,360 --> 01:43:48,840
So subjects were asked to judge the height of various people,

1905
01:43:48,840 --> 01:43:53,480
standing next to a standard object in a college campus.

1906
01:43:53,480 --> 01:43:57,400
And the people were men and women,

1907
01:43:57,400 --> 01:44:00,840
where the men and women were unknown to the subjects.

1908
01:44:00,840 --> 01:44:02,440
There were matched pairs.

1909
01:44:02,440 --> 01:44:04,480
That is, for every six foot man, there

1910
01:44:04,480 --> 01:44:06,320
was also a six foot woman.

1911
01:44:06,320 --> 01:44:09,360
For every five foot one, there was a five foot man.

1912
01:44:09,360 --> 01:44:16,720
And there was a bias towards judging the men to be taller.

1913
01:44:16,720 --> 01:44:20,520
And that is because of sensitivity to background frequencies.

1914
01:44:20,520 --> 01:44:23,080
Now, if you want your, of course,

1915
01:44:23,080 --> 01:44:25,080
the trouble with sensitive to background frequencies

1916
01:44:25,080 --> 01:44:29,400
is they can intrude in an unjust way.

1917
01:44:29,400 --> 01:44:32,720
But if you want your AI to be sensitive to background

1918
01:44:32,720 --> 01:44:36,400
frequencies, you will have to do something

1919
01:44:36,400 --> 01:44:38,320
about it in the way that we try to do something

1920
01:44:38,320 --> 01:44:40,600
about human judgment.

1921
01:44:40,600 --> 01:44:42,800
But wouldn't you say also that the AI, in order

1922
01:44:42,800 --> 01:44:46,640
to be properly functional, has to have those sensitivity

1923
01:44:46,640 --> 01:44:47,520
to background frequencies?

1924
01:44:47,520 --> 01:44:48,160
Yes.

1925
01:44:48,160 --> 01:44:49,560
It has to generalize.

1926
01:44:49,560 --> 01:44:50,060
Yeah.

1927
01:44:50,060 --> 01:44:51,280
It may categories.

1928
01:44:51,280 --> 01:44:55,280
I would hope we could build into it a much better

1929
01:44:55,280 --> 01:44:57,320
facility for dealing with probabilities

1930
01:44:57,320 --> 01:44:59,720
and conditional probabilities than we possess.

1931
01:44:59,720 --> 01:45:00,220
Yeah.

1932
01:45:11,120 --> 01:45:12,120
Great.

1933
01:45:12,120 --> 01:45:17,440
If we say that humans are still evolving,

1934
01:45:17,440 --> 01:45:25,480
and that human evolution is linked with machines and AI.

1935
01:45:25,480 --> 01:45:30,400
And kind of in the same way that when photography came along

1936
01:45:30,400 --> 01:45:32,920
and free painting to do other things,

1937
01:45:32,920 --> 01:45:35,640
then represent the world.

1938
01:45:35,640 --> 01:45:39,400
So it seems like AI has the capacity,

1939
01:45:39,400 --> 01:45:44,880
either to stifle or to enhance this human evolution.

1940
01:45:44,880 --> 01:45:48,040
And maybe some of the ways in which we could evolve

1941
01:45:48,040 --> 01:45:53,960
are toward a greater sense of personal fulfillment,

1942
01:45:53,960 --> 01:46:00,960
artistic creativity, lots of things.

1943
01:46:00,960 --> 01:46:03,000
So is anyone looking at that?

1944
01:46:03,000 --> 01:46:05,360
I mean, if you're telling me my preferences,

1945
01:46:05,360 --> 01:46:10,120
or you're notifying me every time I'm daydreaming,

1946
01:46:10,120 --> 01:46:11,360
that going to stop.

1947
01:46:30,720 --> 01:46:33,920
That is going to happen.

1948
01:46:33,920 --> 01:46:38,800
We are in a coevolutionary relationship

1949
01:46:38,800 --> 01:46:40,200
with the technologies we develop.

1950
01:46:40,200 --> 01:46:44,320
And it's going to happen in this case as well.

1951
01:46:44,320 --> 01:46:49,640
I think in small ways, we can start with small ways

1952
01:46:49,640 --> 01:46:52,000
to answer the question of is it happening now,

1953
01:46:52,000 --> 01:46:53,880
or is it soon to happen?

1954
01:46:53,880 --> 01:46:57,720
I think you can look at cases like assistance

1955
01:46:57,720 --> 01:47:00,160
for the elderly that could be developed,

1956
01:47:00,160 --> 01:47:06,600
where imagine you have an AI-enabled walker that

1957
01:47:06,600 --> 01:47:11,960
helps grandma understand that she needs to lift her feet

1958
01:47:11,960 --> 01:47:13,280
a little bit more.

1959
01:47:13,280 --> 01:47:17,880
And by doing that, she might learn a better gate,

1960
01:47:17,880 --> 01:47:19,960
such that maybe she won't need that walker anymore,

1961
01:47:19,960 --> 01:47:21,880
at least for a while.

1962
01:47:21,880 --> 01:47:25,120
So there are ways that AI can train us

1963
01:47:25,120 --> 01:47:30,400
and help us in ways that ironically make us less dependent

1964
01:47:30,400 --> 01:47:32,200
on this assistance.

1965
01:47:32,200 --> 01:47:33,160
That's just a small thing.

1966
01:47:33,160 --> 01:47:41,320
But I think imagine that we used to be a lot of people

1967
01:47:41,320 --> 01:47:43,120
used to be very good at riding horses.

1968
01:47:43,120 --> 01:47:46,120
And now I think that has atrophied,

1969
01:47:46,120 --> 01:47:49,720
but there's probably something changing in our brain

1970
01:47:49,720 --> 01:47:54,120
that distinguishes us from people

1971
01:47:54,120 --> 01:47:59,960
from the 1800s, where we're better drivers than they would

1972
01:47:59,960 --> 01:48:00,800
have been.

1973
01:48:00,800 --> 01:48:03,120
There's something that has changed in us.

1974
01:48:03,120 --> 01:48:06,240
And I think this is happening in small ways already

1975
01:48:06,240 --> 01:48:08,480
and might happen in very large ways,

1976
01:48:08,480 --> 01:48:13,880
maybe more explicitly as we start to more explicitly implant

1977
01:48:13,880 --> 01:48:18,360
devices in ourselves that are designed

1978
01:48:18,360 --> 01:48:25,840
to enhance recognition.

1979
01:48:25,840 --> 01:48:30,600
OK, my question is, have there been any experiments long term?

1980
01:48:30,600 --> 01:48:34,800
Have there been any successful or not experiments

1981
01:48:34,800 --> 01:48:41,000
with long term training of artificial embodied beings?

1982
01:48:41,000 --> 01:48:44,680
AIs, embodied AIs, like long term training,

1983
01:48:44,680 --> 01:48:49,040
like broader intelligence and just focusing

1984
01:48:49,040 --> 01:48:53,240
about a billion cases just to learn it as a very specific

1985
01:48:53,240 --> 01:48:56,240
task.

1986
01:48:56,240 --> 01:49:00,000
Well, there are some people that work on more general AI,

1987
01:49:00,000 --> 01:49:03,760
rather than narrow and specific to the task.

1988
01:49:06,360 --> 01:49:08,880
Like, maybe aware of these people

1989
01:49:08,880 --> 01:49:12,280
at this company called DeepMind, which is part of Google.

1990
01:49:12,280 --> 01:49:18,200
And they have this algorithm that can

1991
01:49:18,200 --> 01:49:25,000
learn how to play many different games on the screen.

1992
01:49:28,240 --> 01:49:32,840
Just by looking at how these games are played,

1993
01:49:32,840 --> 01:49:35,880
but just by looking at people playing these games.

1994
01:49:35,880 --> 01:49:39,120
And so you can learn not just one game only,

1995
01:49:39,120 --> 01:49:43,120
but many different ones, even with different rules,

1996
01:49:43,120 --> 01:49:48,200
different score functions, you know,

1997
01:49:48,200 --> 01:49:51,400
accumulate the score, the game, the win, and so on.

1998
01:49:51,400 --> 01:49:56,080
And so without communicating the rules of the games,

1999
01:49:56,080 --> 01:50:00,960
these machine learns just by observing many different ones.

2000
01:50:00,960 --> 01:50:05,360
And they think that that could be one, of course,

2001
01:50:05,360 --> 01:50:07,240
a game very specific.

2002
01:50:07,240 --> 01:50:09,520
Just games is not real life.

2003
01:50:09,520 --> 01:50:12,680
It's not a very realistic scenario.

2004
01:50:12,680 --> 01:50:15,520
But they claim that that could be a way

2005
01:50:15,520 --> 01:50:21,560
to build a machine that can learn to do many different things

2006
01:50:21,560 --> 01:50:25,000
that work according to different rules.

2007
01:50:25,000 --> 01:50:27,360
But there are also other people that,

2008
01:50:27,360 --> 01:50:30,120
I mean, whose goal is to work more generally?

2009
01:50:30,120 --> 01:50:32,000
I rather than a specific one.

2010
01:50:32,000 --> 01:50:33,800
But this is when they came to mind.

2011
01:50:33,800 --> 01:50:38,800
I mean, that they are, you know, working hard in expanding that.

2012
01:50:38,800 --> 01:50:41,800
Only only four minutes.

2013
01:50:41,800 --> 01:50:44,800
I just want to be more specific because it was not exactly

2014
01:50:44,800 --> 01:50:45,560
my question.

2015
01:50:45,560 --> 01:50:49,800
I mean, I know people have like been having

2016
01:50:49,800 --> 01:50:54,400
champs from babies and they train them like 15 years.

2017
01:50:54,400 --> 01:50:57,520
And my question is more like if there has been experiments

2018
01:50:57,520 --> 01:51:02,120
successful or not, like training AI is for 15 years

2019
01:51:02,120 --> 01:51:04,920
from nothing to something.

2020
01:51:04,920 --> 01:51:08,640
15 years ago, AI was very different.

2021
01:51:08,640 --> 01:51:10,080
The world was very different.

2022
01:51:10,080 --> 01:51:16,680
I mean, AI just started not long ago, just 50 years ago.

2023
01:51:16,680 --> 01:51:21,440
So I think that I'm not aware of this long training,

2024
01:51:21,440 --> 01:51:23,960
you know, experiments.

2025
01:51:23,960 --> 01:51:25,840
I don't think there have been some attempts

2026
01:51:25,840 --> 01:51:35,240
to learn some simple tasks with computer systems that

2027
01:51:35,240 --> 01:51:37,400
are modeled after the brain.

2028
01:51:37,400 --> 01:51:40,760
I remember some work that was done at IBM probably 10 years

2029
01:51:40,760 --> 01:51:48,760
ago now that wired together a bunch of pseudo neurons

2030
01:51:48,760 --> 01:51:51,680
into something akin to the many columns that

2031
01:51:51,680 --> 01:51:54,840
are in one part of the brain.

2032
01:51:54,840 --> 01:52:01,360
And I think they were trying to teach it to just like to train

2033
01:52:01,360 --> 01:52:02,440
a visual system.

2034
01:52:02,440 --> 01:52:03,760
Can you recognize something?

2035
01:52:03,760 --> 01:52:06,240
Just here's a bunch of circuitry.

2036
01:52:06,240 --> 01:52:08,520
And I'm going to put a bunch of images in front of you

2037
01:52:08,520 --> 01:52:09,600
and can you learn it?

2038
01:52:09,600 --> 01:52:12,640
I don't know how far it got.

2039
01:52:12,640 --> 01:52:18,760
But I know that there have been attempts in other research

2040
01:52:18,760 --> 01:52:20,200
institutions as well to do this.

2041
01:52:20,200 --> 01:52:24,960
But I don't know what was the outcome of these.

2042
01:52:24,960 --> 01:52:25,960
Thank you.

2043
01:52:25,960 --> 01:52:32,240
So one point I haven't heard raised

2044
01:52:32,240 --> 01:52:37,840
has to with the tendency of goals to be redefined

2045
01:52:37,840 --> 01:52:40,120
as a function of the available solutions

2046
01:52:40,120 --> 01:52:42,040
rather than the other way around.

2047
01:52:42,040 --> 01:52:45,800
So for example, Facebook has redefined the notion of friendship.

2048
01:52:45,800 --> 01:52:48,600
And I'm not convinced that it's an improvement

2049
01:52:48,600 --> 01:52:51,240
on the earlier version.

2050
01:52:51,240 --> 01:52:54,480
And for that reason, it's important to me,

2051
01:52:54,480 --> 01:52:59,000
it seems that the question of democracy be addressed.

2052
01:52:59,000 --> 01:53:03,200
And what I've been hearing about the likelihood of half

2053
01:53:03,200 --> 01:53:05,440
of all employment being eliminated.

2054
01:53:05,440 --> 01:53:09,440
And the question is, are the people who

2055
01:53:09,440 --> 01:53:13,400
are likely to be affected by that actually being consulted?

2056
01:53:13,400 --> 01:53:17,560
And is this decision to be made purely

2057
01:53:17,560 --> 01:53:21,160
on the basis of the availability of enormous resources

2058
01:53:21,160 --> 01:53:25,200
on the part of the corporations, Silicon Valley, these five

2059
01:53:25,200 --> 01:53:29,240
corporations that were mentioned and the ones that were not?

2060
01:53:29,240 --> 01:53:32,840
I'm my colleagues at Stanford tell me

2061
01:53:32,840 --> 01:53:35,280
they overhear conversations in the coffee shops.

2062
01:53:35,280 --> 01:53:38,640
And people are very, very seriously

2063
01:53:38,640 --> 01:53:42,080
discussing what they're going to be doing with all these unemployed

2064
01:53:42,080 --> 01:53:42,600
people.

2065
01:53:42,600 --> 01:53:45,160
And they feel concerned about it.

2066
01:53:45,160 --> 01:53:51,360
What I'm more concerned about, I'm more concerned about the absence

2067
01:53:51,360 --> 01:53:56,320
of restraint on the part of the people who exercise this power

2068
01:53:56,320 --> 01:54:00,160
and who have these resources rather than the likelihood

2069
01:54:00,160 --> 01:54:01,840
that machines will get out of control.

2070
01:54:04,240 --> 01:54:06,760
Yeah, I think there is a lot of discussion also

2071
01:54:06,760 --> 01:54:11,400
in terms of regulations of AI, possible regulation.

2072
01:54:11,400 --> 01:54:16,080
And I think that, for example, a few days ago, the White House

2073
01:54:16,080 --> 01:54:20,600
released a very interesting document on the future of AI

2074
01:54:20,600 --> 01:54:33,120
and strategic directions on how to, with the goal of facilitating

2075
01:54:33,120 --> 01:54:37,640
the good development of AI and possibly

2076
01:54:37,640 --> 01:54:42,040
mitigating the undesired consequences.

2077
01:54:42,040 --> 01:54:47,120
And I think that, so even last week, I

2078
01:54:47,120 --> 01:54:49,400
was speaking at the European Parliament.

2079
01:54:49,400 --> 01:54:54,640
And people there are very concerned about specific issues

2080
01:54:54,640 --> 01:54:58,120
about AI, like they apply the C ownership, they said that.

2081
01:54:58,120 --> 01:55:03,200
But they also are concerned about to understand

2082
01:55:03,200 --> 01:55:07,120
how to regulate this very powerful technology

2083
01:55:07,120 --> 01:55:12,280
in a way that it does not stop research and beneficial

2084
01:55:12,280 --> 01:55:17,760
advancement, but also it addresses these other concerns,

2085
01:55:17,760 --> 01:55:21,720
like, for example, the impact on the workforce.

2086
01:55:21,720 --> 01:55:24,520
Well, the European Parliament is not

2087
01:55:24,520 --> 01:55:29,160
a notably democratic body in the European Union as a whole.

2088
01:55:29,160 --> 01:55:35,200
And the question is, where is the public that

2089
01:55:35,200 --> 01:55:39,120
is going to be most profoundly affected

2090
01:55:39,120 --> 01:55:41,480
by taking part in the decisions?

2091
01:55:41,480 --> 01:55:43,080
I think there are some economists

2092
01:55:43,080 --> 01:55:48,400
who are thinking about the impact of AI

2093
01:55:48,400 --> 01:55:53,360
on the economy and displacement and the like.

2094
01:55:53,360 --> 01:55:58,600
I think Eric Brynjolfsson is one such person at MIT.

2095
01:55:58,600 --> 01:56:00,720
And there are others.

2096
01:56:00,720 --> 01:56:04,960
And I think it is important to explore these questions.

2097
01:56:04,960 --> 01:56:07,040
I don't know if we have good answers right now,

2098
01:56:07,040 --> 01:56:08,200
but it's one of those things that we

2099
01:56:08,200 --> 01:56:11,280
have to be conscious of and think about

2100
01:56:11,280 --> 01:56:14,480
and integrate into our thinking.

2101
01:56:14,480 --> 01:56:19,760
One historical example is in the 1930s, Gandhi's campaign

2102
01:56:19,760 --> 01:56:36,880
to resist the industrial encroachment.

