1
00:00:00,000 --> 00:00:02,940
Yeah, wasn't that sad.

2
00:00:02,940 --> 00:00:03,140
But he's not.

3
00:00:03,140 --> 00:00:06,780
I don't have to.

4
00:00:06,780 --> 00:00:15,620
I'm happy too.

5
00:00:15,620 --> 00:00:19,400
Wh Woah.

6
00:00:19,400 --> 00:00:28,160
So I'm happy.

7
00:00:28,160 --> 00:00:32,160
So Ken, what did you think about lecturing?

8
00:00:32,160 --> 00:00:33,160
What's that?

9
00:00:33,160 --> 00:00:38,160
That lecture two days ago on complexity in the task.

10
00:00:38,160 --> 00:00:39,160
No, I couldn't have it.

11
00:00:39,160 --> 00:00:40,160
I couldn't have it.

12
00:00:40,160 --> 00:00:41,160
I couldn't have it.

13
00:00:41,160 --> 00:00:42,160
I'm trying.

14
00:00:42,160 --> 00:00:45,160
I know I saw you talk, but I'm talking about what Tucker was.

15
00:00:45,160 --> 00:00:47,160
I'm wondering, I just think.

16
00:00:47,160 --> 00:00:51,160
This is like, I'm about choice task, and it was a question about levels of complexity.

17
00:00:51,160 --> 00:00:52,160
I'm really excited.

18
00:00:52,160 --> 00:00:57,160
You know, it's like creating models, how people created models of the task.

19
00:00:57,160 --> 00:01:02,160
No, I'm going to be here.

20
00:01:02,160 --> 00:01:03,160
Can people hear me?

21
00:01:03,160 --> 00:01:04,160
I think it's prime age.

22
00:01:04,160 --> 00:01:05,160
Hi, good afternoon, everybody.

23
00:01:05,160 --> 00:01:11,160
I'm Gerald Horowitz, I'm associate director here at the Hill Center.

24
00:01:11,160 --> 00:01:18,160
Welcome to a very interesting roundtable today entitled Math Models Mind.

25
00:01:18,160 --> 00:01:24,160
I wanted to make a quick announcement that coming up in future round tables, we have

26
00:01:24,160 --> 00:01:31,160
a March 9th life in the universe, which we've established actually right now today,

27
00:01:31,160 --> 00:01:32,160
that there is life in the universe.

28
00:01:32,160 --> 00:01:35,160
But there's going to be more to say about that topic.

29
00:01:35,160 --> 00:01:41,160
And we have assembled the following roundtable participants, Caleb Sharf's astrophysicist

30
00:01:41,160 --> 00:01:51,160
at Columbia University, Yale astrophysicist, Priyem Bada, Natar Rayan, Kenneth Dill,

31
00:01:51,160 --> 00:01:58,160
chemist, Stony Brook University, Edward Turner, a physicist from Princeton, and Dennis over

32
00:01:58,160 --> 00:02:01,160
by a New York Times science correspondent.

33
00:02:01,160 --> 00:02:08,160
So that's March 9th, and I'm sure it's going to be really quite a lively, interesting talk.

34
00:02:08,160 --> 00:02:12,160
In April, I don't know if we have an exact date yet, but I don't have it here.

35
00:02:12,160 --> 00:02:18,160
We're going to have a discussion on climate change and the Anthropocene era, which is

36
00:02:18,160 --> 00:02:21,160
among us, which we're living through now.

37
00:02:21,160 --> 00:02:25,160
And on May 18th, you don't want to miss it or else you'll be in trouble.

38
00:02:25,160 --> 00:02:31,160
We're having a talk on shame.

39
00:02:31,160 --> 00:02:40,160
Today's panel, I'll quickly introduce, and you can raise your hand when I mention your

40
00:02:40,160 --> 00:02:41,160
name.

41
00:02:41,160 --> 00:02:43,160
First is Larry Amsel.

42
00:02:43,160 --> 00:02:48,160
And Larry is a clinical research psychiatrist on the faculty of Columbia University.

43
00:02:48,160 --> 00:02:53,160
He's out of long background in mathematics and was an early proponent of using decision

44
00:02:53,160 --> 00:02:57,160
theory, gain theory, and behavioral economics in psychiatric research.

45
00:02:57,160 --> 00:03:01,160
I'm going to keep these short so we have more time to talk.

46
00:03:01,160 --> 00:03:08,160
Cheryl Quarkrin is associate professor of psychiatry and program leader in psychosis

47
00:03:08,160 --> 00:03:13,160
risk at the ICON School of Medicine at Mount Sinai.

48
00:03:13,160 --> 00:03:19,160
Together with Dr. Chechi of IBM, she's identified patterns of language that proceed on set of

49
00:03:19,160 --> 00:03:26,160
psychosis, including reduction in coherence and complexity of the speech.

50
00:03:26,160 --> 00:03:33,160
Andrew Gerber is a psychologist, psychiatrist and psychoanalyst who is now the president

51
00:03:33,160 --> 00:03:38,160
director at Silver Springs Hospital.

52
00:03:38,160 --> 00:03:39,160
Silver Hill Hospital.

53
00:03:39,160 --> 00:03:43,160
I was wondering where is Silver Springs Hospital?

54
00:03:43,160 --> 00:03:45,160
That's what it wrote and that's what I read.

55
00:03:45,160 --> 00:03:46,160
I'm sorry.

56
00:03:46,160 --> 00:03:54,160
Ken Miller is professor of department of neuroscience and department of physiology and is director

57
00:03:54,160 --> 00:03:58,160
for the center of theoretical neurobiology at Columbia University.

58
00:03:58,160 --> 00:04:03,160
He's co-director of Columbia's sports program in theoretical neurobiology, it's center

59
00:04:03,160 --> 00:04:10,160
for theoretical neuroscience as well as its neurobiology and behavioral graduate program.

60
00:04:10,160 --> 00:04:19,160
And John Murray is assistant professor of psychiatry, neuroscience and physics at Yale University

61
00:04:19,160 --> 00:04:24,160
School of Medicine where he directs a research program in computational neuroscience where

62
00:04:24,160 --> 00:04:29,160
they focus on computational models of neuropsychiatric disorders.

63
00:04:29,160 --> 00:04:34,160
He received his PhD in physics at Yale University.

64
00:04:34,160 --> 00:04:42,160
We have one missing member, I'm going to give you his background in case he does show up.

65
00:04:42,160 --> 00:04:43,160
It's our D.J.

66
00:04:43,160 --> 00:04:53,160
Rangan, he's an associate professor in the mathematics department at NYU and he's completed his post-doctoral work at NYU.

67
00:04:53,160 --> 00:04:59,160
His research has focused for many years on computational and theoretical models of sensory processing,

68
00:04:59,160 --> 00:05:02,160
particularly vision and all faction.

69
00:05:02,160 --> 00:05:06,160
So thank you all and we can get started.

70
00:05:06,160 --> 00:05:07,160
Sure.

71
00:05:07,160 --> 00:05:08,160
Sure.

72
00:05:08,160 --> 00:05:09,160
Sure.

73
00:05:09,160 --> 00:05:10,160
Sure.

74
00:05:10,160 --> 00:05:11,160
Sure.

75
00:05:11,160 --> 00:05:12,160
Sure.

76
00:05:12,160 --> 00:05:13,160
Sure.

77
00:05:13,160 --> 00:05:14,160
Sure.

78
00:05:14,160 --> 00:05:15,160
Sure.

79
00:05:15,160 --> 00:05:16,160
Sure.

80
00:05:16,160 --> 00:05:20,160
So, I thought because the topics here are somewhat...

81
00:05:20,160 --> 00:05:23,160
Oh, do you really are?

82
00:05:23,160 --> 00:05:26,160
We just announced your minutes.

83
00:05:26,160 --> 00:05:28,160
Thanks for joining us.

84
00:05:28,160 --> 00:05:31,160
I was about to say that we were just getting started.

85
00:05:31,160 --> 00:05:33,160
Oh, it's not perfect.

86
00:05:33,160 --> 00:05:49,160
The topics, some of the topics were we discussed today are a little arcane and I thought it would be helpful for the participants to maybe give a brief summary of the kind of research they've done and more they're interested in the topic of mathematics

87
00:05:49,160 --> 00:05:55,160
as a search for modeling behavior and the psyche.

88
00:05:55,160 --> 00:05:59,160
So where would you start to find?

89
00:05:59,160 --> 00:06:11,160
So what I've worked on is the circuitry of cerebral cortex, sensory cerebral cortex, primarily visual cortex, but trying to understand how the circuits...

90
00:06:11,160 --> 00:06:29,160
Trying to understand general principles of how the circuits of cortex work and basically trying to understand what operations the circuits are doing to produce the responses to sensory stimuli that we see, how those circuits develop through learning rules based on the activity in the neurons.

91
00:06:29,160 --> 00:06:44,160
And the far away goal is to understand from that really what computation cortex does.

92
00:06:44,160 --> 00:06:59,160
And so there's a sense that there's been a unit of mammalian intelligence that's been developed and then duplicated and applied to almost everything that we do.

93
00:06:59,160 --> 00:07:07,160
Someday I would like to understand what exactly it does to the input it receives and how it transforms it, how it learns from it.

94
00:07:07,160 --> 00:07:25,160
So my research is also kind of focused on theoretical models of neural especially cortical systems. As is Ken's, we primarily focus on computations associated with association cortex in contrast to sensory cortex, looking at kind of fundamental cognitive

95
00:07:25,160 --> 00:07:39,160
limitations such as working memory or decision making. And then we're very interested in understanding how synaptic level disruptions of the circuits could give rise to cognitive impairments as we see in psychiatric disorders such as schizophrenia.

96
00:07:39,160 --> 00:07:47,160
And so we collaborate with experimentalists and study inter-disease processes and pharmacology in these computational models of neural circuits.

97
00:07:47,160 --> 00:08:12,160
And it is on. I too have spent a few years looking at computational models of neural circuits. In this case, I worked a little bit on vision, but I spent most of my last several years working on old-faction trying to understand the kinds of computations that go on inside the old-factory system of insects.

98
00:08:12,160 --> 00:08:19,160
And as Ken was saying there are many similarities between the old-factory systems of a lot of different animals.

99
00:08:19,160 --> 00:08:31,160
And at first I was seduced by this thinking that perhaps there would be a similarity to the computations that these different old-factory systems perform.

100
00:08:31,160 --> 00:08:42,160
However, the more I look at it, the more I've come to realize in recent years that it's almost like there's different operating systems running on the same hardware.

101
00:08:42,160 --> 00:08:45,160
So we can talk more about that later.

102
00:08:45,160 --> 00:08:46,160
No, I'm left.

103
00:08:46,160 --> 00:08:52,160
Okay. So to sort of shift a little bit, I'll work backwards.

104
00:08:52,160 --> 00:09:08,160
My current role in some ways is as a translator between different languages around mental illness. So in leading a psychiatric hospital, my predominant interest is how to get people to talk to one another, not just between our staff and our patients, or the patient's patients, but between the staff and each other.

105
00:09:08,160 --> 00:09:17,160
And one of the languages that's been very important to me is the language of math, and or more broadly speaking, the language of modeling.

106
00:09:17,160 --> 00:09:24,160
And I find, somewhat to my dismay, that that language isn't taught in most clinical programs.

107
00:09:24,160 --> 00:09:40,160
And there's a way in which there's a sort of urgency for an answer, and I think this pervades maybe all of medicine, but particularly in psychiatry and psychology, makes people not understand the value of models that are good but not perfect.

108
00:09:40,160 --> 00:09:49,160
And one of the most common quotes I use, which others may be familiar with, is from George Box, who said all models are wrong, but some are useful.

109
00:09:49,160 --> 00:09:54,160
And to incorporate that is basically one of my main missions.

110
00:09:54,160 --> 00:09:59,160
Prior to being a president of a hospital, I had run the MRI research program up at Columbia.

111
00:09:59,160 --> 00:10:09,160
And so I was particularly interested in the use of structural functional MRI to test various mathematical models of neuro cognitive processing, exactly the stuff that you would like to do.

112
00:10:09,160 --> 00:10:13,160
And I think that's the stuff that you all are talking about.

113
00:10:13,160 --> 00:10:23,160
I'm a believer that there are a set of neuro cognitive processes that underlie our psychiatric disorders that actually will end up looking quite different from the symptomatic descriptions that we now use.

114
00:10:23,160 --> 00:10:36,160
So the terms of depression, anxiety, even psychosis are appeal because they're very experienced near to the clinician, but that the underlying neuro cognitive vulnerabilities probably, in my opinion, will look quite different.

115
00:10:36,160 --> 00:10:39,160
And I'm not quite at the point yet where we understand those.

116
00:10:39,160 --> 00:10:42,160
So that's been the overriding theme of my research.

117
00:10:42,160 --> 00:10:48,160
So I'm a psychiatrist and I collaborate with people who do computational modeling.

118
00:10:48,160 --> 00:10:53,160
And I collaborate with Guizhemo Cheki and his team at IBM.

119
00:10:53,160 --> 00:10:58,160
And he has applied computational analyses to behaviors such as language.

120
00:10:58,160 --> 00:11:08,160
So we think of language as really big data at the level of the individual, the language has semantics and syntax and there's pragmatics.

121
00:11:08,160 --> 00:11:14,160
And probably facial expression also has semantics and syntax as well as gesture.

122
00:11:14,160 --> 00:11:18,160
And we believe this behavior in and of itself can be modeled.

123
00:11:18,160 --> 00:11:26,160
And I think what we do as psychiatrists is we observe people, I do think that data is very important.

124
00:11:26,160 --> 00:11:36,160
And not only can computational scientists help us, but they really feel that we can help them in terms of building the model.

125
00:11:36,160 --> 00:11:42,160
So I've been to a few conferences, the NIPS conference neuro informatics.

126
00:11:42,160 --> 00:11:44,160
Yoshua Bajio was there.

127
00:11:44,160 --> 00:11:47,160
I'm talking about deep learning.

128
00:11:47,160 --> 00:11:55,160
And someone in the audience asked him, you know, what are your thoughts about what you could learn from cognitive neuroscience, human neuro science?

129
00:11:55,160 --> 00:12:00,160
And he said, I don't know and I don't care, which was very interesting.

130
00:12:00,160 --> 00:12:12,160
But the people that I'm working with who really do think about artificial intelligence a great deal feel that we have a lot that we can teach them.

131
00:12:12,160 --> 00:12:15,160
They really want to model what we do.

132
00:12:15,160 --> 00:12:22,160
And the other thing I want to just say briefly, I looked at the kind of description of the history.

133
00:12:22,160 --> 00:12:29,160
And it has this feeling of kind of a guy on a hill, right, and the environment.

134
00:12:29,160 --> 00:12:36,160
But if we think about behavior, behavior at the level of milliseconds is very interactive.

135
00:12:36,160 --> 00:12:43,160
And so we want to model discourse as well in all of what anybody does here who's an analyst or a therapist.

136
00:12:43,160 --> 00:12:47,160
That kind of discourse is kind of key to our therapeutics.

137
00:12:47,160 --> 00:12:59,160
And we'd like to sort of understand what is going on in a successful interaction, therapeutic interaction, if that can be modeled by the computational scientists.

138
00:12:59,160 --> 00:13:06,160
And then with that model, use that to help you all in teaching other people what you do.

139
00:13:06,160 --> 00:13:14,160
Hi, so I'm Larry Absola and I got interested in this stuff 20 years ago.

140
00:13:14,160 --> 00:13:23,160
And actually got interested in it because it seems to me that within the social sciences, the economics was the social science that most used mathematical models.

141
00:13:23,160 --> 00:13:26,160
So I got very interested in economic models.

142
00:13:26,160 --> 00:13:30,160
So I think in this panel we're at wonderfully different levels.

143
00:13:30,160 --> 00:13:34,160
Some people are doing stuff at a neuron level, a small circuit level.

144
00:13:34,160 --> 00:13:40,160
Andrew and I are more interested in the whole, in sort of the patient, and in general, in the whole patient level.

145
00:13:40,160 --> 00:13:48,160
But I think this stuff can be very abstract. And so Jerry and I agreed that we'll have to give like a three minute talk on a trivial example.

146
00:13:48,160 --> 00:13:52,160
And if I can just have you guys help me pass these out.

147
00:13:52,160 --> 00:13:59,160
This is the slide. This is the slide. It's a single slide. But I actually need one back myself.

148
00:13:59,160 --> 00:14:03,160
Thank you. So if we could go get those moving around very quickly, I would appreciate that.

149
00:14:03,160 --> 00:14:13,160
So I would say that what actually happened is that I was interested in watching as the managed care movement was taking over medicine.

150
00:14:13,160 --> 00:14:18,160
And I decided I needed to understand some economics. So I went and took a course in the economics of medicine.

151
00:14:18,160 --> 00:14:24,160
And I bumped into decision theory and game theory and I realized that the economists really have some very interesting insights.

152
00:14:24,160 --> 00:14:35,160
Very interesting insight into human behavior. And the thing that I had come from math, I dropped out of graduate schools, of math graduate school, to go to medicine.

153
00:14:35,160 --> 00:14:43,160
Because I looked around the room and I realized I'm good in math, but I'm not going to have a job. So you had to be great.

154
00:14:43,160 --> 00:14:52,160
So I got interested in these economic models. And then I found myself working in a suicide research lab.

155
00:14:52,160 --> 00:15:01,160
And I started talking to people about the architecture, thinking like an economist, what is the architecture of a suicide decision?

156
00:15:01,160 --> 00:15:14,160
So if you look at this, this is a very, very simple basic model. And the point of this is simply to show a trivial example about how a mathematical application may change the way we think about something.

157
00:15:14,160 --> 00:15:22,160
So in thinking about the architecture of suicide decision, there's three small assumptions that I think people might agree with.

158
00:15:22,160 --> 00:15:27,160
And that is that agents, and that's another word for decision makers, they have preferences over orderings.

159
00:15:27,160 --> 00:15:34,160
You know what you like vanilla, better than chocolate. You have preferences over outcomes. There are outcomes in the world and you have preferences over them.

160
00:15:34,160 --> 00:15:39,160
You like one thing rather than the other. The second is that agents can choose actions, but they can't choose the outcome.

161
00:15:39,160 --> 00:15:45,160
So you can go and order the chocolate ice cream, but you can't define whether it's going to be good. There's always a probability.

162
00:15:45,160 --> 00:15:51,160
It may be good chocolate ice cream, maybe not the chocolate. So I can choose actions, but I can't always choose my outcomes.

163
00:15:51,160 --> 00:15:57,160
And the last one is that when I make my choices, I try to maximize my preferences. I do what I want to do rather than what I don't want to do.

164
00:15:57,160 --> 00:16:08,160
It's very trivial. And yet these three very, very, I think easy assumptions that are not that hard to believe give rise to this decision tree, which is that when someone's facing this decision,

165
00:16:08,160 --> 00:16:17,160
they can either make a suicide attempt or not. If they make no attempt and they remain in the status quo, and that's the branch on top, if they do make an attempt,

166
00:16:17,160 --> 00:16:21,160
those are probabilistic. If they could end up dead or they could end up surviving the attempt.

167
00:16:21,160 --> 00:16:33,160
So there's three possible outcomes in the decision architecture. And here's where the trivial mathematics thing, because if you take discrete mathematics on day one, they will teach you that three things can be ordered in six ways.

168
00:16:33,160 --> 00:16:41,160
So chocolate ice cream, vanilla ice cream, strawberry ice cream, there are six kinds of attitudes that you can have depending on your ordering.

169
00:16:41,160 --> 00:16:49,160
So the mathematics forces me to believe that there are six types of clinical people, six different types of people in their approach to suicide.

170
00:16:49,160 --> 00:16:55,160
Let's see if that's true. Well, the person number one is the healthy normal person is not suicidal. He prefers the status quo.

171
00:16:55,160 --> 00:17:05,160
His second choice would be to survive a suicide attempt, and he really doesn't want to die. That's his last choice. On the opposite end is somebody who death is their first choice,

172
00:17:05,160 --> 00:17:14,160
and they are willing to survive an attempt because the status quo is their absolute last choice. And this is somebody who would make any level lethality attempt.

173
00:17:14,160 --> 00:17:20,160
They don't care about the probability they would make any lethality attempt. The third type, again, this is purely mathematically driven.

174
00:17:20,160 --> 00:17:26,160
The third type is someone who surviving the attempt is their first choice. The status quo is their second choice, and death is their last choice.

175
00:17:26,160 --> 00:17:37,160
And this is what we used to call manipulative suicide, but it leads to people who want, for one reason or another, to have made an attempt, but to survive the attempt, because that would either change them or change the environment that they're in.

176
00:17:37,160 --> 00:17:45,160
We've all treated patients like this who suicide attempt think these people will not make a high level suicide attempt. They will only make a low level suicide attempt.

177
00:17:45,160 --> 00:17:54,160
They're not a danger using a gun. They're only a danger of a low-suicide attempt. The fourth one is somebody who really wants to take their lives.

178
00:17:54,160 --> 00:18:03,160
Let's say they're ill, they're terminally ill people, but they're terrified. We're facing this so much. They're terrified of making an attempt at ending up worse.

179
00:18:03,160 --> 00:18:08,160
This is the reason for the Hamlet Society and the reason for people like Kavorkian. It's the whole idea of euthanasia.

180
00:18:08,160 --> 00:18:14,160
It's the people who feel it's their time to die, but they're terrified of making an unsuccessful attempt. That's the fourth type.

181
00:18:14,160 --> 00:18:23,160
The fifth type is somebody whose whole goal is to get away from the status quo. The status quo is their last option, and anything else is better for them.

182
00:18:23,160 --> 00:18:31,160
Finally, the sixth type is a little bit odd. It's somebody for whom the status quo is fine, but if they were to make an attempt, they would not want to survive the attempt.

183
00:18:31,160 --> 00:18:37,160
That's a samurai or also has to do with dueling cultures. It has to do with honoric cultures.

184
00:18:37,160 --> 00:18:54,160
The point is that I made three very simple assumptions that 90% of people I talk to agree with is the trivial model, and the trivial model because of one piece of mathematics that three things can be ordered six ways, predict six types of suicide attitudes.

185
00:18:54,160 --> 00:19:09,160
Most clinicians I talk to say I recognize those guys. I recognize those people. Those are real categories of people. End of talk.

186
00:19:09,160 --> 00:19:26,160
The kind of applications, mathematical applications, as Laura mentioned, just a few minutes ago from the neuro anatomical cellular all the way to systems actually, not just individuals, but also systems.

187
00:19:26,160 --> 00:19:41,160
I think it involves in a web of relationships with others. I'm straining to think of a way to get ball rolling with a conversation. Because this is dealing, there was a CRI guess on mathematical models.

188
00:19:41,160 --> 00:19:55,160
I thought it would be a way to get started with just the following question. I think a lot of people who are interested in mathematics and get into this field often find themselves a little bit frustrated, where at least historically, of late, they found themselves frustrated

189
00:19:55,160 --> 00:20:09,160
by the lack of mathematical intuition, a lot of people. The people who typically have done the kind of research you've done may not be so well-versed, or interested in mathematics.

190
00:20:09,160 --> 00:20:22,160
I guess I want to know, how do you think mathematics is an improvement over a similar approach to what you've been engaged in that does not involve math?

191
00:20:22,160 --> 00:20:31,160
I can jump in on that. I'm trying to write a popular book on this, and it's very hard. It's really very difficult.

192
00:20:31,160 --> 00:20:38,160
It seems to me I was trying to understand what is the difference between a mathematical model and a verbal model. I grew up in the analytic world.

193
00:20:38,160 --> 00:20:45,160
We had a lot of verbal models. I think the difference is that when you write a mathematical model, you are committed to the consequences of that.

194
00:20:45,160 --> 00:20:56,160
That's kind of a thing. There's an equation you're committed to the consequences of that equation. We saw this in Popper's critique of psychoanalysis.

195
00:20:56,160 --> 00:21:03,160
It was non-falsifiable because the psychoanalytic statements were vague, what the consequences were.

196
00:21:03,160 --> 00:21:11,160
But when you write down a mathematical model, you are committing yourself to all the deductive and computational consequences of that model.

197
00:21:11,160 --> 00:21:15,160
I think that's different than verbal models, but you guys would know better.

198
00:21:15,160 --> 00:21:30,160
What I would say is that by, certainly for me, working on circuits, by exploring a mathematical model of a circuit, you discover things that you would never get just by thinking about it.

199
00:21:30,160 --> 00:21:40,160
I think of a model as a scaffolding that you use to develop new intuitions. Once you've got them, you can apply them without the math, but you'll never get them without the math.

200
00:21:40,160 --> 00:21:51,160
I can give an example. In the brain and the cortex, but across the brain, there are tells that either that are excitatory, they excite other neurons, or they're inhibitory.

201
00:21:51,160 --> 00:22:05,160
They suppress the activity of other neurons. There's a phenomenon called surround suppression, where if you have a visual stimulus, right where a particular cell is looking, it'll respond to that stimulus.

202
00:22:05,160 --> 00:22:11,160
If you put other stuff outside of that region, it'll tend to suppress the response to the center stimulus.

203
00:22:11,160 --> 00:22:20,160
Everybody imagine that the only long range connections are excitatory, so everybody imagined you're sending excitation into the local circuit.

204
00:22:20,160 --> 00:22:28,160
You must be exciting the inhibitory neurons, so they don't get surround suppressed. They get surround enhanced, and then they suppress everybody else.

205
00:22:28,160 --> 00:22:36,160
But then David First, there's an experiment that showed that when you add the surround, the inhibition of the cells received goes down.

206
00:22:36,160 --> 00:22:40,160
The excitation they receive also goes down, and that's what's causing them to get suppressed.

207
00:22:40,160 --> 00:22:51,160
So then we had to figure out how can that happen? How can you add excitation into a local circuit and make both the excitatory cells and inhibitory cells all get suppressed by adding excitation?

208
00:22:51,160 --> 00:22:58,160
It turns out there's a really simple mathematical action of that, which we had to, you know, start off for a little while to understand.

209
00:22:58,160 --> 00:23:09,160
But then once you understand the mechanism, then you have a new understanding of how these things can work, that you then make predictions that you didn't anticipate, then experimentalists can go out and test.

210
00:23:09,160 --> 00:23:19,160
I mean, I would say that, you know, the distinction between verbal models and mathematical models is more important than the distinction between using models or not, right?

211
00:23:19,160 --> 00:23:25,160
There's often the kind of a take of you're using a simplified model, but the reality is that in science, we're always using simplified models, right?

212
00:23:25,160 --> 00:23:36,160
If you don't have a mathematical model, you have some verbal model, some picture model, we've got some diaphragm, arrows, and this is how we kind of synthesize different facts, plan new experiments, et cetera, et cetera, right?

213
00:23:36,160 --> 00:23:41,160
And so there's always, you know, some kind of model that people are using or assuming, even if they're not making it explicit, right?

214
00:23:41,160 --> 00:23:56,160
And so by using a mathematical model, you number one, commit to making an argument for the sufficiency of something such and such mechanisms are sufficient to produce this phenomenon, which is kind of all you can do with models is really make kind of sufficiency

215
00:23:56,160 --> 00:23:58,160
arguments, I would say.

216
00:23:58,160 --> 00:24:16,160
As kimnessing, we discover counterintuitive things because you can use the word kind of emergence that when you have interacting elements in a complex way, you can have resulting phenomena that emerge at a higher level that are not really clear.

217
00:24:16,160 --> 00:24:22,160
It's hard to predict that from even the properties of the low level, property elements.

218
00:24:22,160 --> 00:24:35,160
And I would say that a key role in mathematical modeling is about bridging these different levels, right, where we understand something about, you know, to take Ken's example about the synaptic properties and interactions of excitatory inhibitory neurons.

219
00:24:35,160 --> 00:24:38,160
You know, that's at the real, you know, kind of cellular synaptic level.

220
00:24:38,160 --> 00:24:53,160
And then the phenomenon of surround suppression really emerges in the circuit at the physiological level. And, you know, it's through mathematical models that we're able to show how the elements at the low level produce the phenomenon at the higher level.

221
00:24:53,160 --> 00:25:00,160
Actually, one other thought that occurs to me, you know, you said the, you know, all models are wrong, but some are useful.

222
00:25:00,160 --> 00:25:05,160
And when you said that, it occurs to me, what I actually think is all models are incomplete.

223
00:25:05,160 --> 00:25:15,160
I don't think they're necessarily wrong. Like, for example, I don't think our model of how it is that when you add excitation to the local circuit, everybody gets suppressed.

224
00:25:15,160 --> 00:25:19,160
I don't think it's wrong, but it's incredibly incomplete. It's a very, very, very simplified model.

225
00:25:19,160 --> 00:25:22,160
I think Bob was being intentionally provocative when he said that way.

226
00:25:22,160 --> 00:25:23,160
Yeah.

227
00:25:23,160 --> 00:25:25,160
I think he would agree with that.

228
00:25:25,160 --> 00:25:26,160
Yeah.

229
00:25:26,160 --> 00:25:41,160
So my thought about your question, though, is to try to disentangle a little bit the notion of use of mathematical models from the personality of the person who's asking or applying the model.

230
00:25:41,160 --> 00:25:44,160
Because I feel like these things get conflated often.

231
00:25:44,160 --> 00:25:55,160
And it's one of the struggles, I think, that exists in the clinical, particularly in psychiatry and psychology world, which is, I think there's a disproportionate number of people who get attracted to clinical fields.

232
00:25:55,160 --> 00:26:06,160
Because of a pleasure in thinking in sort of almost rebellious ways that at any time you hear something, you sort of think, well, how's that also not true?

233
00:26:06,160 --> 00:26:10,160
And how's that incomplete? And what's being left out of that?

234
00:26:10,160 --> 00:26:17,160
Which is a personality style, it's a personality style that I like and that I think is very valuable in clinical work.

235
00:26:17,160 --> 00:26:26,160
But it's interesting because it's often seen as a personality style that's at least in people who are non-mathematical as being incompatible with mathematics.

236
00:26:26,160 --> 00:26:28,160
And I don't think that's true at all.

237
00:26:28,160 --> 00:26:41,160
I, in fact, think that when you have a set of rules, when you have a set of practices, constraints, you can actually think out of the box in even still more a provocative way.

238
00:26:41,160 --> 00:26:49,160
And I think getting, that's a nuanced way of describing the use of math that I think many people don't get.

239
00:26:49,160 --> 00:27:03,160
They assume that if you do math, you're content with vast oversimplifications and it never occurs to you that they're, and I see you laughing because I've never met someone who's mathematically inclined who was content for the vast oversimplified.

240
00:27:03,160 --> 00:27:14,160
No, I, to pick up on one of the things that Andrew said about character and the people who were attractive, I mean, one of the things that it seemed to me as I wondered as to why, you know, psychiatry had not become mathematical.

241
00:27:14,160 --> 00:27:19,160
I mean, cardiology is mathematical. There's a lot of more more mathematics in cardiology than there is in psychiatry.

242
00:27:19,160 --> 00:27:22,160
And psychiatry about the brain, so you would think.

243
00:27:22,160 --> 00:27:32,160
And I think that, you know, there was a famous book by C.P. Snow called The Two Cultures talking about, you know, the time when there was a time when people knew everything and then there was a time where people sort of divided into the humanities.

244
00:27:32,160 --> 00:27:34,160
Quote, quote, the humanities and the sciences.

245
00:27:34,160 --> 00:27:41,160
And I always wonder whether people who were drawn to psychiatry were sort of the same sort of people who were sort of thought humanistically.

246
00:27:41,160 --> 00:27:51,160
And that means, that means that they like to think in metaphors rather than informal math models, right, that you think in terms of metaphors that you think are going to be similes.

247
00:27:51,160 --> 00:27:55,160
And that I think is the difference between sort of the humanities and the hard sciences.

248
00:27:55,160 --> 00:28:00,160
And that psychiatrists were generally drawn from those kinds of people, but I could be wrong.

249
00:28:00,160 --> 00:28:03,160
Of course, what is a metaphor, if not a model?

250
00:28:03,160 --> 00:28:09,160
And so that's, you know, people make this argument frequently and I agree with you as a cultural level.

251
00:28:09,160 --> 00:28:19,160
And then I think, why can't we get beyond that? Why can't we see that you can actually apply math to these kinds of more humanities, you know, models without doing any damage?

252
00:28:19,160 --> 00:28:07,160
There's a wonderful quote in the beginning of Freud's biography of Leonardo da Vinci, which I always remember, where he says that some people have criticized him for the fact that he's not a

253
00:28:07,160 --> 00:28:36,160
psychoanalytic study of Leonardo's life. And those of you who read it more recently than I can correct me if I'm wrong here.

254
00:28:36,160 --> 00:28:47,160
But he says, he thinks exactly the opposite. He thinks by applying a psychoanalytic understanding to Leonardo's life, rather than oversimplifying or somehow reducing Leonardo's brilliance.

255
00:28:47,160 --> 00:28:53,160
He's actually expanding and making it even more exciting that Leonardo ended up as he did.

256
00:28:53,160 --> 00:29:02,160
And that's the same relationship I think of math to models that come out of the humanities, which is it actually doesn't reduce them, but rather gives you the opportunity to do with it.

257
00:29:02,160 --> 00:29:12,160
And that's it. This is occurring, in fact, about two years ago, someone came out with a game theoretical analysis of some classic pieces of literature of Jane Austen, for example.

258
00:29:12,160 --> 00:29:19,160
So it was a game theory model of Jane Austen's novels, which is a really beautiful piece of the kind of synthesis that you're talking about.

259
00:29:19,160 --> 00:29:32,160
But I think where I find the math, you have to know, be able to know if you're right or wrong. If you come up with a pretty math model, but you have no way of telling if you're right or wrong, it doesn't get you very far.

260
00:29:32,160 --> 00:29:48,160
Although, you know, that's something physicists debate now with super strength theory, but that's a whole nother topic. But at least for the rest of us, it's important that you be able to, you come up with testable predictions that you wouldn't have thought of otherwise.

261
00:29:48,160 --> 00:29:57,160
And that they can be tested. And without that, you're kind of, you have an insight that might be right, might not be right, and you don't know what to do with it.

262
00:29:57,160 --> 00:30:11,160
And then I think the other element of being successful is really what John said is that, you know, interactions of one, when I first was looking to do theory in biology, I was trained as a physicist, but I was wanted to work in biology.

263
00:30:11,160 --> 00:30:20,160
All the biologists I talked with told me, you can't. Biology is an experimental science. We just find out the facts. And if you want to do theory, you should stay in physics.

264
00:30:20,160 --> 00:30:37,160
And, but why neuroscience started to need theory was when we started to collect enough data at different levels that we had to know how the interactions at one level could create the behavior at another level.

265
00:30:37,160 --> 00:30:49,160
And that, those aren't just facts. You can't just think your way through that. And that's where you really, and then the field of theoretical neuroscience, mathematical neuroscience has exploded since about the time I entered the field.

266
00:30:49,160 --> 00:31:03,160
I happened to enter at a very good time when several people were entering and that's when the field exploded. But that's, it's, yeah, if you're not dealing with sorts, you know, how do the rules of how a neurons activity evolves based on the input it receives,

267
00:31:03,160 --> 00:31:10,160
or inhibitory from other neurons, how does that lead to this behavior, which is more than the behavior that you can get from any one neuron?

268
00:31:10,160 --> 00:31:15,160
It's those kind of questions that where you really can't do it without math.

269
00:31:15,160 --> 00:31:23,160
I think there's a lot in this question of what does it mean in the mental health field to be right or wrong? Because I think that is a place that there's some vulnerability.

270
00:31:23,160 --> 00:31:31,160
One can say, what does it mean to be right or wrong about the ways of a person with psychosis or person with depression functions?

271
00:31:31,160 --> 00:31:41,160
And I think that part is part of why I like George Boxes so much because I don't know that we are at the stage now where we can, we can, we have ways of measuring right or wrong.

272
00:31:41,160 --> 00:31:58,160
We have ways of measuring useful or not useful though. And I think that one of the things the mathematical models do in our field is generate new ideas, generate new hypotheses that I think is one of you were saying, one wouldn't have already, one wouldn't have already just immediately concluded

273
00:31:58,160 --> 00:32:10,160
that emerged from the model, and then you try it. And sometimes it leads to something useful and sometimes it doesn't. And what has to be willing to go down that path even with an absence of certainty?

274
00:32:10,160 --> 00:32:17,160
Yeah, but that's what makes the models right or wrong is that they do make different dissociable predictions. You can't test them yes or no, right?

275
00:32:17,160 --> 00:32:25,160
So I think we can distinguish that that good modeling in neuroscience or behavior is kind of goal oriented.

276
00:32:25,160 --> 00:32:36,160
We're trying to explain a certain phenomenon. We're trying to make some actual testable predictions. And so that's in contrast to, I think, two approaches, right? One would be trying to say, well, I want to build a correct model.

277
00:32:36,160 --> 00:32:52,160
I want as much detail as I can in there and have a complete model, right? And I think the point is that that's a, you know, some of a futile effort and not actually a good path to generating understanding or advancing, you know, what kind of experiments, what we should measure, et cetera.

278
00:32:52,160 --> 00:33:02,160
And that's on one end. And the other is on being kind of too abstract, too much of a toy problem, which, you know, some people from physics and mathematics have a tendency toward.

279
00:33:02,160 --> 00:33:11,160
But then there ends up being a gap between you have a toy model. You can study some interesting properties. But there's a real gap between how you can connect to that to experiment and to real data.

280
00:33:11,160 --> 00:33:17,160
And so I think there are kind of two ends that you can fall off the edge in terms of being useful for modeling.

281
00:33:17,160 --> 00:33:22,160
I mean, just in clinical example, so clinical example, what you're saying in terms of how some of these models actually being clinically applied.

282
00:33:22,160 --> 00:33:34,160
So because of where we're located, we had a huge 9-11 population of firemen and cops after 9-11. And I was in the Toronto section in those years.

283
00:33:34,160 --> 00:33:40,160
And, you know, we had a lot of, I mean, there are validated treatments for PTSD, and we always use those kinds of things.

284
00:33:40,160 --> 00:33:49,160
But what I found was that, you know, one of the things that drove the firemen I was treating completely crazy, because they're straightforward ration, they're straightforward, sort of very straightforward people.

285
00:33:49,160 --> 00:33:58,160
The firemen are very straightforward people. They're not very introspective. They really bothered them that they knew that getting into an elevator was safe, and they also knew they couldn't get into an elevator.

286
00:33:58,160 --> 00:34:04,160
And it was ruining their lives. They lost their jobs over this. And I found you simply take a reward learning model, right?

287
00:34:04,160 --> 00:34:15,160
And I said, let me explain to you how this works. Your brain works in a reward learning model, and you've had this one shot, very negative learning thing, in which getting into an elevator is that it's working in your amygdala.

288
00:34:15,160 --> 00:34:20,160
It's not working in your frontal cortex. So talking about it's not going to change. It only behavior is going to change it.

289
00:34:20,160 --> 00:34:28,160
And giving them that model, giving them that reward learning model really changed things. So it made them feel less crazy.

290
00:34:28,160 --> 00:34:32,160
It made them feel hopeful, and it made a huge difference. That was your idea.

291
00:34:32,160 --> 00:34:47,160
I just want to make one quick, certainly clarifying comment. I hope it's clarifying anyway. Larry and Ken both refer to the idea that mathematics, once you engage in the mathematical model, you sort of committed yourself to the consequences of the mathematics of it.

292
00:34:47,160 --> 00:35:00,160
And I think there could be this misunderstanding. That means that mathematics is rigidly inflexible. And therefore risks being reductionistic and missing something.

293
00:35:00,160 --> 00:35:12,160
And Andrew said something very interesting, I think, about real mathematicians in the real world, or any way, in applied mathematics, where the whole idea is, okay, if there's a remainder, if there's a false, I'm calling it a remainder.

294
00:35:12,160 --> 00:35:20,160
If there's a false outcome, or an hypothesis that's been disproved, put it back in a hopper. Let's use it and build another model based on that.

295
00:35:20,160 --> 00:35:29,160
And that is a way to keep the mathematics from falling into being excessively rigid. Okay. But having said that, I want to ask the following question.

296
00:35:29,160 --> 00:35:42,160
So there's a lot of talk about whether or not computers, whether the brain is a computer. Okay. I want to ask something slightly broader. Of course, let's go to overlap with that question. Is the brain mathematical?

297
00:35:42,160 --> 00:35:53,160
Of course. This whole discussion about mathematical models. What is the difference between a mathematical model and a careful explanation?

298
00:35:53,160 --> 00:36:03,160
Okay. Careful explanations don't go into equations which could be manipulated to give you other consequences. Why not?

299
00:36:03,160 --> 00:36:15,160
Well, that'll become mathematical. Then it becomes mathematical. So there are beatable. So my argument back to you would be to say, at the point where it's exact enough, and exact enough, I would say has two properties.

300
00:36:15,160 --> 00:36:32,160
One is all the variables that you're referring to are measurable things in the universe, agreed measurable things in the universe, and the relationship that they have with each other in the universe corresponds to the relationship that they have in your model, whether it's a verbal model or an equation model, right? So that there's a correspondence.

301
00:36:32,160 --> 00:36:40,160
But if you've got a verbal model that has those two qualities, it's a mathematical model.

302
00:36:40,160 --> 00:36:52,160
I mean, Newton's three laws are written in words. But I'm with you in the sense that I think the word math gets used for a funny purpose here.

303
00:36:52,160 --> 00:37:01,160
It's used as to sort of sit in him for thinking carefully, and that gets very confusing.

304
00:37:01,160 --> 00:37:04,160
I think you have to define what one means when one's having this debate.

305
00:37:04,160 --> 00:37:13,160
I think the times you think you're thinking carefully, and then when someone forces you, I mean, this is sort of when you do an applied math course, right? So you say, oh, I think I have a good model.

306
00:37:13,160 --> 00:37:26,160
And then when you try to put it down in a mathematical way, mathematics, I think the formalism, I'm not disagreeing with you, but I'm saying the formalism of mathematics does force you to make sure that you can probably do that over.

307
00:37:26,160 --> 00:37:34,160
So I do. So I do. I'm an empiricist, right? I do science. I don't do modeling.

308
00:37:34,160 --> 00:37:46,160
To me, what seems like the difference is that when you have a computational or mathematical model, you can look at latent variables that are not evident to the person doing empirical work.

309
00:37:46,160 --> 00:37:49,160
And that seems to me to be the difference.

310
00:37:49,160 --> 00:37:59,160
You could argue second analysis was doing that from the beginning without using math, but was often not doing it in a very precise way.

311
00:37:59,160 --> 00:38:08,160
I wasn't even thinking of psychoanalysis as that sort of adds to it, but I think within sort of research, you know, I use statistics.

312
00:38:08,160 --> 00:38:20,160
You can use very advanced statistics, but to model, you have to, it seems to me you're looking for latent relationships and variables that necessarily are.

313
00:38:20,160 --> 00:38:34,160
Well, that's one thing is that you look for, you know, a much lower dimensional set of variables that explain some high dimensional data, but another is that you're just looking more like a dynamical system.

314
00:38:34,160 --> 00:38:41,160
Understanding how the dynamics of the interactions lead to certain results in this. I wouldn't call that latent variable. That's sort of just a different thing.

315
00:38:41,160 --> 00:38:43,160
That makes sense.

316
00:38:43,160 --> 00:38:47,160
Let me ask the question a different way because I got such a smackdown for the last time.

317
00:38:47,160 --> 00:38:56,160
No, no, no, no. There are things about the way the mind or people that have to be just for the brain cannot be accessed through math.

318
00:38:56,160 --> 00:39:01,160
That's a different way to ask the, I think, the same question.

319
00:39:01,160 --> 00:39:07,160
So here's an example.

320
00:39:07,160 --> 00:39:14,160
So I don't know if everyone is familiar with the recent advances in artificial intelligence to what's called deep nets.

321
00:39:14,160 --> 00:39:23,160
But these are very, very, very loosely modeled on neurons, what they call neural networks.

322
00:39:23,160 --> 00:39:31,160
And by being deep, you just mean you have layers and layers of them. So this one was predicted. This one was predicted. This one was predicted. This one.

323
00:39:31,160 --> 00:39:41,160
And that program has been going on for a long time. And it almost died. There were a few hardy souls who kept it alive through a period when most of the field didn't believe in it.

324
00:39:41,160 --> 00:39:46,160
And then they had a spectacular breakthrough about six years ago, seven years ago.

325
00:39:46,160 --> 00:40:00,160
Now it's completely taken over every field of artificial intelligence. So Jeff Hinton, who was really one of a few leaders, but to my mind, the leader in the field through all those years, brilliant guy.

326
00:40:00,160 --> 00:40:14,160
And he's the one, actually his group made the breakthrough that broke everything open in 2012. But he was asked, so there's a big question with the deep nets, which is that they have like a billion parameters,

327
00:40:14,160 --> 00:40:27,160
a billion synaptic weights that have to be learned from by just knowing for every input what output you want. And then you have an algorithm to make to learn those billion parameters to make this input produce that output.

328
00:40:27,160 --> 00:40:40,160
And he was so there's a big problem is there are black box. We want to know what they're doing. We want to know how they decide that this is a car and this is a German Shepherd.

329
00:40:40,160 --> 00:40:55,160
And we don't. It's just a black box, but it works really well. And so somebody was asking Jeff, you know, are we going to be able to understand these things? And he said, no, he said, these involve a billion parameters.

330
00:40:55,160 --> 00:41:04,160
If it was reducible to some nice simple operation, the problem would have been solved a long time ago. People tried to solve it by using nice simple operations. They didn't get anywhere.

331
00:41:04,160 --> 00:41:16,160
And so that's an example of a level at which, no, you can't reduce it to any nice math. I mean, it is mathematics. You know, there's a mathematical set of equations that are these deep nets, but we can't.

332
00:41:16,160 --> 00:41:28,160
It's not understandable mathematics. It's understandable at the level of what you're trying to optimize and what your learning rules are, but not what it's doing when you present an input at the bottom and the works is way to the top. That part doesn't seem to be understandable.

333
00:41:28,160 --> 00:41:37,160
That's a key thing. I see using that example, as I said, some of the parts of that are understandable, right? The learning rule is, you know, one equation, the loss function.

334
00:41:37,160 --> 00:41:49,160
What is it trying to optimize? And so, in one sense, it's a very simple description of what goes into the model. But again, you're training the seep network to classify, you know, visual images.

335
00:41:49,160 --> 00:41:58,160
And so all of the complexity of the visual world of all of those images is being, you know, parsed and extracted, and that goes into those billion parameters.

336
00:41:58,160 --> 00:42:06,160
And so in some sense, the model has to be complex because it's processing complex data, and that goes into the parameters.

337
00:42:06,160 --> 00:42:13,160
And so in some sense, you know, we understand something about those models. What are they doing? But the final solution is very complex.

338
00:42:13,160 --> 00:42:24,160
And I think a lot of neuroscience will be the same way, right? But it's not even so different from physics, right? We understand, you know, Newton's laws, but, you know, we can't predict the weather, right?

339
00:42:24,160 --> 00:42:30,160
We can't predict the hurricane. There's, you know, but we still think the same basic fundamental interactions are there.

340
00:42:30,160 --> 00:42:37,160
But it's a level of complexity that we're never going to understand on the same intuitive level as, you know, in a simple, inelastic collision of two particles.

341
00:42:37,160 --> 00:42:46,160
And so in the same way in neuroscience and psychology, we often will build kind of, you know, simplified models, where we want to understand some principles.

342
00:42:46,160 --> 00:42:57,160
And then, as you're saying, it goes back to metaphor, but often our metaphors are from, you know, simple models are very constrained behaviors that we can model, that we can study, that we can, you know, look at in humans, in animals, et cetera.

343
00:42:57,160 --> 00:43:03,160
And then there's this process of extrapolation and metaphor to the kind of full richness of human behavior.

344
00:43:03,160 --> 00:43:12,160
Yeah, I think a clarification, and if we were actually worried about this coming into this talk, is the clarification is that, and I always worry about the term computational psychiatry.

345
00:43:12,160 --> 00:43:19,160
I used to call it mathematical when I started doing it, because what I was doing was model it, right? Model a model, a math model of behavior.

346
00:43:19,160 --> 00:43:31,160
But computational psychiatry encompasses two things that are actually at the polar opposite, one in which you have specific models, like a specific reward learning model for the way a mouse is going to behave in a maze.

347
00:43:31,160 --> 00:43:36,160
And you have very specific models. But then you have these computational things, which are absolutely black boxes.

348
00:43:36,160 --> 00:43:43,160
So it encompasses both these things in which not only are there no mathematical models for the beginning to end thing, it's not even possible.

349
00:43:43,160 --> 00:43:51,160
It will never be possible as you point out, because deep networks will never be reducible.

350
00:43:51,160 --> 00:43:53,160
That's why they work.

351
00:43:53,160 --> 00:43:35,160
It seems to me that the subtlety you're talking about though between a substantial levels of complexity and how much you can simplify these models is different than what the

352
00:43:35,160 --> 00:44:03,160
people mean about it mathematical or not.

353
00:44:03,160 --> 00:44:08,160
From that question, at least the way I understood your question, it's all mathematical.

354
00:44:08,160 --> 00:44:14,160
Yes, how many parameters you have, yes, to what extent is it computational versus solvable into a simple equation?

355
00:44:14,160 --> 00:44:15,160
That's different.

356
00:44:15,160 --> 00:44:22,160
But I heard your question and tell me, as being more connected to the question of are we dualists or not?

357
00:44:22,160 --> 00:44:32,160
Because true dualism, and Larry and I have had this debate just recently, to my mind suggests that there is an aspect of the universe,

358
00:44:32,160 --> 00:44:41,160
that does not obey these same laws, that is not, quote unquote, reducible to the sort of principles by which we use.

359
00:44:41,160 --> 00:44:43,160
Now, I am not a dualist.

360
00:44:43,160 --> 00:44:44,160
I'm not a dualist.

361
00:44:44,160 --> 00:44:46,160
So I may be misrepresenting.

362
00:44:46,160 --> 00:44:57,160
But if you don't believe in dualism, if you really believe in monism, that there's one universe and there's one set of principles that governs it, then it's hard for me,

363
00:44:57,160 --> 00:45:02,160
then I don't think that there's anything that isn't at some point amenable to the use of math.

364
00:45:02,160 --> 00:45:03,160
I agree with that.

365
00:45:03,160 --> 00:45:05,160
Are you serious?

366
00:45:05,160 --> 00:45:06,160
Yep, okay.

367
00:45:06,160 --> 00:45:19,160
To me, I don't know anything about philosophy, but to me it seems very obvious that I feel a certain way being a conscious thing.

368
00:45:19,160 --> 00:45:29,160
I don't think that if you make the assumption that consciousness is some emergent property of a complex system, it seems plausible.

369
00:45:29,160 --> 00:45:39,160
I don't think mathematics, as it currently stands, has any foothold on that question.

370
00:45:39,160 --> 00:45:49,160
There's no language that mathematicians have access to today that can begin to untangle why it is that I feel the way that I do about anything.

371
00:45:49,160 --> 00:46:07,160
I mean, and just to answer me to push this analogy a little bit further, if you assume that we're conscious to whatever degree, another similar complicated systems that somehow perpetuate themselves are also conscious, maybe bacteria or sharks or dogs or

372
00:46:07,160 --> 00:46:17,160
cities or the esthetic change, if there's consciousness in these large complicated systems, I have no idea how to address it.

373
00:46:17,160 --> 00:46:30,160
I don't think any mathematician, I don't think math, if you say careful thinking will eventually understand things, sure, but if you look at the current set of tools that mathematicians have access to, no way.

374
00:46:30,160 --> 00:46:39,160
Actually, I think I agree with both of you.

375
00:46:39,160 --> 00:46:50,160
I mean, what you're saying is basically, look, it's a physical system that obeys the laws of physics, and in that sense, you can describe it in principle with those laws.

376
00:46:50,160 --> 00:46:59,160
You can't in practice because it's too complicated, but you can describe bits and pieces of it, but it is a physical system that operates according to some actual laws and isn't just doing whatever it feels like.

377
00:46:59,160 --> 00:47:08,160
It's just doing whatever it feels like doing it in a moment, but there are properties that emerge from that system that are very hard to describe.

378
00:47:08,160 --> 00:47:23,160
When a physicist describes the properties of water and why it's wet, in terms of the atoms in the water, you can derive from statistical physics, the attributes that you can recognize as how wet things behave.

379
00:47:23,160 --> 00:47:30,160
So there you can actually see the emergence. Consciousness?

380
00:47:30,160 --> 00:47:52,160
I tend to think that why material things have a subjective experience is not a scientifically addressable question to why, but the what, which material things end up creating subjective experience with the correlation between the material things and the subjective experience?

381
00:47:52,160 --> 00:47:55,160
I think we'll be able to, in principle, describe that perfectly.

382
00:47:55,160 --> 00:48:02,160
So why isn't it a zombie? Why doesn't it have subjective experience? I think that's not a scientific question. I don't know how science could possibly address that.

383
00:48:02,160 --> 00:48:12,160
So Andrew and I have had this ongoing, and I would say I'm a dualist all the way down, which is to say the turtles all the way down, which is to say I don't think you have to...

384
00:48:12,160 --> 00:48:30,160
I agree with your argument, but I don't think I need human consciousness to do it. I think if you simply look at a single cell protozoan that if you put it in a petri dish, it will swim up the sugar gradient.

385
00:48:30,160 --> 00:48:45,160
So that protozoan swims up the sugar gradient. I can describe the mathematics of the turning of its tail and how it turns, and I can probably get a perfectly good description of how that does.

386
00:48:45,160 --> 00:48:53,160
But that will never replace the description that the protozoan, that creature is swimming towards the sugar.

387
00:48:53,160 --> 00:49:01,160
So the mathematical description will never replace the intuition and the description of what's going on.

388
00:49:01,160 --> 00:49:07,160
And I think those are the two levels, and I think that's exactly what you're saying. So it's not even only the human being consciousness level.

389
00:49:07,160 --> 00:49:18,160
I don't think science can ever, I don't think mathematical things can ever, they can explain the mechanism of it, but you lose something when you move away from the thing of, oh what's going on? Oh, it's swimming towards the sugar.

390
00:49:18,160 --> 00:49:32,160
Well, I'm going to put it in, you can build a mathematical model. But you argue missing something as in a description of observing this, if you leave out the idea that it's swimming up a gradient, and that swimming up a gradient wouldn't be in the mathematics.

391
00:49:32,160 --> 00:49:38,160
Sure, it could be. You're missing something in every model, it depends what the questions are, but you can model those behaviors.

392
00:49:38,160 --> 00:49:46,160
This is what mathematical psychology does, right? You're trying to explain the pattern of behavior in relationship to the stimuli, and you have real kind of, you know, they're truly mechanistic.

393
00:49:46,160 --> 00:50:00,160
They're generative models, but if psychological processes is not of neurons, right? And those are perfectly good quantitative mathematical models meant to address, you know, kind of, again, the level of behavior in relationship to a stimuli or learning, right?

394
00:50:00,160 --> 00:50:06,160
And that's, again, bridging those two levels of analysis, and then there's the challenge, not how does a model connect to different levels?

395
00:50:06,160 --> 00:50:11,160
How do we connect to different modeling levels of analysis, the models of the level of circuits versus models of the level of circuits?

396
00:50:11,160 --> 00:50:28,160
This is an incredible, I think this debate is incredibly clinically relevant, because back to your original scenario, the typical occurrence is a patient comes to see me or one of us and says, I'm having a subjective experience, a deeply subjective, a conscious experience

397
00:50:28,160 --> 00:50:49,160
that I don't understand, or they say that is upsetting to them in some ways, whether they're anxious or they're angry or they're sad. And there's a question on the table as to whether any kind of analytic, not in the psychoanalytic way, but any kind of mathematical or analytic thinking has any value.

398
00:50:49,160 --> 00:51:03,160
Maybe there's nothing I can offer that, that's one hypothesis, that patient, because what they are coming to me with is so intrinsically subjective and doesn't follow any standard set of rules that no amount of experience I have could ever really impact.

399
00:51:03,160 --> 00:51:18,160
I don't believe that. I believe that using my experience, using the models that I have about why some people get sad, why some people get angry, why some people get anxious, actually has a value for them, not just in a kind of, oh, let me tell you a nice story, and therefore you'll feel better.

400
00:51:18,160 --> 00:51:27,160
But rather in that you'll learn something about your subjective experience that is going to be useful to you. And so that's to me why there's a bridge between those two.

401
00:51:27,160 --> 00:51:35,160
Now are there aspects of subjective experience we don't understand yet? Absolutely. Most don't understand. But some of it we understand in small ways.

402
00:51:35,160 --> 00:51:40,160
And I think that's proven to be very useful. And I think that's going to continue to grow.

403
00:51:40,160 --> 00:51:50,160
So this is why in clinical psychiatric research, there's such a push to go beyond kind of subjective evaluation toward actual quantitative behaviors.

404
00:51:50,160 --> 00:52:00,160
Can we relate subjective experiences of anhedonia to the parameters of sensitivity to reward and punishment in a reinforcement learning model that we can actually set?

405
00:52:00,160 --> 00:52:16,160
Can we relate delusions in schizophrenia? Can we encapsulate that in a more quantitative mathematical formalism of Bayesian inference and how we bring prior information in combination with sensory evidence?

406
00:52:16,160 --> 00:52:31,160
And if we can do that, then we have, it's still that metaphor that jump between the simple computation and the full subjective experience.

407
00:52:31,160 --> 00:52:48,160
And that gives us, I think, a foothold in terms of bridging these levels of analyses, which ultimately we need to, in psychiatry, if we're going to talk about how pharmacology can affect brain circuits and ultimately alleviate symptoms.

408
00:52:48,160 --> 00:53:05,160
So if I were to go head to head with your model and evaluating people for delusions, I would win. Sure. So what am I doing that is not in your model?

409
00:53:05,160 --> 00:53:19,160
The interesting thing there is that neural networks are starting to get better at humans at a lot of jobs like that.

410
00:53:19,160 --> 00:53:21,160
Like what? Radiology.

411
00:53:21,160 --> 00:53:23,160
Well, no, no way.

412
00:53:23,160 --> 00:53:24,160
Just go.

413
00:53:24,160 --> 00:53:25,160
Tell me in psychiatry.

414
00:53:25,160 --> 00:53:26,160
Okay.

415
00:53:26,160 --> 00:53:44,160
How about what they need right now to work is they need a huge training database, but if you had a training database of the conclusions you want to reach and the information that goes into it, and you had a lot of that, a machine might learn to do it better than you.

416
00:53:44,160 --> 00:53:56,160
So when I, and all of you, look at a people, look at somebody and their facial expression, you do instantaneous calculation that's better than any computer.

417
00:53:56,160 --> 00:53:57,160
So far.

418
00:53:57,160 --> 00:54:00,160
No, they're not too learning to do that, but they don't, you wouldn't know if that's a good label.

419
00:54:00,160 --> 00:54:02,160
I'm not talking about identity.

420
00:54:02,160 --> 00:54:05,160
No, I know. They're learning recognizing motions too, but they're not as good at it yet.

421
00:54:05,160 --> 00:54:14,160
But, but there are all sorts of nuances. And, and then in terms of context, I mean, computationally, we are very complex.

422
00:54:14,160 --> 00:54:17,160
We are, but we're starting to build machines that are equally complex.

423
00:54:17,160 --> 00:54:25,160
And also, I would point out that, that by virtue of evolution, you've had a training, you have been exposed to training sets, right? Over a billion years, right?

424
00:54:25,160 --> 00:54:31,160
And a billion years of training sets have selected you to be the best facial recognizer, right?

425
00:54:31,160 --> 00:54:33,160
And the people who are not as good facial recognizers.

426
00:54:33,160 --> 00:54:34,160
No, no, no.

427
00:54:34,160 --> 00:54:39,160
So evolution has been a training set over a billion years. We sometimes discount that, that, you know.

428
00:54:39,160 --> 00:54:42,160
No, but evolution has acted on the genes, right?

429
00:54:42,160 --> 00:54:43,160
Right.

430
00:54:43,160 --> 00:54:48,160
That has selected for the genes that can lead to those computational ability.

431
00:54:48,160 --> 00:54:55,160
You know, I'm not Lamarckian, even though, you know, you could do gene epigenetics, right? It's a little bit Lamarckian, that's right.

432
00:54:55,160 --> 00:55:05,160
But I have had several years of pattern recognition, but that even really thinking about it, and I'm awfully good at it.

433
00:55:05,160 --> 00:55:06,160
And I, I...

434
00:55:06,160 --> 00:55:12,160
But that is a huge training data set of, from birth, in an interactive way, a closed loop interaction with the environment.

435
00:55:12,160 --> 00:55:14,160
So, will we be replaced?

436
00:55:14,160 --> 00:55:18,160
Will clinicians be replaced by psychiatrists?

437
00:55:18,160 --> 00:55:25,160
Maybe by computers?

438
00:55:25,160 --> 00:55:31,160
The only things we really know how to do neural networks right now are when we have a huge database of input to output,

439
00:55:31,160 --> 00:55:34,160
then that we can get the machines to learn it, sometimes better than us.

440
00:55:34,160 --> 00:55:35,160
No, no, no, no.

441
00:55:35,160 --> 00:55:36,160
But what it shows is...

442
00:55:36,160 --> 00:55:37,160
You learn wrong.

443
00:55:37,160 --> 00:55:38,160
Excuse me?

444
00:55:38,160 --> 00:55:41,160
These neural networks have never learned the damn thing.

445
00:55:41,160 --> 00:55:45,160
They're trained to perform a classification task. They're not learning anything.

446
00:55:45,160 --> 00:55:47,160
Well, it depends. What's your definition of learning?

447
00:55:47,160 --> 00:55:48,160
They have no values.

448
00:55:48,160 --> 00:55:50,160
This goes to the Chinese room market.

449
00:55:50,160 --> 00:55:51,160
Yeah.

450
00:55:51,160 --> 00:55:52,160
I want to make sure...

451
00:55:52,160 --> 00:55:54,160
Well, I just wanted to finish that.

452
00:55:54,160 --> 00:56:05,160
The point is that some things that we think of as our unique human complex intelligence are starting to be done by machines better than us,

453
00:56:05,160 --> 00:56:11,160
like PlanGo, like PlanGIS, and like some kinds of recognition of objects and so forth.

454
00:56:11,160 --> 00:56:13,160
And I'm not saying that it'll all be replaced.

455
00:56:13,160 --> 00:56:20,160
I'm just trying to say that there's not obvious that there's something in us that is so special that we can't someday get machines to do it better.

456
00:56:20,160 --> 00:56:21,160
I agree.

457
00:56:21,160 --> 00:56:22,160
I agree.

458
00:56:22,160 --> 00:56:23,160
It's a question.

459
00:56:23,160 --> 00:56:27,160
And it also shows how morally complicated this is.

460
00:56:27,160 --> 00:56:33,160
So I think you would agree, you would agree too, Larry, that one of the things that we wish we were better at,

461
00:56:33,160 --> 00:56:35,160
is clinicians predicting who will commit suicide.

462
00:56:35,160 --> 00:56:39,160
That we do our best, and when we work in the emergency room or work on the inpatient unit,

463
00:56:39,160 --> 00:56:44,160
we make those decisions all the time as to who to keep involuntarily and who not to,

464
00:56:44,160 --> 00:56:47,160
and not infrequently, we get strong.

465
00:56:47,160 --> 00:56:49,160
So let me finish the story.

466
00:56:49,160 --> 00:56:51,160
I have a friend who works at Google.

467
00:56:51,160 --> 00:57:02,160
He says that it is kind of an openly known at Google that there are algorithms that they can apply that enable them to predict

468
00:57:02,160 --> 00:57:05,160
who's going to kill themselves to a frighteningly precise degree.

469
00:57:05,160 --> 00:57:10,160
The problem, they have enormous data sets, they have enormous access to data that we have clinicians do not.

470
00:57:10,160 --> 00:57:14,160
And then the question becomes, does Google want to admit that they know that or not?

471
00:57:14,160 --> 00:57:19,160
Facebook's now started to admit that, and doing little things here and there, is it enough?

472
00:57:19,160 --> 00:57:20,160
I'm not sure.

473
00:57:20,160 --> 00:57:22,160
But what do we as a society do?

474
00:57:22,160 --> 00:57:33,160
And the danger to me of too much prioritizing of the clinical superiority would be that we might not take advantage of some of these other data sources

475
00:57:33,160 --> 00:57:35,160
that could help us save a lot.

476
00:57:35,160 --> 00:57:37,160
So I don't want to pit myself against computers.

477
00:57:37,160 --> 00:57:39,160
I meant that to be provocative.

478
00:57:39,160 --> 00:57:41,160
I've done research in this area, right?

479
00:57:41,160 --> 00:57:50,160
So I've done research that shows, again in collaboration with Guillermo, that IBM wants it when it looks at language patterns

480
00:57:50,160 --> 00:57:57,160
and individuals at risk for psychosis, of whom 20% develop psychosis in two years does better than me.

481
00:57:57,160 --> 00:57:59,160
I mean, I've published that.

482
00:57:59,160 --> 00:58:07,160
In terms of suicide, I'm sure Google has much more than what's in the literature, because they have a lot of resources.

483
00:58:07,160 --> 00:58:18,160
But in the literature, there is a good literature that if you look at language, and it's across, it's people texting, talking, all kinds of things.

484
00:58:18,160 --> 00:58:25,160
I reviewed this for a grant that I put in, and Guillermo did an article actually looking at poems, poets who committed suicide

485
00:58:25,160 --> 00:58:31,160
and those who didn't, and looked at the actual language in their poems as to what we predict.

486
00:58:31,160 --> 00:58:39,160
Consistently across the board, it's using words that have semantic content that has similarity with hopelessness and depression.

487
00:58:39,160 --> 00:58:41,160
So, no, these are tools.

488
00:58:41,160 --> 00:58:43,160
I mean, I don't want to put all of us out of a job.

489
00:58:43,160 --> 00:58:51,160
But I just brought that up in a sort of provocative way in terms of the models, because we're also models, right?

490
00:58:51,160 --> 00:58:55,160
Doing computations, we don't understand our own computations, but...

491
00:58:55,160 --> 00:59:01,160
Okay, Adi, I'm curious if you could expand on your idea about learning and why neural nets don't learn?

492
00:59:01,160 --> 00:59:04,160
So, even these words, I feel learning, training, etc.

493
00:59:04,160 --> 00:59:06,160
Even those are very loaded.

494
00:59:06,160 --> 00:59:17,160
As you said before, these deep neural nets are complicated functions with many parameters, hyperparameters, even if you take into account the network architecture.

495
00:59:17,160 --> 00:59:26,160
And there are objective functions that are set up, which are minimized to fit the parameters of that function,

496
00:59:26,160 --> 00:59:30,160
so that at the end, the function has certain specific input-output properties.

497
00:59:30,160 --> 00:59:31,160
Okay.

498
00:59:31,160 --> 00:59:34,160
It's tempting to take a look at the go.

499
00:59:34,160 --> 00:59:35,160
What is this called?

500
00:59:35,160 --> 00:59:37,160
What was the go thing?

501
00:59:37,160 --> 00:59:38,160
Alpha go.

502
00:59:38,160 --> 00:59:39,160
Alpha go.

503
00:59:39,160 --> 00:59:42,160
Or deep blue, and say, wow, it knows how to play chess.

504
00:59:42,160 --> 00:59:44,160
It knows how to play go.

505
00:59:44,160 --> 00:59:49,160
It's a giant lookup table that has been...

506
00:59:49,160 --> 00:59:51,160
Those are constrained.

507
00:59:51,160 --> 00:59:56,160
That has been pruned and refined over many, many iterations, given lots of data.

508
00:59:56,160 --> 01:00:04,160
If you sat Alpha go down on the table opposite of whoever the gentleman was that Alpha go was playing, and you said, okay, guys, new rule.

509
01:00:04,160 --> 01:00:08,160
If you head off the left side of the board, you come on the right side of the board.

510
01:00:08,160 --> 01:00:11,160
You're playing on a cylinder now, or a torus.

511
01:00:11,160 --> 01:00:15,160
Alpha go wouldn't even be able to take the first move.

512
01:00:15,160 --> 01:00:17,160
Its database would be inapplicable.

513
01:00:17,160 --> 01:00:21,160
You sit down, Gary Kasparov, against deep blue, and you say, okay, guys, new rule.

514
01:00:21,160 --> 01:00:23,160
The queen, by the way, it's no longer a queen.

515
01:00:23,160 --> 01:00:25,160
It's like a rook in a bishop.

516
01:00:25,160 --> 01:00:26,160
I'm sorry, a rook in a knight.

517
01:00:26,160 --> 01:00:29,160
It's like a rook in a knight, but it doesn't move like a bishop anymore.

518
01:00:29,160 --> 01:00:30,160
Okay?

519
01:00:30,160 --> 01:00:35,160
Gary Kasparov would still kick my ass, but deep blue wouldn't be able to do anything.

520
01:00:35,160 --> 01:00:56,160
I'm not saying that there won't come a time where the types of functions and the types of parameters that people are able to bake into these neural networks are sophisticated enough that maybe there are some parameters that can be left to be tuned on the fly after the machine sits down and sees the button.

521
01:00:56,160 --> 01:00:58,160
Right now, we're definitely inapplicable.

522
01:00:58,160 --> 01:01:05,160
I'm not there.

523
01:01:05,160 --> 01:01:06,160
This issue is actually in a philosophical issue.

524
01:01:06,160 --> 01:01:09,160
This is philosophical debate about something called the Chinese room problem.

525
01:01:09,160 --> 01:01:13,160
I don't want to get into it, but it's exactly the day as to whether it's something in...

526
01:01:13,160 --> 01:01:14,160
Why not?

527
01:01:14,160 --> 01:01:15,160
Why not?

528
01:01:15,160 --> 01:01:16,160
Why is it?

529
01:01:16,160 --> 01:01:17,160
It's not?

530
01:01:17,160 --> 01:01:18,160
It's not.

531
01:01:18,160 --> 01:01:19,160
It's Johnson.

532
01:01:19,160 --> 01:01:20,160
Look at the describing pre-cream.

533
01:01:20,160 --> 01:01:27,160
The argument is if I have someone in a room who has walled this right, you know the argument right here, and he has all of these dictionaries, Chinese English dictionaries in the room.

534
01:01:27,160 --> 01:01:30,160
And if I give him a sentence written in English, he can translate, right?

535
01:01:30,160 --> 01:01:37,160
He can then look everything up and he can respond properly in Chinese.

536
01:01:37,160 --> 01:01:40,160
Yet, there's nobody who knows Chinese.

537
01:01:40,160 --> 01:01:42,160
So that's the Chinese room argument.

538
01:01:42,160 --> 01:01:43,160
There's nobody who knows Chinese.

539
01:01:43,160 --> 01:01:44,160
But he can't.

540
01:01:44,160 --> 01:01:49,160
And the two sides of the argument are one side says that proves what your point is that the machine doesn't know anything.

541
01:01:49,160 --> 01:01:53,160
And the other side of the argument says no, the entire system knows Chinese.

542
01:01:53,160 --> 01:01:55,160
And that's what we mean by knowing Chinese.

543
01:01:55,160 --> 01:01:56,160
No, no, no.

544
01:01:56,160 --> 01:02:01,160
It applies to the Chinese room because if you, it's apocryphal, right?

545
01:02:01,160 --> 01:02:08,160
All those stories about like the spirit, the flesh is weak, but the spirit is strong, right?

546
01:02:08,160 --> 01:02:16,160
Going into Russian and then being back translated as the meat is rotten and the alcohol is bad, right?

547
01:02:16,160 --> 01:02:19,160
So it doesn't work that way with language.

548
01:02:19,160 --> 01:02:25,160
So whatever he came up with, without knowing, when you put a person in there, they have a face.

549
01:02:25,160 --> 01:02:29,160
And in there, they have a flexibility that the sort of...

550
01:02:29,160 --> 01:02:34,160
In the Chinese room argument, the person is really acting just completely in the middle of the person.

551
01:02:34,160 --> 01:02:35,160
No, no, no, no.

552
01:02:35,160 --> 01:02:39,160
But then you're getting into a strictly rule-based language.

553
01:02:39,160 --> 01:02:44,160
If you believe in Chomsky's sort of hierarchy, now you're at the bottom.

554
01:02:44,160 --> 01:02:48,160
And Chomsky would say for like a human, sophisticated grammar, you need it.

555
01:02:48,160 --> 01:02:50,160
You can't, it has to be probabilistic.

556
01:02:50,160 --> 01:02:51,160
It can be rule-based.

557
01:02:51,160 --> 01:02:54,160
You can build all those things.

558
01:02:54,160 --> 01:02:56,160
But it's complicated.

559
01:02:56,160 --> 01:02:57,160
And that's the Turing test.

560
01:02:57,160 --> 01:03:00,160
The Chinese remiss the Turing test.

561
01:03:00,160 --> 01:03:05,160
The Chinese room problem captures, tries to capture this difference between what do we mean,

562
01:03:05,160 --> 01:03:10,160
what is our intuition when we say somebody knows something or somebody has learned something,

563
01:03:10,160 --> 01:03:14,160
and it challenges that thing, and it challenges, as you did this question of whether we're going to...

564
01:03:14,160 --> 01:03:18,160
That's the purpose of the argument, is to challenge the use of that meaning.

565
01:03:18,160 --> 01:03:22,160
Just want to make one other point that I run into whenever I try to talk about.

566
01:03:22,160 --> 01:03:26,160
The other thing is that we all have folk science in addition to a formal science.

567
01:03:26,160 --> 01:03:29,160
There's folk philosophy that cultures have folk philosophy.

568
01:03:29,160 --> 01:03:31,160
We have folk chemistry and things like that.

569
01:03:31,160 --> 01:03:32,160
And they're not very good.

570
01:03:32,160 --> 01:03:36,160
I mean, people have survived before formal science came in because they had these theories.

571
01:03:36,160 --> 01:03:39,160
But folk psychology is really quite good.

572
01:03:39,160 --> 01:03:40,160
Right?

573
01:03:40,160 --> 01:03:43,160
Pre-scientific psychology is actually much better.

574
01:03:43,160 --> 01:03:45,160
We're pretty good psychologists just intuitively.

575
01:03:45,160 --> 01:03:46,160
We have to be.

576
01:03:46,160 --> 01:03:47,160
Right?

577
01:03:47,160 --> 01:03:48,160
We've evolved to be pretty good psychologists.

578
01:03:48,160 --> 01:03:54,160
And I think that is one of the reasons a lot of these things get confused is because folks' psychology is so good.

579
01:03:54,160 --> 01:03:55,160
Right?

580
01:03:55,160 --> 01:04:02,160
It is hard for scientific psychology to sometimes compete, and there's a much more sophisticated language within folk psychology

581
01:04:02,160 --> 01:04:06,160
than there is, for example, in folk physics or in folk notions of mathematics.

582
01:04:06,160 --> 01:04:08,160
And that gets in the way.

583
01:04:08,160 --> 01:04:12,160
I want to just respond briefly to Audie.

584
01:04:12,160 --> 01:04:18,160
I agree with your criticism that they have a limited range.

585
01:04:18,160 --> 01:04:23,160
And if you change the rules on them, they haven't been taught to adapt.

586
01:04:23,160 --> 01:04:26,160
But it's not to say that they couldn't be taught to adapt.

587
01:04:26,160 --> 01:04:31,160
But the other thing is that I don't think it's right to say they're a lookup table.

588
01:04:31,160 --> 01:04:39,160
Because AlphaGo, well, on any given game, you can't see a board position that no human has ever seen before

589
01:04:39,160 --> 01:04:43,160
and that the machine has never seen before, and it'll still outplay Lisa doll.

590
01:04:43,160 --> 01:04:48,160
Because it's learned somehow implicit in its brilliant parameters.

591
01:04:48,160 --> 01:04:51,160
It's learned some principles of how to play Go.

592
01:04:51,160 --> 01:04:55,160
That it can deal with entirely new situations and outplay any human.

593
01:04:55,160 --> 01:04:58,160
And the same with Alpha Chess or whatever they call it.

594
01:04:58,160 --> 01:04:59,160
AlphaZero.

595
01:04:59,160 --> 01:05:00,160
AlphaZero.

596
01:05:00,160 --> 01:05:01,160
Yeah.

597
01:05:01,160 --> 01:05:02,160
So I don't think it's right.

598
01:05:02,160 --> 01:05:06,160
I think Deep Blue was basically a lookup table, but I think these new things are not.

599
01:05:06,160 --> 01:05:08,160
So they're probabilistic now.

600
01:05:08,160 --> 01:05:15,160
No, but the point is that it really understands some principles in a way that, I mean, Deep Blue just did a deep search

601
01:05:15,160 --> 01:05:20,160
and said, you know, if I look ahead, 10 moves, if I do this, am I going to be better off or worse off?

602
01:05:20,160 --> 01:05:22,160
And it didn't know anything really.

603
01:05:22,160 --> 01:05:27,160
What it knew to evaluate better off or worse off was really simple, but it had this powerful ability to search.

604
01:05:27,160 --> 01:05:30,160
But that's not what these new things are doing.

605
01:05:30,160 --> 01:05:32,160
They don't search nearly so deeply.

606
01:05:32,160 --> 01:05:35,160
They learn principles that they can then outplay any human.

607
01:05:35,160 --> 01:05:40,160
But this principle, the philosophical pushback to you would be when you say it understands, right?

608
01:05:40,160 --> 01:05:41,160
Well, I didn't say it understand.

609
01:05:41,160 --> 01:05:42,160
And the haves as it understands.

610
01:05:42,160 --> 01:05:46,160
I said they're principles built into these billion parameters that can now outplay any human.

611
01:05:46,160 --> 01:05:49,160
But this principle is, I understand we also kind of, you know, consults.

612
01:05:49,160 --> 01:05:54,160
It's observing a pattern and then getting some kind of value function or if it's going to play here or not there.

613
01:05:54,160 --> 01:05:58,160
Not necessarily, you know, rule-based or a particular ball, or a particular ball.

614
01:05:58,160 --> 01:06:00,160
I can't even articulate it.

615
01:06:00,160 --> 01:06:06,160
You know, decisions, but basically kind of, it's a form of pattern recognition.

616
01:06:06,160 --> 01:06:18,160
But, you know, that also underlies a lot of our cognition, the way we recognize emotions, the way, you know, probably a good chess player can just kind of get a flash of the chessboard and get a sense of, is this a good game to play or not?

617
01:06:18,160 --> 01:06:19,160
This a good move or not?

618
01:06:19,160 --> 01:06:23,160
In that kind of intuitive way, a lot of our cognition, we can't necessarily explain.

619
01:06:23,160 --> 01:06:31,160
So they're mathematical models for Gestalt, that kind of Gestalt perception that we're also good at as people.

620
01:06:31,160 --> 01:06:35,160
I mean, I would say a lot of the visual recognition.

621
01:06:35,160 --> 01:06:38,160
The same way that people use, you know, deep networks for this.

622
01:06:38,160 --> 01:06:42,160
You know, we can argue about whether it's kind of training or learning by the same rules.

623
01:06:42,160 --> 01:06:52,160
But, you know, as a metaphor of the general principle of, you know, how does our eventual visual system work for object recognition of it being kind of successive, you know, layers of feed forward problems?

624
01:06:52,160 --> 01:07:10,160
And, you know, layers of feed forward processing with some kind of flexible learning such that the representations at the intermediate layers of your vision, you know, are guided by the final, you know, output recognition and decision in action is, you know, I think where you would say that

625
01:07:10,160 --> 01:07:13,160
model is in some sense a good model of human vision.

626
01:07:13,160 --> 01:07:23,160
So do we already have our own neural net for identification?

627
01:07:23,160 --> 01:07:30,160
I think it's only still a very impoverished analogy to what we do.

628
01:07:30,160 --> 01:07:34,160
But already just with that impoverished analogy, it can outdo us on some test.

629
01:07:34,160 --> 01:07:44,160
We do fairly complex thinking without realizing in advance they know how to do it.

630
01:07:44,160 --> 01:07:49,160
So you could consider that analogy to what some of these computers are doing, perhaps.

631
01:07:49,160 --> 01:07:52,160
Yeah, but you can also bring these back into cognitive neuroscience, right?

632
01:07:52,160 --> 01:07:57,160
So these deep networks, not only do they, you know, do object recognition, it seems like they do it in a similar way as us.

633
01:07:57,160 --> 01:08:11,160
You know, recording from single neurons and monkeys, we can do functional MRI in humans, and we can look at what kind of neural representations that are in, say, different layers in visual processing and compare that to the different layers of processing in these deep networks.

634
01:08:11,160 --> 01:08:13,160
And there are correspondences.

635
01:08:13,160 --> 01:08:17,160
And so, you know, it's considered there, they're trained in impoverished way.

636
01:08:17,160 --> 01:08:19,160
They're not lacking some things.

637
01:08:19,160 --> 01:08:29,160
But where we can actually try to check the actual internal mechanisms, they seem to have some validity in terms of explaining not only our behavior, but even how the brain works, our representation.

638
01:08:29,160 --> 01:08:39,160
I think I would definitely agree that there is some analogy between what we're doing and what strategies are used to make these deep neural networks.

639
01:08:39,160 --> 01:08:40,160
That's true.

640
01:08:40,160 --> 01:08:43,160
But I also agree that it is impoverished and not.

641
01:08:43,160 --> 01:08:53,160
But to Jerry's point about our doing computations that we don't know, I mean, as anybody who has the experience of these phony rocks that are really very light, but they think, you know, you look at the, it looks like a rock, but it's actually foam.

642
01:08:53,160 --> 01:08:58,160
And when you try to pick that up, you completely miss, right, you completely miss the motoric thing.

643
01:08:58,160 --> 01:09:11,160
Because we take absolutely for granted that the incredible computation that goes into a simple motor thing of lifting something up and putting the right pressure on it, and I can fool you by giving you a mis-estimation of what the weight of the thing is.

644
01:09:11,160 --> 01:09:17,160
But so we're doing incredibly complex computations, right, just for that simple motor action.

645
01:09:17,160 --> 01:09:20,160
We have to plan it very carefully, and we have to know what it's doing otherwise.

646
01:09:20,160 --> 01:09:24,160
To give credit for credits too, I think that's one of the early contributions to Psychone else.

647
01:09:24,160 --> 01:09:34,160
Because I think for a motor, the Psychone else is not on motor movement, but to acknowledge the complexity of these models outside of awareness.

648
01:09:34,160 --> 01:09:43,160
Because I think that's the literature that was early literature on what was outside of awareness was that it had to be incredibly simple.

649
01:09:43,160 --> 01:09:46,160
And that more complex processes therefore had to be conscious.

650
01:09:46,160 --> 01:09:56,160
And we now know, and I don't even think it's controversial in the cognitive neuroscience world, that the kinds of processing that can go on completely outside of awareness can be enormously complex.

651
01:09:56,160 --> 01:10:00,160
And this was something that Freud was talking about, you know, hundred more years ago.

652
01:10:00,160 --> 01:10:04,160
And that example highlights one of those key computations is prediction, right?

653
01:10:04,160 --> 01:10:14,160
And so that actually gives us a hugely rich data set, because we're not just, you know, training the human brain on, you know, discrete supervised learning, right, but on predicting the future.

654
01:10:14,160 --> 01:10:21,160
What are the properties of things in the service of predicting how I will interact with them, how they will move in the future, etc.

655
01:10:21,160 --> 01:10:25,160
So, this is a characteristic of various things.

656
01:10:25,160 --> 01:10:27,160
Questions later.

657
01:10:27,160 --> 01:10:28,160
No questions later.

658
01:10:28,160 --> 01:10:29,160
No, no.

659
01:10:29,160 --> 01:10:30,160
No, no.

660
01:10:30,160 --> 01:10:31,160
No, no.

661
01:10:31,160 --> 01:10:32,160
Later.

662
01:10:32,160 --> 01:10:33,160
Later.

663
01:10:33,160 --> 01:10:34,160
Later.

664
01:10:34,160 --> 01:10:35,160
Later.

665
01:10:35,160 --> 01:10:36,160
The, um.

666
01:10:36,160 --> 01:10:41,160
You speak well with the same team about the computation, computation.

667
01:10:41,160 --> 01:10:42,160
Sir, please have a seat.

668
01:10:42,160 --> 01:10:45,160
We have an opportunity later to speak.

669
01:10:45,160 --> 01:10:46,160
Okay.

670
01:10:46,160 --> 01:10:47,160
Thank you.

671
01:10:47,160 --> 01:11:02,160
Larry mentioned earlier that, you know, in mathematical models, we have these variables that are all well defined, in which we can then check on, or be aware of.

672
01:11:02,160 --> 01:11:13,160
And the interesting thing is that when there are all these hidden units, and in some forms of mathematics as well, you can only say that theoretically they can be checked on.

673
01:11:13,160 --> 01:11:14,160
Theoretically what?

674
01:11:14,160 --> 01:11:15,160
Theoretically they can be.

675
01:11:15,160 --> 01:11:20,160
They are possibly accessible to being, you know, accessed.

676
01:11:20,160 --> 01:11:28,160
But in fact, in the process of doing the computation, some of those things are just, they're, they're, x's and y's.

677
01:11:28,160 --> 01:11:30,160
They're not really specified.

678
01:11:30,160 --> 01:11:32,160
I mean, I guess I may need a distinction.

679
01:11:32,160 --> 01:11:33,160
I'm not good at that.

680
01:11:33,160 --> 01:11:34,160
Okay.

681
01:11:34,160 --> 01:11:35,160
Neither.

682
01:11:35,160 --> 01:11:35,160
Okay.

683
01:11:35,160 --> 01:11:36,160
Thank you.

684
01:11:36,160 --> 01:11:37,160
No, I think you're not allowed to.

685
01:11:37,160 --> 01:11:47,160
I think you're a point about it in the deep neural net, and you may understand individual weights, but you don't actually understand how the process is leading to the proper outcome.

686
01:11:47,160 --> 01:11:48,160
So that is it.

687
01:11:48,160 --> 01:11:49,160
Absolutely.

688
01:11:49,160 --> 01:11:50,160
There's an intuition in the black box.

689
01:11:50,160 --> 01:11:52,160
That's a camera for it that is a black box.

690
01:11:52,160 --> 01:12:01,160
There's an intuition we have about what it means to really learn something that I think does involve our knowing the steps quite well, and then having an intuition that we know those steps.

691
01:12:01,160 --> 01:12:05,160
That's what we call, I'll say, for the purpose of this comment.

692
01:12:05,160 --> 01:12:06,160
That's learning.

693
01:12:06,160 --> 01:12:14,160
But when we don't know how those steps are when they're invisible and they're being manipulated all the while, we then have this thought, well, maybe we don't know.

694
01:12:14,160 --> 01:12:16,160
That's how we might be able to do things unconsciously.

695
01:12:16,160 --> 01:12:19,160
Because we're not aware of all the variables being affected.

696
01:12:19,160 --> 01:12:22,160
But I think that sense that we know what we're doing is an illusion.

697
01:12:22,160 --> 01:12:34,160
Because when we've, when, I mean, artificial intelligence for many years until the deep net revolution about six or seven years ago was focused on trying to do things by writing down the rules.

698
01:12:34,160 --> 01:12:41,160
Trying to do vision by writing down the rules, trying to play chess or go by writing down the rules, and it failed miserably.

699
01:12:41,160 --> 01:12:43,160
Because we don't know the rules.

700
01:12:43,160 --> 01:12:49,160
The rules, and actually to go to Jeff Hinton's point, you can't reduce it to the rules.

701
01:12:49,160 --> 01:12:52,160
It's a way more complicated thing than that.

702
01:12:52,160 --> 01:12:57,160
I was trying to capture why we might have an intuition of how learning is different from what computers do.

703
01:12:57,160 --> 01:13:06,160
So whether or not it's good, at it or it's accurate, that may be what gives us the feeling, whether intuition that we've learned something, as opposed to just following the recipe.

704
01:13:06,160 --> 01:13:10,160
Well, I think I learned something when I can teach somebody else.

705
01:13:10,160 --> 01:13:13,160
I don't know if there's an analogy for these nets.

706
01:13:13,160 --> 01:13:17,160
Maybe they have some consciousness, maybe they can feel they can teach other nets.

707
01:13:17,160 --> 01:13:24,160
Well, I can make a bad analogy, which is that the people can take a neural net that's learned to do something, and then they can teach another simpler net to do it.

708
01:13:24,160 --> 01:13:27,160
It can teach something else to do it more compactly.

709
01:13:27,160 --> 01:13:32,160
Teach it in the sense of being the one who tells it what the right answer is.

710
01:13:32,160 --> 01:13:37,160
So people do use one net to teach another in very simple ways right now.

711
01:13:37,160 --> 01:13:38,160
I think that goes in the next one.

712
01:13:38,160 --> 01:13:39,160
I don't really interact with it.

713
01:13:39,160 --> 01:13:40,160
It's just the input out.

714
01:13:40,160 --> 01:13:43,160
It's not what we call teaching.

715
01:13:43,160 --> 01:13:44,160
It's not what we call teaching.

716
01:13:44,160 --> 01:13:45,160
Maybe something like that.

717
01:13:45,160 --> 01:13:53,160
I think that's what I was trying to get at when I talked about the folks' psychological language, is to say that we have this whole language that is that we have evolved that we have evolved into the world.

718
01:13:53,160 --> 01:13:58,160
And it's about ourselves, understanding ourselves and understanding others, our folk psychology.

719
01:13:58,160 --> 01:14:03,160
And then we have these words and then we apply these words, and then those words have very specific human meanings.

720
01:14:03,160 --> 01:14:04,160
So I think you're right there.

721
01:14:04,160 --> 01:14:06,160
When we say learn, we've got to be careful.

722
01:14:06,160 --> 01:14:07,160
We should have two words, right?

723
01:14:07,160 --> 01:14:09,160
We shouldn't say a machine learns.

724
01:14:09,160 --> 01:14:12,160
Because learn has a very specific meaning.

725
01:14:12,160 --> 01:14:16,160
It has a very specific humanistic meaning.

726
01:14:16,160 --> 01:14:19,160
It has to do with experience and it has to do with feeling.

727
01:14:19,160 --> 01:14:31,160
And so I think that we apply the word incorrectly because we take folk psychology, which was developed for human beings, and we apply to machines and those kinds of things.

728
01:14:31,160 --> 01:14:33,160
And we don't have other words.

729
01:14:33,160 --> 01:14:36,160
But we really should have other words.

730
01:14:36,160 --> 01:14:38,160
We do qualify.

731
01:14:38,160 --> 01:14:42,160
There's reinforcement learning and unsupervised learning and supervised learning.

732
01:14:42,160 --> 01:14:56,160
There's different ways that we learn and again, reinforcement learning, like you said, may engage the amygdala in an unconscious way, which may be different than supervised training as well as just kind of general experience or prediction, which may be your less conscious.

733
01:14:56,160 --> 01:15:11,160
It is interesting to me how many conversations, I think, can evolve into arguments around the semantics in exactly the way you're describing between the folk psychological use of a term which people didn't want to defend in a certain way.

734
01:15:11,160 --> 01:15:16,160
And then the other uses which may be narrower, broader, but are carefully defined.

735
01:15:16,160 --> 01:15:27,160
And I think ultimately, since we're not going to discourage people from using those words, you just have to get people to define them when they're using them because people use the same words in different ways.

736
01:15:27,160 --> 01:15:32,160
We can second announce this as had this problem for a long, long time, which is we all talk about transference.

737
01:15:32,160 --> 01:15:42,160
We all talk about libido and we can have very long conversations meaning entirely different things with the same word that then often can be unproductive.

738
01:15:42,160 --> 01:15:45,160
Cheryl mentioned the Turing test, which was the definition of intelligence, right?

739
01:15:45,160 --> 01:15:49,160
So that was a debate, you know, 40 years ago on debates on what does it mean to be intelligent?

740
01:15:49,160 --> 01:15:51,160
And they tried to define that.

741
01:15:51,160 --> 01:15:54,160
The Turing test is, I think, is alive and well.

742
01:15:54,160 --> 01:15:55,160
Say that again?

743
01:15:55,160 --> 01:15:56,160
The Turing test.

744
01:15:56,160 --> 01:15:59,160
The idea of the Turing test, I think, is alive and well.

745
01:15:59,160 --> 01:16:00,160
How alive?

746
01:16:00,160 --> 01:16:03,160
Maybe you should tell people what it is.

747
01:16:03,160 --> 01:16:04,160
Yeah, please.

748
01:16:04,160 --> 01:16:05,160
Oh.

749
01:16:05,160 --> 01:16:26,160
So Alan Turing said that had a test that for artificial intelligence that if a computer could pass as human in conversation with another, with a human, it will have passed the Turing test.

750
01:16:26,160 --> 01:16:30,160
So, you know, you could think chatbots.

751
01:16:30,160 --> 01:16:36,160
They can, as a simulation, for a limited amount of time, seem almost human.

752
01:16:36,160 --> 01:16:42,160
But if you really push, we don't have any machines yet that really pass the test.

753
01:16:42,160 --> 01:16:45,160
My mom, I think my father could pass the Turing test.

754
01:16:45,160 --> 01:16:46,160
Now?

755
01:16:46,160 --> 01:16:51,160
If I talk to your father, I wouldn't think he was human.

756
01:16:51,160 --> 01:16:58,160
If you communicate with him via text or email, I think he would think he was a chatbot.

757
01:16:58,160 --> 01:16:59,160
He might.

758
01:16:59,160 --> 01:17:07,160
So, no, no, but the chatbot is not part of the Turing test.

759
01:17:07,160 --> 01:17:10,160
The Turing test was before chatbots existed.

760
01:17:10,160 --> 01:17:16,160
What I'm saying is a chatbot is getting close in some circumstances to appearing human.

761
01:17:16,160 --> 01:17:21,160
I see.

762
01:17:21,160 --> 01:17:24,160
No one has, where does the status of that?

763
01:17:24,160 --> 01:17:26,160
No machine has passed the Turing test.

764
01:17:26,160 --> 01:17:27,160
No, I don't think so.

765
01:17:27,160 --> 01:17:33,160
I think there was one chatbot which kind of passed by acting as if it were a child who's a non-native English speaker.

766
01:17:33,160 --> 01:17:34,160
Yeah, a load of art.

767
01:17:34,160 --> 01:17:37,160
I think there's a time I have to announce.

768
01:17:37,160 --> 01:17:39,160
I'm actually a robot.

769
01:17:39,160 --> 01:17:45,160
I think we're going to stop now, though it's been such a wonderful and lively conversation and invite people to ask questions.

770
01:17:45,160 --> 01:17:47,160
Why don't you come up to the microphone?

771
01:17:47,160 --> 01:17:49,160
I just assumed everyone's a Turing test.

772
01:17:49,160 --> 01:17:55,160
And please, if you want to see the movie, questions and comments to be kept on the brief side, please.

773
01:17:55,160 --> 01:17:57,160
All right.

774
01:17:57,160 --> 01:18:00,160
Thank you for this engaging and live discussion.

775
01:18:00,160 --> 01:18:09,160
It's interesting because when you were talking about applying mathematical models to psychological states, let's say,

776
01:18:09,160 --> 01:18:19,160
it seemed as though you were mainly talking about behavior, decision making, and action.

777
01:18:19,160 --> 01:18:27,160
And so I could see how models of probability could determine, you know, could be used to determine particular outcomes.

778
01:18:27,160 --> 01:18:33,160
And so the same thing when you were talking about the computer's playing chess, right?

779
01:18:33,160 --> 01:18:35,160
That's how do they make decisions?

780
01:18:35,160 --> 01:18:36,160
How do they act?

781
01:18:36,160 --> 01:18:39,160
How do they behave? What information do they have access to?

782
01:18:39,160 --> 01:18:44,160
But what about mathematical models applied to, let's say, experience?

783
01:18:44,160 --> 01:18:48,160
How one experiences or to consciousness in general?

784
01:18:48,160 --> 01:18:52,160
That seems to be a little bit different than the direction you were heading.

785
01:18:52,160 --> 01:18:55,160
It's like art, humor.

786
01:18:55,160 --> 01:18:56,160
Yeah.

787
01:18:56,160 --> 01:18:58,160
How one experiences art or what creation is, mathematics?

788
01:18:58,160 --> 01:18:59,160
Aesthetics.

789
01:18:59,160 --> 01:19:00,160
Yeah, exactly.

790
01:19:00,160 --> 01:19:03,160
But even experience states, right?

791
01:19:03,160 --> 01:19:06,160
How do I know what I experience and what do I experience?

792
01:19:06,160 --> 01:19:15,160
And is that something that can be described mathematically given you have enough information and data, let's say?

793
01:19:15,160 --> 01:19:29,160
I think right now we couldn't say anything about the in principle when we have, you know, way more ability to watch what's going on in brains.

794
01:19:29,160 --> 01:19:45,160
And to, you know, at some point we're going to be, as I said before, at some point we're going to be able to say these kinds of neural patterns of activity reach consciousness and, you know, lead to this feeling or this or this experience.

795
01:19:45,160 --> 01:19:48,160
And these other kinds of neural activity don't go to consciousness.

796
01:19:48,160 --> 01:19:52,160
But, you know, why they have conscious experience?

797
01:19:52,160 --> 01:20:00,160
I don't know what we'll be able to make a nice morphism between, you know, the behavior of the neurons and what different things feel like.

798
01:20:00,160 --> 01:20:04,160
But that's way far in the sense of future.

799
01:20:04,160 --> 01:20:07,160
We couldn't even touch that now.

800
01:20:07,160 --> 01:20:17,160
I would say that the why question, depending on how you define it, sure, I can see that being far in the future, what I don't think is far in the future is models that are relevant to affective experience.

801
01:20:17,160 --> 01:20:40,160
And, you know, Cheryl's work is one example of that. But there's many other examples now of various relatively simple models that look at the sort of dynamics of various affective states and why one reacts to certain ways to different things and the correspondence of that between, you know, facial expressions and behaviors and so on.

802
01:20:40,160 --> 01:20:49,160
But that's a model of, I agree with you, but that's a model of different level. I guess what I was addressing was the neurons. Can we talk about feelings? No, that's way far in future.

803
01:20:49,160 --> 01:21:05,160
But that's also, when you're discussing modeling affective states, that's also what gives rise to the affective state or the behavior of the affective state, but not the experience of the affective state.

804
01:21:05,160 --> 01:21:16,160
I was drawing this distinction with the why. I think one can always carve out a realm of the why that we don't have access to, but I think that's shrinking.

805
01:21:16,160 --> 01:21:28,160
I mean, the example I'll give of that is I had a classmate, Natalie Training, who remained famous, who we used to have this debate about whether there was possible or worthwhile to measure anything in the psychoanalytic process.

806
01:21:28,160 --> 01:21:41,160
And so he told me that his definition of psychoanalysis was that which I could never measure. So by that definition, as I got better in measuring things, his diagnosis got smaller and smaller.

807
01:21:41,160 --> 01:21:57,160
He was okay with that definition, but I guess what I would argue is that, sure, one can always carve out a why that is inaccessible, but I think it's relevant to the why, the more and more we know about the dynamics of these affective states.

808
01:21:57,160 --> 01:21:59,160
Or the what, actually.

809
01:21:59,160 --> 01:22:07,160
Yeah, well, but philosophers talk about the heart problem. I think that remains obviously an open question.

810
01:22:07,160 --> 01:22:17,160
I don't know whether that's a scientific question or remains forever a philosophical question as to how you can bridge between mechanism and experience.

811
01:22:17,160 --> 01:22:22,160
I think that the heart problem is a heart problem for a good reason.

812
01:22:22,160 --> 01:22:29,160
I guess it's an empirical point as to whether we'll ever be able to bridge that. I'm betting no.

813
01:22:29,160 --> 01:22:30,160
But we'll see.

814
01:22:30,160 --> 01:22:37,160
Can I just go back to an example you said earlier just to see where we stand on things. Let's take your protozoa in the dish going up the chemical gradient.

815
01:22:37,160 --> 01:22:41,160
As Larry pointed out, you can have a, sorry, Ken, sorry.

816
01:22:41,160 --> 01:22:44,160
Ken pointed out, you can, I called you Ken earlier though.

817
01:22:44,160 --> 01:22:45,160
You did.

818
01:22:45,160 --> 01:22:46,160
I'm two for three.

819
01:22:46,160 --> 01:22:55,160
But, but, but, but, I just can point out earlier, you can, you can write a complicated differential equation that has every atom, neuron, whatever in that worm.

820
01:22:55,160 --> 01:22:58,160
And your model will climb up the chemical gradient.

821
01:22:58,160 --> 01:23:04,160
And you can point to every little bit in your model and explain in a particular way why that thing is climbing the chemical gradient.

822
01:23:04,160 --> 01:23:07,160
But would you ever understand how it feels?

823
01:23:07,160 --> 01:23:09,160
I don't think it feels any feels.

824
01:23:09,160 --> 01:23:12,160
How it experiences climbing up the chemical gradient.

825
01:23:12,160 --> 01:23:13,160
And you say, no.

826
01:23:13,160 --> 01:23:14,160
That's what you say.

827
01:23:14,160 --> 01:23:23,160
Well, I will say no, and I will say that there are different languages because by the way, when you say that something's climbing up a chemical gradient, you're giving a teleological explanation, right?

828
01:23:23,160 --> 01:23:24,160
You're explaining, right?

829
01:23:24,160 --> 01:23:26,160
You're explaining something by the end, right?

830
01:23:26,160 --> 01:23:31,160
And classic Aristotelian, teleological model, which is not a scientific model.

831
01:23:31,160 --> 01:23:35,160
You can't describe something that you can't postulate a cause that happens later, right?

832
01:23:35,160 --> 01:23:38,160
That it's climbing up the thing in order to get sugar.

833
01:23:38,160 --> 01:23:40,160
That's a reason and can never be a mechanism.

834
01:23:40,160 --> 01:23:41,160
I mean philosophically.

835
01:23:41,160 --> 01:23:46,160
But you can certainly build a model of things that follow gradients and how they behave.

836
01:23:46,160 --> 01:23:49,160
And you can model it at that level.

837
01:23:49,160 --> 01:23:53,160
But what I describe it, when I describe it, it's doing this for this purpose.

838
01:23:53,160 --> 01:23:54,160
I've gone.

839
01:23:54,160 --> 01:23:55,160
It can still be mechanistic.

840
01:23:55,160 --> 01:24:01,160
So as internal states about the past histories, you can tell whether you're increasing or decreasing the gradient.

841
01:24:01,160 --> 01:24:10,160
And so even in this case, for subjective, affective states, people have done things within, again, the limited context of sequential learning and decision making and understanding.

842
01:24:10,160 --> 01:24:19,160
And so, when you're learning and decision making and reward, you can even say, how does your affective rating of happiness correlate with rewards and reward prediction errors, right?

843
01:24:19,160 --> 01:24:28,160
With humor, I think there is some work on seeing in simplified settings, can we show how humor relates to violations of predictions of expectations a little bit?

844
01:24:28,160 --> 01:24:30,160
It's really pretty inadequate.

845
01:24:30,160 --> 01:24:35,160
But what if instead of a program, it's just not going to speak to the subjective experience.

846
01:24:35,160 --> 01:24:41,160
But what are the kind of computations, right, at the level of inputs and outputs?

847
01:24:41,160 --> 01:24:42,160
That's right.

848
01:24:42,160 --> 01:24:47,160
You can relate these things to actual computations that guide behavior in a mathematical way, I would say.

849
01:24:47,160 --> 01:24:50,160
For humor, you can find some rules.

850
01:24:50,160 --> 01:24:55,160
But then to try to create humor using those rules is where it hits the story.

851
01:24:55,160 --> 01:25:00,160
There's a catastrophe where he modeled humor, which actually works pretty well.

852
01:25:00,160 --> 01:25:12,160
There's a disruption in fact that the theory is a while, in the argument, where pettiliology, and I think that is something that humans, we have reasons for doing things that so far have not been built into sort of computational models.

853
01:25:12,160 --> 01:25:14,160
Oh, no, no, you can model that, absolutely.

854
01:25:14,160 --> 01:25:22,160
You can model agents that have goals and have some relationship between their actions and their goals and their outcomes, and they learn from that.

855
01:25:22,160 --> 01:25:32,160
But in the context of evolution, too, what's selected?

856
01:25:32,160 --> 01:25:38,160
We also say that we have good evidence that there are times when we think we are motivated to do things, and that there's good evidence to suggest that we have told ourselves a story about wanting to do something that we were doing for a completely other reason.

857
01:25:38,160 --> 01:25:41,160
And habits that are not different.

858
01:25:41,160 --> 01:25:42,160
Anishia.

859
01:25:42,160 --> 01:25:45,160
So introduce yourself.

860
01:25:45,160 --> 01:25:46,160
What was that?

861
01:25:46,160 --> 01:25:55,160
Oh, no, I'm Anishia. I work with Cheryl. I'm a neurologist. So from my perspective, there has to be sort of an answer to all of this.

862
01:25:55,160 --> 01:26:02,160
So I guess my question is, you know, one of the limitations I see with models and computers is that they have to be created by humans, right?

863
01:26:02,160 --> 01:26:15,160
So is it conceivable that perhaps the reason some of their limitations are because we as humans are not smart enough, at least now, to really create these models and perhaps with evolution or with human learning, we could learn that we could be able to do things with humans.

864
01:26:15,160 --> 01:26:24,160
And then we could learn to create these models and computers that could account for more complex psychiatric functions.

865
01:26:24,160 --> 01:26:35,160
Well, from the point of view of the history of science, the history of science is kind of on your side, because there were all kinds of things that people said will never be explainable by science, including urea, right?

866
01:26:35,160 --> 01:26:41,160
Right, the production of urea in the human body was something that people thought couldn't be replicated.

867
01:26:41,160 --> 01:26:54,160
So the history of science is sort of pushes to that, but I would say that the human brain is the final frontier, and we don't know whether our historical experience with science being able to progress will be able to do that.

868
01:26:54,160 --> 01:27:02,160
So that would be a limitation of us in our ability to create the model as opposed to a limitation to models in general.

869
01:27:02,160 --> 01:27:09,160
Couldn't you argue that one of the ways of thinking about artificial intelligence is that it is teaching a machine to create new models?

870
01:27:09,160 --> 01:27:19,160
I could imagine, certainly, and maybe this is already happening, building a machine that builds new models that we ourselves as humans couldn't build.

871
01:27:19,160 --> 01:27:26,160
No, it has to appreciate art and tell jokes, and it has to be human. That's a directive.

872
01:27:26,160 --> 01:27:29,160
I wouldn't argue with that.

873
01:27:29,160 --> 01:27:34,160
There's some intrinsic limitation to how complicated a thing humans can create.

874
01:27:34,160 --> 01:27:44,160
It's just a matter of just the whole way science has been built up. You've got to understand some things, and then you build on that, and then you build on that, and then you build on that.

875
01:27:44,160 --> 01:27:49,160
We're ways from getting there, but I don't think there's any intrinsic limit to how far we can go.

876
01:27:49,160 --> 01:27:53,160
Humans are stupid, but humanity is smart.

877
01:27:53,160 --> 01:27:57,160
Humans can be stupid, but humanity is smart.

878
01:27:57,160 --> 01:28:04,160
That's a quote of a word.

879
01:28:04,160 --> 01:28:16,160
When you mentioned the word philosophy, and then before you wanted a definition of the word math, I thought of the confrontation between the analytical philosophers and the continental.

880
01:28:16,160 --> 01:28:29,160
The logical positivist, Witton Stein and Bertrand Russell thought math was the answer. Logic was the answer. Could this be a little bit of what we're dancing around here?

881
01:28:29,160 --> 01:28:38,160
Yeah, Fragan was in there. I would say, yeah, I would say that, and again, Andrew and I have had a number of conversations about this about what are some of the...

882
01:28:38,160 --> 01:28:48,160
What, in conversations like this, what are some of the fundamental things? And Russell had a famous quote about the difference between mind and matter, right?

883
01:28:48,160 --> 01:28:52,160
And he said, what is mind, not matter, what is matter, never mind.

884
01:28:52,160 --> 01:28:58,160
So the mind-body problem underlies all this.

885
01:28:58,160 --> 01:29:04,160
The use of folk psychological language to describe scientific psychology are problems like this.

886
01:29:04,160 --> 01:29:18,160
And I think that those go to the positivist, questions of the positivist about whether you could ultimately come up with a set of definitions and a set of non-falsifiable statements,

887
01:29:18,160 --> 01:29:21,160
whether you could do that or not.

888
01:29:21,160 --> 01:29:38,160
Science hasn't, while philosophy has moved past that and rejected that, I think working scientists have never fully, have never rejected the logical positivist position.

889
01:29:38,160 --> 01:29:53,160
And your question to me was implicit that, is this just kind of skirting around logical positivism as opposed to what else should we perhaps be bringing in and considering? That's the implicit part of your question.

890
01:29:53,160 --> 01:30:08,160
I think, what was the problem with the quote? If we can't, we can't really get to it.

891
01:30:08,160 --> 01:30:09,160
Yeah.

892
01:30:09,160 --> 01:30:13,160
And this will be our last question. Thank you. Please, not too long.

893
01:30:13,160 --> 01:30:27,160
This is seems to us to have gap, abysm between psychology and philosophy because it depends on where we turn our attention and our focus of attention.

894
01:30:27,160 --> 01:30:41,160
Because this, I think this gap is artificial because in order to, I'll give you some example, in order to aware that every experience requires awareness,

895
01:30:41,160 --> 01:30:44,160
requires consciousness, hidden in a way.

896
01:30:44,160 --> 01:30:55,160
For example, Freud and Jung built his theory about unconscious from the point of view, not of unconscious, from the point of view conscious.

897
01:30:55,160 --> 01:31:00,160
Therefore, did you see some animal to write book about biology? No.

898
01:31:00,160 --> 01:31:11,160
Men who is higher ontological creature write book about biology. I wish to only to say that they are not only the, say, I'm a psychiatrist, a baris

899
01:31:11,160 --> 01:31:13,160
not only our science.

900
01:31:13,160 --> 01:31:15,160
So we can ask a question so we can respond.

901
01:31:15,160 --> 01:31:19,160
Excuse me, sir. I didn't wrote so many books in order to only ask questions.

902
01:31:19,160 --> 01:31:29,160
I wish to say some missing great point of contemporary physics, especially in physics because computation upon physics.

903
01:31:29,160 --> 01:31:39,160
Still genes is the massive genes. He tried to deviate this general intention of thought that a universe is not computed.

904
01:31:39,160 --> 01:31:46,160
The universe is a mind because ever mathematics needs of mathematicians.

905
01:31:46,160 --> 01:31:50,160
What presents itself a mathematical equation without interpretation?

906
01:31:50,160 --> 01:31:53,160
Okay. This is not my question.

907
01:31:53,160 --> 01:31:57,160
My question, my team was to say something I think very important.

908
01:31:57,160 --> 01:32:11,160
Now we are aware that the whole science meets time, temporality, because see, every, all the computers and all the living systems process information in time.

909
01:32:11,160 --> 01:32:14,160
Time, substitute the, time is like work.

910
01:32:14,160 --> 01:32:16,160
We are not actually.

911
01:32:16,160 --> 01:32:20,160
So you don't want to listen to the, why sir?

912
01:32:20,160 --> 01:32:23,160
Sir, do you have some esteem?

913
01:32:23,160 --> 01:32:28,160
Yes. I have to stop now unfortunately.

914
01:32:28,160 --> 01:32:33,160
I did, I did, because the time went out. So I must. I'm sorry. I apologize.

915
01:32:33,160 --> 01:32:42,160
Do we have the panel responder or do you want to just, okay, then sit down and let's hear what the response we get, please.

916
01:32:42,160 --> 01:32:47,160
Sir, afterwards you'll be able to ask the panelists independently.

917
01:32:47,160 --> 01:32:51,160
I told you all the living systems information in time.

918
01:32:51,160 --> 01:32:53,160
Okay. Let me ask the question.

919
01:32:53,160 --> 01:32:58,160
The human brain is all the process time into the flow of information.

920
01:32:58,160 --> 01:32:59,160
Okay.

921
01:32:59,160 --> 01:33:01,160
You make something version. It's very important.

922
01:33:01,160 --> 01:33:05,160
Do you have now read to, to.

923
01:33:05,160 --> 01:33:07,160
Yes. I'll stop you now.

924
01:33:07,160 --> 01:33:09,160
It is actually an interesting point.

925
01:33:09,160 --> 01:33:14,160
If you have a response then they will not have knowledge and function.

926
01:33:14,160 --> 01:33:15,160
Okay.

927
01:33:15,160 --> 01:33:19,160
Because to be cautious that means double status.

928
01:33:19,160 --> 01:33:23,160
I'll say it's time. It's all time plus the 50 days.

929
01:33:23,160 --> 01:33:25,160
And the future takes them together.

930
01:33:25,160 --> 01:33:28,160
Okay. Now it happens that your comment, that one of the-

931
01:33:28,160 --> 01:33:30,160
My comment, concern about peak time.

932
01:33:30,160 --> 01:33:31,160
Thank you.

933
01:33:31,160 --> 01:33:32,160
Thank you.

934
01:33:32,160 --> 01:33:33,160
Thank you.

935
01:33:33,160 --> 01:33:39,160
Now one of the people that I, that we reference in the description of today's talk was on Ray Bergson,

936
01:33:39,160 --> 01:33:48,160
who did supply that similar sort of analysis to making a connection between the mathematics and, and subjective experience.

937
01:33:48,160 --> 01:33:51,160
So in that sense, you're in a good company.

938
01:33:51,160 --> 01:33:52,160
Thank you very much.

939
01:33:52,160 --> 01:33:58,160
And thank you everyone else here today for a really wonderful round table.

940
01:33:58,160 --> 01:33:59,160
Thank you.

941
01:33:59,160 --> 01:34:00,160
Thank you.

942
01:34:00,160 --> 01:34:01,160
Thank you.

943
01:34:01,160 --> 01:34:02,160
Thank you.

944
01:34:02,160 --> 01:34:03,160
Thank you.

945
01:34:03,160 --> 01:34:04,160
Thank you.

946
01:34:04,160 --> 01:34:05,160
Thank you.

947
01:34:05,160 --> 01:34:06,160
Thank you.

948
01:34:06,160 --> 01:34:07,160
It's a good opportunity.

949
01:34:07,160 --> 01:34:08,160
I see that.

950
01:34:08,160 --> 01:34:09,160
Yeah.

951
01:34:09,160 --> 01:34:10,160
Thank you.

952
01:34:10,160 --> 01:34:11,160
Thank you.

953
01:34:11,160 --> 01:34:12,160
Thank you.

954
01:34:12,160 --> 01:34:13,160
Thank you.

955
01:34:13,160 --> 01:34:14,160
Oh, thank you.

956
01:34:14,160 --> 01:34:15,160
Yeah.

957
01:34:15,160 --> 01:34:16,160
Thank you.

