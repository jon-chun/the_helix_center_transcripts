1
00:00:00,000 --> 00:00:29,960
Welcome to the Hillic Center for the Program on Mechanization of Matter.

2
00:00:30,000 --> 00:00:43,000
Before I introduce the participants today, let you know what we are planning for December and November and December.

3
00:00:43,000 --> 00:00:52,000
Harold Ann Spucker from Switzerland has proposed a program called Mathematics and many other realities.

4
00:00:52,000 --> 00:01:09,000
That will be on December 7th and on November 16th Alberto Manguel has proposed and is planning roundtable on emergence of empathy and countering the other through fiction.

5
00:01:09,000 --> 00:01:22,000
Today's roundtable was proposed by Michael Harris and I will introduce the participants.

6
00:01:22,000 --> 00:01:28,000
So please lift your hand so people know who you are.

7
00:01:28,000 --> 00:01:38,000
Stephanie Dick is Assistant Professor of History and Sociology of Science at the University of Pennsylvania.

8
00:01:38,000 --> 00:01:44,000
She received her PhD in History of Science from Harvard University in 2015.

9
00:01:44,000 --> 00:01:50,000
Was a junior fellow at Harvard Society of Fellows prior to joining the faculty at Penn.

10
00:01:50,000 --> 00:01:57,000
Her work sits at the intersection of mathematics and computing primarily in the 20th century United States.

11
00:01:57,000 --> 00:02:23,000
She is currently in the process of completing her first book, A History of Automated Mathematical Theory and Proving with an eye to how the concepts of mathematical reasoning and knowledge were fashioned in that field.

12
00:02:23,000 --> 00:02:35,000
Francesca Rossi is the AI global, AI ethics global leader and a distinguished research staff member at IBM research.

13
00:02:35,000 --> 00:02:46,000
Her research interests focus on constraint reasoning preferences multi agent systems computational social choice and collective decision making.

14
00:02:46,000 --> 00:02:55,000
On this topic she has published over 200 scientific articles and journals and conference proceedings and as book chapters.

15
00:02:55,000 --> 00:03:01,000
She's a AI and a European AI fellow.

16
00:03:01,000 --> 00:03:10,000
She has been president of IJCAA, AI and the editor in chief of JAI, JAR.

17
00:03:10,000 --> 00:03:23,000
She's the executive she's in the executive committee of the IEE global initiative on ethical considerations on the development of autonomous and intelligent systems.

18
00:03:23,000 --> 00:03:39,000
And she's a board member on the board of directors of partnership on AI where she represents IBM as one of the founding partners.

19
00:03:39,000 --> 00:03:59,000
Brandon Fiddlson is distinguished professor of philosophy at Northeastern University before teaching at Northeastern Brandon health teaching positions at Rutgers Berkeley, San Jose State Stanford and visiting positions at the Munich Center for mathematical philosophy at LMU Munich

20
00:03:59,000 --> 00:04:08,000
and the Institute for logic language and computation at the University of Amsterdam.

21
00:04:08,000 --> 00:04:14,000
Thomas Hales is the professor, the Mellon professor of mathematics at the University of Pittsburgh.

22
00:04:14,000 --> 00:04:26,000
He received a bachelor's of science and an MS degree from Stanford at three post part three from Cambridge University and a PhD from Princeton.

23
00:04:26,000 --> 00:04:39,000
He has post his health postdoctoral and faculty appointments at MSRI Harvard University University of Chicago Institute for advanced study and the University of Michigan.

24
00:04:39,000 --> 00:04:52,000
In 1998, with the help of his graduate student Samuel Ferguson proved Kepler 1611 conjecture on the most efficient way to stack oranges.

25
00:04:52,000 --> 00:05:03,000
In 2014 he has his coworkers gave a formal proof of the Kepler conjecture in the computer proof assistant, a whole light.

26
00:05:03,000 --> 00:05:17,000
He has received the chauvinette prize of the MAA, the more prize the robin's prize of the AMS the less their fourth prize of the MA and the falcerson prize of the MPS and AMS.

27
00:05:17,000 --> 00:05:21,000
He's an inaugural fellow of the AMS 2012.

28
00:05:21,000 --> 00:05:35,000
Michael Harris, who's been here before, is professor of mathematics at Columbia University and is on extended leave from University, the University of the Pareddi D'Hou, where he taught for 20 years.

29
00:05:35,000 --> 00:05:39,000
Before that he was a professor at Bandai's University.

30
00:05:39,000 --> 00:05:46,000
He obtained his PhD in 77 from Harvard University under the direction of Barry Mazur.

31
00:05:46,000 --> 00:06:06,000
He has organized or co-organized more than 20 conferences, workshop special programs in his field of number theory and until 2018 directed the European Research Council project arithmetic of automorphic motives at the Institute de Hôts et Fütze on Tefic outside Paris.

32
00:06:06,000 --> 00:06:17,000
He's been a visiting professor at Bethlehem University in Palestine and the National Academy of Sciences Exchange Scholar at the Stekloff Institute in Moscow.

33
00:06:17,000 --> 00:06:35,000
Projects he initiated or helped to initiate include the signs for the people, signs for the people Nicaragua project, the London Paris number theory seminar, the Paris book project and the trace formula and Shimura varieties,

34
00:06:35,000 --> 00:06:44,000
and the Association of University of Pareddi D'Hou, Pareddi D'Hou, in Paris.

35
00:06:44,000 --> 00:06:54,000
His books Mathematics Without Apologies won the 2016 Prose Award in Mathematics from the Association of American Publishers.

36
00:06:54,000 --> 00:06:59,000
There's more on all of them, but I will stop here. Thank you.

37
00:06:59,000 --> 00:07:08,000
You have a broad topic. Do you have any comments?

38
00:07:08,000 --> 00:07:19,000
Yeah, a few opening comments, probably more than need to be made, but I just wanted to just to set the atmosphere a little bit.

39
00:07:19,000 --> 00:07:30,000
In the 1920s, David Hilbert famously declared that no one could expel us from Tantor's paradise.

40
00:07:30,000 --> 00:07:38,000
He was referring to the set theory of Georg Contre, which is used as the basis for at least informally mathematics.

41
00:07:38,000 --> 00:07:53,000
These days, and this is the theme of today's roundtable, these days, mathematicians are facing according to some people, not expulsion from Kantor's paradise, but self-deportation,

42
00:07:53,000 --> 00:07:58,000
and not just from Kantor's paradise, but from mathematics altogether.

43
00:07:58,000 --> 00:08:12,000
Now, this is the Hollywood version of a real discussion that's going on among mathematicians, but there are people who have been arguing in this way.

44
00:08:12,000 --> 00:08:24,000
For example, Paul Cohen, according to Reuben Hirsch, who unfortunately couldn't be here, Paul Cohen claimed that at some point in the indefinite future,

45
00:08:24,000 --> 00:08:41,000
all mathematics would be done by computers. Reuben Hirsch said that enraged him, and that's around that time that he began writing books and articles that were at the beginning,

46
00:08:41,000 --> 00:08:56,000
what's now called the humanistic mathematics movement, which as his name suggests, implies as the belief that mathematics is something that human beings do.

47
00:08:56,000 --> 00:09:14,000
And so, I'm really sorry that Reuben could not come here. He was originally planning just a few weeks ago, a few months, maybe a month or so ago, he said he wouldn't be able to, and I was hoping to meet him finally after having read his books for so many years.

48
00:09:14,000 --> 00:09:34,000
I hope he's watching in New Mexico, and if he is, I hope he will recognize his influence in what I have to say. But to return to Paul Cohen's predictions, I want to suggest that the discussion not take certain directions.

49
00:09:34,000 --> 00:09:50,000
And so, usually when we talk about the role of computers and mathematics, it's framed by stark oppositions, whether it's some desirable, undesirable, whether it's possible or impossible, or always by the question, isn't really mathematics.

50
00:09:50,000 --> 00:10:04,000
And I'm interested in shifting the terms of the debate to questions that are more promising for philosophical consideration, particularly specifically, and I'm going to quote from the presentation of the round table.

51
00:10:04,000 --> 00:10:05,000
Once positioned, I'm going to read it.

52
00:10:05,000 --> 00:10:11,000
Once positioned on the future mechanization of proof is a function of one's view of mathematics itself.

53
00:10:11,000 --> 00:10:20,000
Is it a means to an end that can be achieved as well or better by a competent machine as by a human being? And if so, what is that end?

54
00:10:20,000 --> 00:10:24,000
And why are machines seen as more reliable than humans?

55
00:10:24,000 --> 00:10:27,000
Or is mathematics rather an end in itself?

56
00:10:27,000 --> 00:10:33,000
A human practice that is pursued for its intrinsic value, as humanistic mathematics would suggest.

57
00:10:33,000 --> 00:10:42,000
So, what could that value be? And can it ever be shared with machines?

58
00:10:42,000 --> 00:10:45,000
I guess that maybe a few ideas.

59
00:10:45,000 --> 00:10:54,000
Well, I think that to me, that or that you put in the two options, so what is mathematics?

60
00:10:54,000 --> 00:11:19,000
I want to say that is mutually exclusive. I think that it can be a means to an end, but it definitely is an end also because I see that by proposing or thinking about theorem statements and proving them.

61
00:11:19,000 --> 00:11:41,000
It's also a way to frame, you know, I mean, have a certain framework for our own life, you know, like, like, and so to me, it is also an end, but not just mathematics, also other sciences.

62
00:11:41,000 --> 00:11:50,000
People that work in a certain science, then they have a certain frame of mind, not just when they are proving the theorem, but in general.

63
00:11:50,000 --> 00:12:09,000
And I think that, you know, for example, the one of the frameworks given by mathematics is a framework of being rigorous, yes, but also being very creative, because, yeah, maybe,

64
00:12:09,000 --> 00:12:20,000
there is a way to mechanize some proofs, but then somebody has to propose some statements to be proved.

65
00:12:20,000 --> 00:12:32,000
And that is something very creative, in my view, and very inherent in our human capabilities.

66
00:12:32,000 --> 00:12:48,000
So it's not just that we could be, the machines are better than human beings in maybe proving once the statement is there, but sometimes when we, and I'm not a mathematician, I'm a computer scientist, but of course, I worked in theoretical computer science,

67
00:12:48,000 --> 00:13:05,000
so I've stated theorems and I proved theorems and so on. So it's not just the task of mechanizing the proof, which, but it's the whole environment of working,

68
00:13:05,000 --> 00:13:17,000
especially in my case, for example, I usually worked in my career with a team of people, and the whole interaction with these people, trying to come up with the right statement,

69
00:13:17,000 --> 00:13:28,000
and maybe then by discussing you discover that is not the statement that you really want, so it's not that there is a reality that you are trying to prove.

70
00:13:28,000 --> 00:13:40,000
The reality is not given to you, you have to create that virtual reality, and that's something very typical and very inherent to human beings, and I hope it remains to you.

71
00:13:40,000 --> 00:13:52,000
Having said that, of course, machines, like I always say in AI, with artificial intelligence, the goal is to help people do better whatever they need to do.

72
00:13:52,000 --> 00:14:06,000
So also in scientific discoveries and proofs and etc, there is a role for machines and mechanization and automation to be able to help people,

73
00:14:06,000 --> 00:14:21,000
but I don't see that, I don't know, you say you don't want to go into that direction, if it's possible or not, whatever, but my goal, not just in automatizing mathematics,

74
00:14:21,000 --> 00:14:38,000
but also in many other things, machines should help people so that people can devote their time and effort and thinking into what is inherent to human, and maybe leave some success to machines.

75
00:14:38,000 --> 00:14:54,000
I don't want to discourage people from talking about whether it's possible, I just don't want the discussion to fall into a trap of yes, no, it isn't.

76
00:14:54,000 --> 00:15:13,000
Okay, this is going to be a really random sounding place to start, but there's a book that's a classical text from the history technology called More Work for Mother, and it's about the irony of the way that their housekeeping technologies were sold to women in the middle of the 20th century,

77
00:15:13,000 --> 00:15:30,000
so this is going to save you so much time, this is going to make your job so much more efficient, machines are going to be so much better at doing all these things that you've been toiling away doing, and that was the story, and then it turned out that in fact, housewives had to do a whole bunch more work to maintain these

78
00:15:30,000 --> 00:15:46,000
things, and it's a really classic story about how the way that technologies are often packaged for their users is being these incredibly liberating devices, sometimes turns out not to be true in really interesting ways, and there's a great parallel, I think, to

79
00:15:46,000 --> 00:16:02,000
attempt to automate mathematics, and in particular, I'm thinking about the development of one of the first automated algebraic systems in the 1960s, the Maxima system that was developed at MIT, and unlike all of the numerical support calculating systems that came before it, this was meant to assist

80
00:16:02,000 --> 00:16:20,000
mathematicians in doing symbolic and algebraic and non-numeric work, so supposed to be able to multiply matrices and factorise and simplify and take on all of what was seen as this kind of menial labour that mathematicians were wasting so much of their time at, but using the system, especially in the beginning,

81
00:16:20,000 --> 00:16:37,000
turned out to be so profoundly difficult and frustrating, in part because mathematicians had to work within representational systems that might be incredibly unnatural, or that might not be well suited to the problems that they were trying to solve, and in sort of describing these different choices

82
00:16:37,000 --> 00:16:56,000
about representation, the designers of Maxima, in particular this manjal Moses who still at MIT, used political language to talk about the different choices, there were radical representation systems that forced everybody to make use of sort of one type of equation, like a polynomial or something,

83
00:16:56,000 --> 00:17:11,000
solve certain class of problems, there were Catholic systems which were more like whatever tools you need, you should be able to use them in your automated system, there were liberal systems, there were conservative systems, there were conservative systems

84
00:17:11,000 --> 00:17:34,000
that we probably shouldn't be automating at all, and I think I just wanted to pick up on this point to say that we are not the way that mathematics is done and the tools that are used to do it don't remain stable as we develop automated systems, people have to do a lot of work to discipline themselves in order to gain access to the kinds of freedom

85
00:17:34,000 --> 00:17:51,000
that are often sold in association with these automated systems, so the question might not be should we automate, what do we lose, can we automate, what do we gain, the question might also be how much work do people have to do to sort of discipline their own mode of thinking, to discipline their own approach to problem solving

86
00:17:51,000 --> 00:18:06,000
in order to make these automated systems useful, because often the sort of freedom and liberating potential comes at a very high cost of sort of disciplining your own practice and the way you think and the way you approach problem solving, and in the maximum case, the goal really was to make

87
00:18:06,000 --> 00:18:19,000
mathematicians into people who think about mathematical problem solving like computer programmers, and the reason the system was hard to use is because there's a real friction between those two ways of thinking and modes of doing, at least in the

88
00:18:19,000 --> 00:18:35,000
1960s, although the chasm might be closing and that's one of the things that we're seeing, so I think I just wanted to point out the freedom comes with lots of self discipline, and the tools you use to think about problem solving are part of what's at stake in this conversation, I think

89
00:18:35,000 --> 00:18:56,000
I guess that's your cue, because you know more than anybody here, and maybe anybody anywhere about just what habits have to be changed in order to switch from working in this familiar framework of mathematics to the framework of proof verification

90
00:18:56,000 --> 00:19:11,000
Yeah, I can talk about that, let me just preface it a little bit with a discussion of what some of the activities are that we do when we talk about doing a computer assisted proof.

91
00:19:11,000 --> 00:19:25,000
At the most elementary level, we can use a computer as a calculator as part of a proof to do simple calculations, or they're the computer algebra systems.

92
00:19:25,000 --> 00:19:29,000
That we use often as part of a proof.

93
00:19:29,000 --> 00:19:42,000
But I think one thing that we want to discuss today are formalized methods of doing mathematical proof.

94
00:19:42,000 --> 00:19:59,000
So there are really two different groups of products there, what we call ATP, or automated, they're improving, and with that, the computer really does all of the work, and there's very little human interaction.

95
00:19:59,000 --> 00:20:14,000
There might be some configuration by the user before the computer starts its work, but once the computer starts, it takes over and tries to do proof entirely on its own.

96
00:20:14,000 --> 00:20:35,000
And then there's what we call ITP, or interactive theorem proving, and in that case, the user and the computer are more or less in constant interaction, the user will type a line, hit return, and wait for the computer's feedback.

97
00:20:35,000 --> 00:20:55,000
And with many of these systems, you take all of the basic rules of logic, and you put them into the computer, and all the basic axioms of mathematics, and put it into the computer, and you really require the computer to check every single step of a proof.

98
00:20:55,000 --> 00:21:07,000
And so the interaction really depends on which of these products that you're using on the computer.

99
00:21:07,000 --> 00:21:24,000
There are really a lot of mathematicians these days who use computer algebra systems, and it's just part of the everyday interaction and research endeavor with something like

100
00:21:24,000 --> 00:21:48,000
these formal proof systems, they really have a much smaller group of users, and some of these systems can take up to a year to learn how to use proficiently, and the estimates might be something like

101
00:21:48,000 --> 00:21:59,000
a week of work to transform a single page of mathematics into a format that can be accepted by the computer.

102
00:21:59,000 --> 00:22:17,000
And in the case of these interactive theorem proving systems, it really takes an enormous amount of dedication and persistence to learn how to use these systems, and then to get the computers to accept

103
00:22:17,000 --> 00:22:25,000
the proofs on the other end.

104
00:22:25,000 --> 00:22:36,000
Well, I can speak as a mathematician who has never used any of the computer algebra systems, and the reason is that I've always found that I've tried a few times.

105
00:22:36,000 --> 00:22:55,000
Every time it occurred to me to do that, I found that by the time I had reframed my question in language that I could even type into this rather elementary computer technology, I would have solved my problem myself.

106
00:22:55,000 --> 00:23:13,000
The problem was, so then there would be no point in actually going through the next step. I have had worked with colleagues who are able to do that sort of thing, but I don't deal in really complicated calculations.

107
00:23:13,000 --> 00:23:28,000
And so if there's a conceptual question, I don't see how by reformulating it in a, well, I think the concept is the obstacle, finding the way to reformulate the concept is the obstacle.

108
00:23:28,000 --> 00:23:47,000
Now, I don't know whether that is a barrier to future integration of the more sophisticated technologies into mathematical practice.

109
00:23:47,000 --> 00:23:58,000
You say it takes a week of translating a single page. That means that you're analyzing the concept, you're breaking them down.

110
00:23:58,000 --> 00:24:09,000
In fact, you're actually doing all the work, but then you have somebody has programmed the computer to say, yes, you did it at the right.

111
00:24:09,000 --> 00:24:14,000
Yes, it's right. Your reinterpretation is correct.

112
00:24:14,000 --> 00:24:26,000
Can I jump, Mr. K, if I jump in? I think it just seems that it might be helpful that sort of behind a lot of the comments here is one philosopher used to call it the context of justification versus the context of discovery.

113
00:24:26,000 --> 00:24:35,000
So the automated theorem pervers are about discovering solving open problems. I've been using those tools for a long time to do that to solve open problems, and that's about discovery.

114
00:24:35,000 --> 00:24:48,000
So actually, most of my use of these tools has been to discover new mathematical results or new logical results. On the other hand, there's justification. There's things you already know or you think you know, and then you want a rigorous verification.

115
00:24:48,000 --> 00:24:54,000
I think that's a useful way to divide, because I think those are very different tasks. It's not just that the use of the systems is different.

116
00:24:54,000 --> 00:25:00,000
The goals are much different. I find it personally much more exciting to discover new things than to justify.

117
00:25:00,000 --> 00:25:07,000
And I say this is a logician, which is a little weird. I'm not super interested in the context of justification myself. I'm more interested in discovering new things.

118
00:25:07,000 --> 00:25:12,000
But you can do both with the technology. And I think it's partly because those tasks are very different.

119
00:25:12,000 --> 00:25:16,000
The goals are very different. The experience of using the things is different.

120
00:25:16,000 --> 00:25:22,000
I mean, to me, it's worth putting in the effort to learn all the different theorem-proofing things and all the different languages.

121
00:25:22,000 --> 00:25:30,000
If you can solve problems, which I've done with much people. And that to me makes it much more worthwhile.

122
00:25:30,000 --> 00:25:39,000
It'd be a lot harder for me to convince myself to motivate myself to just do the 200,000 line of code and the verification of the Paris Harrington theorem or something.

123
00:25:39,000 --> 00:25:47,000
I'm just speaking personally. But I think there are two different personalities, two of people who work on these things because of the discovery versus justification thing.

124
00:25:47,000 --> 00:25:52,000
Could you say something more about the discovery? Because this, of course, is what's most interesting to me.

125
00:25:52,000 --> 00:26:04,000
The context of justification is very much secondary. But you have to set the parameters of what you want to discover, of what kind of thing you want to discover.

126
00:26:04,000 --> 00:26:14,000
I would be surprised if, but I'm certainly willing to be surprised if the system actually discovered something that you were not expecting at all.

127
00:26:14,000 --> 00:26:17,000
Oh, that happens to me all the time when I used the tools.

128
00:26:17,000 --> 00:26:18,000
Yeah. So I mean...

129
00:26:18,000 --> 00:26:27,000
No, but I think you were referring to discovery as the activity of given a statement. Let's see whether it's true or not.

130
00:26:27,000 --> 00:26:31,000
But somebody has to put the statement there. Somebody has to write the statement, right?

131
00:26:31,000 --> 00:26:42,000
Well, that's one use. I mean, actually, so I was thinking in a slightly more general way. I don't know if this is appropriate, but I was thinking about the mechanization of scientific inference in general, not just deductive inferences like in mathematics, but also in doctorate.

132
00:26:42,000 --> 00:26:48,000
So inductive, this is what Francesca and the people who do machine learning work on. They're trying to automate. They're trying to mechanize inductive reasoning.

133
00:26:48,000 --> 00:26:55,000
If you thought deduction was hard, try to mechanize induction. I mean, that's much harder, much more difficult.

134
00:26:55,000 --> 00:27:03,000
No one really knows how induction works, right? So what does it even mean to mechanize? It's not even clear.

135
00:27:03,000 --> 00:27:11,000
So when I think about this as a philosopher of science, I tend to think of it much more generally. Not just about mathematics per se, but if you will also apply mathematics.

136
00:27:11,000 --> 00:27:20,000
So about automating, not just deductive inference, maybe, but also inductive inference. So I don't know if that's appropriate to this. I don't want to get off track.

137
00:27:20,000 --> 00:27:32,000
I think it's worth noting also that what it means to have established that a statement is correct or what it means to solve a problem or what it means to prove a theorem in mathematics is one of the moving targets in this history, right?

138
00:27:32,000 --> 00:27:44,000
So there's historical conundrum. Descartes figures out that you can use algebra to describe geometric problems and solve them in this way. And yet he never accepts an algebraic answer to a geometric question.

139
00:27:44,000 --> 00:27:51,000
For him, at the end, you always had to go and do the construction. You had to still do the Euclidean construction in the end.

140
00:27:51,000 --> 00:28:04,000
And that's because for Descartes, geometry was not about solving equations. It was about cultivating the right kind of internal knowing and understanding of what geometric figures are and how they operate.

141
00:28:04,000 --> 00:28:12,000
And this debate between analytic geometers who think algebra is obviously better because it generalizes. You can just follow some steps and get the answer right.

142
00:28:12,000 --> 00:28:21,000
Everybody can do it. You don't have to spend a decade of your life futzing about with these weird Euclidean constructions that, by the way, you could never explain how to know how to do them.

143
00:28:21,000 --> 00:28:33,000
Obviously, algebra is better. There were whole schools of mathematicians throughout Europe all through the 19th century who insisted that synthetic approaches Euclidean constructions were not just better.

144
00:28:33,000 --> 00:28:38,000
That was what mathematical knowledge consisted in, was knowing how to establish these things.

145
00:28:38,000 --> 00:28:49,000
Matt Jones, who's a historian of mathematics at Columbia, has done work on how for Leibniz, for Pascal, for Descartes, mathematical knowledge was about cultivating a certain kind of inner life, right?

146
00:28:49,000 --> 00:28:58,000
It was about being a better Christian, among other things. It wasn't just about churning out answers to problems or establishing that things are correct syntactically.

147
00:28:58,000 --> 00:29:07,000
And I think we're having a similar kind of conversation right now, where for some mathematicians, they don't just want a certificate that a statement is correct.

148
00:29:07,000 --> 00:29:21,000
They don't just want a black box that outputs certificates for theory. They want to understand why things are true and what they think that understanding consistent is the proper province of mathematical work.

149
00:29:21,000 --> 00:29:26,000
And so I think it's not just that there's the context of discovering the context of justification.

150
00:29:26,000 --> 00:29:33,000
It's what it even means to solve a problem in the first place is one of the things that is a moving target in this landscape of automation.

151
00:29:33,000 --> 00:29:40,000
Oh, yeah, absolutely. So, yeah, mathematical understanding. What is that? I'm not sure anyone knows, but philosophers talk about that.

152
00:29:40,000 --> 00:29:50,000
Oh, sure. Yeah. And, you know, I've actually been one of the things I've been interested in doing with some of these tools is trying to use them to get better explanations.

153
00:29:50,000 --> 00:29:56,000
Better explanations from a human perspective. And that can, they can be helpful for that. See, these are, to me, these are just tools.

154
00:29:56,000 --> 00:30:07,000
It depends how you use them. If you're clever about how you use them, you can use them to actually get better explanations than to actually not just have proofs, but to have explanatory proofs, more explanatory proofs from a human perspective.

155
00:30:07,000 --> 00:30:13,000
And therefore, to increase understanding, I mean, I want to understand. I'm a philosopher. So definitely I want to understand for sure.

156
00:30:13,000 --> 00:30:23,000
I think the tools can be useful for that, too. I know some mathematicians look at a lot of the inner, the ITP stuff maybe, and they say, oh, that's not conferring understanding.

157
00:30:23,000 --> 00:30:35,000
It's really just this tiny little step-by-step thing that goes along. I don't know. I think that can be debated, but I do think certainly the tools can be used to help confer understanding.

158
00:30:35,000 --> 00:30:41,000
If you're clever about how you use them, I mean, they're just like any other tools I feel, you know, they can be helpful for understanding.

159
00:30:41,000 --> 00:30:50,000
You can use them to help reverse engineer proofs that humans would like. That's something that I've been working on to try to do in real cases.

160
00:30:50,000 --> 00:30:59,000
So, yeah, absolutely. So ultimately, we want to understand. I didn't mean, I just meant that might be a useful distinction for part of that discussion.

161
00:30:59,000 --> 00:31:01,000
Certainly, it's only a small part of the story.

162
00:31:01,000 --> 00:31:14,000
Yeah, so to me, I mean, besides the discovery, besides the certification, besides the understanding, there is also the learning that happens in a human being while you state a statement over here.

163
00:31:14,000 --> 00:31:24,000
I mean, you try to prove it with various techniques that you gather from all your, you know, multi-year decades experience.

164
00:31:24,000 --> 00:31:43,000
And this process of learning is then reused to be creative for another statement and other proofs and other understanding of how things work in another part of this virtual reality that you are building.

165
00:31:43,000 --> 00:31:57,000
So to me, the learning process, it's very important in stating theorems and proving them and trying to understand whether they're false or true or so on.

166
00:31:57,000 --> 00:32:15,000
So, and that I find it difficult that it can be, you know, that that can be, and I'm not saying replace, but can be, the machine can be useful in that regard.

167
00:32:15,000 --> 00:32:22,000
But, but every said that I'm not familiar with all the various tools that are available right now, you know.

168
00:32:22,000 --> 00:32:35,000
But I want to say something about this effort in preparing the input to that tool. I mean, but yeah, I mean, it's very dynamic process.

169
00:32:35,000 --> 00:32:41,000
What happens in mathematics, but it's also very dynamic process with the technology.

170
00:32:41,000 --> 00:32:52,000
So I think that the more we go, we advance the technology and the more the technology can be actually adapting to us rather than the opposite.

171
00:32:52,000 --> 00:33:00,000
So I am hopeful that in the future, maybe this effort can be, you know, decreased.

172
00:33:00,000 --> 00:33:18,000
I want to get back to understanding, I would want to talk a little bit about the motivations or draw out your thoughts about the motivations for this, this, these developments in the first place, and understanding and what you were talking about

173
00:33:18,000 --> 00:33:31,000
are part of human life that we don't necessarily want to attribute that to anything else, or even if computers understand, and they don't understand in a human way.

174
00:33:31,000 --> 00:33:39,000
So a human understanding is part of human life. We don't have to define it. It's just something that it's a word that we use.

175
00:33:39,000 --> 00:33:51,000
And it's used routinely in talking about, in talking about a lecturer and talking about teaching and writing letters of recommendation.

176
00:33:51,000 --> 00:34:08,000
I'll get back to letters of recommendation a little bit. The kinds of words, the values that are privileged by mathematicians are easy to recognize because you just read a lot of letters of recommendation and you see which ones are positive

177
00:34:08,000 --> 00:34:16,000
and which ones are not. And they're all rather philosophically difficult to pin down.

178
00:34:16,000 --> 00:34:29,000
So understanding is certainly a motivation and to the extent that mechanization of mathematics can contribute to understanding, obviously, I'm not going to raise any objections.

179
00:34:29,000 --> 00:34:46,000
Now, historically, as I understand, and this Stephanie will correct me, mechanizing mathematics is one of the very first tasks that was opposed in the development of computers.

180
00:34:46,000 --> 00:35:03,000
I guess it was Herbert Simon who mentioned three milestones, or writing music, and I suppose been achieved playing chess and then proving a mathematical theory.

181
00:35:03,000 --> 00:35:09,000
But each of these was qualified in a certain way, so it was to be valuable, not trivial. So that's one source.

182
00:35:09,000 --> 00:35:19,000
It's a challenge to computer science. All right. This is not something that's necessarily internally of importance to mathematicians.

183
00:35:19,000 --> 00:35:27,000
Within mathematics, a lot of people have been paying more attention to this because they're concerned about mistakes.

184
00:35:27,000 --> 00:35:35,000
They're concerned that they have written complicated proofs and then they want to be sure they're correct.

185
00:35:35,000 --> 00:35:47,000
And some, there are two kinds of experiences. There's the experience of Vio Wotski who found many years after paper had been published that there was a mistake and this upset him.

186
00:35:47,000 --> 00:35:58,000
And then there's the experience of Tom Hales who was unable to get a human referee to confirm that what he had done was correct.

187
00:35:58,000 --> 00:36:18,000
And so those are two different kinds of experiences. And there's a third which has been raised by my colleague, Kevin Buzzard, which is that the way mathematics is published is based to a large extent on expert ascent.

188
00:36:18,000 --> 00:36:30,000
Even the referees are checking the proofs are going to be experts. Well, when these experts disappear, will anybody be able to reconstruct the validation?

189
00:36:30,000 --> 00:36:40,000
So he's been working and learning this and he's enjoying it. It's a lot of work, but he's enjoying it. So that's fine.

190
00:36:40,000 --> 00:36:59,000
Those are motivations. But understanding is a very different one. And so I may be, maybe Tom, you can't say whether you've understood a lot in formalizing.

191
00:36:59,000 --> 00:37:21,000
I can talk a little bit about understanding and about reliability of proofs. So I want my mathematical proofs to be understandable in the sense that they're surveyable, that I want to have some high level understanding of everything that's going on inside the proof.

192
00:37:21,000 --> 00:37:32,000
And if part of the proof uses an algorithm and I understand the algorithm, then generally I'm pretty happy to accept the output of the computer.

193
00:37:32,000 --> 00:37:41,000
And I can still consider the proof as surveyable if I know what the computer is doing in general terms.

194
00:37:41,000 --> 00:37:53,000
So I take a fairly broad view of what I mean by surveyable there. I also want my proofs to be reproducible.

195
00:37:53,000 --> 00:38:06,000
That means that 10 years from now, I want to be possible to still run the same computer code and get the same answer.

196
00:38:06,000 --> 00:38:17,000
This is a real problem in the software industry that there's a thing called a code rot. And it's very real that you write computer code.

197
00:38:17,000 --> 00:38:24,000
And 10 years later, the systems that support the software are no longer available or they're new versions.

198
00:38:24,000 --> 00:38:31,000
And you can no longer run the software. And for computer proof, this is a real problem.

199
00:38:31,000 --> 00:38:38,000
If it's not reproducible and if it has a very short shelf life.

200
00:38:38,000 --> 00:38:48,000
So something like Euclid has had, well, it's lasted through centuries.

201
00:38:48,000 --> 00:39:00,000
We have to really worry whether proof written in a particular system will still be around 50 years now.

202
00:39:00,000 --> 00:39:17,000
And on the other hand, Michael points out that mathematicians die and they often don't record the full knowledge of what it is that you need to know to reconstruct a proof.

203
00:39:17,000 --> 00:39:29,000
So there's a problem on the human side as well with reproducibility. People worry about classification of finite simple groups.

204
00:39:29,000 --> 00:39:36,000
It's an old crowd now. What will happen when those people are no longer around?

205
00:39:36,000 --> 00:39:49,000
I will be able to reconstruct everything that we need to know to have the classification.

206
00:39:49,000 --> 00:40:04,000
I also want Pus to be reliable. So as part of the formalization of the Kepler conjecture, we found hundreds of mistakes in the original paper proof that Sam Ferguson and I did.

207
00:40:04,000 --> 00:40:16,000
And I just have no question whatsoever that these formal methods are easily an order of magnitude more reliable than anything that humans can do.

208
00:40:16,000 --> 00:40:26,000
People have done very extensive tests in the software industry about error rates.

209
00:40:26,000 --> 00:40:39,000
And I think the number is that people writing computer code make on average 1.5 errors per line when they're first writing out computer code.

210
00:40:39,000 --> 00:40:48,000
And even by the time computer code gets to the market, there's maybe one error per every 100 lines of code.

211
00:40:48,000 --> 00:40:57,000
So I have three mathematical papers and my experience is that you find an error on every page.

212
00:40:57,000 --> 00:41:17,000
So these are very real issues. And I think that by developing the mechanization of mathematics, we can reduce those error rates to something more acceptable.

213
00:41:17,000 --> 00:41:21,000
And I think that's a very important motivation coming from outside.

214
00:41:21,000 --> 00:41:28,000
My understanding that everybody at this table is in favor of human understanding and in favor of human life and just the persistence.

215
00:41:28,000 --> 00:41:39,000
Because there's a question, there's also a trend and supposedly I haven't actually met people who think this way.

216
00:41:39,000 --> 00:41:47,000
I didn't go to the transhumanism panel, so I don't know very much about people.

217
00:41:47,000 --> 00:41:50,000
But that people are coming to the end of their shelf life.

218
00:41:50,000 --> 00:41:58,000
For whatever reason, we've exhausted the resources.

219
00:41:58,000 --> 00:42:03,000
We're no longer able to write reliable proofs or to understand them.

220
00:42:03,000 --> 00:42:09,000
So maybe we need to be replaced by something better.

221
00:42:09,000 --> 00:42:18,000
And there are actually, of course, we know some of the names of the people who are actually counting on that and collecting their billions.

222
00:42:18,000 --> 00:42:23,000
In the hope that they'll be part of the first wave.

223
00:42:23,000 --> 00:42:29,000
But much more down to Earth is the question of what can, but it overlaps with this.

224
00:42:29,000 --> 00:42:39,000
Why is there are other people in Google, for example, who are repeating what Paul Cohen said 40 years ago,

225
00:42:39,000 --> 00:42:49,000
that at some point in the not definite future, machines will be doing mathematics and people will not,

226
00:42:49,000 --> 00:42:52,000
like many other, like driving trucks and so on.

227
00:42:52,000 --> 00:42:56,000
All the other things that people do, machines will do better.

228
00:42:56,000 --> 00:43:04,000
That perspective is not represented at this table, but it is out there.

229
00:43:04,000 --> 00:43:12,000
And when articles are written in the press, financial times, or lost feet journal,

230
00:43:12,000 --> 00:43:16,000
that's the framework that, if they talk about mathematics at all,

231
00:43:16,000 --> 00:43:25,000
there'll be one of the many things that people do that will be better done in the future by some machines.

232
00:43:25,000 --> 00:43:30,000
And one of the advantages, of course, is that whoever owns the machines will be able to collect,

233
00:43:30,000 --> 00:43:33,000
will be able to monetize this.

234
00:43:33,000 --> 00:43:43,000
Mathematics as it stands, mathematical research, for the most part, does not profit anybody, except the people who do it.

235
00:43:43,000 --> 00:43:51,000
So, yeah, from my point of view, I mean, that hypothesis is really very, very far in the future.

236
00:43:51,000 --> 00:44:01,000
There are still a lot of challenges that need to be addressed to make machines more capable of having, like,

237
00:44:01,000 --> 00:44:09,000
a horizontal kind of intelligent, but doing it better than human beings.

238
00:44:09,000 --> 00:44:15,000
And, you know, right now, you know, the state of computer science and artificial intelligence,

239
00:44:15,000 --> 00:44:22,000
although it has a lot of applications and a lot of, you know, successful applications,

240
00:44:22,000 --> 00:44:24,000
but it's still very, very far.

241
00:44:24,000 --> 00:44:27,000
There's Ernie Davis is here, hi Ernie.

242
00:44:27,000 --> 00:44:35,000
He just published a book together with Gary Marfus telling us really that we are very far from that moment.

243
00:44:35,000 --> 00:44:40,000
We need to understand how to embed common sense into machines.

244
00:44:40,000 --> 00:44:45,000
We need machines to be able to deal with causality, causal information.

245
00:44:45,000 --> 00:44:53,000
They are not very good at doing that by now to understand very well correlations between data, but not causality.

246
00:44:53,000 --> 00:45:00,000
And so, already these two things are big challenges that many people are working on,

247
00:45:00,000 --> 00:45:09,000
but until we solve them, we don't really know how, you know, how we can make these intelligence or capabilities,

248
00:45:09,000 --> 00:45:12,000
intelligence, we don't even know what it means.

249
00:45:12,000 --> 00:45:18,000
Capabilities of machine-batch broader and horizontal rather than very specific and narrow as they are now.

250
00:45:18,000 --> 00:45:26,000
So, that aspect of machines, you know, being able to do everything better, but having said that, you know,

251
00:45:26,000 --> 00:45:31,000
machine can do better than a human being already now in a very specific thing.

252
00:45:31,000 --> 00:45:44,000
But again, mathematics and proving and discovering is to me a kind of task or collection of tasks that really requires

253
00:45:44,000 --> 00:45:54,000
a lot of analogies and memories and drawing from experience, getting from previous knowledge and adapting it and so on.

254
00:45:54,000 --> 00:46:02,000
So, it needs a lot of capabilities that a very narrow system does not have.

255
00:46:02,000 --> 00:46:07,000
So, that's, you know, we're really very far from that.

256
00:46:07,000 --> 00:46:14,000
But having said that, the other point that you made before, this one, the fact that there are some proofs that people write

257
00:46:14,000 --> 00:46:23,000
of certain statements or conjectures that almost nobody can check, you know, like even recently,

258
00:46:23,000 --> 00:46:33,000
there was another proof that P is equal to NP, which is one of the, you know, the main computer science open question.

259
00:46:33,000 --> 00:46:38,000
And I don't know of people that have been checking this.

260
00:46:38,000 --> 00:46:52,000
So, that definitely, you know, but again, being able to do that, it may require capabilities that right now we don't have in machines probably.

261
00:46:52,000 --> 00:47:02,000
So, but can one capability of the machine contribute to finding and adding, in other words, can the machine itself

262
00:47:02,000 --> 00:47:05,000
figure out how to have common sense?

263
00:47:05,000 --> 00:47:14,000
Well, we don't, that capability is not there in a machine yet.

264
00:47:14,000 --> 00:47:28,000
But again, one of the things that in specific fields you can do is to try to exploit the complementarity of machines and humans.

265
00:47:28,000 --> 00:47:35,000
Because machines know how to reason with causality and common sense and so human beings.

266
00:47:35,000 --> 00:47:49,000
Machines can do other things much better, so it's usually the most successful results are when you try to succeed in combining these capabilities that are very complementary.

267
00:47:49,000 --> 00:47:51,000
Yeah.

268
00:47:51,000 --> 00:47:58,000
You see nobody so far wants to say that there may yet be things that humans can do mathematically, that machines cannot,

269
00:47:58,000 --> 00:48:02,000
that no one's made that to the bold claims all further out there.

270
00:48:02,000 --> 00:48:12,000
But to me, to me again, doing mathematics or proving to you, whether it's about mathematics areas or computer science, you know,

271
00:48:12,000 --> 00:48:19,000
it's something that is not just proving that to you and that statement that somebody gave me.

272
00:48:19,000 --> 00:48:25,000
It has, it needs a lot of analogies, common sense, social interaction.

273
00:48:25,000 --> 00:48:33,000
Most of my best work are done together with other people, you know, ideas come from one person and another one in the team.

274
00:48:33,000 --> 00:48:36,000
So, and that is typically human.

275
00:48:36,000 --> 00:48:40,000
And that's part of being a mathematician too.

276
00:48:40,000 --> 00:48:41,000
Right, right.

277
00:48:41,000 --> 00:48:45,000
So that part, the machines are just disqualified.

278
00:48:45,000 --> 00:48:53,000
Unless, unless we can't, unless they manage to pass themselves off as humans.

279
00:48:53,000 --> 00:49:03,000
But I think it's worth noting that even human faculties get redefined and experienced and manifested in new ways when we seek to automate.

280
00:49:03,000 --> 00:49:09,000
So, one of the first systems that I studied when I was working on this project was called the ORA, the Automated Reasoning Assistant.

281
00:49:09,000 --> 00:49:14,000
It was an interactive theorem prover that was developed at the Argonne National Laboratory starting in the 70s.

282
00:49:14,000 --> 00:49:16,000
And it was quite, it was quite powerful.

283
00:49:16,000 --> 00:49:22,000
It was one of the earliest systems to successfully help solve open problems about whether different axiom sets were independent.

284
00:49:22,000 --> 00:49:25,000
And minimal and stuff like that.

285
00:49:25,000 --> 00:49:34,000
And it was built by this team of people led by Larry Watts, who may be some of you who have encountered, he's quite a character, who really just believes that intuition, yeah.

286
00:49:34,000 --> 00:49:37,000
Intuition cannot be automated.

287
00:49:37,000 --> 00:49:41,000
Human intuition cannot be reduced to any set of rules whatsoever.

288
00:49:41,000 --> 00:49:49,000
If you want an automated system to participate in theorem proving, you're going to have to impart human intuitions to it for it to be useful at all.

289
00:49:49,000 --> 00:49:55,000
So, the system's really good at inference, let it do inference, and we'll guide its inference with intuition.

290
00:49:55,000 --> 00:49:57,000
That's the way that they set it up.

291
00:49:57,000 --> 00:50:09,000
But so ironically, in making human intuition useful and usable to a technical system, it had to be translated into terms that the computer could make use of.

292
00:50:09,000 --> 00:50:17,000
So, human intuition, this sort of ephemeral esoteric, eureka moment, mathematicians wake up in the middle of the night knowing how to solve a problem.

293
00:50:17,000 --> 00:50:23,000
They have ideas in the shower while they're washing the dishes, gets translated into a weighting mechanism.

294
00:50:23,000 --> 00:50:35,000
So, the user can impart at different moments in a proof run a weighting template that says things like, you know, prefer shorter clauses over longer ones, or prefer this logical operation over another.

295
00:50:35,000 --> 00:50:45,000
And so, this human intuition gets reduced to the weighting template, which is something quite different from what Larry Watts describes human intuition to be.

296
00:50:45,000 --> 00:50:58,000
So, even if what we're looking for is an interface where uniquely human capabilities are sort of put in conversation with the machine, we are still reimagining what our faculties are and translating them into the terms of the machine.

297
00:50:58,000 --> 00:51:10,000
And the flip side of the story was that Waus and his colleagues were extremely excited that working with a system like the ORA would help them develop new and otherwise impossible intuitions about a problem domain.

298
00:51:10,000 --> 00:51:19,000
But after reading all their work, it looks to me like what they developed intuitions about is actually the behavior of the theorem proving system and not intuitions about mathematics.

299
00:51:19,000 --> 00:51:26,000
Like, oh, I'm pretty sure that if we constrain the inference in this way, we make like addition more important than subtraction.

300
00:51:26,000 --> 00:51:29,000
That seems to work really well for getting the outputs that we want.

301
00:51:29,000 --> 00:51:39,000
And so, our own faculties are not stable as we develop these technologies to interact with, but we remake them, we translate them, we put them into the terms of the machine.

302
00:51:39,000 --> 00:51:51,000
We automate our own practice, we behave more machine like one of the great wonderful things about the history of technology is that Charles Babbage and Karl Marx inhabited the same 19th century London.

303
00:51:51,000 --> 00:51:54,000
I'm doing exactly the opposite project.

304
00:51:54,000 --> 00:52:01,000
And Marx saw Babbage presenting his, you know, different engine and his calculating machines in different contexts.

305
00:52:01,000 --> 00:52:09,000
And Marx's response was, why are you so quick to answer, premorpise your machine, talk about its memory, talk about its intelligence.

306
00:52:09,000 --> 00:52:21,000
At the same time as you were so quick to dehumanize people, namely the ones working in the factory, Babbage really famously wanted to do for mental labor, what factory automation had done for physical labor.

307
00:52:21,000 --> 00:52:30,000
And it just put him on the side of the machine as the one with the human faculties, whereas people just became cogs in the machine that was the factory.

308
00:52:30,000 --> 00:52:36,000
And I think we risk subjecting even our higher faculties like mathematical intuition to this automating impulse.

309
00:52:36,000 --> 00:52:51,000
When we imagine both that they can be developed by working with the machine, but also that we could translate them to be useful to the machine, this performs a kind of reduction that I think has, we have to pay attention to that, will we develop the tools that we want to work with.

310
00:52:51,000 --> 00:53:05,000
I just want to insert a quotation I'd like from William Burroughs on this, which is, this is what this is the sort of development to avoid from naked lunches.

311
00:53:05,000 --> 00:53:10,000
The junk merchant does not sell his product to the consumer, he sells the consumer to his product.

312
00:53:10,000 --> 00:53:15,000
He does not improve and simplify his merchandise, he degrades and simplifies the client.

313
00:53:15,000 --> 00:53:25,000
And I guess you can say that a lot of social media has managed to do that with human interactions of various kinds.

314
00:53:25,000 --> 00:53:38,000
One of my concerns is that mathematics, even though it has had many different forms in many different places at different times, I want to protect mathematics from that sort of development.

315
00:53:38,000 --> 00:53:45,000
Can I jump in on some of these stories, I worked at Argonne with Larry Wass.

316
00:53:45,000 --> 00:53:48,000
I'll read it again, yes.

317
00:53:48,000 --> 00:53:49,000
Oh, sorry.

318
00:53:49,000 --> 00:53:55,000
The junk merchant does not sell his product to the consumer, he sells the consumer to his product.

319
00:53:55,000 --> 00:54:01,000
He does not improve and simplify his merchandise, he degrades and simplifies the client.

320
00:54:01,000 --> 00:54:10,000
I was just hoping to follow up on Stephanie's story, so I worked at Argonne with Larry Wass. Larry Wass is a remarkable character, he was the first blind of PhD in Mathematics in the United States.

321
00:54:10,000 --> 00:54:15,000
I didn't know he was blind until three years into our collaboration because we talked on the phone.

322
00:54:15,000 --> 00:54:23,000
So the story about Larry is the story about many people who work on, I've met different kinds of people who work on mathematics, and when we talk about intuition, I don't think it's a univocal thing.

323
00:54:23,000 --> 00:54:34,000
So some people work more syntactically, just naturally, some people do mathematics in a very syntactical way, Larry's one of those people. He would sit at his braille terminal and look at these incredibly long formulas of proofs we were working on.

324
00:54:34,000 --> 00:54:39,000
And he just felt it, he just knew he just had this intuition for syntax.

325
00:54:39,000 --> 00:54:49,000
So just to do a little background about Larry, I think he really views his early statements as being vindicated because his intuitions really are syntactical.

326
00:54:49,000 --> 00:55:02,000
He's not the only one, I've met many mathematicians, just to throw this out there, I've met many mathematicians who have a much more syntactical, symbolic way of approaching mathematics, as opposed to, let's say, a more pictorial or other kinds of intuitive ways that may be more synthetic.

327
00:55:02,000 --> 00:55:04,000
I think that's worth getting out there.

328
00:55:04,000 --> 00:55:11,000
So yeah, I think Larry just views himself as having been vindicated, but he has peculiar intuitions, but so do many mathematicians.

329
00:55:11,000 --> 00:55:20,000
So, Karu Meredith, who was a magician, a Irish magician that worked with the Polish school, he had this unbelievable knack for doing axiomatic proofs.

330
00:55:20,000 --> 00:55:35,000
And Larry and I tried to get one of his proofs automatically, and it took 10 years, this was in the 90s, it took us 10 years to even get a proof of this result using theorem proving, and he could just do it in his head.

331
00:55:35,000 --> 00:55:40,000
So, when we talk about mathematicians in tuition, I think there's a variation in that.

332
00:55:40,000 --> 00:55:45,000
People approach humans, approach mathematics in very different ways, and it's important to keep in mind.

333
00:55:45,000 --> 00:55:49,000
And some people are just very syntactical, Larry, with his braille terminal as one of those guys.

334
00:55:49,000 --> 00:55:59,000
So just quickly, how can you communicate if you, to a syntact, how can you communicate a syntactic intuition to an audience?

335
00:55:59,000 --> 00:56:11,000
If it's the apprehension of the capacity to apprehend very, very long formulas and to interpret them, how can you communicate that if you're at a blackboard, for example?

336
00:56:11,000 --> 00:56:23,000
Oh, I'm not sure. I mean, I don't share that. I'm not like Larry. I tend to. I mean, I'm not like that at all myself, but I just point out that I've worked with people like Larry who are like that.

337
00:56:23,000 --> 00:56:29,000
And I don't really understand how a mind like that would work, but I know they're out there. And there are many of them.

338
00:56:29,000 --> 00:56:40,000
I mean, I think there are many mathematicians who are like this. So there's a presupposition that's sometimes in these discussions about the way humans think about mathematics, which may or may not be true for some people.

339
00:56:40,000 --> 00:56:45,000
It is. Some people are more syntactical. I wanted to just one little point about your question from before.

340
00:56:45,000 --> 00:56:53,000
I think even if the computers could take over, of course, they can't, but let's suppose they could in the following sense, make it analogous to chess.

341
00:56:53,000 --> 00:57:02,000
There are more people playing chess now than used to play it. I actually get more out of chess now, but it's totally, I mean, the computers are way better at than we are.

342
00:57:02,000 --> 00:57:10,000
That doesn't mean we're just going to stop doing it. In fact, I actually, as I said, I get more pleasure out of it now. I feel I have a deeper understanding of chess because of the computers.

343
00:57:10,000 --> 00:57:15,000
So this is the kind of thing I'm talking about, but that happens in mathematics too for me anyway.

344
00:57:15,000 --> 00:57:25,000
One thing about human understanding is that you might simply put it that when people understand something, they could say, now I know how to go on.

345
00:57:25,000 --> 00:57:32,000
Right? And that's something that seems to be so much intrinsic to human nature, maybe not machine nature.

346
00:57:32,000 --> 00:57:38,000
I wonder if that sort of makes sense, you all. How do you make sense of that?

347
00:57:38,000 --> 00:57:49,000
In the earliest text, the Egyptian and Babylonian texts, the end of an argument is C, now you see.

348
00:57:49,000 --> 00:57:56,000
I think that's very important. I think that's, you would not ask the computer whether it sees or not.

349
00:57:56,000 --> 00:58:06,000
It sees everything at the same time. It's not an understanding that unfolds in time.

350
00:58:06,000 --> 00:58:20,000
No, but I mean, the continues need to understand that I think you were referring to that is, you know, so in here, in human nature, you know, like to make sense of things.

351
00:58:20,000 --> 00:58:32,000
And so understand, create and understand continuously is what drives us and I think what drives mathematicians as well, to create new turems, improve them and then go on, as you say.

352
00:58:32,000 --> 00:59:00,000
And of course, I mean, that could be an objective function that can be put into a machine, but I mean, I'm not sure how it will get all the sub criteria that we have inside ourselves to really have that drive, you know, that makes us, you know, look for new turems, for new results, for new discoveries of other parts of the physical

353
00:59:00,000 --> 00:59:04,000
or virtual, you know, world around us.

354
00:59:04,000 --> 00:59:24,000
I've been said that, but however, to go back to games like chess or others, machines can be, I mean, in the case of chess, it was mostly computing power, you know, that allowed machines to be better than human beings.

355
00:59:24,000 --> 00:59:37,000
In the case of Go in 2016, it was not computing power because computer power alone could not have brought to make machines better than the best human beings playing Go.

356
00:59:37,000 --> 00:59:51,000
So, and in that case, it was a very clever combination of various techniques of machines also learning and reasoning and learning by playing against themselves and so on.

357
00:59:51,000 --> 01:00:14,000
And, you know, also being somewhat creative and surprising, because, for example, there is this famous move that the machine made that really shocked the human being playing against the machine and was instrumental in getting to the victory of that particular

358
01:00:14,000 --> 01:00:26,000
play. So, so there are aspects of into, not into which of creativity of surprise that machines can can achieve.

359
01:00:26,000 --> 01:00:41,000
But I wouldn't say like the kind of intuition. So, and the fact that they also in go the machine won against the human being doesn't mean that the machine has the kind as all the capabilities that that human being has.

360
01:00:41,000 --> 01:00:54,000
Sure, sure, but just to pick up on that, I mean, they're now using machine learning techniques in for automated theorem proving to instead of just maybe appealing to like WOSN intuitions about syntax, you just do a similar thing like you do with Go.

361
01:00:54,000 --> 01:01:06,000
I'd better apply to like a proof space, a search space for proofs. And, and you know, it's early days, but I think that's very exciting research that we should, we should really be excited about and we should be working supporting.

362
01:01:06,000 --> 01:01:19,000
So, I agree, this is still in the very early days. You know, we're still waiting for machine learning to prove its first big mathematical theorem that hasn't happened so far.

363
01:01:19,000 --> 01:01:31,000
And I think machine learning is still at a very early stage when it comes to understanding of mathematics, just to give one example.

364
01:01:31,000 --> 01:01:52,000
So, of course, computers can add its program into the computer. But suppose that the computer isn't given an algorithm to add numbers together, but we want the computer to learn how to add numbers and we give some big collection of data and we just say, okay, here's a machine learning project.

365
01:01:52,000 --> 01:02:03,000
Can you learn the algorithm for addition? As far as I understand neural networks are currently incapable of adding numbers together.

366
01:02:03,000 --> 01:02:13,000
And so the same with simple tasks like primality detection on small integers with small number of digits.

367
01:02:13,000 --> 01:02:30,000
And still machine learning is not capable of carrying out these relatively simple mathematical tasks. And so, you know, I think it's very easy right now to be swept away by all this happening in machine learning and artificial intelligence,

368
01:02:30,000 --> 01:02:53,000
because there really are some spectacular advances going on right now, but we really need to stay grounded in what is currently possible today's technology and realize that, you know, true mathematical understanding by computers of mathematics, maybe a few decades away.

369
01:02:53,000 --> 01:03:05,000
Yeah, and I think that because, you know, true mathematical understanding is not that different from my point of view, a true understanding of the world around us.

370
01:03:05,000 --> 01:03:20,000
So it is not like a subset of capabilities that you need. So, and to do that, as you say, machine learning is very spectacular successes and applications and especially in perception capabilities.

371
01:03:20,000 --> 01:03:37,000
But it's very primitive in giving capabilities of all kinds of machine. That's why I think that at this point, most of the researchers in AI are convinced that what you need is not to just focus just on machine learning,

372
01:03:37,000 --> 01:03:51,000
but to combine the learning and reasoning capabilities of the machine in a way that it's kind of similar to how we combine our perception, but also our logical reasoning capabilities.

373
01:03:51,000 --> 01:04:03,000
So, so more and more. And again, I want to cite Hermes book because that's the book that really advocates for that is called rebooting AI because it's really

374
01:04:03,000 --> 01:04:19,000
for many decades AI has been focused on the logical reasoning capabilities that could get to a certain point, but not not more than that because they were not working well on the perception abilities under, you know, interpreting text interpreting

375
01:04:19,000 --> 01:04:42,000
or vocal commands interpreting images and so on. Then people started using machine learning and they were so successful. Oh my God. So then we can do everything with machine learning. But then now we realize, well, maybe not. Maybe you need to, you know, combine the two kinds of main approaches, because otherwise you're not going to be able to do everything with just logical

376
01:04:42,000 --> 01:04:52,000
reasoning, but not even with just machine learning. So you really need both capabilities. Otherwise, you're going to stay very primitive in the two kinds of

377
01:04:52,000 --> 01:05:14,000
things. I get the impression that though you say things are a lot further into the future, but nobody seems to say it's just not going to happen. So just a question, but it may not be 50 years, it may be 200 years, but there's a sense when they say this that this is where we are going.

378
01:05:14,000 --> 01:05:31,000
Well, we actually know because we will have been transformed. Yeah. So by that process. And so we won't know whether what we were expecting it to be is what it is because it will be something else.

379
01:05:31,000 --> 01:05:44,000
Right. When you say it happens, we redefine it so that the machine can accomplish it. Right. That's how it goes every time. Now, does it.

380
01:05:44,000 --> 01:05:57,000
One of the dividing lines between mathematicians and computer scientists, you know, we share a lot. One difference is that mathematicians don't include time in the, in the as a criteria.

381
01:05:57,000 --> 01:06:07,000
So Tom and I both have worked in the Langlands program. The Langlands formulated his program. Oh, in the 1960s.

382
01:06:07,000 --> 01:06:20,000
And it's a program that's meant to take centuries. Although I think he would probably be happy to see it done now, but it's, but as things stand, it could very well take centuries.

383
01:06:20,000 --> 01:06:35,000
But nobody doubts that at this point that it's the ideas are going to be found to pull together, if at least if mathematics is continues to be practiced by human beings.

384
01:06:35,000 --> 01:06:43,000
It's not clear at all that that machines have it would have any interest in the Langlands program.

385
01:06:43,000 --> 01:07:02,000
If they go, they may have other priorities. They may, they, if mathematics is redefined to be the kind of of of of activity at which machines excel, then that may just be left left aside.

386
01:07:02,000 --> 01:07:21,000
So, so, so that's the, that's the, that's the, the more general question, you know, is whether the values of contemporary mathematics will can be trans can, can, can be implemented implemented is a terrible word.

387
01:07:21,000 --> 01:07:38,000
Can, can be shared with, with machines of any kind. And what would it take, what would the machine have to be like in order to share the contemporary values bearing in mind that the values of contemporary mathematics are not at all the same as the values of the mathematicians

388
01:07:38,000 --> 01:07:41,000
of the 18th century or the 19th century.

389
01:07:41,000 --> 01:07:55,000
Well, it seems to be one of the clues that there's a little bit of a problem still is that we're relying on a lot of algorithmic complexity and the ability of computers to do a bazillion calculations in very little time.

390
01:07:55,000 --> 01:08:07,000
And, but doesn't that algorithmic complexity, like for example, that it seems so much complexity required just for a computer learn how to learn on its own how to perform mathematics or arithmetic.

391
01:08:07,000 --> 01:08:19,000
That, isn't that a clue that there's something very different about the way we think the computers that, that, that so much power required to get to the same.

392
01:08:19,000 --> 01:08:28,000
Yeah, I just find it a little strange. That's where this discussion is going. I just tend to think of these things as tools. And then the question is how to use the most optimally for human advancement.

393
01:08:28,000 --> 01:08:31,000
I mean, so that's the way I think I'm not. Are you thinking about computers?

394
01:08:31,000 --> 01:08:40,000
You don't think they're going to think about this tool. I just don't think about that. I'm just more interested in what are better currently. That's not the case.

395
01:08:40,000 --> 01:08:53,000
And, you know, there are tools and I think we should be focused on the kind of thing that Frances was talking about building the kinds of systems that our existing technology would be the best at advancing our interest in understanding mathematics and other areas of science.

396
01:08:53,000 --> 01:08:58,000
That's what I'm focused on. So I just don't think about these questions at all.

397
01:08:58,000 --> 01:09:12,000
Yeah, and we have to keep in mind that it's not that these machines are like an alien coming from another planet here. I mean, we designed them. We gave them the objective function, the criteria, the values, as you say.

398
01:09:12,000 --> 01:09:29,000
So the point is that it's not that clear how to define our own values and then to model them so that you can code them into a machine or have the machine learn them, but or a combination of them. But I think we designed them.

399
01:09:29,000 --> 01:09:47,000
So it's not that they wake up one day and they start having a completely, but I mean, said that, you know, they can do very surprising and maybe undesired things already now, you know, especially those that are based on statistics and probabilities.

400
01:09:47,000 --> 01:10:05,000
So they're not deterministic. You're not sure exactly what they will do. They will have, they may have some surprising, you know, and undesired results, which in some high stake, the main that may be, you know, even harmful.

401
01:10:05,000 --> 01:10:19,000
Okay. I'm a little bit troubled by what looks to me like a kind of epistemological collapse, we might call it as the machine learning mode of thought sort of eats the world.

402
01:10:19,000 --> 01:10:28,000
What machine learning systems are really good at doing is taking in a bunch of unstructured data and outputting classifiers or prediction rules for certain kinds of phenomena.

403
01:10:28,000 --> 01:10:37,000
And we think that with a tool like that, we can be better at sentencing criminals, we can be better at figuring out what kinds of people are going to graduate from college.

404
01:10:37,000 --> 01:10:41,000
We're going to be able to prove mathematical theorems, we're going to be able to make better medical diagnoses.

405
01:10:41,000 --> 01:10:50,000
And I'm not sure that prediction rules or classifiers in the way that neural networks can produce them is actually the kind of knowledge that we want to be seeking in every domain.

406
01:10:50,000 --> 01:11:03,000
And as soon as there's a critique of machine learning from the outside, it's demonstrated by ProPublica that there's demonstrable racial bias in the error rates of the Compass Risk Assessment score that's being used in almost every state in the country.

407
01:11:03,000 --> 01:11:11,000
And their response is, oh, we just need to figure out how to encode what we mean by fairness and equality in machine learning terms.

408
01:11:11,000 --> 01:11:19,000
We then need to figure out how do we encode these longstanding ethical conundrums about self-driving cars, should they protect the consumer or the pedestrian or whatever.

409
01:11:19,000 --> 01:11:22,000
We just need to encode those in machine learning terms.

410
01:11:22,000 --> 01:11:31,000
And I'm just not sure I buy the idea that all of our values can in fact be translated into mathematical formalism.

411
01:11:31,000 --> 01:11:37,000
And it's an ethical question, but because I'm a historian of science, to me it's also an epistemological question.

412
01:11:37,000 --> 01:11:45,000
The only lesson from the history of science is that there is more than one way to know there are syntactical forms of intuition.

413
01:11:45,000 --> 01:11:58,000
There's this beautiful story recovered by Lorraine Dastin, who's a historian of mathematics, about how in the 17th century calculation is really held up as sort of a sign of mathematical genius.

414
01:11:58,000 --> 01:12:04,000
The ability to work in your mind quickly with numbers makes you a god among men.

415
01:12:04,000 --> 01:12:09,000
But by the 19th century, calculation has been relegated to the realm of the merely mechanical.

416
01:12:09,000 --> 01:12:22,000
It becomes women's work, it becomes the proper position for African Americans, where people aren't educated, for people aren't imaginative and creative, and the position of human computer becomes the realm of calculation.

417
01:12:22,000 --> 01:12:31,000
And part of what happens in that transition is the advent of the calculating machine that makes clear that if the machine can do it, it must not belong to genius.

418
01:12:31,000 --> 01:12:48,000
And so we are closing off certain forms of knowing or certain epistemological possibilities or certain value systems for knowledge or for doing by collapsing everything onto the terms that were currently in a position to translate into our technical systems.

419
01:12:48,000 --> 01:12:54,000
We're doing it in science, we're doing it in mathematics, we're doing it in the law, we're doing it everywhere, and that's what troubles me.

420
01:12:54,000 --> 01:13:01,000
I mean of course to me, let's do what we can do with the tools that we have, but maybe let's not.

421
01:13:01,000 --> 01:13:11,000
Maybe these aren't the right tools for doing certain kinds of work in the world because we don't know how to translate our values into these systems or because they cannot be so translated.

422
01:13:11,000 --> 01:13:17,000
Except the optimism that that's possible is rampant and problematic and the stakes are very high.

423
01:13:17,000 --> 01:13:27,000
It's seen as a future source of profit, which may be deductive reasoning, not so much.

424
01:13:27,000 --> 01:13:34,000
Yeah, I was just thinking of a very limited set of tools that are applied to maybe just doing a little bit better in mathematics.

425
01:13:34,000 --> 01:13:36,000
I mean absolutely ethical AI.

426
01:13:36,000 --> 01:13:38,000
I got to put an plug for my wife Tina, Halle Aasirad.

427
01:13:38,000 --> 01:13:40,000
That's her project, the Adjust Machine Learning.

428
01:13:40,000 --> 01:13:48,000
So I'm all 100% behind that for sure. I was thinking of something much more limited to myself, just about scientific discovery.

429
01:13:48,000 --> 01:13:57,000
But of course, that's a really good point that you raised because can you just restrict yourself to when you say you're just restricting myself to scientific discovery?

430
01:13:57,000 --> 01:14:00,000
Maybe you can't really do that, maybe that's an illusion too.

431
01:14:00,000 --> 01:14:09,000
Even the idea that you can just restrict it to that because inevitably they're going to be other effects and other uses of, let's say off-label uses of whatever technology you're doing.

432
01:14:09,000 --> 01:14:13,000
Whatever technology you might develop, that's certainly true.

433
01:14:13,000 --> 01:14:18,000
It'd be interesting if that were to happen with the kind of theorem for things.

434
01:14:18,000 --> 01:14:21,000
It probably will. I mean it's happened with everything else.

435
01:14:21,000 --> 01:14:27,000
You both mentioned, you both referred to statistical methods.

436
01:14:27,000 --> 01:14:32,000
And I can imagine, this is not a fantasy scenario.

437
01:14:32,000 --> 01:14:45,000
I can imagine developing on the basis of interaction with machine learning technology that mathematics would develop in a direction that privileges statistical deduction.

438
01:14:45,000 --> 01:14:49,000
So inductive reasoning over proofs.

439
01:14:49,000 --> 01:14:58,000
There are people who have talked like that and they've been considered provocateurs and outsiders.

440
01:14:58,000 --> 01:15:03,000
But that's a possible direction.

441
01:15:03,000 --> 01:15:08,000
It's not inconceivable in view of how mathematics has changed since the 18th century, for example.

442
01:15:08,000 --> 01:15:12,000
What's considered important, what's considered valuable?

443
01:15:12,000 --> 01:15:15,000
Yeah, it's much more empirical now with all simulations.

444
01:15:15,000 --> 01:15:21,000
And so computers are used in so many ways in mathematics simulation and not just in probabilistic proofs, like in the case of primality.

445
01:15:21,000 --> 01:15:23,000
I think those confer knowledge.

446
01:15:23,000 --> 01:15:26,000
I think I can know that something's prime based on probabilistic proof.

447
01:15:26,000 --> 01:15:28,000
I'm 100% behind that.

448
01:15:28,000 --> 01:15:32,000
So yeah, to me, this goes back to the point, it's about mathematical knowledge, generally.

449
01:15:32,000 --> 01:15:34,000
It's not just about mathematical knowledge, it's not just about mathematical knowledge, it's some narrow sense.

450
01:15:34,000 --> 01:15:41,000
And this is where it becomes, you're right, it's going to rub up against all kinds of important ethical issues because what really matters is scientific knowledge generally.

451
01:15:41,000 --> 01:15:43,000
And that touches on everything I think.

452
01:15:43,000 --> 01:15:46,000
I could give a Bayesian proof of the Langlands program right now.

453
01:15:46,000 --> 01:15:55,000
I mean it's, you know, it's so, it's so, all of these coincidences are so unlikely that it has to be true.

454
01:15:55,000 --> 01:15:56,000
It's just that.

455
01:15:56,000 --> 01:16:03,000
But I think probabilistic proofs are a little more secure than that, but we can, I mean, I don't want to get to in the woods, but in the mind, that's not fair.

456
01:16:07,000 --> 01:16:10,000
You, I think alluded to someone like Elon Musk before.

457
01:16:10,000 --> 01:16:22,000
So if Max Tegmark or Elon Musk were sitting here, what would they say to what you are saying about AI and what will happen?

458
01:16:22,000 --> 01:16:51,000
About what part of AI, I think that Max, you know, understands that, I mean, he wrote a book on AI that he explained his point of view, but I think that he is very, I mean, he's focused, they spend some of his time, you know, to focus on these artificial general, intelligent idea, which means, you know, when

459
01:16:51,000 --> 01:17:11,000
machines can be, you know, with the same capability, even better than human beings, but he's also very focused on concerns about current AI, and also about other concerns, like nuclear and other things, bio, you know, and so on.

460
01:17:11,000 --> 01:17:31,000
So, so to me, I see, Max has very, very constructive person that even in his book, you may have seen that he has a table where he demystifies a lot of myths about this idea of the artificial general intelligence.

461
01:17:31,000 --> 01:17:54,000
So, of course, he has this idea that this can happen, but again, I'm not sure exactly what it means that this, what is this that can happen because unless it happens today, which is not the case, but if it happens like in 100 years, it will not be what we imagine now, because the whole society and people, infrastructure,

462
01:17:54,000 --> 01:18:05,000
and everything will be changed. So, it's not that, you know, we remain static, and then, and then someday we wake up and there is this super intelligence.

463
01:18:05,000 --> 01:18:21,000
So, so he has this idea that, yes, maybe it's very improbable, but even if the probability is very small, we should still, you know, worry about it and think about it, because it's like a cosmologist.

464
01:18:21,000 --> 01:18:41,000
So, he always makes this analogy with the asteroid that, you know, maybe it's very, very probable that in 100 years, the asteroid will come and destroy Earth, but, you know, if there is some probability, then we should start now thinking about it.

465
01:18:41,000 --> 01:18:50,000
So, he always makes that. And then another thing that he always says that, I think he would say here as well, because I haven't seen any talk when he didn't say that.

466
01:18:50,000 --> 01:19:02,000
So, it's this idea that as the capabilities and intelligence capabilities of AI grow, we need to make also our wisdom grow.

467
01:19:02,000 --> 01:19:19,000
And so to compensate and to make sure that we build a system of wisdom and trust and to compensate, you know, to be in parallel with augmenting the capabilities of AI.

468
01:19:19,000 --> 01:19:28,000
Yeah, and so I just want to plug philosophy is important for that. And we should work with it. Seriously, we should be able to cross many disciplines.

469
01:19:28,000 --> 01:19:32,000
It's including philosophy, not just ethics though, but philosophy more generally.

470
01:19:32,000 --> 01:19:42,000
And I think this is going to require, because the technology is so powerful and far-reaching, it's going to require vast interdisciplinary projects, I think, to really, press to really get a handle on it.

471
01:19:42,000 --> 01:19:45,000
And I think that's a good challenge. I think that's a good thing.

472
01:19:45,000 --> 01:19:49,000
Okay, we can go to questions.

473
01:19:49,000 --> 01:19:55,000
Sure.

474
01:19:55,000 --> 01:20:08,000
One of the things that smart people do, and I'm thinking about AI, that AI doesn't do, is smart people ask questions.

475
01:20:08,000 --> 01:20:28,000
And I'm, it's very curious that that didn't come up. You know, when you talk with other people, I think we can trust the questions they ask more than the statements that they make.

476
01:20:28,000 --> 01:20:44,000
I mean, for one thing, they cut very deeply and tell you a great deal about that person. So, and I think that's true of any phenomenon where thought is involved.

477
01:20:44,000 --> 01:20:57,000
Yeah, of course. And that's what I meant in some sense when I said, you know, like the process of even deciding which statement you want to prove is asking a question.

478
01:20:57,000 --> 01:21:09,000
And I'm saying, okay, I would like to understand whether this thing is true or not. So I'm, I'm identifying a question that I want to answer for.

479
01:21:09,000 --> 01:21:25,000
And that process, even the process of identifying that question is a very social process that comes maybe with the people of your team, but even if it's not, or from outside, from other papers, other, you know, talks or people.

480
01:21:25,000 --> 01:21:38,000
So, so it's really a very collective process to be able to ask interesting and questions that go in the direction of this continuous learning and understanding.

481
01:21:38,000 --> 01:21:45,000
And I agree that for now, I don't see that machines are into doing that.

482
01:21:45,000 --> 01:21:52,000
Right. Right. So when we're talking about understanding, there's formulating the questions. Then there's finding answers and then there's checking answers.

483
01:21:52,000 --> 01:22:00,000
So maybe there's really those three things. And discovering both the formulation of the question and the discovering the answer to the question once formulated.

484
01:22:00,000 --> 01:22:12,000
So I think that's a helpful. That's helpful. I would stress the fact that the questions that are asked within mathematics as it's practiced are rooted in the history of mathematics.

485
01:22:12,000 --> 01:22:24,000
It's very, very unusual that a completely new kind of question is raised. And then that that represents a turning point in the history.

486
01:22:24,000 --> 01:22:35,000
But one of the ways to distinguish between human mathematics and mechanical mathematics may be that the machines may very well want to ask different kinds of questions.

487
01:22:35,000 --> 01:22:47,000
They may want to ask the kinds of questions for which their capabilities, the capabilities they have now or 20 years from now, have prepared them.

488
01:22:47,000 --> 01:23:03,000
And that's again, is that something we want to force them to think the way we do or do we want mathematics to differentiate into a human kind of mathematics and a mechanical kind of mathematics with different.

489
01:23:03,000 --> 01:23:11,000
Machines, I wrote an article, one of the things I was trying to imagine what machines intuition would be based on.

490
01:23:11,000 --> 01:23:17,000
We'll be based on, for example, doing the same thing over and over and over again. People don't like to do that.

491
01:23:17,000 --> 01:23:24,000
Computers have been designed to do the same kind of thing over and over again. So that's a different kind of mathematics.

492
01:23:24,000 --> 01:23:36,000
So I've got sort of a broad question and I'm curious what anyone here would think about this. And sure all, mathematical experts, certainly more than I am.

493
01:23:36,000 --> 01:23:46,000
I had my last math class more than 40 years ago. I was an English major, but I'm a science writer and I actually have to write about mathematics every now and then.

494
01:23:46,000 --> 01:24:00,000
And it seems to me that one of the problems that you're addressing and trying to mechanize mathematics is that checking proofs, especially as time goes on, is getting harder and harder.

495
01:24:00,000 --> 01:24:07,000
And there's more and more specialization in mathematics. There are very few generalists out there anymore.

496
01:24:07,000 --> 01:24:20,000
And when it comes to something like the supposed proof of Fermets last theorem, there's a very small group of people in the world were qualified to determine whether it is, in fact, a proof.

497
01:24:20,000 --> 01:24:31,000
So I guess my question is, is mathematics, you already have to be sort of a special person to do mathematics.

498
01:24:31,000 --> 01:24:43,000
Mathematics outrunning our cognitive capacity. And is that one reason why we are forced to mechanize it to a certain extent?

499
01:24:43,000 --> 01:24:44,000
Thank you.

500
01:24:44,000 --> 01:24:52,000
Thank you for you.

501
01:24:52,000 --> 01:25:16,000
So the first thing I want to say is that proofs that are complicated in one century may not be complicated a century later because we continually revise and update our understanding and invent new concepts that make very difficult proofs easier to understand as time goes by.

502
01:25:16,000 --> 01:25:27,000
Another issue that was brought up was just how do we check proofs and the process of refereeing and how that relates to mechanization and mathematics.

503
01:25:27,000 --> 01:25:36,000
And I think it's fair to say that for many mathematicians, refereeing other people's work is at a very low priority.

504
01:25:36,000 --> 01:25:42,000
This is when we try to say what our values are. This is not one of our values.

505
01:25:42,000 --> 01:25:45,000
I'm not a general president.

506
01:25:45,000 --> 01:25:53,000
You know, we might want to understand the ideas in the paper, but we don't want to go through the tedious details of checking other people's work.

507
01:25:53,000 --> 01:26:07,000
And so when we look to the future, one thing that we might really want to invest in would be tools for better refereeing mathematics and to relieve mathematicians of that burden.

508
01:26:07,000 --> 01:26:15,000
We want to judge whether it's important or significant, but we don't want to check whether it's correct or not.

509
01:26:15,000 --> 01:26:30,000
Another, now that you say this thing, but it's not really related. You made me think about something that in my career I saw that was different between computer science, even theoretical computer science and mathematics.

510
01:26:30,000 --> 01:26:40,000
And in computer science, once you have a statement and somebody proved it and people are more or less convinced that that's correct proof, that's it.

511
01:26:40,000 --> 01:26:44,000
Nobody's going to prove it again. Nobody's going to give a different proof.

512
01:26:44,000 --> 01:26:53,000
In mathematics, that's not the case. I've seen several times the same statement and I don't mean an incredible...

513
01:26:53,000 --> 01:27:07,000
But the same statement with different proofs and new papers were published and peer reviewed and accepted just because they had a different proof of the same statement that already people knew that it was true.

514
01:27:07,000 --> 01:27:07,000
So to me, even more that shows that it is not just a means to an end, it's the end as well. Because again, writing a more elegant proof, meaning with less concepts, more

515
01:27:07,000 --> 01:27:17,000
It's a value by itself because it allows your mind to also learn more and they reuse that what you learn in other ways.

516
01:27:17,000 --> 01:27:46,000
So that's something that I remember even when I was much younger that I saw this, why is this guy really writing the proof of the...

517
01:27:46,000 --> 01:27:56,000
Another proof of the same theorem and that's not something that happens. At least I haven't seen usually happening in computer science.

518
01:27:56,000 --> 01:28:03,000
Let me just combine these two with respect to from our last theorem in particular because it's a good example.

519
01:28:03,000 --> 01:28:18,000
The theorem that it was proved and people have been working on it on the ideas ever since. It's not just the proof is not just a certificate.

520
01:28:18,000 --> 01:28:37,000
An independent object that stands by itself is an object for analysis and for discussion. It raises more questions, namely why does this proof give this result?

521
01:28:37,000 --> 01:28:52,000
I can understand and then there's the part in between why is this a root to this piece of the Langlands program, so to speak. And that has been studied by many, many people by hundreds, if not thousands of people.

522
01:28:52,000 --> 01:29:12,000
And so that part now can be said has already been simplified considerably and 100 years from now it's not at all, I mean if there are people doing mathematics and those are the priorities, then it's not at all impossible that it can be taught in an advanced undergraduate course.

523
01:29:12,000 --> 01:29:22,000
I think that's true of scientific understanding generally as we go as we evolve where we get better at explaining things simpler and more illuminating terms, not just in mathematics, that's generally the case.

524
01:29:22,000 --> 01:29:34,000
And one of my fears about both formalization and automation is that it captures really well only the last stage of what is otherwise like a very messy process.

525
01:29:34,000 --> 01:29:42,000
And historians of science don't believe that you can write the history of how knowledge is produced, if all you do is read published papers.

526
01:29:42,000 --> 01:29:57,000
Because if you don't go to the archive, if you don't see what people were uncertain about, if you don't read their messy notebooks and their correspondence and their failed grant applications, if you don't try to recover the actual practice with which they came up with what then was fashioned as a really clean final product,

527
01:29:57,000 --> 01:30:15,000
you don't actually understand what it is that scientists do at all. And formalization and with it automation seems to fix in place and standardize what we all know actually takes place with the friction between systems that are incompatible with questions you don't seem to have tools that can answer

528
01:30:15,000 --> 01:30:27,000
interpersonally, and that if we're so focused on formalization and automation, we might have closed down all of the avenues that open up in the mess that comes before, you know?

529
01:30:27,000 --> 01:30:35,000
That can happen, but you know, one of my favorite book by Larry Woss is his experimenters notebook, which is all about showing you what he did.

530
01:30:35,000 --> 01:30:57,000
So he's all about that. He's not just showing you a finished product, he's not just showing you a certificate, he's trying to explain to you, hey, I'm a practitioner, I use these tools, here's how one uses them, you can do it well, you can do it badly, let me tell you about some false starts, let me tell you about some dead hands, let me tell you about some success stories, all, you know, warts and all.

531
01:30:57,000 --> 01:31:07,000
That's my favorite book of Larry Woss, just to throw that in there. It's a very interesting conversation, so thank you. But I noticed no one mentioned the incompleteness theorem.

532
01:31:07,000 --> 01:31:17,000
So I was just wondering, Godot's theorem, what would happen if an automated theorem prover is given an undecidable question?

533
01:31:17,000 --> 01:31:32,000
I interviewed Michael Rabina a number of times when I was a graduate student who did some of the early work in theorizing how difficult problems are, and I asked him a version of this question, and he said, oh, you will run into the practical limitations of computing so much

534
01:31:32,000 --> 01:31:49,000
sooner than you will run into the limitations of formal systems that it barely matters, there are lots of uncomputable problems, but the practical limitations of polynomial running time algorithms are so much more constraining than the constraints of the limitations of formal

535
01:31:49,000 --> 01:31:54,000
systems are incomplete systems, that's my understanding anyways, that doesn't come up actually all that much.

536
01:31:54,000 --> 01:32:03,000
I mean, take systems for which there are decision procedures, they're usually intractable, so they're not really helpful for anything. The theory is totally decidable.

537
01:32:03,000 --> 01:32:12,000
So I'm not sure I got with you guys for saying this now, just saying we're in a smaller sphere already, and we're just constrained by technology and the...

538
01:32:12,000 --> 01:32:16,000
Right, even systems that are fully decidable are completely intractable.

539
01:32:16,000 --> 01:32:25,000
Because we haven't explored all the decidable ones yet. There are decidable ones, but the point is the algorithms are so complex for deciding the questions that there's not useful.

540
01:32:25,000 --> 01:32:36,000
But the under-undecidable or decidable ones, if that falls into that category, where humans feel quite confident they know the answer, or maybe not perfectly, not 100%.

541
01:32:36,000 --> 01:32:47,000
So the decidability is relative to a particular system, so just because something is undecidable from the point of view of a particular system doesn't mean there couldn't be another system that explains why it's true anyway.

542
01:32:47,000 --> 01:33:05,000
Chinese-American logician by the name of Hao Wang, who also said that everybody was so focused on the incompleteness theorem, they forgot that actually one of its corollaries is that it opened up all of this new interest in the decidable subsets of different domains, actually logicians who work in automation are really key to.

543
01:33:05,000 --> 01:33:08,000
So there was a closing down, but also an opening up.

544
01:33:08,000 --> 01:33:11,000
Okay.

545
01:33:11,000 --> 01:33:17,000
This is not really a question, but just some comments. So I'm a computer scientist like Francesca.

546
01:33:17,000 --> 01:33:26,000
And I think from the perspective of computer science, it is obvious that proofs can be mechanized.

547
01:33:26,000 --> 01:33:34,000
Another way of saying it is that if you are a believer in constructive logic, then constructive logic and computation are just different sizes.

548
01:33:34,000 --> 01:33:37,000
So from that point of view, it is obvious that you can mechanize.

549
01:33:37,000 --> 01:33:41,000
Then maybe the question is, you know, how good is the mechanization and so forth.

550
01:33:41,000 --> 01:33:46,000
But I wanted to also mention a couple of things which we don't know how to do.

551
01:33:46,000 --> 01:33:50,000
Okay, so like intuition. So what is mathematical intuition?

552
01:33:50,000 --> 01:33:53,000
I'm not sure whether we know how to formalize it.

553
01:33:53,000 --> 01:33:57,000
I'm not saying that it cannot be formalized, but I'm not sure whether this has been done.

554
01:33:57,000 --> 01:34:04,000
Sometimes you want to suddenly result, you ask, oh, you know, what's the real reason for this result?

555
01:34:04,000 --> 01:34:10,000
And for some complicated reasons, actually, you can ask the expert and they cannot tell you the reason.

556
01:34:10,000 --> 01:34:13,000
It is, oh, look at the proof.

557
01:34:13,000 --> 01:34:18,000
So you're going to have to read this in a 50 page proof or whatever, and you still may be not much better off.

558
01:34:18,000 --> 01:34:21,000
But in some other cases, you have some intuition.

559
01:34:21,000 --> 01:34:25,000
So we don't understand those things very well, explanations, all those things.

560
01:34:25,000 --> 01:34:33,000
It doesn't mean they cannot be done, but maybe we just don't know the techniques.

561
01:34:33,000 --> 01:34:41,000
Then on the previous question, well, I mean, it just depends on the axioms you put into the proof system.

562
01:34:41,000 --> 01:34:44,000
You put the right axioms, it's just like that.

563
01:34:44,000 --> 01:34:46,000
So no problem in some sense.

564
01:34:46,000 --> 01:34:49,000
It starts to not get to the point that you want to get.

565
01:34:49,000 --> 01:34:52,000
I mean, every axiom builds on other axioms.

566
01:34:52,000 --> 01:34:56,000
So depends on what you choose to believe.

567
01:34:56,000 --> 01:35:01,000
Things are not cast in stone.

568
01:35:01,000 --> 01:35:04,000
That many problems are just hot.

569
01:35:04,000 --> 01:35:07,000
And to put it in another way, many problems are just undecidable.

570
01:35:07,000 --> 01:35:12,000
And whether it's machine or human, it doesn't mean that we not do it any better.

571
01:35:12,000 --> 01:35:15,000
So maybe you want to have human machine cooperation.

572
01:35:15,000 --> 01:35:25,000
Just like in chess, okay, for, let's say, professional chess players, they always use the machine.

573
01:35:25,000 --> 01:35:28,000
Like, they don't like, oh, I do it on myself.

574
01:35:28,000 --> 01:35:32,000
No, even the win, you better use the machine.

575
01:35:32,000 --> 01:35:36,000
And the human is good for some things, and the machine is good for some other things.

576
01:35:36,000 --> 01:35:39,000
And there are, in some sense, at the moment, complementary.

577
01:35:39,000 --> 01:35:41,000
So that's just some...

578
01:35:41,000 --> 01:35:42,000
Can I jump in on the explanation point?

579
01:35:42,000 --> 01:35:45,000
So there's some really great work in philosophy of mathematics.

580
01:35:45,000 --> 01:35:47,000
Let me give a shout out to a couple people.

581
01:35:47,000 --> 01:35:53,000
Paolo Mancosu at Berkeley has done great historical and philosophical work on mathematical explanation

582
01:35:53,000 --> 01:35:56,000
of what makes one proof more explanatory than a Mark Steiner has written.

583
01:35:56,000 --> 01:35:58,000
Several really good books on that.

584
01:35:58,000 --> 01:36:02,000
So I encourage, if you're interested in that, there's some really good work in philosophy mathematics on that.

585
01:36:02,000 --> 01:36:06,000
And I think that gives some hope towards, if not formalizing it at least,

586
01:36:06,000 --> 01:36:09,000
discovering some systematic structure in the nature of mathematical explanation

587
01:36:09,000 --> 01:36:13,000
as we think we've done for parts of scientific explanation in the empirical sciences.

588
01:36:13,000 --> 01:36:15,000
But I think that...

589
01:36:15,000 --> 01:36:19,000
I think that's the reason for this thing.

590
01:36:19,000 --> 01:36:21,000
Sometimes...

591
01:36:21,000 --> 01:36:23,000
You know what can happen?

592
01:36:23,000 --> 01:36:35,000
It could be a whole branch of mathematics can develop around trying to explain why such and such a proof is effective.

593
01:36:35,000 --> 01:36:39,000
Oh yeah, I know I just meant there are people thinking about that stuff and I think that's what we ought to be doing.

594
01:36:39,000 --> 01:36:43,000
And then not necessarily trying to formalize it, but certainly trying to find some law-like structure in it

595
01:36:43,000 --> 01:36:45,000
because that's what science does.

596
01:36:45,000 --> 01:36:47,000
I want to make a couple of points.

597
01:36:47,000 --> 01:36:56,000
One is that there is an area of experimental mathematics, which has conferences and journals and so on.

598
01:36:56,000 --> 01:37:04,000
And they use mathematics to prove theorems and they've proved some nice, rather new gen-like identities and so on.

599
01:37:04,000 --> 01:37:10,000
You know, it works better in some areas, at least so far it has worked better in some.

600
01:37:10,000 --> 01:37:17,000
It works nicely for its real analysis of certain kinds, not so well with abstract algebra's A.

601
01:37:17,000 --> 01:37:32,000
And it would be interesting to see whether the technology of proof verification will lead to interesting mathematics in that kind of way.

602
01:37:32,000 --> 01:37:40,000
And the other was just to follow up on the emphasize point which Francesca raised, which is that my feeling is that we're not going to get

603
01:37:40,000 --> 01:37:48,000
machines that really understand mathematics until we understand until they grasp not just pure mathematics, but applied mathematics.

604
01:37:48,000 --> 01:37:53,000
They understand how the mathematical symbols relate to the realities of the world.

605
01:37:53,000 --> 01:37:57,000
Hmm.

606
01:37:57,000 --> 01:38:00,000
That all seems to be possible to me.

607
01:38:00,000 --> 01:38:04,000
I mean, I like the experimental math stuff myself. I find it really interesting.

608
01:38:04,000 --> 01:38:07,000
Some of it's a little weird, but which is cool.

609
01:38:07,000 --> 01:38:12,000
Like, Zileberger was my colleague at Rutgers and he does a lot of interesting stuff, also a lot of weird stuff.

610
01:38:12,000 --> 01:38:16,000
Stephen Wolfram has been championing experimental mathematics for many years.

611
01:38:16,000 --> 01:38:18,000
I think that's great.

612
01:38:18,000 --> 01:38:22,000
But to me, again, that's just an example of thinking of these things as tools and there's different ways they can be helpful.

613
01:38:22,000 --> 01:38:26,000
I would try to exploit all the different ways they can be helpful.

614
01:38:26,000 --> 01:38:28,000
No other questions?

615
01:38:28,000 --> 01:38:29,000
Okay.

616
01:38:29,000 --> 01:38:30,000
Thank you.

617
01:38:30,000 --> 01:38:31,000
Thank you.

618
01:38:31,000 --> 01:38:32,000
Thank you.

619
01:38:32,000 --> 01:38:33,000
Thank you.

620
01:38:33,000 --> 01:38:34,000
Thank you.

621
01:38:34,000 --> 01:38:37,000
That's definitely an interesting point.

622
01:38:37,000 --> 01:38:52,000
Yeah.

623
01:38:52,000 --> 01:39:07,000
Thank you.

