1
00:00:00,000 --> 00:00:04,000
I'm Ed Nersesian, the Director of the Center.

2
00:00:04,000 --> 00:00:08,000
Today's program, Mind Matters,

3
00:00:08,000 --> 00:00:12,000
Past, Present and Future, and the

4
00:00:12,000 --> 00:00:16,000
participants are, please raise your hand when I call your name.

5
00:00:16,000 --> 00:00:20,000
John Krakower, who is Professor of

6
00:00:20,000 --> 00:00:24,000
Neurology at Neuroscience at Johns Hopkins University.

7
00:00:24,000 --> 00:00:28,000
Jonathan Kramnick,

8
00:00:28,000 --> 00:00:32,000
who is Maynard Mack, Professor of English at Yale University.

9
00:00:32,000 --> 00:00:36,000
George McCarry, who is Director of the

10
00:00:36,000 --> 00:00:40,000
Wheat Wallace Institute for the History of Psychiatry and Professor at

11
00:00:40,000 --> 00:00:44,000
Wile Cornell, Ken Miller, Professor of Neuroscience at

12
00:00:44,000 --> 00:00:48,000
Columbia University, and Barbara Montero,

13
00:00:48,000 --> 00:00:52,000
Professor of Philosophy at the University of New York.

14
00:00:52,000 --> 00:00:56,000
They are aware that they just start spontaneously

15
00:00:56,000 --> 00:01:00,000
with a conversation and we'll see where they end up.

16
00:01:00,000 --> 00:01:04,000
Thank you. Sorry. Maybe we could just begin by saying something about the perspective that

17
00:01:04,000 --> 00:01:08,000
we bring to the question. One of the interesting and

18
00:01:08,000 --> 00:01:12,000
attractive features of this afternoon, for me at least, is that

19
00:01:12,000 --> 00:01:16,000
I am sitting with people who are bringing

20
00:01:16,000 --> 00:01:20,000
expertise from a variety of different disciplines

21
00:01:20,000 --> 00:01:24,000
and none of them my own. So I'd be curious to know

22
00:01:24,000 --> 00:01:28,000
simply how you all approach this issue, which is for me a literary

23
00:01:28,000 --> 00:01:32,000
and cultural question, though I'm always excited to

24
00:01:32,000 --> 00:01:36,000
think about questions of mind, consciousness, and mental life from

25
00:01:36,000 --> 00:01:40,000
an interdisciplinary perspective and bringing into account

26
00:01:40,000 --> 00:01:44,000
developments in neuroscience, cognitive science, and philosophy.

27
00:01:44,000 --> 00:01:48,000
So now I will pass the torch off to someone else.

28
00:01:48,000 --> 00:01:52,000
So, in understanding the mind, are you looking

29
00:01:52,000 --> 00:01:56,000
at literature? Are you looking at

30
00:01:56,000 --> 00:02:02,000
does this literary story reveal the nature of the mind?

31
00:02:02,000 --> 00:02:06,000
Or does the author of the literary story have insight?

32
00:02:06,000 --> 00:02:10,000
Or who's inside are you

33
00:02:10,000 --> 00:02:14,000
benefiting from in that project?

34
00:02:14,000 --> 00:02:18,000
Well, I'm probably the authors, but I kind of focus on questions

35
00:02:18,000 --> 00:02:22,000
of the representation of mental life and work to literature.

36
00:02:22,000 --> 00:02:26,000
Yeah. And how it's changed, I guess.

37
00:02:26,000 --> 00:02:29,000
How it's changed or how it took shape in a particular moment in time. And one of the

38
00:02:29,000 --> 00:02:33,000
exciting things about working in my particular historical field, which is the 18th

39
00:02:33,000 --> 00:02:37,000
century, is that at that moment in time,

40
00:02:37,000 --> 00:02:41,000
literary writers were in pretty close dialogue with scientists and philosophers.

41
00:02:41,000 --> 00:02:45,000
It's part of the emergence of empiricism and all kinds of work into

42
00:02:45,000 --> 00:02:49,000
the nature of cognition and the nature of representation, perception,

43
00:02:49,000 --> 00:02:53,000
and other kinds of questions in and around mental life that have

44
00:02:53,000 --> 00:02:57,000
posed problems that people are still interested in addressing.

45
00:02:57,000 --> 00:03:01,000
Sounds interesting.

46
00:03:01,000 --> 00:03:05,000
And so to me, that side of it is simple.

47
00:03:05,000 --> 00:03:09,000
I think how consciousness arises from me is not

48
00:03:09,000 --> 00:03:14,000
simple, and in fact I don't even see how you can approach that as a question.

49
00:03:14,000 --> 00:03:19,000
But it's obvious to me that this piece of meat that evolved is

50
00:03:19,000 --> 00:03:24,000
the basis of our mind.

51
00:03:24,000 --> 00:03:31,000
I guess I approached it from two vantage points, which is that I'm a working

52
00:03:31,000 --> 00:03:36,000
psychiatrist, and that does no doubt in form of some of my perspective.

53
00:03:36,000 --> 00:03:41,000
But my work has been in the history of ideas, so I would really take often away from

54
00:03:41,000 --> 00:03:46,000
your point of view and say one of the things that intrigued me and

55
00:03:46,000 --> 00:03:51,000
made me write my last book was the notion that we've been living with this problem

56
00:03:51,000 --> 00:03:57,000
for many, many years, with centuries, and that to think of it as an inside of a problem

57
00:03:57,000 --> 00:04:03,000
was fine, but not as interesting as a problem that in a way had its own history,

58
00:04:03,000 --> 00:04:07,000
had political history, had a scientific history, had a philosophic history,

59
00:04:07,000 --> 00:04:12,000
that impacted us as we couldn't solve it, and moved forward to different iterations

60
00:04:12,000 --> 00:04:14,000
of ways of thinking about it.

61
00:04:14,000 --> 00:04:18,000
So I really look at it in a way much like you do.

62
00:04:18,000 --> 00:04:22,000
I was interested in what happens once we take the, let's say, I don't think it's quite

63
00:04:22,000 --> 00:04:27,000
innocent to say it comes from the meat, but then all the difficult problems, as you say,

64
00:04:27,000 --> 00:04:29,000
come after the meat.

65
00:04:29,000 --> 00:04:31,000
I guess it comes with a dessert.

66
00:04:31,000 --> 00:04:33,000
We don't know what makes it dessert.

67
00:04:33,000 --> 00:04:38,000
I think it's a mad story and we've got to go through the lens of history of ideas.

68
00:04:38,000 --> 00:04:39,000
Sean, please.

69
00:04:39,000 --> 00:04:41,000
Oh, gosh.

70
00:04:41,000 --> 00:04:46,000
Yeah, I always try to think to myself how I feel being an audience, and I think topics

71
00:04:46,000 --> 00:04:53,000
like this are so amorphous and abstract that, so how does one get some

72
00:04:53,000 --> 00:05:04,000
concreteness on it?

73
00:05:04,000 --> 00:05:10,000
That's what people think of me by mind.

74
00:05:10,000 --> 00:05:15,000
I mean, it's a slightly human, self-absorbed idea that that's what's making us create

75
00:05:15,000 --> 00:05:18,000
all the specialties that we represent.

76
00:05:18,000 --> 00:05:24,000
And then that's what Ken just said, which is he's a neuroscientist.

77
00:05:24,000 --> 00:05:27,000
In neuroscience, we don't talk about the mind.

78
00:05:27,000 --> 00:05:29,000
We talk about the brain.

79
00:05:29,000 --> 00:05:33,000
So the mind is what psychologists and cognitive certain neuroscientists talk about,

80
00:05:33,000 --> 00:05:37,000
and it's what we all consider, as I said, the creator of all the things we care about,

81
00:05:37,000 --> 00:05:41,000
and then the neuroscientists talk about brain, and then we get this tedious thing.

82
00:05:41,000 --> 00:05:44,000
How are we going to bridge the brain and the mind?

83
00:05:44,000 --> 00:05:49,000
And the answer to that is, we're not.

84
00:05:49,000 --> 00:05:52,000
And what do I mean by that?

85
00:05:52,000 --> 00:05:56,000
It means that, and I'll give you a tiny little anecdote.

86
00:05:56,000 --> 00:05:57,000
Ken probably knows this.

87
00:05:57,000 --> 00:06:01,000
Génelia, does everyone know what Génelia research campus is?

88
00:06:01,000 --> 00:06:06,000
It's a big campus for neuroscience, and they've just decided that they're going to switch

89
00:06:06,000 --> 00:06:11,000
directions and do something they call mechanistic cognitive neuroscience.

90
00:06:11,000 --> 00:06:19,000
Meaning that they're going to spend 15 years to do the bridge between the brain of a fruit fly

91
00:06:19,000 --> 00:06:21,000
and cognition.

92
00:06:21,000 --> 00:06:30,000
So they're going to try and operationally define what could possibly be the bridge between mind and brain.

93
00:06:30,000 --> 00:06:36,000
And the way they're going to do it is they're going to have to come up with some operational definition of cognition.

94
00:06:36,000 --> 00:06:43,000
So I think we can have a fruitful discussion here that we agree that we're going to talk about cognition.

95
00:06:43,000 --> 00:06:48,000
Thought, if we will, conscious of that, that's it.

96
00:06:48,000 --> 00:06:49,000
Cognition.

97
00:06:49,000 --> 00:06:57,000
Now, depending on how you operation define cognition, you can decide that we are going to have a scientific theory of the mind.

98
00:06:57,000 --> 00:07:03,000
That is neuroscientific, not psychological, and just the last point I want to make is that it's extremely important

99
00:07:03,000 --> 00:07:10,000
to say that all the interesting work on the mind to date, in my view, has been psychological, not neuroscientific.

100
00:07:10,000 --> 00:07:19,000
And in philosophy, we talk about functionalist explanations versus reductionist mechanistic explanations.

101
00:07:19,000 --> 00:07:26,000
If you say to somebody, why is that woman crying there in the corner, and you say it's because she's sad,

102
00:07:26,000 --> 00:07:31,000
because she lost a loved one, that is a perfectly adequate explanation.

103
00:07:31,000 --> 00:07:40,000
To say it's because circuits in her head are misfiring, or dopamine levels are low, is not an adequate explanation.

104
00:07:40,000 --> 00:07:46,000
Now, anyone who says, oh, and you know, and they're molecules of the moment, oxytocin is very big at the moment,

105
00:07:46,000 --> 00:07:55,000
dopamine was really big, and these molecules get this totemic meaning that we think are bridging, and they're not.

106
00:07:55,000 --> 00:08:05,000
So as soon as we move away from causal bridges between the brain and the mind, and decide the kind of work we want to do,

107
00:08:05,000 --> 00:08:12,000
I think you have to be perfectly happy with saying the woman is sad, and never talk about dopamine.

108
00:08:12,000 --> 00:08:21,000
And any attempt to think that you'll explain away the notion of sadness by some reduced circuit or molecular-based exclamation

109
00:08:21,000 --> 00:08:26,000
is a philosophical aporia. It's a non-place.

110
00:08:26,000 --> 00:08:29,000
And that, I think, is where we're going to happen.

111
00:08:29,000 --> 00:08:34,000
And I think many people in the audience really hope that the neuroscientists will solve the mind.

112
00:08:34,000 --> 00:08:37,000
It's a non-starting.

113
00:08:37,000 --> 00:08:43,000
Okay. Well, it's interesting for me to hear different perspectives on this.

114
00:08:43,000 --> 00:08:51,000
My work is very interdisciplinary, so I take into consideration literature.

115
00:08:51,000 --> 00:08:57,000
That's why I was interested in how you, you know, think about it, because I've looked at Shakespeare and said,

116
00:08:57,000 --> 00:09:05,000
well, you know, how did he understand the mind, which is different how the characters sometimes seem to see the springs of action.

117
00:09:05,000 --> 00:09:14,000
And so, and I look at neuroscience and psychology and common sense.

118
00:09:14,000 --> 00:09:18,000
I've even conducted an experiment.

119
00:09:18,000 --> 00:09:21,000
And then my own case, too, introspection.

120
00:09:21,000 --> 00:09:27,000
I use that too, and I mix it all together and see what comes out.

121
00:09:27,000 --> 00:09:37,000
So, well, so one thing Ken said was, certainly the brain is the basis of the mind.

122
00:09:37,000 --> 00:09:47,000
And, you know, one sense who wants to say, yeah, but one question I'm interested in what it means to be the basis of something else.

123
00:09:47,000 --> 00:10:07,000
So, because it seems to me from different perspectives, you can see different, you could say in a different context, someone might say our socialization is the basis of the mind,

124
00:10:07,000 --> 00:10:16,000
because that's what makes us who we are, or you could, in a different context, our DNA, or so, yeah,

125
00:10:16,000 --> 00:10:34,000
so there's a, so one thing, so my work tends to, like, focus on some very, well, I have a variety of areas of research, but in my most, like strictly focusing on the mind and the mind body problem,

126
00:10:34,000 --> 00:10:48,000
focuses on the meaning of that, what can we mean when we're looking for the basis, and also the question of, well, what is that base?

127
00:10:48,000 --> 00:10:54,000
So, we all want to be somehow materialist or physicalist, but what does that mean?

128
00:10:54,000 --> 00:11:17,000
And somehow is there, like, I mean, we think about the problem of reducing the mind to the brain, but, you know, we haven't reduced chemistry either to physics, so what is there something special about the mind, and its relation to the lower level features that it somehow emerges out of, or is based on, or is maybe

129
00:11:17,000 --> 00:11:34,000
reduced, so as opposed to other, the other stratifications of nature, or is it just, do we have the same problems throughout, or are they just different perspectives we take on reality, so, those are some of my thoughts.

130
00:11:34,000 --> 00:11:50,000
It's just that you're not, the question why is this woman crying, it's not asked, this is a question of, like, what people are interested in when they pose a question, what kind of answer they want, right?

131
00:11:50,000 --> 00:12:05,000
So, if you're in a religious office, maybe if someone's crying all the time, nonstop, maybe you need to figure that out, so it might be depending on your content.

132
00:12:05,000 --> 00:12:23,000
It's annoying, it's annoying, what I'm saying is, is that, you know, it's philosophy, explanation, and understanding, and causality have different meanings. Physicists have understood notions of emergence and aggregate behavior and complexity for ages.

133
00:12:23,000 --> 00:12:42,000
Neuroscientists in particular, and I say this, have a kind of philosophical, sophomoric way of speaking, right, where, because they're sophisticated with their tools and their techniques, they think by inheritance, they're also intelligent about talking about these things, and they're not.

134
00:12:42,000 --> 00:13:08,000
Okay, and what I'm saying is that it's not, lots of people have dealt with these problems of levels of analysis, you know, as you said, physics, chemistry, biology, phase, transitions in science, complexity, I mean, you know, I think it gets down to you, you know, the book that we're writing, I think the title I'm using right now is, what does neuroscience want to know?

135
00:13:08,000 --> 00:13:13,000
And I think members of the audience are asking, why is that woman sad?

136
00:13:13,000 --> 00:13:18,000
But what the answer to sound like something like dopamine?

137
00:13:18,000 --> 00:13:34,000
And what I'm saying is, as long as you're in that track where you're asking a question that's never going to yield the kind of answer you feel like, it's like the number 42, I mentioned that in Hitchhiker's Guide to the Larrazie, what's the meaning of the universe?

138
00:13:34,000 --> 00:13:38,000
Well, that's my face, I want something that felt like my question.

139
00:13:38,000 --> 00:13:49,000
The problem is, is that we have a feeling when we ask our question and we demand that the answer have the same feel as the question, and it will not.

140
00:13:49,000 --> 00:14:00,000
And as long as we accept that, we're fine, you can do causal neurology, yes, you can do cheat Parkinson's in depression and stroke, that's causal work, that's where the levels matter.

141
00:14:00,000 --> 00:14:04,000
But if you're asking, understanding explanation, why is this person a good cook?

142
00:14:04,000 --> 00:14:10,000
It's because they salt well, it's because they read recipes well, it's because they have a knack.

143
00:14:10,000 --> 00:14:17,000
And notice that those explanations for why a cook is good, all on the Daniel Dennett's intentional level of cooking.

144
00:14:17,000 --> 00:14:22,000
If you said they're good cook, they've got really good spatial working memory, right?

145
00:14:22,000 --> 00:14:25,000
Or they have good selective spatial attention.

146
00:14:25,000 --> 00:14:37,000
You're no longer explaining why they're a good cook, all you're doing is talking about the, perhaps, necessary but not sufficient cognitive capacities that you require to be a good composer as well.

147
00:14:37,000 --> 00:14:47,000
So, to me, it's all a bit of a non-question that we like, because we all would like the feeling of an answer that we're never going to get.

148
00:14:47,000 --> 00:14:56,000
I completely agree with you, but I'm much more curious rather than bored by the question, because in fact, the non-question becomes a culture.

149
00:14:56,000 --> 00:15:02,000
As you know, the NIMH now demands that studies about sadness have biomarkers.

150
00:15:02,000 --> 00:15:06,000
You cannot get a grant at the National Institute of Mental Health.

151
00:15:06,000 --> 00:15:14,000
And so these problems, we have this capacity to sometimes manage better, in the 19th century, Hughlings Jackson said exactly what you said.

152
00:15:14,000 --> 00:15:21,000
We're going to basically defer psychic causality, and we're going to look for neural causality, and we're going to start to do the same thing with psychology.

153
00:15:21,000 --> 00:15:27,000
Everyone is mixing mind and brain in a mash of nonsense.

154
00:15:27,000 --> 00:15:29,000
He said we're just going to look for psychological causality.

155
00:15:29,000 --> 00:15:37,000
The problem is some fields and some people, like psychiatrists and their patients, walk into a situation where they're demanded.

156
00:15:37,000 --> 00:15:43,000
It seems like there is a great deal of pressure to think on both sides and to prematurely integrate.

157
00:15:43,000 --> 00:15:46,000
Those integrations have been usually disastrous.

158
00:15:46,000 --> 00:15:56,000
And presently, I think we're in the midst of another chapter in that with the recent NIMH demand that everyone's going to have to make up a biomarker for every study they do.

159
00:15:56,000 --> 00:15:59,000
You will not be able to do a clinical study on sadness.

160
00:15:59,000 --> 00:16:01,000
Yeah, so that sounds right to me.

161
00:16:01,000 --> 00:16:10,000
And also, I think there's something that's been historically perplexing about the relationship between physical matter and cognition or the mind in the brain over long spans of time.

162
00:16:10,000 --> 00:16:20,000
So there's something, at least in the curiosity or the fad or the fetish that you're describing, John, that has cultural significance, if not also political significance.

163
00:16:20,000 --> 00:16:31,000
And there's a reason why we started off in this vein, even though with the possible exception of Ken, it seems like no one here really is committed to a kind of program of reduction.

164
00:16:31,000 --> 00:16:40,000
And yet, it's what bugs you, even though none of us really actually are committed to explaining in the way that you're afraid of.

165
00:16:40,000 --> 00:16:43,000
Well, as I said, that's not true of NIMH.

166
00:16:43,000 --> 00:16:44,000
Well, no, exactly.

167
00:16:44,000 --> 00:16:45,000
Do you need the neuroscience?

168
00:16:45,000 --> 00:16:46,000
Yes, right.

169
00:16:46,000 --> 00:16:57,000
Genuinely believe that if you can understand heading direction through a ring attractor, a fly's brain, that you will then be able to extrapolate from that other forms of cognition.

170
00:16:57,000 --> 00:16:59,000
They really do believe that.

171
00:16:59,000 --> 00:17:01,000
Right?

172
00:17:01,000 --> 00:17:08,000
And it's now, look, if they could explain in an argument, you know, Ken just said, I believe one day you will.

173
00:17:08,000 --> 00:17:10,000
That's the religion, as far as I understand.

174
00:17:10,000 --> 00:17:11,000
Right?

175
00:17:11,000 --> 00:17:19,000
But if you're going to come up with a framework as to how that extrapolate, I mean, if you were to say, look, here's an ant.

176
00:17:19,000 --> 00:17:20,000
Yeah.

177
00:17:20,000 --> 00:17:21,000
One little ant.

178
00:17:21,000 --> 00:17:29,000
And I'm going to be so intelligent that I will actually infer ant colonies and termite mounts from that one ant.

179
00:17:29,000 --> 00:17:30,000
Good luck.

180
00:17:30,000 --> 00:17:31,000
You never would.

181
00:17:31,000 --> 00:17:32,000
Yeah.

182
00:17:32,000 --> 00:17:34,000
I'm going to agree with everything you're saying.

183
00:17:34,000 --> 00:17:38,000
And unfortunately, neuroscience, we study the ant because we've got great tools.

184
00:17:38,000 --> 00:17:40,000
We can go deep into the ant's brain.

185
00:17:40,000 --> 00:17:46,000
We will infer termite mounts, which is what cognition would be in an ant colony, as present in the present.

186
00:17:46,000 --> 00:17:47,000
Right, right.

187
00:17:47,000 --> 00:17:49,000
It's an emergent behavior when you have thousands of ants.

188
00:17:49,000 --> 00:17:50,000
Correct.

189
00:17:50,000 --> 00:17:54,000
But somehow we're going to work out how that thousands of ants work.

190
00:17:54,000 --> 00:18:04,000
And there are coarse grain descriptions of complex systems like ant colonies that are very explanatory, but they're not single ants.

191
00:18:04,000 --> 00:18:05,000
Right?

192
00:18:05,000 --> 00:18:07,000
And you substitute ants for neurons and you can see the problem.

193
00:18:07,000 --> 00:18:08,000
Yeah.

194
00:18:08,000 --> 00:18:09,000
Right?

195
00:18:09,000 --> 00:18:18,000
And yet neuroscientists hate this kind of argument because they're deeply in love with the reductionist work.

196
00:18:18,000 --> 00:18:20,000
And it's good work, by the way.

197
00:18:20,000 --> 00:18:22,000
I think at the level of the work, ants are interesting.

198
00:18:22,000 --> 00:18:23,000
Neurons are interesting.

199
00:18:23,000 --> 00:18:24,000
Right.

200
00:18:24,000 --> 00:18:26,000
Parts of neurons are interesting.

201
00:18:26,000 --> 00:18:27,000
Right?

202
00:18:27,000 --> 00:18:34,000
But the idea that if you study and study and study and study and make some magical nonlinear extrapolatory moment, you'll have this new thing.

203
00:18:34,000 --> 00:18:35,000
It's never happened.

204
00:18:35,000 --> 00:18:36,000
I've never seen it.

205
00:18:36,000 --> 00:18:37,000
Yeah.

206
00:18:37,000 --> 00:18:38,000
That's the problem.

207
00:18:38,000 --> 00:18:39,000
Right.

208
00:18:39,000 --> 00:18:47,000
I don't agree that it's an insoluble, in principle, separation between the physical and the mental.

209
00:18:47,000 --> 00:18:49,000
That will never get there.

210
00:18:49,000 --> 00:19:06,000
And just as one simple example that John would know very well, but we have the experience that to see this cup and to reach for this cup and to know it's a cup and to be able to reach for it is the same thing.

211
00:19:06,000 --> 00:19:10,000
Well, from study of the brain, we now know it's not the same.

212
00:19:10,000 --> 00:19:18,000
That there's one system that I use to say that's a cup and it's a sort of round in a cylinder and to describe its shape.

213
00:19:18,000 --> 00:19:24,000
And there's a completely different system that I use to form my hands to reach for it, know where to reach it and be able to pick it up.

214
00:19:24,000 --> 00:19:26,000
And those are two separable systems.

215
00:19:26,000 --> 00:19:28,000
You can lead in one or lead in the other separately.

216
00:19:28,000 --> 00:19:37,000
So that's a very tiny example of where knowing something about the brain gives us some new insight into the mind that we just wouldn't get by introspection.

217
00:19:37,000 --> 00:19:52,000
And I think that doesn't go very far into our daily life, but it's an example of how understanding how the real thing works is going to piece by piece little by little over a very, very long time going to revise our understanding of our mind works.

218
00:19:52,000 --> 00:20:05,000
Well, actually, let me ask you though, what have we learned about the mind by knowing that the action of reaching for a cup and of gazing at a cup are served by two different systems?

219
00:20:05,000 --> 00:20:11,000
We've learned about the underlying neural architecture of each and how they're distinct from each other.

220
00:20:11,000 --> 00:20:13,000
Well, you mentioned that they're separate processes.

221
00:20:13,000 --> 00:20:15,000
Right, but they're separate and neural.

222
00:20:15,000 --> 00:20:16,000
Unified.

223
00:20:16,000 --> 00:20:20,000
No, no, just to defend, I mean, just to be, this is the crux of matter to be clear.

224
00:20:20,000 --> 00:20:21,000
Yeah.

225
00:20:21,000 --> 00:20:24,000
What Ken is talking about is a form of modularity of process.

226
00:20:24,000 --> 00:20:25,000
Right, exactly.

227
00:20:25,000 --> 00:20:26,000
Okay.

228
00:20:26,000 --> 00:20:37,000
Now, psychologists, Sternberg and many others who have worried about the decomposition into components of what seems like the unified brain.

229
00:20:37,000 --> 00:20:42,000
So that's not implementational level work.

230
00:20:42,000 --> 00:20:53,000
In other words, showing that there's dissociation of modules, showing that the seemingly unified brain mind is actually made up of these competing subsystems.

231
00:20:53,000 --> 00:20:54,000
Right.

232
00:20:54,000 --> 00:20:57,000
It's incredibly interesting, but that's psychology.

233
00:20:57,000 --> 00:20:58,000
Right.

234
00:20:58,000 --> 00:21:01,000
And then cognitive neuroscience is sort of psychology, but imaging.

235
00:21:01,000 --> 00:21:02,000
Right.

236
00:21:02,000 --> 00:21:03,000
But.

237
00:21:03,000 --> 00:21:09,000
But I think for the benefit of the audience, actually, we might want to clarify that what we're talking about here is still actually below any level of consciousness.

238
00:21:09,000 --> 00:21:10,000
Yes.

239
00:21:10,000 --> 00:21:20,000
So this is about a mental process, a cognitive process in which these two modules are distinct from each other, but one can't's introspect on that at all.

240
00:21:20,000 --> 00:21:35,000
So that this is in some sense what I was asking. I think many people in the audience, when they hear questions of mind are interested in something that is available to introspection is consciously experienced.

241
00:21:35,000 --> 00:21:39,000
Right. And I think it's very important. I'm sure Ken and I would take the view.

242
00:21:39,000 --> 00:21:47,000
I think that, and I think Dan Dennett, I was using the point to introspection.

243
00:21:47,000 --> 00:21:52,000
Yes, in certain circumstances, self-report is useful in psychology.

244
00:21:52,000 --> 00:21:53,000
Right.

245
00:21:53,000 --> 00:22:04,000
But it's a dangerous thing to base science on, whether it's cognitive neuroscience with its computational modules, or whether it's neuroscience where you actually talk about implementation circuits.

246
00:22:04,000 --> 00:22:05,000
Right.

247
00:22:05,000 --> 00:22:11,000
I think they're both ways to study the brain. I think the bridge, as I said, between the brain and mind is the notion of cognition.

248
00:22:11,000 --> 00:22:19,000
The best way to go after cognition, I think, is psychological experiments. Sometimes there's psychological experiments benefit from some self-report.

249
00:22:19,000 --> 00:22:34,000
But I think it's very important that I do not take the view that the introspective route alone, even I think members of the audience who care about free will, consciousness, thinking would, I think, prefer cognitive neuroscience to introspection.

250
00:22:34,000 --> 00:22:35,000
Okay.

251
00:22:35,000 --> 00:22:42,000
And I would... The one thing I disagree is, this isn't... The dissociative... I just talked about it's not just psychology.

252
00:22:42,000 --> 00:22:50,000
Without... We discovered that by understanding how two separate brain systems process things and how when you're lesion one of them, you lose one faculty.

253
00:22:50,000 --> 00:22:52,000
That's neuropsychology. Neuropsychology.

254
00:22:52,000 --> 00:22:53,000
Neuropsychology.

255
00:22:53,000 --> 00:22:59,000
No, right. I'm just saying that if you had area A and area B and they dissociated, it's fascinating.

256
00:22:59,000 --> 00:23:06,000
But unless you care that it was dorsal stream versus ventral stream, it could have been occipital cortex, could have been temporal cortex.

257
00:23:06,000 --> 00:23:12,000
It's the dissociation that's interesting, not the exact anatomical location of it at the current time.

258
00:23:12,000 --> 00:23:14,000
I agree with that.

259
00:23:14,000 --> 00:23:15,000
Yes.

260
00:23:15,000 --> 00:23:22,000
Wouldn't you have the cognitive dissociation be at some level of explanatory removed from the neural...

261
00:23:22,000 --> 00:23:29,000
Why would it occur in different forms of neural location?

262
00:23:29,000 --> 00:23:31,000
Well, the point is we discover it by studying the brain.

263
00:23:31,000 --> 00:23:34,000
We don't discover it by studying the mind without studying the brain.

264
00:23:34,000 --> 00:23:35,000
Right.

265
00:23:35,000 --> 00:23:36,000
It's a leading...

266
00:23:36,000 --> 00:23:37,000
Yes, you know, I understand.

267
00:23:37,000 --> 00:23:45,000
It seems to me that on the one hand, we have the limits epistemologically of neural reductionism, which you made the case for very clearly.

268
00:23:45,000 --> 00:23:50,000
And on the other, we do have the limits of self-report, which are obvious.

269
00:23:50,000 --> 00:23:56,000
For the people who are in the lab doing this, do we not need a new model of science to understand the mind?

270
00:23:56,000 --> 00:23:59,000
Are we not stuck between bad models?

271
00:23:59,000 --> 00:24:01,000
I think...

272
00:24:01,000 --> 00:24:07,000
Well, one thing I've found recently, and I've just...

273
00:24:07,000 --> 00:24:15,000
A recent project, but I've been writing about the fact that we seem to forget the sensation of pain.

274
00:24:15,000 --> 00:24:21,000
And there's quite a large psychology and some neuroscience on this.

275
00:24:21,000 --> 00:24:30,000
And looking at it, it's interesting how so much of the neuroscience is still in...

276
00:24:30,000 --> 00:24:35,000
Really, I mean, because what else can we do, but maybe there's a way to think about moving forward into something else,

277
00:24:35,000 --> 00:24:39,000
but has to speak in terms in behavioral terms.

278
00:24:39,000 --> 00:24:42,000
So about the two components of pain.

279
00:24:42,000 --> 00:24:50,000
There is a distinction and maybe a dissociation, though it's not clear that there's ever been a full dissociation

280
00:24:50,000 --> 00:24:54,000
between the feeling of pain and your emotional reaction.

281
00:24:54,000 --> 00:24:59,000
But both are described in terms of behavior. They're identified in behavior.

282
00:24:59,000 --> 00:25:01,000
So it's very difficult.

283
00:25:01,000 --> 00:25:11,000
Just, I think, to get... It would be nice in my paper, I suggest, you know, that the literature on memory of pain

284
00:25:11,000 --> 00:25:18,000
is also very confusing because it is hard to identify this sensory component outside of behavioral terms.

285
00:25:18,000 --> 00:25:25,000
So it would be nice, I think, for the science to move forward, for the neuroscience to move forward,

286
00:25:25,000 --> 00:25:38,000
to allow for neuroscientists to feel a little bit more free about not needing to identify everything behaviorally.

287
00:25:38,000 --> 00:25:44,000
Now, how do you do the study? You know, you have to find some thing that goes on.

288
00:25:44,000 --> 00:25:51,000
But... And this is where, I think, maybe first introspection and the neuroscience, the fMRI,

289
00:25:51,000 --> 00:25:57,000
it's going to have to work hand in hand because maybe, okay, we're not going to be able to see some behavior

290
00:25:57,000 --> 00:26:04,000
that a subject in a study is going to events, but we're going to have to take their word for it

291
00:26:04,000 --> 00:26:12,000
that they are remembering a sensation. But it's... And we'll look at what's going on in the brain at that point.

292
00:26:12,000 --> 00:26:22,000
So I feel that in moving forward, and then just in looking at that research, I wanted to suggest that it would be nice

293
00:26:22,000 --> 00:26:31,000
to have a way to get subjects to identify a conscious component of what's going on.

294
00:26:31,000 --> 00:26:38,000
Versus, well, what I found in the literature was a distinction between remembered pain and known pain.

295
00:26:38,000 --> 00:26:44,000
But the poor subjects in these studies, there's like a 500-word description of making this distinction,

296
00:26:44,000 --> 00:26:49,000
and I'm sure that's very difficult for any subject to read and really understand.

297
00:26:49,000 --> 00:26:54,000
So I thought... Here I thought it was a place where philosophers who like to think a lot about this subjectivity

298
00:26:54,000 --> 00:27:04,000
and what they call qualia and consciousness could help out in trying to create a protocol to offer to subjects

299
00:27:04,000 --> 00:27:14,000
and studies for them to try to identify different conscious aspects of their mind, the consciousness.

300
00:27:14,000 --> 00:27:21,000
So in looking at what's the core, you know, what we see is correlated with activity in the brain.

301
00:27:21,000 --> 00:27:25,000
I mean, that seemed to be clear in the pain.

302
00:27:25,000 --> 00:27:30,000
But even in the end, you're just going to have correlations, which are not explanations.

303
00:27:30,000 --> 00:27:31,000
Oh, right.

304
00:27:31,000 --> 00:27:36,000
Just to get to your question, right, I mean, there's the meteorological fallacy.

305
00:27:36,000 --> 00:27:42,000
You know, PMS, Hackler, Wittgenstein, he talked about, you know, wings don't fly, birds do.

306
00:27:42,000 --> 00:27:48,000
Okay, the amygdala does not feel pain, people do.

307
00:27:48,000 --> 00:27:54,000
All right, so soon, now lots of, again, neuroscientists don't understand that logical fallacy.

308
00:27:54,000 --> 00:27:59,000
Right, they think that parts feel rather than the... Yeah.

309
00:27:59,000 --> 00:28:01,000
...the glagration of the path.

310
00:28:01,000 --> 00:28:07,000
So there, you could say we don't need the difference, because there's a philosophical misunderstanding.

311
00:28:07,000 --> 00:28:08,000
Okay.

312
00:28:08,000 --> 00:28:13,000
But on the other hand, we do lack, in complexity science, a bridging hypothesis.

313
00:28:13,000 --> 00:28:21,000
It is true that it's difficult to come up with, if I were to ask you, do you understand New York City?

314
00:28:21,000 --> 00:28:23,000
I could ask everyone in this room.

315
00:28:23,000 --> 00:28:27,000
What does that question mean? Do I understand New York City?

316
00:28:27,000 --> 00:28:32,000
So to understand the mind is as stupid as that question, right?

317
00:28:32,000 --> 00:28:35,000
Right? Do you understand New York City?

318
00:28:35,000 --> 00:28:37,000
You can utter that sentence, right?

319
00:28:37,000 --> 00:28:38,000
But I beg to differ.

320
00:28:38,000 --> 00:28:42,000
Now, what's ever asked me that question before, I've been asked the question if I understand the mind a million times.

321
00:28:42,000 --> 00:28:43,000
Right.

322
00:28:43,000 --> 00:28:44,000
Why?

323
00:28:44,000 --> 00:28:45,000
I'm just saying it's the same in times.

324
00:28:45,000 --> 00:28:46,000
No, but let me finish.

325
00:28:46,000 --> 00:28:48,000
Why do neuroscientists keep making the same mistake?

326
00:28:48,000 --> 00:28:50,000
It's not because they don't understand the problems.

327
00:28:50,000 --> 00:28:53,000
It's because there's a great deal of pressure on them.

328
00:28:53,000 --> 00:28:55,000
Social, political, human, bless you.

329
00:28:55,000 --> 00:28:57,000
Make that leap.

330
00:28:57,000 --> 00:29:01,000
And so for 150 years, they keep going off the same cliff.

331
00:29:01,000 --> 00:29:06,000
People keep asking them, but what about the thing that I think organizes my day every day?

332
00:29:06,000 --> 00:29:11,000
And that's got to be related to these gray matters that you're talking about in these neurons.

333
00:29:11,000 --> 00:29:13,000
But how is it related?

334
00:29:13,000 --> 00:29:16,000
And again and again, the pressure of answering that question.

335
00:29:16,000 --> 00:29:17,000
Exactly what I'm saying is...

336
00:29:17,000 --> 00:29:18,000
It's a seduction.

337
00:29:18,000 --> 00:29:19,000
I know, it's a seduction.

338
00:29:19,000 --> 00:29:21,000
But why don't we just get over it?

339
00:29:21,000 --> 00:29:26,000
In other words, what I'm saying is we can have very important causal connections made.

340
00:29:26,000 --> 00:29:29,000
Causality matters, right?

341
00:29:29,000 --> 00:29:37,000
But just like explaining ant colonies and weather and cities, you can't do it.

342
00:29:37,000 --> 00:29:39,000
I completely agree with you, but I'll tell you the answer.

343
00:29:39,000 --> 00:29:40,000
Let's just get over this.

344
00:29:40,000 --> 00:29:42,000
The reason we don't get over it is you won't get grants.

345
00:29:42,000 --> 00:29:44,000
Yeah, but that's why you won't get over it.

346
00:29:44,000 --> 00:29:47,000
But that's not a very intellectual position to take.

347
00:29:47,000 --> 00:29:50,000
I can tell my friends that they're laughing.

348
00:29:50,000 --> 00:29:55,000
I would say I'm not going to be always not interrupted, just because I'm a woman.

349
00:29:55,000 --> 00:29:57,000
So I'm going to interrupt a little bit.

350
00:29:57,000 --> 00:30:03,000
And also I want to bring up somewhere where I think Jonathan can add, okay, I would say

351
00:30:03,000 --> 00:30:10,000
things are even worse or maybe more complicated than John has just pointed out with explanation.

352
00:30:10,000 --> 00:30:21,000
I mean, I would say it's partly a sociological linguistic fact that we say people think

353
00:30:21,000 --> 00:30:23,000
instead of the brain.

354
00:30:23,000 --> 00:30:26,000
And I think it could, I mean, we could change.

355
00:30:26,000 --> 00:30:31,000
I'm very always influenced by Chomsky on his thoughts about this.

356
00:30:31,000 --> 00:30:38,000
And he often points out, well, you know, in English we say birds fly and airplanes fly.

357
00:30:38,000 --> 00:30:41,000
But you know, in some other languages, airplanes don't fly.

358
00:30:41,000 --> 00:30:47,000
No, you know, so it's, you know, well in English and maybe today in most languages we talk about

359
00:30:47,000 --> 00:30:50,000
people as thinking and not the brain.

360
00:30:50,000 --> 00:30:52,000
But you know, that could change.

361
00:30:52,000 --> 00:30:57,000
I mean, it's interesting you see also, I mean, I noticed in my writing, I don't want an

362
00:30:57,000 --> 00:30:58,000
argument to state something.

363
00:30:58,000 --> 00:31:02,000
But a lot of people say, you know, so maybe arguments are now state.

364
00:31:02,000 --> 00:31:08,000
So it might be somehow also just a social fact also.

365
00:31:08,000 --> 00:31:09,000
I'm not sure.

366
00:31:09,000 --> 00:31:16,000
I mean, I'm willing to say there's some flexibility in what we take as a fact of the matter in this case.

367
00:31:16,000 --> 00:31:18,000
Well, I would agree entirely.

368
00:31:18,000 --> 00:31:19,000
I think it is.

369
00:31:19,000 --> 00:31:20,000
I knew you know.

370
00:31:20,000 --> 00:31:23,000
I think it's very much a social fact.

371
00:31:23,000 --> 00:31:27,000
But also with an interesting history, I mean the mind-body problem goes, you know, it goes

372
00:31:27,000 --> 00:31:28,000
all the way back.

373
00:31:28,000 --> 00:31:32,000
And people have written interesting things about it for a very long time.

374
00:31:32,000 --> 00:31:38,000
And it has affected our culture in recent years in ways that are, you know, I guess good and bad.

375
00:31:38,000 --> 00:31:42,000
And certainly our arts in ways that are interesting.

376
00:31:42,000 --> 00:31:49,000
There's good novels have been written about, you know, brain-based accounts of behavior and psychology.

377
00:31:49,000 --> 00:31:51,000
E.M.E.Q. and Saturdays, lovely novel.

378
00:31:51,000 --> 00:31:53,000
The science is wrong.

379
00:31:53,000 --> 00:31:54,000
No doubt.

380
00:31:54,000 --> 00:32:02,000
But it's preoccupied with, you know, the problem of reducing the experience to the brain.

381
00:32:02,000 --> 00:32:09,000
But also actually I think even in everyday speech, again, going way back, you will pick up phrases

382
00:32:09,000 --> 00:32:14,000
like, you know, my brain hurts or something like that when you're talking about having to think really hard about a problem.

383
00:32:14,000 --> 00:32:20,000
The notion that the brain is the seat of the mind and is involved in cognition has been, you know,

384
00:32:20,000 --> 00:32:24,000
obvious in one way or another for people for a very long time.

385
00:32:24,000 --> 00:32:30,000
And there's all sorts of kind of like folk cultural ways of making the connections pretty quick.

386
00:32:30,000 --> 00:32:35,000
And none of that I think poses any problems for your fundamental irritation with your own discipline,

387
00:32:35,000 --> 00:32:36,000
though.

388
00:32:36,000 --> 00:32:38,000
I think that it's actually quite compatible with it.

389
00:32:38,000 --> 00:32:41,000
There's in the sense that they're not doing any explanatory work.

390
00:32:41,000 --> 00:32:49,000
But it's part of the culture in which people will refer to the brain as a kind of chunky way of talking about, you know, the mind.

391
00:32:49,000 --> 00:32:59,000
Yeah, I mean, if you're just, you know, in an article last year, you know, involved in cognition, that word involved, doesn't know work.

392
00:32:59,000 --> 00:33:05,000
It's just a correlation or that are causal link, which none of them doubt.

393
00:33:05,000 --> 00:33:06,000
Right.

394
00:33:06,000 --> 00:33:11,000
In other words, if you're doing causal work, manipulative, interventional work, that's great science.

395
00:33:11,000 --> 00:33:12,000
Right.

396
00:33:12,000 --> 00:33:21,000
And we do, you know, but it's going to be more difficult when you say, hey, how does the brain lead to the mind?

397
00:33:21,000 --> 00:33:23,000
And I'm going to go, you really go, oh, thank you.

398
00:33:23,000 --> 00:33:25,000
I'm not going to have to worry about that again.

399
00:33:25,000 --> 00:33:26,000
I've had my aha moment.

400
00:33:26,000 --> 00:33:27,000
Yeah.

401
00:33:27,000 --> 00:33:30,000
It's a nice compressed sentence has been uttered.

402
00:33:30,000 --> 00:33:31,000
Yeah.

403
00:33:31,000 --> 00:33:35,000
I've understood just the way you say how does a car engine work.

404
00:33:35,000 --> 00:33:37,000
I think I know how a car engine works.

405
00:33:37,000 --> 00:33:40,000
I want there to be a way to say how the brain leads to the mind.

406
00:33:40,000 --> 00:33:46,000
When I'm saying it is not going to ever sound like the way you're being told how a car engine works.

407
00:33:46,000 --> 00:33:47,000
Yeah.

408
00:33:47,000 --> 00:33:50,000
And you just have to look at that.

409
00:33:50,000 --> 00:33:51,000
Yeah, no, no.

410
00:33:51,000 --> 00:33:58,000
And I'm just saying that most people, I think, intuited better than the reductionist scientists.

411
00:33:58,000 --> 00:33:59,000
Again, I agree entirely with that.

412
00:33:59,000 --> 00:34:00,000
Yeah.

413
00:34:00,000 --> 00:34:05,000
I also think that there are cultural reasons and political reasons along the lines of which

414
00:34:05,000 --> 00:34:12,000
order is suggesting as to why this conversation now in 2018 seems so pressing.

415
00:34:12,000 --> 00:34:19,000
At least in my experience over the last decade, universities and funding agencies and everything

416
00:34:19,000 --> 00:34:26,000
else, I've been preoccupied with two things in particular, neurological reduction and

417
00:34:26,000 --> 00:34:30,000
what you might call kind of data or computational reduction.

418
00:34:30,000 --> 00:34:37,000
So the money is in either kind of information systems or studying, using computers to sort

419
00:34:37,000 --> 00:34:43,000
of provide numbers for things or it is, as you suggested, George, in putting a squiggly F

420
00:34:43,000 --> 00:34:46,000
in front of something or just generally using the word neuron.

421
00:34:46,000 --> 00:34:48,000
And that's an interesting sociological question.

422
00:34:48,000 --> 00:34:53,000
Yeah, there's a wonderful book of anyone interested called Neuro published by Princeton University Press.

423
00:34:53,000 --> 00:35:01,000
Think about five years ago by two writers where they very, very beautifully outline the sociology

424
00:35:01,000 --> 00:35:03,000
of the neuro fad.

425
00:35:03,000 --> 00:35:04,000
Right.

426
00:35:04,000 --> 00:35:09,000
And they very much link it to what they call the neuromolecular view of the mind, which

427
00:35:09,000 --> 00:35:12,000
very much came on the heels of molecular biology and molecular genetics.

428
00:35:12,000 --> 00:35:18,000
So the hubris was that we told DNA makes RNA, makes protein.

429
00:35:18,000 --> 00:35:20,000
We're going to do this for neurosar.

430
00:35:20,000 --> 00:35:22,000
Complete failure, obviously.

431
00:35:22,000 --> 00:35:25,000
But nevertheless, it was very compelling.

432
00:35:25,000 --> 00:35:30,000
The Caltech story, Mary McKay wrote a history of Caltech.

433
00:35:30,000 --> 00:35:34,000
And how Linus Pauli said that we studied proteins, we would extrapolate from proteins.

434
00:35:34,000 --> 00:35:39,000
So this has happened over and over again, this sort of extrapolation hubris.

435
00:35:39,000 --> 00:35:40,000
Right.

436
00:35:40,000 --> 00:35:45,000
And now the current one isn't neurons, it's not genes, it's circuits.

437
00:35:45,000 --> 00:35:46,000
Yeah, exactly.

438
00:35:46,000 --> 00:35:49,000
This nauseating word that you hear over and over again.

439
00:35:49,000 --> 00:35:52,000
Now it's not again, it's not that it isn't used to work.

440
00:35:52,000 --> 00:35:54,000
Which is also kind of a neuro term as well.

441
00:35:54,000 --> 00:35:58,000
Yeah, it's just that there's underline this good work.

442
00:35:58,000 --> 00:36:06,000
There's this additional strange totemic view that this will be what gets us to the promised

443
00:36:06,000 --> 00:36:07,000
land.

444
00:36:07,000 --> 00:36:08,000
Right.

445
00:36:08,000 --> 00:36:09,000
And it's bizarre.

446
00:36:09,000 --> 00:36:13,000
So I get to where I don't quite know what we're talking about.

447
00:36:13,000 --> 00:36:18,500
Well, I think it's what people really wanted to know whether the things that we talk about

448
00:36:18,500 --> 00:36:22,840
as mind will ever be sub explained.

449
00:36:22,840 --> 00:36:23,840
And okay, so to address.

450
00:36:23,840 --> 00:36:24,840
That's what it's about.

451
00:36:24,840 --> 00:36:31,000
So I'm saying not at the level of neural circuits.

452
00:36:31,000 --> 00:36:33,840
So there's different things there.

453
00:36:33,840 --> 00:36:40,040
So for example, I mean to understand New York City in its totality.

454
00:36:40,040 --> 00:36:41,040
Yeah.

455
00:36:41,040 --> 00:36:45,760
No, but we can understand a lot and we can understand more and more and we can understand

456
00:36:45,760 --> 00:36:49,440
why, you know, why are the traffic patterns the way they are.

457
00:36:49,440 --> 00:36:51,440
We can understand the forces that lead to that.

458
00:36:51,440 --> 00:36:56,000
We can, there's a lot you can understand without saying that you ever grasp the whole thing.

459
00:36:56,000 --> 00:36:57,000
But the word, right.

460
00:36:57,000 --> 00:36:58,000
It seems to me you're saying if we can't.

461
00:36:58,000 --> 00:37:00,840
No, no, no, no, no, no, it's more subtle than that.

462
00:37:00,840 --> 00:37:04,120
When you look at work on cities, for example, Jeffrey West, the former president of Santa

463
00:37:04,120 --> 00:37:08,680
V Institute, has written a wonderful book which I highly recommend called Scale, where he talks

464
00:37:08,680 --> 00:37:12,760
about how the complexity of cities arises.

465
00:37:12,760 --> 00:37:21,520
Any science does have ways of producing explanations about aggregate behavior, but it is non-reductionist

466
00:37:21,520 --> 00:37:22,520
science.

467
00:37:22,520 --> 00:37:28,320
What I'm saying is when you talk about complexity, you can find reduced forms of explanation

468
00:37:28,320 --> 00:37:30,760
to discuss complex systems.

469
00:37:30,760 --> 00:37:31,760
Okay.

470
00:37:31,760 --> 00:37:37,080
But no one's going to talk about galaxy formation by talking about particle physics, right.

471
00:37:37,080 --> 00:37:38,760
It's a different discipline.

472
00:37:38,760 --> 00:37:43,200
But all I'm saying is, of course you can study cities.

473
00:37:43,200 --> 00:37:47,440
The Santa Fe Institute has been studying cities for ages, but what they wouldn't say is to

474
00:37:47,440 --> 00:37:51,000
understand cities to study one family.

475
00:37:51,000 --> 00:37:57,840
Look, of course there's many, many levels and the very bottom level doesn't give a good

476
00:37:57,840 --> 00:37:59,560
explanation of the top level.

477
00:37:59,560 --> 00:38:01,280
But that's what neuroscientists think.

478
00:38:01,280 --> 00:38:02,920
I don't think so.

479
00:38:02,920 --> 00:38:05,000
I don't see that myself.

480
00:38:05,000 --> 00:38:07,920
I think in popular culture and popular writing, there's a lot of that.

481
00:38:07,920 --> 00:38:12,920
I don't see it really in neuroscientists myself.

482
00:38:12,920 --> 00:38:18,240
But just to say another thing, so you were talking about what people want to know is

483
00:38:18,240 --> 00:38:22,120
the mind they experience, their consciousness.

484
00:38:22,120 --> 00:38:26,000
From my point of view, that's literally the tip of the iceberg.

485
00:38:26,000 --> 00:38:30,480
And the iceberg being all the stuff that goes on in the brain.

486
00:38:30,480 --> 00:38:32,480
It's not literally the tip of the iceberg though.

487
00:38:32,480 --> 00:38:44,400
As the English professor, I'm constantly thinking, I'm going to be guilty.

488
00:38:44,400 --> 00:38:50,400
And the reason why that matters is because what we experience has a lot of structure

489
00:38:50,400 --> 00:38:55,280
that we don't understand, that we try to get at by introspection, by meditation, by all

490
00:38:55,280 --> 00:38:56,800
kinds of means.

491
00:38:56,800 --> 00:39:00,720
But a lot of that structure comes from all the stuff that's beneath the surface and not

492
00:39:00,720 --> 00:39:03,440
conscious.

493
00:39:03,440 --> 00:39:10,480
So to me, to understand the mind, to understand the conscious mind is a tiny piece of that.

494
00:39:10,480 --> 00:39:14,640
And it's not the central question really, although for ourselves personally, that's

495
00:39:14,640 --> 00:39:17,280
what we'd like to know because that's what we experience.

496
00:39:17,280 --> 00:39:21,400
But we're not going to understand that without understanding this huge amount of substructure

497
00:39:21,400 --> 00:39:24,520
and all the many, many levels from the bottom of the top.

498
00:39:24,520 --> 00:39:26,000
That's a very, very long project.

499
00:39:26,000 --> 00:39:28,440
But it's not that we're never going to get any insight.

500
00:39:28,440 --> 00:39:32,720
I mean, the one thing that I would agree with, or maybe I don't know if you're saying this,

501
00:39:32,720 --> 00:39:41,120
John, but I don't think we're ever going to have a scientific explanation of why the

502
00:39:41,120 --> 00:39:46,240
operations of the brain and the mind and all of their structure that leads us to act in

503
00:39:46,240 --> 00:39:50,640
the way that we act, why that's accompanied by subjective experience.

504
00:39:50,640 --> 00:39:55,320
Why we have a conscious experience, why we're not zombies, that whole thing of a question.

505
00:39:55,320 --> 00:39:57,040
I don't think that's a scientific question.

506
00:39:57,040 --> 00:39:59,040
I don't think we're ever going to answer that.

507
00:39:59,040 --> 00:40:03,000
We experience that when our mind does these things, they enter consciousness.

508
00:40:03,000 --> 00:40:04,880
We have a subjective awareness of them.

509
00:40:04,880 --> 00:40:10,360
But if you take away the subjective awareness part and just take all of the processing,

510
00:40:10,360 --> 00:40:17,840
all of the things that go on by which our brain is associating ideas and understand

511
00:40:17,840 --> 00:40:22,720
anything about the world and leading us to act in certain ways, then I don't see why

512
00:40:22,720 --> 00:40:26,800
we can't ultimately understand that in terms of the brain.

513
00:40:26,800 --> 00:40:31,280
For example, the problem is we're at such an early stage.

514
00:40:31,280 --> 00:40:35,720
At the time of Aristotle, if you wanted to understand why water felt wet, you really

515
00:40:35,720 --> 00:40:36,960
didn't have the means.

516
00:40:36,960 --> 00:40:40,120
Somebody could say, we're never going to understand that in physical terms.

517
00:40:40,120 --> 00:40:45,440
Now, any way you can describe the property of wetness, we have an explanation in terms

518
00:40:45,440 --> 00:40:48,480
of quantum mechanics and atoms and hydrogen bonds.

519
00:40:48,480 --> 00:40:51,920
We have explanations of why it has all the properties of wetness.

520
00:40:51,920 --> 00:40:54,840
In Aristotle's time, that was totally out of reach, but it didn't mean it was out of

521
00:40:54,840 --> 00:40:55,840
reach and principle.

522
00:40:55,840 --> 00:40:58,200
I think we're in Aristotle's time in terms of neurosis.

523
00:40:58,200 --> 00:40:59,600
No, just one...

524
00:40:59,600 --> 00:41:03,960
I don't agree actually about the subjective experience, but I actually feel that's a

525
00:41:03,960 --> 00:41:04,960
red herring.

526
00:41:04,960 --> 00:41:08,320
I think as it can shine again pointed out, it's amazing that you decide to move your

527
00:41:08,320 --> 00:41:09,320
arm and it goes up.

528
00:41:09,320 --> 00:41:11,160
It's amazing that you get touched and you feel it.

529
00:41:11,160 --> 00:41:15,880
So I actually think, like Nicholas Humphrey has written, that actually knowing where your

530
00:41:15,880 --> 00:41:21,920
limits in space when you close your eye is probably on the way to subjective conscious

531
00:41:21,920 --> 00:41:25,040
experience and you have a lovely argument of that kind.

532
00:41:25,040 --> 00:41:28,800
What I'm saying is that you keep using the word understand and what I'm saying can is

533
00:41:28,800 --> 00:41:33,840
you remove causal relations from that word.

534
00:41:33,840 --> 00:41:34,840
What does it mean?

535
00:41:34,840 --> 00:41:37,040
It means understanding the substance.

536
00:41:37,040 --> 00:41:39,040
No, don't use understanding to explain understanding.

537
00:41:39,040 --> 00:41:43,880
It means understanding the substructure that leads to the structure.

538
00:41:43,880 --> 00:41:45,680
Leads, but what does needs do?

539
00:41:45,680 --> 00:41:48,600
Is that causality?

540
00:41:48,600 --> 00:41:52,000
It's getting a deeper understanding of how the pieces are put together.

541
00:41:52,000 --> 00:41:53,360
You can't be going back to that word.

542
00:41:53,360 --> 00:41:55,360
What you're saying is what does it mean?

543
00:41:55,360 --> 00:41:57,360
What is causality?

544
00:41:57,360 --> 00:42:02,680
What I'm saying is that in the philosophy of science, in words explain and understand

545
00:42:02,680 --> 00:42:08,160
and cause are not the same thing or predict.

546
00:42:08,160 --> 00:42:12,360
When a sycamore leaf is dropped, we know the physics of the movement of a sycamore.

547
00:42:12,360 --> 00:42:15,360
It's extremely difficult to predict what its trajectory is going to be.

548
00:42:15,360 --> 00:42:20,480
Similarly, you can have converse situations where you can predict but not understand things.

549
00:42:20,480 --> 00:42:24,720
What I'm saying is the problem here is that you're using the word understanding.

550
00:42:24,720 --> 00:42:27,360
When you say, why is that woman crying?

551
00:42:27,360 --> 00:42:28,360
It's because she's sad.

552
00:42:28,360 --> 00:42:32,640
That's an explanation and you go, I now understand why she's crying.

553
00:42:32,640 --> 00:42:36,240
Causality is not in there in the scientific sense.

554
00:42:36,240 --> 00:42:41,040
What I'm saying is understanding isn't synonymous with causality.

555
00:42:41,040 --> 00:42:47,200
As soon as one goes after a scientist, like I'm going after you, to remove any causal sounding

556
00:42:47,200 --> 00:42:50,120
words from your sentences.

557
00:42:50,120 --> 00:42:52,840
Your left bereft.

558
00:42:52,840 --> 00:42:55,960
That's what I'm saying is what's so difficult about this.

559
00:42:55,960 --> 00:42:58,760
To me, it's a matter of these things.

560
00:42:58,760 --> 00:43:01,760
There's very, very complicated structure underlying it.

561
00:43:01,760 --> 00:43:08,040
The more you get insight, it's not a total understanding.

562
00:43:08,040 --> 00:43:12,760
You gain insight, the more you understand the structure underneath the surface.

563
00:43:12,760 --> 00:43:14,760
A body of correlations is not nothing.

564
00:43:14,760 --> 00:43:16,440
I think that's what you mean.

565
00:43:16,440 --> 00:43:19,120
You gain more and more and more correlations.

566
00:43:19,120 --> 00:43:20,120
They aren't causal.

567
00:43:20,120 --> 00:43:42,280
I think perhaps John

568
00:43:42,280 --> 00:43:55,800
has a question.

569
00:43:55,800 --> 00:44:00,520
You can also say that whatever event that this poor woman had in her life before she

570
00:44:00,520 --> 00:44:07,280
started crying had some causal relationship to her crying in a way that we could understand

571
00:44:07,280 --> 00:44:11,200
in some scientific fashion, perhaps though not just in terms of we wouldn't be in the

572
00:44:11,200 --> 00:44:12,200
language of neuroscience.

573
00:44:12,200 --> 00:44:17,440
In the language of psychology or sociology, events in one's life that have an effect

574
00:44:17,440 --> 00:44:18,960
on your emotions.

575
00:44:18,960 --> 00:44:21,600
That's not outside of science, I don't think.

576
00:44:21,600 --> 00:44:22,600
Is it?

577
00:44:22,600 --> 00:44:23,600
No, I'm just saying-

578
00:44:23,600 --> 00:44:25,320
It's just that science is not neuroscience.

579
00:44:25,320 --> 00:44:30,040
Right, I'm just saying that there are, as a general, intentional levels, there are words

580
00:44:30,040 --> 00:44:32,680
that do explain them that mean things.

581
00:44:32,680 --> 00:44:37,720
All I'm saying is that if you want it to sound like an explanation or understanding,

582
00:44:37,720 --> 00:44:43,760
by dropping down to what is doing good, correlational causal work and wanted to also give you,

583
00:44:43,760 --> 00:44:49,120
along with its correlational causal truth, the feeling of explanation and understanding,

584
00:44:49,120 --> 00:44:53,680
in addition when you subtract the way the correlational causality, we don't have a way

585
00:44:53,680 --> 00:44:54,680
of doing that.

586
00:44:54,680 --> 00:44:55,680
No, and I agree entirely.

587
00:44:55,680 --> 00:44:59,120
You could say that in some ways, there's just a dependency relationship between these

588
00:44:59,120 --> 00:45:00,680
levels of explanation.

589
00:45:00,680 --> 00:45:02,200
You can't have one without the other.

590
00:45:02,200 --> 00:45:06,400
We're not going to have crying people who have no brains.

591
00:45:06,400 --> 00:45:12,560
Nevertheless, the explanation that's going to satisfy the question of whether it was

592
00:45:12,560 --> 00:45:16,520
or someone crying is simply not going to be done at some lower level of dependency that

593
00:45:16,520 --> 00:45:20,880
needs to be there, but it doesn't do any explanatory causal work.

594
00:45:20,880 --> 00:45:22,880
It doesn't.

595
00:45:22,880 --> 00:45:25,080
And I don't have to- I don't claim any of the answer.

596
00:45:25,080 --> 00:45:31,120
I'm just saying that I don't think we should fool ourselves with these words and use them.

597
00:45:31,120 --> 00:45:36,240
Well philosophers of science have spent a lot of time arguing about what it means to be

598
00:45:36,240 --> 00:45:43,640
an explanation and to have understanding, and they've come to no answers on this or no

599
00:45:43,640 --> 00:45:44,640
understanding.

600
00:45:44,640 --> 00:45:49,000
But some of them, like Nathan Salmon, he did argue that an explanation is just providing

601
00:45:49,000 --> 00:45:50,000
a cause.

602
00:45:50,000 --> 00:45:57,800
Boss von Frostin, one of my favorite, he says an explanation is an answer to a Y question.

603
00:45:57,800 --> 00:46:03,040
So it's sort of a social thing also when we stop sort of asking the question, maybe we

604
00:46:03,040 --> 00:46:06,040
have the explanation.

605
00:46:06,040 --> 00:46:16,120
Now Ken, I wanted to say you're just really not being bold enough, I would say.

606
00:46:16,120 --> 00:46:23,120
I am another thing that boss von Frostin says that I like to quote quite often.

607
00:46:23,120 --> 00:46:26,280
So he says there are no science stoppers.

608
00:46:26,280 --> 00:46:27,440
And I really believe that.

609
00:46:27,440 --> 00:46:34,080
I mean, well yes, from our perspective now, consciousness, subjective experience that's

610
00:46:34,080 --> 00:46:39,240
beyond the Ken, but you know, why, who knows?

611
00:46:39,240 --> 00:46:46,120
I mean, how can we, at our point, really, I mean, there have been such major turnovers

612
00:46:46,120 --> 00:46:52,000
in science, but we couldn't have predicted at all in the past, maybe in the future, we

613
00:46:52,000 --> 00:46:57,760
will stop, you know, wondering why consciousness, why are we subjective, and it will just be

614
00:46:57,760 --> 00:47:05,000
something that seems, somehow we've come up with some psychologically satisfying answer

615
00:47:05,000 --> 00:47:06,000
to it.

616
00:47:06,000 --> 00:47:11,480
Now, of course, there is the other certain science stoppers do exist in terms of grant

617
00:47:11,480 --> 00:47:12,480
money.

618
00:47:12,480 --> 00:47:17,240
You know, maybe it's not reasonable to be giving a lot of money right now to the neuroscientists

619
00:47:17,240 --> 00:47:21,320
studying subjectivity because we're nowhere close enough.

620
00:47:21,320 --> 00:47:30,120
But in principle, why like sort of say, absolutely not, isn't it useful to leave open, except

621
00:47:30,120 --> 00:47:32,720
for maybe economic reasons?

622
00:47:32,720 --> 00:47:38,280
So, okay, I don't think it's really the most important question to worry about, but the

623
00:47:38,280 --> 00:47:45,960
reason I say that is what science does, I mean, we start with subjective experience

624
00:47:45,960 --> 00:47:47,560
of an objective world.

625
00:47:47,560 --> 00:47:52,520
What science does is it isolates the things that are objectively reproducible, objectively

626
00:47:52,520 --> 00:47:55,640
definable, operationally definable.

627
00:47:55,640 --> 00:48:01,440
And so you can, I believe consciousness, science, consciousness, neuroscience is a real field

628
00:48:01,440 --> 00:48:08,400
because what it has to do with understanding what brain processes and what brain systems

629
00:48:08,400 --> 00:48:13,640
are active, in what ways when you are versus are not conscious, when you do versus don't

630
00:48:13,640 --> 00:48:15,960
consciously perceive something.

631
00:48:15,960 --> 00:48:21,760
It's knowing, you know, ultimately, hopefully we will completely be able to say by observing,

632
00:48:21,760 --> 00:48:25,680
you know, the brain, whether someone was conscious of a certain thing or not.

633
00:48:25,680 --> 00:48:27,320
I think that's totally unreached.

634
00:48:27,320 --> 00:48:34,320
What I don't think is unreached is to say why there's a subjective experience associated

635
00:48:34,320 --> 00:48:38,520
with it because all that science can talk about is the objective processes underlying.

636
00:48:38,520 --> 00:48:42,920
Or how you get from these objective processes to this objective experience, not just why.

637
00:48:42,920 --> 00:48:47,720
I'm impressed that you know what science can do, maybe that's what it can do today.

638
00:48:47,720 --> 00:48:53,120
But come on, look at what Aristotle would have said, all this was what science can do.

639
00:48:53,120 --> 00:48:57,560
He would have really had a much different, none of what we are doing today would have

640
00:48:57,560 --> 00:48:58,560
he said.

641
00:48:58,560 --> 00:49:03,280
But I mean, prediction is difficult, especially about the future.

642
00:49:03,280 --> 00:49:08,800
I think someday you have an explanation that makes us satisfied and stop worrying about

643
00:49:08,800 --> 00:49:16,080
it, you know, so that to say when this subsystem of the brain acts in this way, it makes the

644
00:49:16,080 --> 00:49:20,440
brain imagine that it's experiencing something and that's consciousness.

645
00:49:20,440 --> 00:49:24,880
You know, and you could imagine that we will tell ourselves a story that will satisfy ourselves

646
00:49:24,880 --> 00:49:29,080
and will be 100% correlated with when you're conscious and when you're not.

647
00:49:29,080 --> 00:49:33,800
That's different from saying why dead matter has subjective experience.

648
00:49:33,800 --> 00:49:37,600
That's just that we can get a satisfying explanation, but that's a different issue.

649
00:49:37,600 --> 00:49:39,400
I think it's an important one though.

650
00:49:39,400 --> 00:49:43,680
I think it might seem termological and not that interesting, but what's happened over

651
00:49:43,680 --> 00:49:48,160
time was when neuroscience says that's not a scientific question and subjectivity and

652
00:49:48,160 --> 00:49:52,280
consciousness have gotten that treatment, it ends up flipping over into mysticism and

653
00:49:52,280 --> 00:49:53,280
religion.

654
00:49:53,280 --> 00:49:57,080
Usually there's a great deal of pressure for it to flip over into religion and mysticism.

655
00:49:57,080 --> 00:50:00,360
So I think it's easy to say this is part of the natural world.

656
00:50:00,360 --> 00:50:03,960
We probably need a different way of thinking about science and different technological

657
00:50:03,960 --> 00:50:09,560
kind of advances to be able to master this, but the problems don't go away.

658
00:50:09,560 --> 00:50:13,600
And one of the central problems subjectivity is if you think of it as an epiphenomena,

659
00:50:13,600 --> 00:50:15,000
it's not very interesting.

660
00:50:15,000 --> 00:50:21,320
But if you think of intention and top down regulation as critical to human behavior and

661
00:50:21,320 --> 00:50:25,920
to expand it outward to the ant in a colony, to political life, to life, liberty and the

662
00:50:25,920 --> 00:50:31,400
pursuit of happiness, if you think all of that is critical, then you don't want to walk

663
00:50:31,400 --> 00:50:36,280
away from it as potentially open to reliable and valid knowledge, AKA science.

664
00:50:36,280 --> 00:50:38,520
I don't want to look at any of that.

665
00:50:38,520 --> 00:50:43,360
It's only any objective process you must have to name on.

666
00:50:43,360 --> 00:50:45,320
Intentionality won't make me quite.

667
00:50:45,320 --> 00:50:48,920
Yeah, any objective process you can put a name on, I think we can investigate.

668
00:50:48,920 --> 00:50:57,160
It's just why that objective process is not, you know, look, when a computer program is

669
00:50:57,160 --> 00:51:02,160
complicated as we can make it today, I think we all feel confident does not have any subjective

670
00:51:02,160 --> 00:51:03,160
experience.

671
00:51:03,160 --> 00:51:06,960
A car engine, I think we all feel confident does not have any subjective experience.

672
00:51:06,960 --> 00:51:11,240
So you keep adding more and more and more complexity of the matter, and at some point

673
00:51:11,240 --> 00:51:13,200
the matter has subjective experience.

674
00:51:13,200 --> 00:51:16,600
I think we can talk about all the complexity of the matter, all the processing it does,

675
00:51:16,600 --> 00:51:19,880
we can talk about which of them are associated with subjective experience.

676
00:51:19,880 --> 00:51:27,020
But the fact that the dead meat requires subjective experience, how do you, there isn't a

677
00:51:27,020 --> 00:51:29,600
fact, that's a fact, it's a phenomenon.

678
00:51:29,600 --> 00:51:33,720
Well, I think I recognize that it's a paradox.

679
00:51:33,720 --> 00:51:35,880
How do we have an objective science of subjectivity?

680
00:51:35,880 --> 00:51:36,880
I honestly feel...

681
00:51:36,880 --> 00:51:38,880
It's a real problem.

682
00:51:38,880 --> 00:51:45,000
I mean, there's a lot more that's concerning about understanding the link between mind

683
00:51:45,000 --> 00:51:49,640
and brain than this old sore of subjectivity and consciousness.

684
00:51:49,640 --> 00:51:51,480
I think one of the...

685
00:51:51,480 --> 00:51:55,280
I think one has to distinguish.

686
00:51:55,280 --> 00:51:59,720
It's extremely important to distinguish between making statements about science never getting

687
00:51:59,720 --> 00:52:00,720
somewhere.

688
00:52:00,720 --> 00:52:01,720
I think you're totally right.

689
00:52:01,720 --> 00:52:08,840
And at no point was I trying to say that versus logical conceptual errors that are

690
00:52:08,840 --> 00:52:12,880
being made and being mistaken for lack of scientific progress.

691
00:52:12,880 --> 00:52:15,840
And all I'm saying is they're not the same.

692
00:52:15,840 --> 00:52:21,200
There's a lot of logical conceptual error in the way we think about these questions that

693
00:52:21,200 --> 00:52:27,040
is invariant to how far we are along in terms of our technologies and our techniques.

694
00:52:27,040 --> 00:52:28,320
Of course we don't know.

695
00:52:28,320 --> 00:52:33,080
And actually, when it comes to the subjectivity argument, that's actually what I don't think

696
00:52:33,080 --> 00:52:34,080
it's such a big deal.

697
00:52:34,080 --> 00:52:38,180
I think it's a big deal because of us posing it.

698
00:52:38,180 --> 00:52:42,760
We give special status to subjectivity over pain, over proprioception.

699
00:52:42,760 --> 00:52:44,560
But that's cultural.

700
00:52:44,560 --> 00:52:49,520
I actually think this obsession with consciousness subjectivity is a cultural bias, more than

701
00:52:49,520 --> 00:52:51,760
a difference in kind.

702
00:52:51,760 --> 00:52:52,760
And I just...

703
00:52:52,760 --> 00:53:03,080
I hope this is a complex discussion that understanding things like language, pain, movement, attention,

704
00:53:03,080 --> 00:53:07,960
all of those are difficult to know what the right level of analysis is.

705
00:53:07,960 --> 00:53:09,480
Do we do psychological modules?

706
00:53:09,480 --> 00:53:10,480
Do we do circuits?

707
00:53:10,480 --> 00:53:11,480
Do we do neural?

708
00:53:11,480 --> 00:53:15,880
I'm trying to say that all those areas that aren't the sexy area of consciousness of subjectivity

709
00:53:15,880 --> 00:53:22,000
are all still in play in terms of we're not sure what the level of granularity is to do

710
00:53:22,000 --> 00:53:24,280
the work.

711
00:53:24,280 --> 00:53:25,280
That's what I'm saying.

712
00:53:25,280 --> 00:53:30,120
It's true in all areas, even movement.

713
00:53:30,120 --> 00:53:35,400
And what I'm saying is that the moment in neuroscience we've got this reductionist turn

714
00:53:35,400 --> 00:53:39,960
where we feel like we can play God on small circuits, which we can.

715
00:53:39,960 --> 00:53:42,960
It's remarkable what we can do now on small circuits.

716
00:53:42,960 --> 00:53:46,360
We are like gods, right?

717
00:53:46,360 --> 00:53:53,400
Has led to a kind in my view premature belief that this is going to be extrapolated to be

718
00:53:53,400 --> 00:53:58,680
as good as what our current psychological functional explanations look like.

719
00:53:58,680 --> 00:54:00,680
So you see global points.

720
00:54:00,680 --> 00:54:01,680
Right.

721
00:54:01,680 --> 00:54:08,720
So if I understand what you're saying now, the complaint or the concern about reducing

722
00:54:08,720 --> 00:54:15,080
subjectivity is for you at least less of a concern or is a red herring in the way of

723
00:54:15,080 --> 00:54:22,280
actually what you're really worried about, which is reducing and being kind of a sort

724
00:54:22,280 --> 00:54:26,800
of neurological greediness at other levels of the mind, which aren't even conscious actually.

725
00:54:26,800 --> 00:54:27,800
That's correct.

726
00:54:27,800 --> 00:54:34,280
And what we're really dealing with is how are we going to do neuroscience versus psychology?

727
00:54:34,280 --> 00:54:36,240
One is functional, one is mechanistic.

728
00:54:36,240 --> 00:54:38,800
And that's a genuinely difficult bridging question.

729
00:54:38,800 --> 00:54:41,440
And that's true across the board.

730
00:54:41,440 --> 00:54:49,120
And I think that there's a tendency to believe that we will, even for complex cognitive phenomena,

731
00:54:49,120 --> 00:54:54,800
be able to explain away some of our functional designations with lower level ones.

732
00:54:54,800 --> 00:54:58,560
And at the current time, that has not succeeded.

733
00:54:58,560 --> 00:55:02,880
Now, whether there's some breakthrough, we shall see.

734
00:55:02,880 --> 00:55:07,400
But that's right, but that's so that we're agreeing because as I said, we're, you know,

735
00:55:07,400 --> 00:55:11,000
we're centuries or millennia away from those kinds of connections.

736
00:55:11,000 --> 00:55:14,080
But that's different from saying in principle, it can't be done, which is what I thought

737
00:55:14,080 --> 00:55:15,080
you were saying.

738
00:55:15,080 --> 00:55:16,080
No, I wasn't saying principle.

739
00:55:16,080 --> 00:55:20,840
I was saying I want to distinguish between what is still open to being discovered versus

740
00:55:20,840 --> 00:55:28,440
what is a illogical kind of framework to begin with that needs to just be dismantled a little

741
00:55:28,440 --> 00:55:29,440
bit.

742
00:55:29,440 --> 00:55:33,680
So that's saying one day we'll understand the ether, there is no ether.

743
00:55:33,680 --> 00:55:34,680
Right?

744
00:55:34,680 --> 00:55:39,840
And what I'm saying is we just, I want to say that there are lots of philosophical, logical

745
00:55:39,840 --> 00:55:48,320
mistakes at work today, which by the way, the neurological fallacy that Haka talks about,

746
00:55:48,320 --> 00:55:53,040
Aristotle was the one who first came up with that idea that it's not the idea that the

747
00:55:53,040 --> 00:55:55,560
part is doing what the whole is doing.

748
00:55:55,560 --> 00:56:00,440
Aristotle was the first one to point that out and he's correct up until this day.

749
00:56:00,440 --> 00:56:02,440
Wings don't fly, birds do.

750
00:56:02,440 --> 00:56:03,440
Right?

751
00:56:03,440 --> 00:56:04,440
In some language, yes, sir.

752
00:56:04,440 --> 00:56:06,440
The amygdala does not feel pain.

753
00:56:06,440 --> 00:56:07,440
Right?

754
00:56:07,440 --> 00:56:08,920
Well, yeah, I don't think...

755
00:56:08,920 --> 00:56:11,480
You could say the heart doesn't pump blood, a person does.

756
00:56:11,480 --> 00:56:14,560
No, I think we all feel comfortable saying the heart pumps blood.

757
00:56:14,560 --> 00:56:17,080
You could say, you know, the brain doesn't think the person does.

758
00:56:17,080 --> 00:56:20,680
Well, that's because we identify the person with what the brain does.

759
00:56:20,680 --> 00:56:23,600
So in that sense, we feel more comfortable saying the person thinks.

760
00:56:23,600 --> 00:56:25,840
It's just sensible to say the brain.

761
00:56:25,840 --> 00:56:27,840
Now, the amygdala, you wouldn't feel the pain.

762
00:56:27,840 --> 00:56:31,080
Yeah, but without pain sensors and your periphery, you wouldn't.

763
00:56:31,080 --> 00:56:33,520
So are they feeling pain in your skin?

764
00:56:33,520 --> 00:56:35,520
That's not the question.

765
00:56:35,520 --> 00:56:39,320
No, I'm saying we don't consider your skin as feeling pain.

766
00:56:39,320 --> 00:56:40,320
We don't.

767
00:56:40,320 --> 00:56:41,320
Right?

768
00:56:41,320 --> 00:56:44,160
Or you do when you burn your hand, your skin is moving.

769
00:56:44,160 --> 00:56:46,760
No, it's not your skin feeling pain.

770
00:56:46,760 --> 00:56:48,760
It's not your skin feeling pain.

771
00:56:48,760 --> 00:56:51,440
And then why do you say my skin hurts?

772
00:56:51,440 --> 00:56:56,680
No, because we're very subject to this neurological fallacy, which Aristotle first pointed out.

773
00:56:56,680 --> 00:56:57,680
Yes, right.

774
00:56:57,680 --> 00:57:01,520
There's an importance to that because we start with saying it's my...

775
00:57:01,520 --> 00:57:05,360
I just burnt my finger, my finger is there for hurting.

776
00:57:05,360 --> 00:57:11,000
And out of that observation comes a series of other development and progress that gets

777
00:57:11,000 --> 00:57:16,280
us to the point where you can say, this part, these circuits and this and that got us to

778
00:57:16,280 --> 00:57:18,080
the fact that now I'm perceiving this.

779
00:57:18,080 --> 00:57:19,080
But there's a causal...

780
00:57:19,080 --> 00:57:28,560
There is a... in terms of worrying about subjectivity and the consciousness, there is a gain at

781
00:57:28,560 --> 00:57:32,040
the end, just as there is a gain about understanding any of the...

782
00:57:32,040 --> 00:57:35,280
Interesting facts and causality that we gathered along the way.

783
00:57:35,280 --> 00:57:40,040
What I'm saying is the answers aren't of the form of the original question.

784
00:57:40,040 --> 00:57:43,960
Your five government interests in five governments.

785
00:57:43,960 --> 00:57:45,440
Can't hear you in.

786
00:57:45,440 --> 00:57:47,040
Why is the government interested in...

787
00:57:47,040 --> 00:57:51,640
Well, it's... well, there was a leader, Thomas O'Hil, who was the head of the NIMH and he

788
00:57:51,640 --> 00:57:53,600
had a very narrow view.

789
00:57:53,600 --> 00:57:59,960
I think he would have benefited from a conversation with you about the epistemologic problems of

790
00:57:59,960 --> 00:58:05,280
neuroscience and he felt that we were wasting our time looking for correlations, wasting

791
00:58:05,280 --> 00:58:09,720
our time doing clinical research that didn't have clear biomarkers or neural circuits.

792
00:58:09,720 --> 00:58:12,000
These are the two big terms.

793
00:58:12,000 --> 00:58:17,880
And so he mandated, kind of from the top down, that any grant that would be coming to the

794
00:58:17,880 --> 00:58:23,040
NIMH must make a claim about a biomarker or a neural circuit that was underlying it in

795
00:58:23,040 --> 00:58:24,720
a causal manner.

796
00:58:24,720 --> 00:58:28,480
Now in psychiatry, we have none.

797
00:58:28,480 --> 00:58:31,640
So that puts researchers in a rather strange position.

798
00:58:31,640 --> 00:58:33,760
They have to make something up.

799
00:58:33,760 --> 00:58:36,840
Mr. Ensel, a doctor Ensel, has gone to Google.

800
00:58:36,840 --> 00:58:38,680
We're no doubt he's doing very well.

801
00:58:38,680 --> 00:58:39,680
He left...

802
00:58:39,680 --> 00:58:40,680
He left Google already.

803
00:58:40,680 --> 00:58:42,760
He left the company now.

804
00:58:42,760 --> 00:58:46,360
Okay, but the NIMH to this date, we don't know, they have a new director.

805
00:58:46,360 --> 00:58:48,640
We'll see if they continue with this.

806
00:58:48,640 --> 00:58:58,720
Whether they do or not, it is a really made manifest the way science can become epistemologically

807
00:58:58,720 --> 00:59:03,320
unsound and ideological essentially.

808
00:59:03,320 --> 00:59:04,320
This was essentially ideology.

809
00:59:04,320 --> 00:59:06,640
It wasn't science.

810
00:59:06,640 --> 00:59:09,600
And it's really a hamper research.

811
00:59:09,600 --> 00:59:15,600
The irony was the great victory of Tom Ensel's NIMH came out as he was on his way out the

812
00:59:15,600 --> 00:59:16,600
door.

813
00:59:16,600 --> 00:59:21,680
It was a five-year study that showed that early intervention with young people who were psychotic,

814
00:59:21,680 --> 00:59:27,960
with psychotherapy and family support and medication led to better outcomes.

815
00:59:27,960 --> 00:59:32,400
It was a study that could not have been done with a biomarker or a neural circuit.

816
00:59:32,400 --> 00:59:38,240
So his greatest victory actually seemed to imply that he should not have made these

817
00:59:38,240 --> 00:59:41,720
decisions, but it's presently the law of the land at the NIMH.

818
00:59:41,720 --> 00:59:43,520
As far as I understand.

819
00:59:43,520 --> 00:59:47,880
Maybe I'll say if it's okay.

820
00:59:47,880 --> 00:59:55,600
Okay, I mean I also, in my research, I also, and I'm writing a book really why we need

821
00:59:55,600 --> 01:00:03,400
to move beyond this problem actually of just the subjectivity and consciousness actually.

822
01:00:03,400 --> 01:00:05,000
So mind without matter.

823
01:00:05,000 --> 01:00:08,920
We don't need to be thinking now about that relationship, but I just have to say something

824
01:00:08,920 --> 01:00:17,000
about this, still this idea that there's this insuperable problem about science's objective

825
01:00:17,000 --> 01:00:19,400
and the consciousness is subjective.

826
01:00:19,400 --> 01:00:25,920
Well, you know, no one else, as philosophers, no philosopher has been able to explain what

827
01:00:25,920 --> 01:00:28,560
that means to say science is objective.

828
01:00:28,560 --> 01:00:32,680
It's very difficult to say what that exactly means.

829
01:00:32,680 --> 01:00:38,560
So it's not, it sounds like there's a problem, but it's not clear what that problem is at

830
01:00:38,560 --> 01:00:41,160
all if there is a problem.

831
01:00:41,160 --> 01:00:47,080
So I wouldn't take that as a, okay, we're never going to get there because we have science

832
01:00:47,080 --> 01:00:51,080
as objective and if we don't really know what that means.

833
01:00:51,080 --> 01:00:56,720
I mean, just to defend Ken a little bit about, just so you don't think I'm anti-circuit.

834
01:00:56,720 --> 01:00:58,720
I mean, these are subtle issues.

835
01:00:58,720 --> 01:01:03,160
But if you look at the Sherryton's work on the stretch reflex, in other words, Shankton

836
01:01:03,160 --> 01:01:07,160
at the end of the 19th, early 20th century was very interested in the loop between the

837
01:01:07,160 --> 01:01:09,920
sensation and the reflex, you know, the nature that everyone says.

838
01:01:09,920 --> 01:01:16,720
And we do have a mechanistic understanding that goes beyond correlation or causation.

839
01:01:16,720 --> 01:01:21,840
I mean, when you talk about understanding in science, it's about this object, pushing

840
01:01:21,840 --> 01:01:24,000
on that object and moving it.

841
01:01:24,000 --> 01:01:26,160
It's about interaction between things.

842
01:01:26,160 --> 01:01:30,360
Although not in physics because they don't even talk about causality and physics.

843
01:01:30,360 --> 01:01:35,360
Well, well, I mean, quantum mechanics is a causal to degree, but that's not true at

844
01:01:35,360 --> 01:01:37,280
the meso level they do, right?

845
01:01:37,280 --> 01:01:41,040
So in other words, what I'm saying is, is you can say, and Sherryton did, that this is

846
01:01:41,040 --> 01:01:45,920
a stimulus, it hits the 1A apron, it goes and it makes a monosynaptic connection to the

847
01:01:45,920 --> 01:01:50,280
motor neuron, which then innovates the muscle, and that is really nice.

848
01:01:50,280 --> 01:01:52,760
It's just the kind of work that we like.

849
01:01:52,760 --> 01:01:56,520
And then you look at eye movement, so the very famous engineer at Hopkins called David

850
01:01:56,520 --> 01:02:01,480
Robinson, who beautifully integrated eye movements with the circuitry underlying them in the

851
01:02:01,480 --> 01:02:04,040
brainstem and a mathematical model.

852
01:02:04,040 --> 01:02:10,320
There was an isomorphic click between the model, the anatomy and the behavior.

853
01:02:10,320 --> 01:02:15,160
So there are beautiful successes in neuroscience where you do get the perfect link between

854
01:02:15,160 --> 01:02:18,200
the circuit, the behavior and a computational model.

855
01:02:18,200 --> 01:02:24,400
The problem is, is can we repeat that success when it comes to what we all call the mind

856
01:02:24,400 --> 01:02:25,680
or cognition?

857
01:02:25,680 --> 01:02:29,880
When I mentioned Janelia at the beginning of this session, they say, look, what if we

858
01:02:29,880 --> 01:02:35,840
find the cognitive primitive that is the equivalent of the stretch reflex or an eye movement,

859
01:02:35,840 --> 01:02:38,400
but it's not about the sensory motor, we'll move those out.

860
01:02:38,400 --> 01:02:41,000
It's about that decision in between, right?

861
01:02:41,000 --> 01:02:45,280
That it's the cognitive decision that is invariant to whether you decide to pick up

862
01:02:45,280 --> 01:02:48,320
the apple because you saw it, you smelled it, you can use your mouth, you can use your

863
01:02:48,320 --> 01:02:49,320
hand.

864
01:02:49,320 --> 01:02:54,520
The decision is invariant to the sensory stimulus and to the effector and if we could just understand

865
01:02:54,520 --> 01:03:00,720
the cognitive equivalent of the stretch reflex, we're on our way, right?

866
01:03:00,720 --> 01:03:03,920
And we have the technology now that Sherryton didn't have and David Robinson didn't have,

867
01:03:03,920 --> 01:03:06,440
we can move beyond the sensory motor.

868
01:03:06,440 --> 01:03:08,080
That's what's going on.

869
01:03:08,080 --> 01:03:13,120
And what I'm saying is, is there's a possibility, just like we're seeing with deep mind, we

870
01:03:13,120 --> 01:03:17,040
don't know what all those synaptic weights are doing, but when it gets to cognition of

871
01:03:17,040 --> 01:03:22,320
the form we're interested in, it won't be that cliquey little isomorphic feeling that

872
01:03:22,320 --> 01:03:26,240
eye movements and stretch reflex have.

873
01:03:26,240 --> 01:03:29,680
That's the big dilemma is will we be able to extrapolate, because even for the stretch

874
01:03:29,680 --> 01:03:32,720
reflex, we haven't been able to extrapolate to motor cortex, we still don't know what

875
01:03:32,720 --> 01:03:36,040
motor cortex does and we've had stretch reflex for 100 years.

876
01:03:36,040 --> 01:03:43,640
So I'm just voicing a little bit of skepticism about the simple version and then scaling it

877
01:03:43,640 --> 01:03:44,640
up.

878
01:03:44,640 --> 01:03:50,120
But again, it's, there's a question of in principle, is it impossible or is it, are

879
01:03:50,120 --> 01:03:52,400
we just way premature?

880
01:03:52,400 --> 01:03:56,600
And you know, just as another example, I mean, you know, memory.

881
01:03:56,600 --> 01:04:03,880
So we, that's a psychological process and from neuropsychology, but the neuro was critical.

882
01:04:03,880 --> 01:04:06,240
We've discovered it breaks up into pieces, right?

883
01:04:06,240 --> 01:04:10,760
There's the episodic memory and there's the short term working memory and there's the

884
01:04:10,760 --> 01:04:15,280
short term and long term episodic memory and the ways those that psychology that's neuro

885
01:04:15,280 --> 01:04:18,320
psychology, but we didn't and that's that I love that stuff.

886
01:04:18,320 --> 01:04:19,640
That separates from procedural memory.

887
01:04:19,640 --> 01:04:22,680
And there's not agreement that they don't have agreement about that.

888
01:04:22,680 --> 01:04:26,320
But we never knew any of those things separated until there was a jam, right?

889
01:04:26,320 --> 01:04:30,560
So until there was a guy who lost his ability to make new episodic memories.

890
01:04:30,560 --> 01:04:35,760
Well, that research though is though also I think we can't talk about that research

891
01:04:35,760 --> 01:04:40,160
after you have you read the journalist's book about the research on HM.

892
01:04:40,160 --> 01:04:42,120
I don't think it's, yeah.

893
01:04:42,120 --> 01:04:44,120
That's not going to happen anymore.

894
01:04:44,120 --> 01:04:45,120
Well done.

895
01:04:45,120 --> 01:04:49,000
It was like, no, I mean, you're talking about, you know, the ethics of how it was done.

896
01:04:49,000 --> 01:04:50,480
I'm just talking about the results.

897
01:04:50,480 --> 01:04:51,480
Right.

898
01:04:51,480 --> 01:04:53,480
I mean, we can maybe separate them.

899
01:04:53,480 --> 01:04:55,480
We'll leave them there.

900
01:04:55,480 --> 01:05:03,040
But my point was that a psychological process starts to break up into pieces we were not

901
01:05:03,040 --> 01:05:05,120
aware of that give us insight.

902
01:05:05,120 --> 01:05:06,120
Oh, I love that stuff.

903
01:05:06,120 --> 01:05:07,520
And that's never going to stop.

904
01:05:07,520 --> 01:05:11,240
We're never going to stop breaking it up into pieces that are we didn't know about and

905
01:05:11,240 --> 01:05:14,480
that are going to give us insight into how it works and give us more and more power to

906
01:05:14,480 --> 01:05:15,480
manipulate it.

907
01:05:15,480 --> 01:05:16,480
No, just agreement there.

908
01:05:16,480 --> 01:05:24,840
You know, neuropsychological decomposition into modules is not the same as circuit implementation

909
01:05:24,840 --> 01:05:25,840
in neuroscience.

910
01:05:25,840 --> 01:05:26,840
They're not there.

911
01:05:26,840 --> 01:05:27,840
They're related.

912
01:05:27,840 --> 01:05:28,840
Well, okay.

913
01:05:28,840 --> 01:05:29,840
But that's so a third cause.

914
01:05:29,840 --> 01:05:30,840
All right.

915
01:05:30,840 --> 01:05:33,840
I'm going to turn it on.

916
01:05:33,840 --> 01:05:40,840
Please come to the microphone for asking any questions that you may have.

917
01:05:40,840 --> 01:05:47,840
No, I don't want to worry in principle.

918
01:05:47,840 --> 01:05:50,840
Should there be limits to what we can know?

919
01:05:50,840 --> 01:05:52,640
I'm thinking of evolution.

920
01:05:52,640 --> 01:05:56,640
Obviously, you know, a monkey can't know answer these questions.

921
01:05:56,640 --> 01:06:01,640
And if you think of evolution, our brains have evolved to deal with everyday life.

922
01:06:01,640 --> 01:06:03,640
I'm assuming evolution hasn't stopped.

923
01:06:03,640 --> 01:06:08,480
Something species have come along with brains bigger than ours and can't figure out why

924
01:06:08,480 --> 01:06:10,360
we can't understand things.

925
01:06:10,360 --> 01:06:14,520
So why is it obvious to me that there are questions that we haven't ever answered?

926
01:06:14,520 --> 01:06:15,880
Well, that's a good question.

927
01:06:15,880 --> 01:06:21,640
I mean, I would say we shouldn't take there to be in principle science toppers.

928
01:06:21,640 --> 01:06:22,640
Why?

929
01:06:22,640 --> 01:06:23,720
Say you can't do it.

930
01:06:23,720 --> 01:06:28,960
It's not useful psychologically from a psychological perspective.

931
01:06:28,960 --> 01:06:31,720
Like since we don't know.

932
01:06:31,720 --> 01:06:35,920
We so, yes, of course our brains are a certain size.

933
01:06:35,920 --> 01:06:37,640
We have certain resources.

934
01:06:37,640 --> 01:06:39,440
They're going to be limits.

935
01:06:39,440 --> 01:06:44,800
But why say we can't right now, we're at a point where we can't because in the past

936
01:06:44,800 --> 01:06:50,320
we've seen so many times where scientists have said, oh, we've figured it all out now.

937
01:06:50,320 --> 01:06:53,560
This is something we can never figure out.

938
01:06:53,560 --> 01:06:55,280
And we have after time.

939
01:06:55,280 --> 01:06:56,280
Not these questions.

940
01:06:56,280 --> 01:06:59,520
So the thing that we're just spending your money should do where you think you can have

941
01:06:59,520 --> 01:07:00,520
the most chances of the things.

942
01:07:00,520 --> 01:07:04,640
Yes, so that's why there are practical reasons.

943
01:07:04,640 --> 01:07:10,880
So maybe, you know, this is why maybe it's an argument since some people want the money

944
01:07:10,880 --> 01:07:13,000
and they're trying to argue for it.

945
01:07:13,000 --> 01:07:14,960
So that's a reasonable argument.

946
01:07:14,960 --> 01:07:22,880
But I don't think we can predict the future well enough given how much science has changed

947
01:07:22,880 --> 01:07:23,880
in the past.

948
01:07:23,880 --> 01:07:24,880
It's going to change in the future too.

949
01:07:24,880 --> 01:07:31,800
I think to say this is one area where we're absolutely certain we'll not make progress.

950
01:07:31,800 --> 01:07:32,800
So yes.

951
01:07:32,800 --> 01:07:33,800
I don't think you know what I'm saying.

952
01:07:33,800 --> 01:07:34,800
I think there's another.

953
01:07:34,800 --> 01:07:35,800
No one said that.

954
01:07:35,800 --> 01:07:40,360
Yeah, there's another answer which is that this isn't just about pure science.

955
01:07:40,360 --> 01:07:42,320
It's about pragmatic problems.

956
01:07:42,320 --> 01:07:46,560
You know, as a psychiatrist, if someone comes to me and I take the epistemological standards

957
01:07:46,560 --> 01:07:49,680
that we've been talking about today, I say, I really have no idea.

958
01:07:49,680 --> 01:07:53,680
I have no idea about almost anything at this level of certainty and causal certainty.

959
01:07:53,680 --> 01:07:56,720
And yet, here is a person who needs help.

960
01:07:56,720 --> 01:08:02,040
So I have to try to think in a very dirty way from a scientific point of view about what's

961
01:08:02,040 --> 01:08:09,600
pragmatically and what's pragmatically effective, what kind of correlations exist, what things

962
01:08:09,600 --> 01:08:16,240
like my experience have shown me that give me some sort of dirty attempt to try to help

963
01:08:16,240 --> 01:08:17,240
this person.

964
01:08:17,240 --> 01:08:18,840
So I think that's just one experience.

965
01:08:18,840 --> 01:08:24,760
I think there's a whole array of other pragmatic reasons why saying this isn't unanswerable

966
01:08:24,760 --> 01:08:29,560
leads to, it doesn't have to lead to fake answers.

967
01:08:29,560 --> 01:08:35,120
It can lead to hypotheses that you know are not confirmed that are working in hypotheses

968
01:08:35,120 --> 01:08:37,400
that help you work through a problem.

969
01:08:37,400 --> 01:08:41,000
Well, I would someone study consciousness for instance.

970
01:08:41,000 --> 01:08:43,880
I don't think there's any sort of human knowledge benefit.

971
01:08:43,880 --> 01:08:47,400
And for me, that's obvious one where we're not going to have an answer.

972
01:08:47,400 --> 01:08:48,400
It's really one possible.

973
01:08:48,400 --> 01:08:55,000
Well again, we're going to have lots of answers about what neuroprocesses happen when you

974
01:08:55,000 --> 01:08:56,800
are and aren't conscious.

975
01:08:56,800 --> 01:08:59,120
But so those will have certain kinds of answers.

976
01:08:59,120 --> 01:09:00,360
I think, well, of course.

977
01:09:00,360 --> 01:09:02,960
And also, I mean, why study consciousness?

978
01:09:02,960 --> 01:09:06,600
I mean, there's several, there's different ways in which one can study consciousness.

979
01:09:06,600 --> 01:09:10,840
I mean, since consciousness is actually available for introspection, it is something we all

980
01:09:10,840 --> 01:09:14,600
experience because it is experience.

981
01:09:14,600 --> 01:09:19,920
It certainly is available for all kinds of investigation and explanation of a gist of

982
01:09:19,920 --> 01:09:25,480
a different sort than you might have with respect to its neural substructure.

983
01:09:25,480 --> 01:09:28,280
So then why do it?

984
01:09:28,280 --> 01:09:30,960
Well, because actually it's the world in which we all live.

985
01:09:30,960 --> 01:09:37,120
I mean, it was something that we all care deeply about because it's something we are

986
01:09:37,120 --> 01:09:41,000
intimately familiar with from waking up to going to sleep.

987
01:09:41,000 --> 01:09:46,320
So certain jetties and consciousnesses, if it's irrelevant, seems to basically leave

988
01:09:46,320 --> 01:09:48,920
behind the entire world.

989
01:09:48,920 --> 01:09:53,040
And who would want to do that?

990
01:09:53,040 --> 01:09:55,640
So like, you know, it could be out there with the science of correlations like you were

991
01:09:55,640 --> 01:10:02,440
describing, the whole neural-carlate program, which I think is underway for a while now,

992
01:10:02,440 --> 01:10:08,840
and or it could be some other form of phenomenological science where actually you really look hard

993
01:10:08,840 --> 01:10:14,360
on your own experiences and those are the reports of others.

994
01:10:14,360 --> 01:10:17,080
And also, you don't know what type of spin-offs it might have.

995
01:10:17,080 --> 01:10:21,520
With pure research now, we don't really know what these correlations would do.

996
01:10:21,520 --> 01:10:23,280
You don't know what type of...

997
01:10:23,280 --> 01:10:26,760
And if people are motivated, if there's some people that are interested, well then they'll

998
01:10:26,760 --> 01:10:31,600
probably work really hard and maybe do some good research.

999
01:10:31,600 --> 01:10:39,200
I have a wonderful scientist, MIT, Harvard, Emory Brown, who've done some fascinating

1000
01:10:39,200 --> 01:10:45,440
work on the differences between being under anesthesia and versus being asleep, for example,

1001
01:10:45,440 --> 01:10:48,560
and being sedated and doing what Ken was talking about.

1002
01:10:48,560 --> 01:10:54,360
Very interesting differences in the sub-component neuroanatomy and physiological processes versus

1003
01:10:54,360 --> 01:10:58,080
immuno-reuse sleep, sedated, or under anesthesia of different kinds.

1004
01:10:58,080 --> 01:11:04,200
It says, wonderful, basic work being done in hospitals where patients are being put under

1005
01:11:04,200 --> 01:11:05,200
anesthesia.

1006
01:11:05,200 --> 01:11:09,200
In fact, Emory Brown's work to me is some of the nicest work showing how you can do in

1007
01:11:09,200 --> 01:11:16,280
parallel mechanistic modular work and work that's going to be important for patients.

1008
01:11:16,280 --> 01:11:19,680
The problem with translation, I mean, with a wonderful paper that came out last year by

1009
01:11:19,680 --> 01:11:26,160
Unida Senjoyna at how little of what the NIH has funded has translated.

1010
01:11:26,160 --> 01:11:31,320
And one of the reasons they put forward is because it's this horrible reductionist neuromolecular

1011
01:11:31,320 --> 01:11:36,240
program that we're going to solve disease, one molecule at a time, one gene at a time.

1012
01:11:36,240 --> 01:11:42,000
And actually, if you notice now, many people are considerably their sanguine now about

1013
01:11:42,000 --> 01:11:47,080
what they think this is going to lead to, the former head of the NIH and Blanchion's name

1014
01:11:47,080 --> 01:11:50,040
now, he was interviewed recently about Thomas Insol.

1015
01:11:50,040 --> 01:11:54,800
No, I mean, he was saying, you know, we've got hundreds and hundreds of genes associated

1016
01:11:54,800 --> 01:11:55,800
with all these things.

1017
01:11:55,800 --> 01:11:59,960
We haven't got the slightest clue what to do about it.

1018
01:11:59,960 --> 01:12:06,320
Now, when it comes to things like diabetes, hypertension, stroke, heart disease, and,

1019
01:12:06,320 --> 01:12:11,960
you know, obesity, cigarette smoking, opioid addiction, you know, these chronic, low-end

1020
01:12:11,960 --> 01:12:17,520
lifelong conditions that require behavioral intervention, we're very bad because we want

1021
01:12:17,520 --> 01:12:22,680
a delta function, we want a pill or a surgery or a zap.

1022
01:12:22,680 --> 01:12:27,400
The idea, and just so you understand the absurdity of it, if I had said to you, take this pill,

1023
01:12:27,400 --> 01:12:33,400
sir, swallow it, and you'll be an expert pianist, right?

1024
01:12:33,400 --> 01:12:36,960
Take this pill and you don't need to go to college, all four years of college, you've

1025
01:12:36,960 --> 01:12:38,280
been compressed into this pill.

1026
01:12:38,280 --> 01:12:42,240
You rightly go, there's something nonsensical about that.

1027
01:12:42,240 --> 01:12:47,880
But once you get to medicine, switch you being healthy and taking a pill to be an expert,

1028
01:12:47,880 --> 01:12:51,440
violinist or pianist, but take this pill and we'll cure your multiple sclerosis, your

1029
01:12:51,440 --> 01:12:52,440
Alzheimer's.

1030
01:12:52,440 --> 01:12:56,480
That suddenly makes more sense, right?

1031
01:12:56,480 --> 01:12:57,480
Right.

1032
01:12:57,480 --> 01:13:01,560
So what I'm saying is it's going to turn out that we're going to have to do a lot more

1033
01:13:01,560 --> 01:13:07,080
human-behavioral modular work than mass models of Alzheimer's.

1034
01:13:07,080 --> 01:13:08,080
I'm not against them.

1035
01:13:08,080 --> 01:13:11,480
I do think there'll be pharmacological interventions.

1036
01:13:11,480 --> 01:13:13,640
I do mouse studies.

1037
01:13:13,640 --> 01:13:22,160
But again, you're totally correct that there is an equivalent fallacy in medicine of a

1038
01:13:22,160 --> 01:13:27,160
reductionist kind, just like there is in neuroscience, right?

1039
01:13:27,160 --> 01:13:32,280
That we're going to solve diseases, one drug, one molecule, one gene at a time.

1040
01:13:32,280 --> 01:13:37,160
And if you look at these papers by Unidists and others, it's been for the most part of

1041
01:13:37,160 --> 01:13:38,160
failure.

1042
01:13:38,160 --> 01:13:41,360
Now, I'm going to get into lots of trouble saying that, but I'm just quoting them.

1043
01:13:41,360 --> 01:13:42,360
Yeah.

1044
01:13:42,360 --> 01:13:51,760
Well, the other sort of pie-in-the-sky translational outcome of studying consciousness through neuroscience,

1045
01:13:51,760 --> 01:13:56,680
of course, would have to do with end-of-life issues, whether one of patients in a coma.

1046
01:13:56,680 --> 01:14:02,240
So you need to, you know, I mean, it's a lot of complicated issues here.

1047
01:14:02,240 --> 01:14:05,200
Do we, is this over?

1048
01:14:05,200 --> 01:14:07,400
Is there anything worth living for?

1049
01:14:07,400 --> 01:14:12,360
You know, is, is, if there is subject to consciousness, does that make life, you know,

1050
01:14:12,360 --> 01:14:17,360
so it's a very complicated thing to think about that requires a lot of interdisciplinary

1051
01:14:17,360 --> 01:14:18,360
thought.

1052
01:14:18,360 --> 01:14:24,680
But that could be, you know, a principle, a possible translational issue.

1053
01:14:24,680 --> 01:14:29,000
Hey, this is a question that Sam Harris addresses.

1054
01:14:29,000 --> 01:14:34,280
I think in the book, Consciousness, but don't hold me to that.

1055
01:14:34,280 --> 01:14:41,160
If we can never understand this subject, or if it's a long way off, you never say never,

1056
01:14:41,160 --> 01:14:49,680
either way, pattern, you want to go by, how can we be responsible for our actions?

1057
01:14:49,680 --> 01:14:55,240
Well, I'm not sure if I actually see the connection, to be honest.

1058
01:14:55,240 --> 01:15:02,760
I mean, I'm, yeah, your question is the perfect question, because it's exactly the perfect

1059
01:15:02,760 --> 01:15:08,680
question, because it's exactly the way not to think about these things.

1060
01:15:08,680 --> 01:15:15,000
We're talking about a social psychological being, we call the you.

1061
01:15:15,000 --> 01:15:21,960
When we talk about, you know, is the brain deterministic, or, you know, where, where does

1062
01:15:21,960 --> 01:15:27,520
that choice live, nearly, that's a whole nother level that I don't think we have any good

1063
01:15:27,520 --> 01:15:28,520
clear answers.

1064
01:15:28,520 --> 01:15:32,640
But it doesn't, it doesn't, I mean, when we talk, it's a practical matter.

1065
01:15:32,640 --> 01:15:38,400
We talk about people being responsible, in part, because we're either trying to, you

1066
01:15:38,400 --> 01:15:42,680
know, cause interventions to change their behavior, or cause interventions to lock them

1067
01:15:42,680 --> 01:15:47,880
away so they don't bother us again, or we're doing things in the human world on the basis

1068
01:15:47,880 --> 01:15:50,600
of morality and responsibility.

1069
01:15:50,600 --> 01:15:55,400
And the fact that things have physical causes is at a whole completely different level.

1070
01:15:55,400 --> 01:15:59,520
And Jonathan, I'm sure, could talk about this far better, I mean, it's more 19th century

1071
01:15:59,520 --> 01:16:09,320
than 18th century, but the sort of medicalization of, you know, behavioral states, I mean, you

1072
01:16:09,320 --> 01:16:14,520
know, homosexuality, for example, it went from being a legal issue to being the most

1073
01:16:14,520 --> 01:16:16,000
medicalized issue.

1074
01:16:16,000 --> 01:16:18,800
Everyone had a theory about this, right?

1075
01:16:18,800 --> 01:16:23,320
And I'm saying that the equivalent of this medicalization of things that shouldn't be

1076
01:16:23,320 --> 01:16:30,760
medicalized, I think in a way, neuroscience is doing the same thing.

1077
01:16:30,760 --> 01:16:35,160
It's walking over areas it should not be trespassing on.

1078
01:16:35,160 --> 01:16:41,120
I would, okay, I need to say something, because I would say it's not the exact wrong question.

1079
01:16:41,120 --> 01:16:42,960
I think there is an issue here.

1080
01:16:42,960 --> 01:16:49,840
And I do think as society in courts of law, and it does, sometimes we do take away responsibility

1081
01:16:49,840 --> 01:16:52,720
when we find an underlying physical cause.

1082
01:16:52,720 --> 01:16:58,960
And I think this is something that it's a very, I mean, I think we're not going to always

1083
01:16:58,960 --> 01:17:04,640
say it's just the person we always have free will, but it's a very delicate balance.

1084
01:17:04,640 --> 01:17:12,840
And we have to think about it carefully, and just make sure that, I mean, yeah, sometimes

1085
01:17:12,840 --> 01:17:13,840
we do take it away.

1086
01:17:13,840 --> 01:17:14,840
But it's a mistake.

1087
01:17:14,840 --> 01:17:16,840
I don't know if it's always a mistake.

1088
01:17:16,840 --> 01:17:19,880
Well, but I mean, you know, it's not always a mistake.

1089
01:17:19,880 --> 01:17:22,760
Sometimes it makes sense, I think, to take it away.

1090
01:17:22,760 --> 01:17:28,360
It's no different in kind from the question of my responsible, if my childhood experience,

1091
01:17:28,360 --> 01:17:29,840
you know, let me down this path.

1092
01:17:29,840 --> 01:17:31,240
It's just, it's a difficult question.

1093
01:17:31,240 --> 01:17:32,240
And it's a difficult question.

1094
01:17:32,240 --> 01:17:35,440
There's, you have to take it to so many issues.

1095
01:17:35,440 --> 01:17:40,960
Yeah, it's a similar, we don't have a solution, but one way or the other, it's complicated.

1096
01:17:40,960 --> 01:17:46,560
It depends on part of, to go back to Baslan-Drasen's language, like what question you're asking.

1097
01:17:46,560 --> 01:17:52,720
So there's a, like, for example, they just, sure, John and Ken read the article today,

1098
01:17:52,720 --> 01:17:55,840
or yesterday's times, about the brain of the shooter in Las Vegas.

1099
01:17:55,840 --> 01:17:56,840
Yeah, I see.

1100
01:17:56,840 --> 01:17:58,960
They sliced it, diced it, looked at it.

1101
01:17:58,960 --> 01:17:59,960
Interesting.

1102
01:17:59,960 --> 01:18:01,600
You know, and they came up with it.

1103
01:18:01,600 --> 01:18:05,360
No, we can't tell anything that's different about this brain, that would explain it.

1104
01:18:05,360 --> 01:18:10,800
But I know it's, but even if they had, but my point in the end, I think it's a little

1105
01:18:10,800 --> 01:18:15,200
bit of a, my point is both to kind of yet another example of neuromania, but also, but

1106
01:18:15,200 --> 01:18:20,160
also because, because if you think about what they were asking, it was not if we actually

1107
01:18:20,160 --> 01:18:26,160
found some sort of underlying, you know, abnormality that would explain the recent, you know,

1108
01:18:26,160 --> 01:18:30,840
that would explain the atrocity that we then therefore excuse him for it in retrospect.

1109
01:18:30,840 --> 01:18:35,200
It's more like, would this provide an account of the, you know, the change in behavior that

1110
01:18:35,200 --> 01:18:39,960
led up to an atrocity that he would have he had survived, still be put in jail for?

1111
01:18:39,960 --> 01:18:42,760
I think that's a reasonable question.

1112
01:18:42,760 --> 01:18:45,960
That's a, it's a question different, however, from a legal question.

1113
01:18:45,960 --> 01:18:48,120
And I think sometimes they often get confused.

1114
01:18:48,120 --> 01:18:52,320
And I think that, then especially when you bring in something as, you know, as difficult

1115
01:18:52,320 --> 01:18:55,720
as free will talk, it then just becomes a mess.

1116
01:18:55,720 --> 01:18:56,720
Yeah.

1117
01:18:56,720 --> 01:18:58,240
And we would love to keep them separate.

1118
01:18:58,240 --> 01:19:03,880
It's much easier to say the moral, political, social world is dominated by notions of free

1119
01:19:03,880 --> 01:19:04,880
will.

1120
01:19:04,880 --> 01:19:05,880
We need to operate that way.

1121
01:19:05,880 --> 01:19:11,200
If we work at the level of biological determinism, that doesn't really correspond to that.

1122
01:19:11,200 --> 01:19:13,880
And so we'll, this will go over here and that'll go over there.

1123
01:19:13,880 --> 01:19:18,560
But then if you want to think of a field, forensic psychiatry is when the two mix.

1124
01:19:18,560 --> 01:19:23,360
And you know, the, someone is going to be making decisions about whether someone had

1125
01:19:23,360 --> 01:19:29,200
intentionality and free will and, and civilized society since the 17th century have said,

1126
01:19:29,200 --> 01:19:34,200
you know, if you're start raving mad and you commit a crime, that's going to be different

1127
01:19:34,200 --> 01:19:36,200
than otherwise.

1128
01:19:36,200 --> 01:19:40,040
You know, I think embarrassingly some of these studies are like, I mean, these people go

1129
01:19:40,040 --> 01:19:42,120
to eat a Chinese restaurants too much.

1130
01:19:42,120 --> 01:19:44,440
They think you open the brain, it's going to be like a fortune cookie and insights.

1131
01:19:44,440 --> 01:19:48,800
I wanted to kill a lot of people, but they never find anything like that.

1132
01:19:48,800 --> 01:19:55,840
And just to get to the notion of explanation so you can get some sense, you know, Fred

1133
01:19:55,840 --> 01:19:58,600
is a teatotler.

1134
01:19:58,600 --> 01:20:00,080
Why?

1135
01:20:00,080 --> 01:20:02,040
His father was an alcoholic.

1136
01:20:02,040 --> 01:20:04,040
Ah, yes.

1137
01:20:04,040 --> 01:20:06,040
Do you see what I'm saying?

1138
01:20:06,040 --> 01:20:07,040
Yeah.

1139
01:20:07,040 --> 01:20:11,040
We can create these pseudo explanations right at any level, right?

1140
01:20:11,040 --> 01:20:13,040
And it's very tempting.

1141
01:20:13,040 --> 01:20:15,040
We live by them.

1142
01:20:15,040 --> 01:20:33,040
And what I'm, and what I'm saying is, as I think I, and the demonstrate, they're not,

1143
01:20:33,040 --> 01:20:34,040
right?

1144
01:20:34,040 --> 01:20:38,320
But we, we like them.

1145
01:20:38,320 --> 01:20:44,160
And we now have a new kind of nonsensical explanation like that, which is neuromania,

1146
01:20:44,160 --> 01:20:45,160
right?

1147
01:20:45,160 --> 01:20:52,160
We like to have this new addition to these pseudo explanations that as you say, we live

1148
01:20:52,160 --> 01:20:53,160
by.

1149
01:20:53,160 --> 01:20:59,000
You know, from, at least from the literary sort of history of science and philosophy perspective

1150
01:20:59,000 --> 01:21:00,000
that I would have.

1151
01:21:00,000 --> 01:21:04,200
I don't think I said anything like consciousness is in the brain.

1152
01:21:04,200 --> 01:21:11,800
I think I would just, you know, would say that consciousness requires, for our species,

1153
01:21:11,800 --> 01:21:17,760
requires a brain, but is embodied and is about, you know, interacting with the world.

1154
01:21:17,760 --> 01:21:22,320
And in other kinds of creatures like, you know, the famous octopus, for example, it's

1155
01:21:22,320 --> 01:21:25,120
a much more of a whole body phenomenon.

1156
01:21:25,120 --> 01:21:30,000
And arguably is out there in the world too, if you take the kind of panpsychists take

1157
01:21:30,000 --> 01:21:33,760
on the, on the heart problem, which I think is very attractive and interesting.

1158
01:21:33,760 --> 01:21:35,760
So why not?

1159
01:21:35,760 --> 01:21:42,800
I mean, you know, but as I look at this far as I understand it, I think professional

1160
01:21:42,800 --> 01:21:46,560
it's taken saying that I just find it, something kind of cool about it.

1161
01:21:46,560 --> 01:21:50,880
So, so yeah, so I, you know, at least for us, it's like it requires the brain.

1162
01:21:50,880 --> 01:21:51,880
That's all I would say.

1163
01:21:51,880 --> 01:21:53,600
But I'm sure these folks who disagree.

1164
01:21:53,600 --> 01:21:59,080
I mean, brain is a particular organization of neurons and the nervous system.

1165
01:21:59,080 --> 01:22:03,680
It's limo type of nervous system too, that, you know, and plants have certain kinds of

1166
01:22:03,680 --> 01:22:08,760
memory and they have it through physical processes in their cells that, you know, don't correspond

1167
01:22:08,760 --> 01:22:10,760
to neurons, but it's their own system.

1168
01:22:10,760 --> 01:22:16,200
So there's lots of ways of building memories out of, out of cells and ours gets organized

1169
01:22:16,200 --> 01:22:17,200
into brains.

1170
01:22:17,200 --> 01:22:23,480
And embodiment is one of those very slippery, very cool subjects right now.

1171
01:22:23,480 --> 01:22:28,520
I mean, Andy Klartz written famously about this, but I think one way to think about it

1172
01:22:28,520 --> 01:22:33,840
is Dennis Noble talks about top-down causality when it comes to the heartbeat.

1173
01:22:33,840 --> 01:22:38,760
So you can take a cardiac cell and it will start beating in the right medium.

1174
01:22:38,760 --> 01:22:44,080
And then you say, what's the cause of the heartbeat?

1175
01:22:44,080 --> 01:22:49,240
Now you can look at the ion channels in that myocardial cell and go, oh, look, those ion

1176
01:22:49,240 --> 01:22:54,200
channels, it's through the flow of ions through those channels that needs to a change in the

1177
01:22:54,200 --> 01:22:56,000
membrane of the heart cell.

1178
01:22:56,000 --> 01:22:59,440
But then you have to go, well, if those ion channels were just floating around alone,

1179
01:22:59,440 --> 01:23:00,960
there'd be no heartbeat.

1180
01:23:00,960 --> 01:23:04,800
They have to be embedded in a heart cell.

1181
01:23:04,800 --> 01:23:07,000
So then where do you point to the heartbeat?

1182
01:23:07,000 --> 01:23:08,440
What's the cause of the heartbeat?

1183
01:23:08,440 --> 01:23:11,600
The whole thing is the heartbeat.

1184
01:23:11,600 --> 01:23:15,880
There is no thing that you can point at, which is more fundamental of the heartbeat.

1185
01:23:15,880 --> 01:23:18,080
You need the whole thing, right?

1186
01:23:18,080 --> 01:23:20,400
And that's what embodiment is.

1187
01:23:20,400 --> 01:23:25,680
Is that you can't avoid being trans-level when you talk about the heartbeat.

1188
01:23:25,680 --> 01:23:28,840
The membrane is not doing it alone, the ion channels are not doing it alone, the ions

1189
01:23:28,840 --> 01:23:29,840
aren't doing it alone.

1190
01:23:29,840 --> 01:23:30,840
It's the whole thing.

1191
01:23:30,840 --> 01:23:35,640
So that's really what embodiment is, is that you need the whole thing in its organizational

1192
01:23:35,640 --> 01:23:40,880
structure to do the thing that you're seeing in its totality, right?

1193
01:23:40,880 --> 01:23:45,680
And that's, I think, the reasonable way of thinking about embodiment is ended up having

1194
01:23:45,680 --> 01:23:52,720
a sort of hokey, slightly holistic thing that's attached to it, but that's separate.

1195
01:23:52,720 --> 01:23:53,720
Okay.

1196
01:23:53,720 --> 01:23:54,720
Thank you.

1197
01:23:54,720 --> 01:23:55,720
Thank you.

1198
01:23:55,720 --> 01:23:56,720
Thank you.

1199
01:23:56,720 --> 01:23:57,720
Thank you.

1200
01:23:57,720 --> 01:23:58,720
Thank you.

1201
01:23:58,720 --> 01:23:59,720
Thank you.

1202
01:23:59,720 --> 01:24:12,280
Thank you.

