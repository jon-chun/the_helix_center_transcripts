1
00:00:00,000 --> 00:00:01,040
My name is Beth Haus.

2
00:00:01,040 --> 00:00:02,760
I'm a psychiatrist in private practice

3
00:00:02,760 --> 00:00:04,640
and a member of the executive committee

4
00:00:04,640 --> 00:00:06,440
for the Helix Center.

5
00:00:06,440 --> 00:00:08,640
And I've been asked to moderate this a little bit last minute.

6
00:00:08,640 --> 00:00:10,320
So bear with me.

7
00:00:10,320 --> 00:00:11,200
It's not my area.

8
00:00:11,200 --> 00:00:13,600
But we're here today to talk about secrecy, privacy,

9
00:00:13,600 --> 00:00:17,240
transparency, openness, and related concepts.

10
00:00:17,240 --> 00:00:19,560
Since Freud's early twin psychopathology

11
00:00:19,560 --> 00:00:21,800
stemmed from secrets that one part of the mind kept

12
00:00:21,800 --> 00:00:26,040
from another to win a cots idea for the development of self

13
00:00:26,040 --> 00:00:27,760
and to allow the child to overcome shame

14
00:00:27,760 --> 00:00:29,920
and narcissistic humiliation, psychoanalysis

15
00:00:29,920 --> 00:00:32,000
has always been interested in secrets.

16
00:00:32,000 --> 00:00:33,400
And Western literature also has been

17
00:00:33,400 --> 00:00:36,720
interested in secrets from medipists to Prometheus Bound,

18
00:00:36,720 --> 00:00:39,240
where the main character spends the entire play resisting

19
00:00:39,240 --> 00:00:41,640
the attempt of one character after another

20
00:00:41,640 --> 00:00:43,440
to get a secret out from him.

21
00:00:43,440 --> 00:00:45,800
So the secret and its gradual revelation

22
00:00:45,800 --> 00:00:50,160
serves many, many literary and psychoanalytic functions.

23
00:00:50,160 --> 00:00:52,280
And liberal political thought is obviously

24
00:00:52,280 --> 00:00:55,360
predicated on the autonomy of individual selves

25
00:00:55,360 --> 00:00:58,320
and the maintenance of secrets as an aspect of selfhood.

26
00:00:58,320 --> 00:01:00,640
So we're here to talk about these ideas today

27
00:01:00,640 --> 00:01:03,200
and an age of NSA surveillance and prison access

28
00:01:03,200 --> 00:01:07,600
to our Google and Facebook and Yahoo records, drones, big data,

29
00:01:07,600 --> 00:01:10,400
and the internet creation of secret and revealed selves.

30
00:01:10,400 --> 00:01:12,040
And we will look to our wonderful panelists

31
00:01:12,040 --> 00:01:14,600
to help us open up these concepts of secrecy

32
00:01:14,600 --> 00:01:16,920
and transparency and their current relevance.

33
00:01:16,920 --> 00:01:18,760
So I'm going to introduce the panelists now.

34
00:01:18,760 --> 00:01:21,520
We are pleased to have here with us today.

35
00:01:21,520 --> 00:01:24,280
Alex Abdo on the far left here.

36
00:01:24,280 --> 00:01:26,760
Alex is a staff attorney for the ACLU

37
00:01:26,760 --> 00:01:29,160
and for their national security project.

38
00:01:29,160 --> 00:01:32,080
He is the current counsel to the ACLU's challenge

39
00:01:32,080 --> 00:01:35,280
to the NSA's phone records program

40
00:01:35,280 --> 00:01:37,880
and has been involved in the litigation of cases concerning

41
00:01:37,880 --> 00:01:41,560
the Patriot Act and the Foreign Intelligence Surveillance

42
00:01:41,560 --> 00:01:44,440
Act, the International Emergency Economic Powers

43
00:01:44,440 --> 00:01:47,200
Act, and the treatment of detainees at Guantanamo

44
00:01:47,200 --> 00:01:48,640
and elsewhere.

45
00:01:48,640 --> 00:01:50,840
Alex is a graduate of Yale University and Harvard Law

46
00:01:50,840 --> 00:01:53,720
School, and prior to working at the ACLU

47
00:01:53,720 --> 00:01:56,480
clerked for several different judges.

48
00:01:56,480 --> 00:01:58,200
And he recently participated in a panel

49
00:01:58,200 --> 00:02:00,440
at the University of Pennsylvania,

50
00:02:00,440 --> 00:02:03,640
which was called on the very idea of secret laws,

51
00:02:03,640 --> 00:02:07,280
transparency and publicity, and deliberative democracy.

52
00:02:07,280 --> 00:02:09,160
So welcome, Alex.

53
00:02:09,160 --> 00:02:10,840
Next, we have Jack Braddich over here,

54
00:02:10,840 --> 00:02:13,360
who is Associate Professor and Chair of the Journalism

55
00:02:13,360 --> 00:02:16,840
and Media Studies Department at Rutgers University.

56
00:02:16,840 --> 00:02:19,000
His work applies autonomous social theory

57
00:02:19,000 --> 00:02:22,680
to popular culture and social movement media.

58
00:02:22,680 --> 00:02:25,720
He is the author of Conspiracy Panics,

59
00:02:25,720 --> 00:02:29,960
Political Rationality in Popular Culture in 2008.

60
00:02:29,960 --> 00:02:33,120
And his most recent publications include Adventures

61
00:02:33,120 --> 00:02:35,560
in the Public Secrets Sphere, Police,

62
00:02:35,560 --> 00:02:38,760
Sovereign Networks, and Communications Warfare

63
00:02:38,760 --> 00:02:41,040
in Cultural Studies, Critical Methodologies.

64
00:02:41,040 --> 00:02:41,560
Is that right?

65
00:02:41,560 --> 00:02:42,840
OK.

66
00:02:42,840 --> 00:02:45,360
He's also a Zion librarian at the ABC in Nel Rio

67
00:02:45,360 --> 00:02:49,080
and has co-taught courses at Blue Stocking's Bookstore

68
00:02:49,080 --> 00:02:50,320
in New York City.

69
00:02:50,320 --> 00:02:51,040
So welcome, Anna.

70
00:02:51,040 --> 00:02:53,120
Welcome.

71
00:02:53,120 --> 00:02:55,760
Our next panelist is Ted Jacobs.

72
00:02:55,760 --> 00:02:57,640
Ted is a psychiatrist and psychoanalyst

73
00:02:57,640 --> 00:02:59,720
at Albert Einstein College of Medicine

74
00:02:59,720 --> 00:03:01,960
in the NYU School of Medicine.

75
00:03:01,960 --> 00:03:04,000
He's also a training and supervising analyst

76
00:03:04,000 --> 00:03:06,360
for adults and children, both here at Nipsey

77
00:03:06,360 --> 00:03:07,920
and at four other psychoanalytic

78
00:03:07,920 --> 00:03:09,760
institutes around the country.

79
00:03:09,760 --> 00:03:13,480
And the editor of at least four analytic journals.

80
00:03:13,480 --> 00:03:16,520
Dr. Jacobs has published over 60 papers and book reviews,

81
00:03:16,520 --> 00:03:21,000
and among them are Secrets, Alliances, and Family Fictions,

82
00:03:21,000 --> 00:03:25,280
Psychoanalytic Observations, and Notes on the Unknowable,

83
00:03:25,280 --> 00:03:28,560
Analytic Secrets, and the Transfer Ensnorosis.

84
00:03:28,560 --> 00:03:31,120
He has two new books out for 2013,

85
00:03:31,120 --> 00:03:32,760
neither of which have much to do with Secrets.

86
00:03:32,760 --> 00:03:35,360
One is The Year of Du Roche, a novel

87
00:03:35,360 --> 00:03:37,840
about the turmoil created when the manager of the Brooklyn

88
00:03:37,840 --> 00:03:40,840
Dodgers moved to a different team, the New York Giants,

89
00:03:40,840 --> 00:03:43,800
in 1948, and the possible profession,

90
00:03:43,800 --> 00:03:45,920
the analytic process of change.

91
00:03:45,920 --> 00:03:47,280
So welcome.

92
00:03:47,280 --> 00:03:48,960
And finally, we have Michael Lewis.

93
00:03:48,960 --> 00:03:51,280
Michael is a university distinguished professor

94
00:03:51,280 --> 00:03:53,800
of pediatrics in psychiatry and director

95
00:03:53,800 --> 00:03:56,080
of the Institute for the Study of Child Development

96
00:03:56,080 --> 00:03:58,680
at Rutgers Robert Wood Johnson School.

97
00:03:58,680 --> 00:04:01,240
His research has focused on normal and deviant

98
00:04:01,240 --> 00:04:04,520
emotional and intellectual development.

99
00:04:04,520 --> 00:04:07,840
And among his books are Lies and Deception in Everyday Life,

100
00:04:07,840 --> 00:04:10,080
which he has brought with us here,

101
00:04:10,080 --> 00:04:12,880
and a handbook of emotional development, which

102
00:04:12,880 --> 00:04:15,800
was awarded the Critics' Choice Award.

103
00:04:15,800 --> 00:04:18,360
Most recently, his latest book, which he also has with him,

104
00:04:18,360 --> 00:04:20,200
is The Rise of Consciousness and the Development

105
00:04:20,200 --> 00:04:23,520
Devotion, Emotional Life, which is after 2014.

106
00:04:23,520 --> 00:04:26,920
And he's also working on a new book, My Life as Development.

107
00:04:26,920 --> 00:04:27,800
So these are our panelists.

108
00:04:27,800 --> 00:04:29,280
I hope that they will help us get access

109
00:04:29,280 --> 00:04:31,760
to the psychology and national psychology of secrecy

110
00:04:31,760 --> 00:04:33,920
and transparency.

111
00:04:33,920 --> 00:04:36,840
I wanted to start out just with a quote from JFK.

112
00:04:36,840 --> 00:04:40,200
The word secrecy is repugnant and free and open society.

113
00:04:40,200 --> 00:04:42,440
And we as a people are inherently and historically

114
00:04:42,440 --> 00:04:45,200
opposed to secret societies, secret oaths,

115
00:04:45,200 --> 00:04:47,280
and secret proceedings.

116
00:04:47,280 --> 00:04:50,080
It seems like JFK was not particularly up to date,

117
00:04:50,080 --> 00:04:52,680
but I would start out with the question related to that.

118
00:04:52,680 --> 00:04:55,240
What is our modern conception of secrecy, privacy,

119
00:04:55,240 --> 00:04:56,600
transparency, and openness?

120
00:04:56,600 --> 00:04:58,840
And is there something uniquely American about the way

121
00:04:58,840 --> 00:04:59,640
that we approach it?

122
00:04:59,640 --> 00:05:04,000
So leave it to you guys to open that.

123
00:05:04,000 --> 00:05:09,880
Well, I will be talking just before we came up.

124
00:05:09,880 --> 00:05:16,440
And many of you will remember that Eisenhower was

125
00:05:16,440 --> 00:05:19,280
going to meet with the Russian Premier.

126
00:05:19,280 --> 00:05:21,720
It was a bridge level.

127
00:05:21,720 --> 00:05:22,520
It was Khrushchev.

128
00:05:22,520 --> 00:05:23,640
I don't remember.

129
00:05:23,640 --> 00:05:25,800
Must have been Khrushchev.

130
00:05:25,800 --> 00:05:32,760
And right before they were to meet a YouTube plane

131
00:05:32,760 --> 00:05:38,520
with powers flew over Russia taking pictures.

132
00:05:38,520 --> 00:05:41,280
And they shot the plane down.

133
00:05:41,280 --> 00:05:44,720
And so they had us.

134
00:05:44,720 --> 00:05:50,880
Eisenhower was asked, did you have knowledge of this?

135
00:05:50,880 --> 00:05:51,320
He said yes.

136
00:05:54,280 --> 00:06:01,520
That terminated the first meeting since Yolta or Otslan

137
00:06:01,520 --> 00:06:06,040
between American President, Russian Premier.

138
00:06:06,040 --> 00:06:13,120
Because in diplomacy, it is recognized

139
00:06:13,120 --> 00:06:20,080
that indeed if you tell the truth, that's an insult.

140
00:06:20,080 --> 00:06:26,240
Have he said, I didn't know if I have this as you do this large

141
00:06:26,240 --> 00:06:29,400
government and all sorts of things happen,

142
00:06:29,400 --> 00:06:34,680
Khrushchev would have, the meeting would have taken place.

143
00:06:34,680 --> 00:06:39,320
That he chose Eisenhower, the all-American,

144
00:06:39,320 --> 00:06:47,920
chose to tell the truth, which from a diplomatic point of view

145
00:06:47,920 --> 00:06:55,120
was not the thing to do to have that meeting take place.

146
00:06:55,120 --> 00:07:03,080
So I guess I'm going to start off as sort of the other side

147
00:07:03,080 --> 00:07:04,640
of the view.

148
00:07:04,640 --> 00:07:10,480
I think that secrecy, deception, and lying

149
00:07:10,480 --> 00:07:15,560
have a very important place in human life.

150
00:07:15,560 --> 00:07:23,760
And I spend my time at as legal or helping folks

151
00:07:23,760 --> 00:07:28,600
with their problems, media issues.

152
00:07:28,600 --> 00:07:30,440
I study development.

153
00:07:30,440 --> 00:07:34,360
I study infants and their development.

154
00:07:34,360 --> 00:07:36,640
And one of the extraordinary things

155
00:07:36,640 --> 00:07:40,680
is how early children lie.

156
00:07:44,280 --> 00:07:48,280
By two years of age, children have pretend

157
00:07:48,280 --> 00:07:51,920
plague, which is a kind of self-deception.

158
00:07:51,920 --> 00:07:56,440
They lie to spare the feelings of others,

159
00:07:56,440 --> 00:08:04,320
and they lie not to get punished for transgressions.

160
00:08:04,320 --> 00:08:08,480
So and we are not unique.

161
00:08:08,480 --> 00:08:09,480
This goes on.

162
00:08:09,480 --> 00:08:10,760
We studied in Japan.

163
00:08:10,760 --> 00:08:13,440
We studied in Europe.

164
00:08:13,440 --> 00:08:18,440
This is all children as best we could say.

165
00:08:18,440 --> 00:08:27,720
So extraordinary early in life, secrecy, deception

166
00:08:27,720 --> 00:08:31,000
is part of human psyche.

167
00:08:31,000 --> 00:08:33,200
I'll stop.

168
00:08:33,200 --> 00:08:37,120
I agree with you that there is a very important place

169
00:08:37,120 --> 00:08:40,480
for secrecy and deception in our private lives.

170
00:08:40,480 --> 00:08:42,720
But I think in America, we've always

171
00:08:42,720 --> 00:08:45,120
had a double standard when it comes to secrecy.

172
00:08:45,120 --> 00:08:49,400
We expect to maintain our privacy fears,

173
00:08:49,400 --> 00:08:50,920
but we expect that the government will not

174
00:08:50,920 --> 00:08:53,560
maintain a significant privacy fear.

175
00:08:53,560 --> 00:08:56,400
We expect that the government will, to the extent possible,

176
00:08:56,400 --> 00:08:58,040
I think, be open and transparent.

177
00:08:58,040 --> 00:09:00,200
Although you're right, that there's obviously even

178
00:09:00,200 --> 00:09:03,200
there in a very important role for secrecy to play.

179
00:09:03,200 --> 00:09:07,760
Diplomacy doesn't work in the open generally.

180
00:09:07,760 --> 00:09:12,600
Many national security policies can't function if entirely open.

181
00:09:12,600 --> 00:09:14,960
But there's always been a deep-seated hostility,

182
00:09:14,960 --> 00:09:17,680
I think, toward excessive secrecy in government.

183
00:09:17,680 --> 00:09:20,960
And the question that, when we were discussing a moment ago,

184
00:09:20,960 --> 00:09:22,800
that we at the ACLU are concerned about

185
00:09:22,800 --> 00:09:28,400
is the question of when excessive government

186
00:09:28,400 --> 00:09:30,960
makes democratic self-governance difficult.

187
00:09:30,960 --> 00:09:32,520
And maybe another way of thinking about it,

188
00:09:32,520 --> 00:09:34,400
though, is just to take it on the terms

189
00:09:34,400 --> 00:09:36,120
that you've started us out with.

190
00:09:36,120 --> 00:09:40,920
If secrecy is so important for private development

191
00:09:40,920 --> 00:09:43,320
and private life, which I agree with you, it is,

192
00:09:43,320 --> 00:09:46,240
you can think of surveillance, government surveillance,

193
00:09:46,240 --> 00:09:48,960
as being what undoes private secrecy,

194
00:09:48,960 --> 00:09:52,000
what unmasks self-deception.

195
00:09:52,000 --> 00:09:54,560
It gets behind the veil that we construct for ourselves

196
00:09:54,560 --> 00:09:57,480
in maintaining a private and dignified life.

197
00:09:57,480 --> 00:09:59,480
And so there's a question as to what extent

198
00:09:59,480 --> 00:10:00,680
should we allow that surveillance.

199
00:10:00,680 --> 00:10:03,680
And obviously, we have decided there should be some of it.

200
00:10:03,680 --> 00:10:05,480
And to what extent should we allow secrecy

201
00:10:05,480 --> 00:10:07,920
about the extent of that surveillance, which

202
00:10:07,920 --> 00:10:09,880
is another interrelated question?

203
00:10:12,520 --> 00:10:15,760
Yeah, I've been following up on that, too.

204
00:10:15,760 --> 00:10:20,040
Around democracy, I really like the figure of Eisenhower.

205
00:10:20,040 --> 00:10:21,920
So he could have said, as you said,

206
00:10:21,920 --> 00:10:23,760
he could have said, yes, we have secrets,

207
00:10:23,760 --> 00:10:25,680
but I didn't know about this one.

208
00:10:25,680 --> 00:10:26,800
It's an acknowledgment.

209
00:10:26,800 --> 00:10:29,680
But he said, yes, we have secrets, and I know about them.

210
00:10:29,680 --> 00:10:30,840
So I mean, those are two different ways

211
00:10:30,840 --> 00:10:33,600
of revealing one's knowledge about secrecy.

212
00:10:33,600 --> 00:10:38,400
And I think it's that tactical approach

213
00:10:38,400 --> 00:10:42,760
to revelation that interests me as part of US history,

214
00:10:42,760 --> 00:10:44,560
so around democracy.

215
00:10:44,560 --> 00:10:48,960
So you think about the emergence of the American Revolution.

216
00:10:48,960 --> 00:10:51,880
So much of it was also hatched in things like lodges,

217
00:10:51,880 --> 00:10:54,520
and free Masonic lodges, and secret societies.

218
00:10:54,520 --> 00:10:58,720
So here's this fundamental moment of the foundation

219
00:10:58,720 --> 00:11:01,400
of democracy itself that's also partially hatched.

220
00:11:01,400 --> 00:11:03,440
In some people saying, well, there's

221
00:11:03,440 --> 00:11:05,120
a certain kind of secrecy that we don't like,

222
00:11:05,120 --> 00:11:07,240
monarchical secrecy.

223
00:11:07,240 --> 00:11:09,640
But we're going to start plotting some things in secret,

224
00:11:09,640 --> 00:11:12,480
because we have to do that, because it's a revolution.

225
00:11:12,480 --> 00:11:17,080
So there are moments that, to me, the issue

226
00:11:17,080 --> 00:11:21,640
is who gets to decide when secrecy and transparency

227
00:11:21,640 --> 00:11:23,480
are allowable.

228
00:11:23,480 --> 00:11:26,920
So to me, is the interest around sovereignty.

229
00:11:26,920 --> 00:11:29,120
Is there something, a difference between,

230
00:11:29,120 --> 00:11:31,560
say, a popular secrecy and something

231
00:11:31,560 --> 00:11:32,720
more like state secrecy?

232
00:11:32,720 --> 00:11:34,120
Can we think about something about secrets

233
00:11:34,120 --> 00:11:37,680
that belong to a population that's part of democracy

234
00:11:37,680 --> 00:11:40,400
and not just antithetical to democracy?

235
00:11:43,040 --> 00:11:48,160
In some sense, secrets, just like I

236
00:11:48,160 --> 00:11:53,280
think who it was who said that all politics

237
00:11:53,280 --> 00:11:55,000
is local.

238
00:11:55,000 --> 00:11:58,680
So it relates to the community, and even

239
00:11:58,680 --> 00:12:01,400
for its national politics, people

240
00:12:01,400 --> 00:12:05,920
tend to experience it in terms of their own community,

241
00:12:05,920 --> 00:12:07,720
their own private lives.

242
00:12:07,720 --> 00:12:12,880
Secrets also are always, in some deep sense, always personal,

243
00:12:12,880 --> 00:12:17,120
in the sense that they resonate with our own experience

244
00:12:17,120 --> 00:12:20,560
with the secrets and the deepest part of ourselves.

245
00:12:20,560 --> 00:12:25,520
As you've said, Dr. Lewis, that secrets are always

246
00:12:25,520 --> 00:12:28,440
a part of development and a necessary,

247
00:12:28,440 --> 00:12:31,360
can you imagine if a child didn't have any secrets

248
00:12:31,360 --> 00:12:35,400
that the parents knew everything about the child?

249
00:12:35,400 --> 00:12:37,060
There'd be no sense of separateness,

250
00:12:37,060 --> 00:12:39,560
no sense of development.

251
00:12:39,560 --> 00:12:43,240
And the larger question that Alex raises

252
00:12:43,240 --> 00:12:47,040
is when does this necessary aspect of secret

253
00:12:47,040 --> 00:12:51,960
become intolerable, and he says, well,

254
00:12:51,960 --> 00:12:53,760
a certain amount of state secrets

255
00:12:53,760 --> 00:12:56,120
are necessary for any government to exist?

256
00:12:59,880 --> 00:13:04,600
If the Germans knew where we were going to land and D-day,

257
00:13:04,600 --> 00:13:07,760
we would have lost thousands of more troops,

258
00:13:07,760 --> 00:13:10,440
or maybe even the war.

259
00:13:10,440 --> 00:13:14,080
But at what point does the individual

260
00:13:14,080 --> 00:13:20,640
right to privacy conflict with the national interest

261
00:13:20,640 --> 00:13:27,360
in, let's say, discovering secret plots that may go on?

262
00:13:27,360 --> 00:13:33,440
If the government had been able to discover the secrets

263
00:13:33,440 --> 00:13:39,640
of the 9-11 bombers as they were preparing their plans,

264
00:13:39,640 --> 00:13:41,360
we would have been saved a great deal.

265
00:13:41,360 --> 00:13:47,160
So let me ask Alex, in your work that you've been doing,

266
00:13:47,160 --> 00:13:49,640
have you been able to define at all

267
00:13:49,640 --> 00:13:54,040
that borderline between the necessary and the intrusive

268
00:13:54,040 --> 00:13:57,520
and immoral aspects of secrets?

269
00:13:57,520 --> 00:13:58,520
Let me, yeah.

270
00:14:01,720 --> 00:14:11,040
I think let's some agreement between

271
00:14:11,040 --> 00:14:16,360
having them and how much is a good thing.

272
00:14:16,360 --> 00:14:21,280
So I think that's a lot of progress, really,

273
00:14:21,280 --> 00:14:24,640
and just as an opening, because we

274
00:14:24,640 --> 00:14:30,520
could have folks sitting here as panelists who

275
00:14:30,520 --> 00:14:34,280
would say that secrets aren't good.

276
00:14:34,280 --> 00:14:38,560
So for example, there's a very famous philosopher,

277
00:14:38,560 --> 00:14:43,440
a boy who talks about lying, as I said,

278
00:14:43,440 --> 00:14:48,840
secrets as a first-door-no-moral failure.

279
00:14:48,840 --> 00:14:52,800
No, one could argue with that.

280
00:14:52,800 --> 00:14:58,520
What I think might concern us is what

281
00:14:58,520 --> 00:15:01,320
has changed for those of us who have lived longer

282
00:15:01,320 --> 00:15:08,040
than in terms of what is public and what was not public.

283
00:15:08,040 --> 00:15:13,240
So how many of us have not experienced

284
00:15:13,240 --> 00:15:17,760
walking down the street and having someone in full voice

285
00:15:17,760 --> 00:15:22,880
in a conversation on the phone with someone to which we could

286
00:15:22,880 --> 00:15:27,760
listen in to or go to the movies where people talk

287
00:15:27,760 --> 00:15:33,120
and the idea of private versus public behavior?

288
00:15:33,120 --> 00:15:41,360
So I think there's been a real transformation in this move

289
00:15:41,360 --> 00:15:46,840
toward public behavior, that private behavior.

290
00:15:46,840 --> 00:15:49,640
And I think it's part of this movement

291
00:15:49,640 --> 00:15:56,880
that is probably at least 50 to 70 years old in this society

292
00:15:56,880 --> 00:16:03,120
in which we are not supposed to have secrets,

293
00:16:03,120 --> 00:16:06,960
that good relationships don't involve privacy.

294
00:16:09,840 --> 00:16:12,640
If you go to a dinner party and it

295
00:16:12,640 --> 00:16:17,520
wasn't a good dinner or pleasant company,

296
00:16:17,520 --> 00:16:23,560
and you can say something about it,

297
00:16:23,560 --> 00:16:26,040
instead of what we used to do was write,

298
00:16:26,040 --> 00:16:28,160
remember we actually wrote things,

299
00:16:28,160 --> 00:16:32,040
and we sent the thank you note for inviting us

300
00:16:32,040 --> 00:16:33,600
to their home and so on.

301
00:16:33,600 --> 00:16:36,760
So I think there's an enormous change.

302
00:16:36,760 --> 00:16:40,440
And I think there's a lot of pathology

303
00:16:40,440 --> 00:16:44,280
that goes on with both the society at large, which

304
00:16:44,280 --> 00:16:47,760
will mean government, will be communicative,

305
00:16:47,760 --> 00:16:49,720
and individuals.

306
00:16:49,720 --> 00:16:55,000
I think that we're supposed to not have a secret.

307
00:16:55,000 --> 00:16:56,760
If you get a present that you don't

308
00:16:56,760 --> 00:16:59,880
like from your grandmother, who has

309
00:16:59,880 --> 00:17:04,040
netted you with arthritic fingers, a sweater,

310
00:17:04,040 --> 00:17:07,320
you're supposed to say you like it.

311
00:17:07,320 --> 00:17:10,880
But people chafe with that idea.

312
00:17:10,880 --> 00:17:13,360
You're not being honest with your grandmother.

313
00:17:13,360 --> 00:17:17,120
You should tell your grandmother the truth.

314
00:17:17,120 --> 00:17:23,080
So I think there's a strange thing going on in our society.

315
00:17:23,080 --> 00:17:27,400
In this domain of public private, which, of course,

316
00:17:27,400 --> 00:17:30,560
is some sense what we're talking about.

317
00:17:30,560 --> 00:17:34,720
As you were saying, private, we have to have private.

318
00:17:34,720 --> 00:17:39,240
But this idea is we're not supposed to.

319
00:17:39,240 --> 00:17:45,240
My wife comes home with a jacket that I don't like,

320
00:17:45,240 --> 00:17:47,720
and I'm supposed to tell her I don't like it.

321
00:17:47,720 --> 00:17:52,720
So all sorts of things are going on.

322
00:17:52,720 --> 00:17:54,720
And I think at multiple levels.

323
00:17:58,720 --> 00:18:03,720
And I think that we have to understand them in that context.

324
00:18:03,720 --> 00:18:08,720
I don't know historically whether in this country there

325
00:18:08,720 --> 00:18:12,720
was ever a period of time when the government didn't have secrets.

326
00:18:12,720 --> 00:18:16,720
I mean, it's inconceivable to me that you could have a government

327
00:18:16,720 --> 00:18:20,720
in the same way that they would say you can have a person,

328
00:18:20,720 --> 00:18:23,720
a child, who didn't have secrets.

329
00:18:23,720 --> 00:18:30,720
Any functionally organization has to have secret handshake,

330
00:18:30,720 --> 00:18:38,720
secret code, secret language, in fact.

331
00:18:38,720 --> 00:18:43,720
I don't know that I disagree with your kind of description

332
00:18:43,720 --> 00:18:45,720
of how a society has changed over the last 50 or 60 years.

333
00:18:45,720 --> 00:18:48,720
I'm not sure I know enough to agree or disagree.

334
00:18:48,720 --> 00:18:51,720
But one of the questions that concerns us so much

335
00:18:51,720 --> 00:18:56,720
is to what extent the government gets to capitalize on those changes.

336
00:18:56,720 --> 00:19:00,720
So if you look back at the late 1700s,

337
00:19:00,720 --> 00:19:03,720
one of the reasons that the early Americans rebelled

338
00:19:03,720 --> 00:19:05,720
was actually about secrecy.

339
00:19:05,720 --> 00:19:09,720
One of the last straws was the fact that King George insisted

340
00:19:09,720 --> 00:19:13,720
on using similar authority that was being used in England,

341
00:19:13,720 --> 00:19:17,720
what were known as general warrants in the colonies as well.

342
00:19:17,720 --> 00:19:21,720
And these were issued by courts and they gave constables the authority

343
00:19:21,720 --> 00:19:25,720
to enter homes at will to search for tax evaders

344
00:19:25,720 --> 00:19:29,720
and people who were evading import taxes.

345
00:19:29,720 --> 00:19:33,720
And that was the foundation of the Fourth Amendment.

346
00:19:33,720 --> 00:19:35,720
That was where our right to privacy came from,

347
00:19:35,720 --> 00:19:42,720
was a notion that unregulated government access to our secret lives is wrong.

348
00:19:42,720 --> 00:19:45,720
But then for the next 200 years, the primary protector of secrecy

349
00:19:45,720 --> 00:19:47,720
was not actually legal.

350
00:19:47,720 --> 00:19:50,720
The primary protector of secrecy in the country was practical.

351
00:19:50,720 --> 00:19:54,720
The government couldn't record all of the information that was being spread around in public

352
00:19:54,720 --> 00:19:56,720
and maintain it in that basis.

353
00:19:56,720 --> 00:20:01,720
And I think the real governmental shift over the last 15 years

354
00:20:01,720 --> 00:20:05,720
is that, you know, and this is actually a really kind of fundamental shift

355
00:20:05,720 --> 00:20:07,720
in the way intelligence gathering takes place,

356
00:20:07,720 --> 00:20:11,720
is that it is now possible for the government to keep a record

357
00:20:11,720 --> 00:20:15,720
of all of these conversations that are spoken in public

358
00:20:15,720 --> 00:20:19,720
or to keep track of all of the digital trails we leave

359
00:20:19,720 --> 00:20:22,720
whenever we interact with an internet service.

360
00:20:22,720 --> 00:20:25,720
Because those things are, in a sense, made public.

361
00:20:25,720 --> 00:20:28,720
We share them with Google, we share them with Yahoo.

362
00:20:28,720 --> 00:20:31,720
But now the government can keep track of them, can keep them forever.

363
00:20:31,720 --> 00:20:36,720
And that search first, you know, search first suspicion later approach

364
00:20:36,720 --> 00:20:39,720
to surveillance is really, you know, a revolution

365
00:20:39,720 --> 00:20:42,720
in the way our agencies conduct intelligence gathering.

366
00:20:42,720 --> 00:20:45,720
And so the question is, is that okay?

367
00:20:45,720 --> 00:20:48,720
And if it's not, why isn't it?

368
00:20:48,720 --> 00:20:52,720
If you're right that this is information that we're just volunteering anyway,

369
00:20:52,720 --> 00:20:55,720
why do we care if the government keeps a repository?

370
00:20:55,720 --> 00:21:01,720
And, you know, one answer is that, you know, when you,

371
00:21:01,720 --> 00:21:06,720
and this is a phrase I heard recently, when you calcify the present,

372
00:21:06,720 --> 00:21:11,720
you might say to yourself, suppose you had the world's best detective

373
00:21:11,720 --> 00:21:14,720
and he were always able to reconstruct the past.

374
00:21:14,720 --> 00:21:15,720
Would that be a bad thing?

375
00:21:15,720 --> 00:21:16,720
You know, maybe that's a great thing.

376
00:21:16,720 --> 00:21:19,720
Maybe you want a police officer who's always able to reconstruct

377
00:21:19,720 --> 00:21:21,720
the investigative path.

378
00:21:21,720 --> 00:21:23,720
And a lot, you know, one logical leap from that is,

379
00:21:23,720 --> 00:21:26,720
well, why not preserve the path now so you don't have to rely

380
00:21:26,720 --> 00:21:28,720
on the superhuman investigator.

381
00:21:28,720 --> 00:21:31,720
And then you can just give the government access to these databases of information.

382
00:21:31,720 --> 00:21:34,720
And the answer to that question I think is that when you calcify the present

383
00:21:34,720 --> 00:21:43,720
in that way, when you get rid of the practical protection for secrecy we had for 200 years,

384
00:21:43,720 --> 00:21:45,720
you also encumber the present.

385
00:21:45,720 --> 00:21:50,720
Every current decision is encumbered by the possibility of future liability,

386
00:21:50,720 --> 00:21:56,720
not necessarily legal liability, but the possibility that in 20 years something you said

387
00:21:56,720 --> 00:22:02,720
will be uncovered in a way that never existed, that there was in essence a right to be forgotten

388
00:22:02,720 --> 00:22:07,720
for most of our country's history, and that is being practically threatened.

389
00:22:07,720 --> 00:22:14,720
You know, we now live in a world where virtually everything we do is digitally tracked in some way or other.

390
00:22:14,720 --> 00:22:19,720
And we have to wrestle with the question of whether we want to preserve this practical ability

391
00:22:19,720 --> 00:22:21,720
to have those actions forgotten or not.

392
00:22:21,720 --> 00:22:28,720
Well, I don't want to want to talk about the government,

393
00:22:28,720 --> 00:22:33,720
but you don't object to governments having secrets,

394
00:22:33,720 --> 00:22:39,720
we object to them knowing about our secrets, that is knowing about our private life.

395
00:22:39,720 --> 00:22:41,720
Is that the argument?

396
00:22:41,720 --> 00:22:43,720
Are governments allowed to have secrets?

397
00:22:43,720 --> 00:22:46,720
Because I don't see how they could function.

398
00:22:46,720 --> 00:22:48,720
I think they have a question they can have secrets.

399
00:22:48,720 --> 00:22:51,720
The question is what level of secrecy is allowable in a democracy?

400
00:22:51,720 --> 00:22:53,720
And how about recoverable?

401
00:22:53,720 --> 00:22:54,720
Recoverable?

402
00:22:54,720 --> 00:23:01,720
Well, in the sense that you know what they know about you, freedom of information, act, and so on.

403
00:23:01,720 --> 00:23:03,720
There was an attempt there.

404
00:23:03,720 --> 00:23:09,720
The government makes records, takes, gets information about people,

405
00:23:09,720 --> 00:23:12,720
and you have a right to know what they know.

406
00:23:12,720 --> 00:23:15,720
Is that a safe gone?

407
00:23:15,720 --> 00:23:20,720
I think it is, but it's kind of telling in our country that that's a statutory right and not a constitutional one.

408
00:23:20,720 --> 00:23:25,720
You would think in a democracy that the Constitution would protect access to information about government.

409
00:23:25,720 --> 00:23:28,720
But in fact, the Supreme Court has held it doesn't.

410
00:23:28,720 --> 00:23:31,720
There's not actually a constitutional right to that information.

411
00:23:31,720 --> 00:23:36,720
And the statutory right, and as someone who litigates a lot under the Freedom of Information Act,

412
00:23:36,720 --> 00:23:38,720
I can tell you is a weak one.

413
00:23:38,720 --> 00:23:44,720
And so the question, if you went back to the 70s,

414
00:23:44,720 --> 00:23:46,720
which is when the Foreign Intelligence Surveillance Act was passed,

415
00:23:46,720 --> 00:23:50,720
and this is the main law that the government has used for national security intelligence gathering

416
00:23:50,720 --> 00:23:56,720
for the last quarter century, 35 years, you could have made an argument that in 1978,

417
00:23:56,720 --> 00:24:04,720
that statute itself should have been secret, that our prospects in the Cold War would have been better,

418
00:24:04,720 --> 00:24:13,720
had the KGB not been able to read the law that governed the intelligence activities of the NSA.

419
00:24:13,720 --> 00:24:14,720
But we rejected that.

420
00:24:14,720 --> 00:24:18,720
We didn't, because there was a recognition that certain things should be public.

421
00:24:18,720 --> 00:24:22,720
And so I agree, the government's need secrets.

422
00:24:22,720 --> 00:24:27,720
The very difficult question is when the secret becomes illegitimate.

423
00:24:27,720 --> 00:24:33,720
And to take the modern day example, you have Edward Snowden who leaked the existence of a program

424
00:24:33,720 --> 00:24:37,720
in which every phone record, every single day, is being captured by the NSA.

425
00:24:37,720 --> 00:24:41,720
The phone call that you're talking about made on the streets of New York City,

426
00:24:41,720 --> 00:24:45,720
by tomorrow morning, will be in an NSA database.

427
00:24:45,720 --> 00:24:50,720
Is that a secret that the government should be allowed to keep?

428
00:24:50,720 --> 00:24:53,720
And I think that's a difficult question in a democracy.

429
00:24:53,720 --> 00:24:55,720
We have a strong view of what the answer is.

430
00:24:55,720 --> 00:24:59,720
But not everyone agrees with us. And it's a difficult question.

431
00:24:59,720 --> 00:25:05,720
At the same time that there has been a kind of maybe defanging of FOIA,

432
00:25:05,720 --> 00:25:08,720
has there also been an increase, my understanding,

433
00:25:08,720 --> 00:25:12,720
is an increase in classification of certain kinds of domains of government activity.

434
00:25:12,720 --> 00:25:16,720
So at the same moment that public is disempowered,

435
00:25:16,720 --> 00:25:21,720
increasing disempowered from accessing things, more and more things are becoming inaccessible.

436
00:25:21,720 --> 00:25:24,720
Is that a kind of characterization of the recent?

437
00:25:24,720 --> 00:25:30,720
Every government official to have left government in the past 30 years has complained about overclassification,

438
00:25:30,720 --> 00:25:31,720
but even more of late.

439
00:25:31,720 --> 00:25:35,720
And they're now, I think, I forgot what the exact number is,

440
00:25:35,720 --> 00:25:39,720
but millions of people who have security clearances to access this trove of information

441
00:25:39,720 --> 00:25:41,720
that is growing exponentially.

442
00:25:41,720 --> 00:25:45,720
Particularly in the digital age, things are now born classified, they say,

443
00:25:45,720 --> 00:25:48,720
because they're derivative of other digital documents.

444
00:25:48,720 --> 00:25:51,720
You email something around internally on a classified network,

445
00:25:51,720 --> 00:25:55,720
and all of a sudden you have a whole new classified chain of information.

446
00:25:55,720 --> 00:25:57,720
So yeah, you're right.

447
00:25:57,720 --> 00:26:01,720
To me, some of the changes I think that are happening in the political environment include,

448
00:26:01,720 --> 00:26:05,720
precisely what you've been talking about, an expansion of a national security state,

449
00:26:05,720 --> 00:26:09,720
specifically in the last dozen years or so around terror war, right?

450
00:26:09,720 --> 00:26:15,720
And a kind of war context and a war environment that makes every question about freedom,

451
00:26:15,720 --> 00:26:18,720
democracy suddenly tied to security.

452
00:26:18,720 --> 00:26:24,720
And the security suddenly becomes the most important kind of value through which all these others have to be run through.

453
00:26:24,720 --> 00:26:29,720
So that kind of discursive environment is something I think that's changed over the last 15 years.

454
00:26:29,720 --> 00:26:33,720
The other thing that I think has changed a bit, which is my interest,

455
00:26:33,720 --> 00:26:40,720
is how does the do get revealed take place in a public sphere, and it almost doesn't matter, right?

456
00:26:40,720 --> 00:26:47,720
So one of the things I look at is the early Bush years, and some of the things that did get revealed,

457
00:26:47,720 --> 00:26:50,720
and all the books that came out around, you know,

458
00:26:50,720 --> 00:26:55,720
Bush is a liar, Bush is this and that, and then it made no difference for the election in 2004.

459
00:26:55,720 --> 00:27:01,720
Because it's almost like there's a belief somehow in American belief that if you expose secrets that leads to action,

460
00:27:01,720 --> 00:27:05,720
and I think that is the connection that I'm interested in, how do we move from information to action,

461
00:27:05,720 --> 00:27:09,720
and does exposure and revelation actually lead to action,

462
00:27:09,720 --> 00:27:13,720
or is there something else going on that might prevent that from happening?

463
00:27:13,720 --> 00:27:14,720
So that's sort of my interest.

464
00:27:14,720 --> 00:27:19,720
And my favorite figure during that time period is one of the masters of revealing secrets,

465
00:27:19,720 --> 00:27:24,720
and that was Donald Rumsfeld, right, who was this, I mean, just listen to these press conferences,

466
00:27:24,720 --> 00:27:29,720
and he's almost a wizard at this, where he says things like,

467
00:27:29,720 --> 00:27:34,720
remember that list of the known and the unknown that he brought up,

468
00:27:34,720 --> 00:27:38,720
but there are these known knowns, then there are the known unknowns about Iraq,

469
00:27:38,720 --> 00:27:41,720
where these are justifications, the unknown unknowns.

470
00:27:41,720 --> 00:27:46,720
Right, what he forgot though, he only mentioned three, right, and then that two by two mono hybrid,

471
00:27:46,720 --> 00:27:50,720
there's a fourth, right, which is the unknown knowns.

472
00:27:50,720 --> 00:27:51,720
Is that right?

473
00:27:51,720 --> 00:27:52,720
Yeah, right.

474
00:27:52,720 --> 00:27:53,720
So he didn't mention that one.

475
00:27:53,720 --> 00:27:55,720
Okay, well, what is that one about?

476
00:27:55,720 --> 00:28:00,720
And so that for me is the interesting moment of things that we know are happening,

477
00:28:00,720 --> 00:28:03,720
but we forget, right, or we deny, or we repress.

478
00:28:03,720 --> 00:28:08,720
So things like Abu Ghraib, I mean, it was a revelation of particular activities that were being done,

479
00:28:08,720 --> 00:28:15,720
but atrocities during warfare are something that we almost have to forget in order to engage in it as a society.

480
00:28:15,720 --> 00:28:21,720
So these moments of things that get revealed at the same time other things get concealed at the very same moment,

481
00:28:21,720 --> 00:28:24,720
I was sort of tracking that as a cultural media phenomenon.

482
00:28:24,720 --> 00:28:26,720
And I think a lot of that has changed too.

483
00:28:26,720 --> 00:28:28,720
So we get revelations.

484
00:28:28,720 --> 00:28:32,720
I like before the panel started, the roundtable panel on the screen we had,

485
00:28:32,720 --> 00:28:40,720
my friend Trevor Paglin was up there, the artist who studies government secrets and tries to map them and visualize them.

486
00:28:40,720 --> 00:28:46,720
He's collected the patches of units in the military and such that are secret units, right,

487
00:28:46,720 --> 00:28:48,720
and he finds their patches and he's collected them.

488
00:28:48,720 --> 00:28:52,720
He did some work on Area 51 before it was acknowledged that it existed, right,

489
00:28:52,720 --> 00:28:57,720
which just happened in the last year or two, right, before that Area 51 never existed.

490
00:28:57,720 --> 00:28:59,720
So he wouldn't try to give tours of it.

491
00:28:59,720 --> 00:29:02,720
Great artists, performance artists around this.

492
00:29:02,720 --> 00:29:09,720
Anyway, so that kind of question too, like what, you know, even if things get revealed, what happens?

493
00:29:09,720 --> 00:29:15,720
What happens now that Area 51 is revealed or, you know, more documents about the Kennedy assassination system start coming out, right?

494
00:29:15,720 --> 00:29:19,720
I mean, what is the action component that to me is the crucial part of democracy,

495
00:29:19,720 --> 00:29:23,720
not just the things that we know about the government, but the things that we can do about it.

496
00:29:23,720 --> 00:29:27,720
And I think that's one of the things that's so frightening about the NSA leaks,

497
00:29:27,720 --> 00:29:32,720
is that people have this feeling now that they're releasing this massive amount of information and anonymity,

498
00:29:32,720 --> 00:29:34,720
and that someday in the future it's going to come back and bite them,

499
00:29:34,720 --> 00:29:38,720
but it's very hard for them to conceptually keep it in mind as to when would that be.

500
00:29:38,720 --> 00:29:48,720
Let's take this back down to interpersonal, to sort of get it back,

501
00:29:48,720 --> 00:29:55,720
and I think Ted and I represent more that perspective.

502
00:29:55,720 --> 00:30:07,720
So it is not possible for a parent to go on the computer of their child and find out things about their child.

503
00:30:07,720 --> 00:30:19,720
So let's talk about parents and kids and the technology that allows parents to find out things about their kids

504
00:30:19,720 --> 00:30:22,720
and what we might feel about that.

505
00:30:22,720 --> 00:30:24,720
Because you couldn't do that before.

506
00:30:24,720 --> 00:30:32,720
Now, since you would open mail and so on, but it is now possible with computers in the home to do that.

507
00:30:32,720 --> 00:30:39,720
Then I think there would be more argument about whether that was a good thing or a bad thing,

508
00:30:39,720 --> 00:30:42,720
or how to regulate that and so on.

509
00:30:42,720 --> 00:30:44,720
I don't have an answer to that.

510
00:30:44,720 --> 00:30:50,720
I'm just sort of trying to bring your point about new technology,

511
00:30:50,720 --> 00:30:57,720
changing the complexity of the problem we have to making it now and to personally.

512
00:30:57,720 --> 00:31:04,720
I mean, we could even do it not therapeutically, but now with brain imaging and all this kind of work

513
00:31:04,720 --> 00:31:15,720
that is leading to find out what is going on in someone's head without asking them or without interacting with them.

514
00:31:15,720 --> 00:31:27,720
So what we have is a technology that is allowing all sorts of challenges to this idea of privacy

515
00:31:27,720 --> 00:31:34,720
or secrecy, whether it be at the level of the individual to the government or the level of the person to themselves,

516
00:31:34,720 --> 00:31:38,720
to the therapist, to parents and so on.

517
00:31:38,720 --> 00:31:40,720
That technology is changing that.

518
00:31:40,720 --> 00:31:50,720
The people have responded so much to this idea of the government listening in on phone calls,

519
00:31:50,720 --> 00:31:57,720
listening, being able to obtain private messages that people send to one another.

520
00:31:57,720 --> 00:32:01,720
It's a sense of violation that goes back to early childhood.

521
00:32:01,720 --> 00:32:04,720
I mean, that child is entitled to have some privacy.

522
00:32:04,720 --> 00:32:13,720
The parent who is looking at the diary, looking at the computer is violating something very essential in the child's development.

523
00:32:13,720 --> 00:32:17,720
But it's an interesting question, Alex.

524
00:32:17,720 --> 00:32:28,720
Let's just say, for argument's sake, that there are some people in this country sent by some foreign power

525
00:32:28,720 --> 00:32:36,720
and they're planning another attack of a major kind, a 9-11.

526
00:32:36,720 --> 00:32:44,720
And I think most people would say if we can identify those individuals or have some suspicion,

527
00:32:44,720 --> 00:32:55,720
we're entitled to tap their phones, get whatever information we can to avoid this massive disaster.

528
00:32:55,720 --> 00:33:02,720
And what if the government says, well, but we can't actually do this without listening to everybody,

529
00:33:02,720 --> 00:33:08,720
because maybe they have some contacts here with private citizens or US citizens

530
00:33:08,720 --> 00:33:12,720
that we won't know about unless we listen to everybody.

531
00:33:12,720 --> 00:33:17,720
Is that justified on the basis of a clear and present danger?

532
00:33:17,720 --> 00:33:23,720
Or should we say, no, you can't do that regardless, even if there's a danger,

533
00:33:23,720 --> 00:33:28,720
because that's just too invasive of an individual's rights.

534
00:33:28,720 --> 00:33:33,720
You know, I would dispute the factual premise of the question,

535
00:33:33,720 --> 00:33:38,720
which is that you actually would ever need to listen to everyone's communications

536
00:33:38,720 --> 00:33:41,720
in order to conduct a targeted investigation.

537
00:33:41,720 --> 00:33:46,720
But even if it were the case, you could prevent an extraordinary amount of crime in this country.

538
00:33:46,720 --> 00:33:54,720
If you gave police officers authority to open up houses without warrants, to install video cameras in houses,

539
00:33:54,720 --> 00:34:00,720
and then only access them later on if a need arose, you know, maybe through judicial authorization,

540
00:34:00,720 --> 00:34:04,720
maybe through an automated scan of what's going on in the house,

541
00:34:04,720 --> 00:34:07,720
so it's just a computer looking, it's not a human.

542
00:34:07,720 --> 00:34:12,720
And I think that, you know, there are a lot of people who kind of observe very provocatively

543
00:34:12,720 --> 00:34:17,720
that the optimal level of crime in a society is not zero,

544
00:34:17,720 --> 00:34:22,720
because the only way you can achieve zero crime in a society is to become a surveillance state,

545
00:34:22,720 --> 00:34:27,720
and that we've rejected that path, and that's a weird cost-benefit way of looking at it,

546
00:34:27,720 --> 00:34:33,720
that we tolerate a certain amount of civil disobedience or crime

547
00:34:33,720 --> 00:34:38,720
in order to not to sacrifice every bit of freedom that we have.

548
00:34:38,720 --> 00:34:42,720
And that question becomes a lot more difficult when you ratchet up the one side,

549
00:34:42,720 --> 00:34:48,720
when you make it about, you know, a terrorist attack in the country.

550
00:34:48,720 --> 00:34:55,720
But, you know, I spent a lot of time listening to people who conduct these investigations,

551
00:34:55,720 --> 00:34:58,720
and I've looked at all of the examples that the government has made public in any event

552
00:34:58,720 --> 00:35:04,720
about how these programs are used, and there's not really a convincing case

553
00:35:04,720 --> 00:35:08,720
that pervasive surveillance is necessary in a strong sense.

554
00:35:08,720 --> 00:35:10,720
It's, of course, useful.

555
00:35:10,720 --> 00:35:12,720
It makes the job easier sometimes.

556
00:35:12,720 --> 00:35:15,720
It's always easier if you don't have to go to a judge to get a warrant

557
00:35:15,720 --> 00:35:17,720
and why trouble the police officers.

558
00:35:17,720 --> 00:35:26,720
But that inefficiency has a purpose, and it's to protect the private sphere from, you know,

559
00:35:26,720 --> 00:35:28,720
unjustified invasion.

560
00:35:28,720 --> 00:35:33,720
And to get back to the question of when do we accept that invasion?

561
00:35:33,720 --> 00:35:37,720
And, you know, the balance that was originally drawn in the 1700s,

562
00:35:37,720 --> 00:35:41,720
you know, with the ratification of the Fourth Amendment was with cause.

563
00:35:41,720 --> 00:35:46,720
And the question always is what's sufficient cause, and we can have that debate.

564
00:35:46,720 --> 00:35:48,720
But with cause means something.

565
00:35:48,720 --> 00:35:51,720
It means that you don't accept pervasive surveillance at the outset.

566
00:35:51,720 --> 00:35:54,720
You don't accept dragnet surveillance.

567
00:35:54,720 --> 00:35:57,720
You know, that's what the founding was, a rejection of, you know,

568
00:35:57,720 --> 00:36:00,720
of King George's dragnet surveillance.

569
00:36:00,720 --> 00:36:08,720
You know, I used to do a lot of, most of my early work at the ACLU was relating to mistreatment

570
00:36:08,720 --> 00:36:09,720
of detainees.

571
00:36:09,720 --> 00:36:13,720
And so we heard the same argument there, which was, well, maybe torture does work.

572
00:36:13,720 --> 00:36:18,720
Maybe in that one situation, the ticking time bomb scenario, you'd want to use torture.

573
00:36:18,720 --> 00:36:27,720
And, you know, the kind of civil society groups that work on these issues are divided

574
00:36:27,720 --> 00:36:30,720
over whether to engage in that hypothetical.

575
00:36:30,720 --> 00:36:34,720
Because torture is immoral no matter what is the argument.

576
00:36:34,720 --> 00:36:37,720
I mean, we shouldn't engage on this practical question.

577
00:36:37,720 --> 00:36:43,720
But they always, you know, at the end of the day you want to convince people that it doesn't actually work.

578
00:36:43,720 --> 00:36:47,720
And even if it did, this ticking time bomb scenario is actually just a hypothetical.

579
00:36:47,720 --> 00:36:49,720
It doesn't actually exist.

580
00:36:49,720 --> 00:36:51,720
Those situations never really do exist.

581
00:36:51,720 --> 00:36:56,720
So I don't think we'll find ourselves in a situation where the but for limit,

582
00:36:56,720 --> 00:37:04,720
on cracking a terrorist attack, is collecting every American's communication.

583
00:37:04,720 --> 00:37:12,720
But if we did, I think our principles would reject pervasive surveillance to accomplish that result.

584
00:37:12,720 --> 00:37:19,720
Why do you think pervasive surveillance is going on now since there doesn't seem to be a clear and present danger?

585
00:37:19,720 --> 00:37:21,720
But it's still going on.

586
00:37:21,720 --> 00:37:24,720
You know, there's been a shift in the way the NSA conceives of its role.

587
00:37:24,720 --> 00:37:31,720
It really is switched from an agency that used to be targeted in its investigations to one that tries to collect everything at the outset

588
00:37:31,720 --> 00:37:33,720
to preserve for later searching.

589
00:37:33,720 --> 00:37:38,720
And that technological shift has just happened to happen, you know, occurred over the last 20 years.

590
00:37:38,720 --> 00:37:44,720
The war of terrorism, they've redefined so that there's always a threat.

591
00:37:44,720 --> 00:37:53,720
And given there's always a threat, you always can be in this mode of trying to prevent it, which then allows us to move.

592
00:37:53,720 --> 00:38:00,720
Great, it allows them to do this.

593
00:38:00,720 --> 00:38:14,720
There was another, and we were talking about it briefly earlier, which is the role of people in the society to uncover what in fact the secrets

594
00:38:14,720 --> 00:38:16,720
or what the government is doing.

595
00:38:16,720 --> 00:38:21,720
And there, we were talking about investigating, reporting.

596
00:38:21,720 --> 00:38:30,720
Well, that is all but disappeared, given the state that the print media is in now.

597
00:38:30,720 --> 00:38:44,720
Now, hopefully, the new technology, the Internet blogging will pick up this, but we get in our home items every day,

598
00:38:44,720 --> 00:38:51,720
and it says, then, all the news that's fit to print, well, that's silly.

599
00:38:51,720 --> 00:39:06,720
Clearly, we know that they selectively don't tell things as they've admitted, but investigated for reporting has, for the most part, disappeared.

600
00:39:06,720 --> 00:39:16,720
And now what we have are individuals who take it upon themselves to reveal some of these secrets that are going on.

601
00:39:16,720 --> 00:39:22,720
Of course, they'll label this traitors, and they have to flee the country and so on.

602
00:39:22,720 --> 00:39:34,720
But I have a concern that given that there has to be a certain tension between the public and private always,

603
00:39:34,720 --> 00:39:42,720
one of the regulators of that is investigative reporting.

604
00:39:42,720 --> 00:39:46,720
And there is much, I think, much less of that.

605
00:39:46,720 --> 00:39:50,720
It's not much less of it, in sense, it isn't happening.

606
00:39:50,720 --> 00:40:03,720
But, you know, I read Mother Jones, well, Mother Jones, I don't know if it has 50,000 subscribers or the nation where things break much earlier than,

607
00:40:03,720 --> 00:40:16,720
or I have stone in another generation, where, in fact, there were people who took up the reporters, the media who took it upon themselves to, in fact, report,

608
00:40:16,720 --> 00:40:23,720
and to try to regulate this private, public kind of activity.

609
00:40:23,720 --> 00:40:32,720
And I'm really placing, and I understand the position of preventing the government,

610
00:40:32,720 --> 00:40:39,720
I see it as a broader issue between a public and a private, what is private and what is public.

611
00:40:39,720 --> 00:40:43,720
And I think we're in trouble with this.

612
00:40:43,720 --> 00:40:49,720
There is much too much private behavior that's public now.

613
00:40:49,720 --> 00:40:52,720
For some of us, we're uncomfortable with it.

614
00:40:52,720 --> 00:41:00,720
We don't really want to hear the conversation, the regulation of conversation in restaurants.

615
00:41:00,720 --> 00:41:13,720
For those of us who are New Yorkers, come on, I mean, we can't go in, the noise level is so high, and it's because people don't modulate their voices any longer.

616
00:41:13,720 --> 00:41:19,720
And so, but investigative reporting, what's happened to that?

617
00:41:19,720 --> 00:41:28,720
You know better than I, how many people get their news from, you know, from television?

618
00:41:28,720 --> 00:41:34,720
How many get it from newspaper or from the internet?

619
00:41:34,720 --> 00:41:42,720
In fact, we know how little people know about what's going on in the world, our citizens.

620
00:41:42,720 --> 00:41:52,720
I've forgotten the figures, but I said I could name who the vice president is, and probably 90 couldn't name who was the last vice president.

621
00:41:52,720 --> 00:42:03,720
So, is something happening in this balance now that needs writing?

622
00:42:03,720 --> 00:42:18,720
Interesting legal ACLU, such organizations attempt to try to maintain this balance, but it's a lonely place to be.

623
00:42:18,720 --> 00:42:21,720
And you would pay much more help, right?

624
00:42:21,720 --> 00:42:32,720
And if there are a lot of people out there who could spot in their job reveal these things and, you know, confront some of these issues.

625
00:42:32,720 --> 00:42:50,720
I mean, this last event with, I remember his name at the moment, but, you know, had to reveal what Ennis, the NSA, what's doing, and then he had to certainly leave the country.

626
00:42:50,720 --> 00:42:58,720
So, that's kind of investigative reporting, but then you get classified as a traitor.

627
00:42:58,720 --> 00:43:09,720
So, I don't know, I don't even know how, I mean, from a legal point of view, obviously it has to be fought to regulate it.

628
00:43:09,720 --> 00:43:19,720
No one could argue with that, but I think if we take a broader perspective of, and maybe it is, what happens when technology changes?

629
00:43:19,720 --> 00:43:29,720
And indeed, it's only going to get more amazing with this balance between public and private.

630
00:43:29,720 --> 00:43:41,720
It clearly is, we're in the midst of change, which in a single lifetime we can witness, sometimes it's quite scary.

631
00:43:41,720 --> 00:43:46,720
I think the right to privacy has been based on the idea of habeas corpus, typically, right?

632
00:43:46,720 --> 00:44:01,720
And there's been these metaphors of the body or the home or, like, concrete things that are in violet that you have ownership over, and I was curious what are the rights to information that we have now, and what is considered to be owned by the person.

633
00:44:01,720 --> 00:44:08,720
I mean, in terms of language or data or things like that, what are the legal principles behind ownership in that way?

634
00:44:08,720 --> 00:44:15,720
One of the people that I talked to for this panel is starting a company where you would actually own your data and could monetize bits of information that are used.

635
00:44:15,720 --> 00:44:18,720
Because that is something that belongs to you.

636
00:44:18,720 --> 00:44:22,720
You know, Vick can find, start that once you put something in language, it no longer belongs to you.

637
00:44:22,720 --> 00:44:26,720
So, you know, how is that sort of tension being played out between people?

638
00:44:26,720 --> 00:44:43,720
As a legal matter, I think it's difficult to answer this question of what preserves our ownership of information, but as a practical matter, it's a lot easier to describe it, which is that, you know, if you think of your relationships with various internet companies, if you're not paying for the service, chances are that you're not the customer.

639
00:44:43,720 --> 00:44:49,720
That's the adage that's been cropping up these days, that if you're not paying for it, you're actually the product.

640
00:44:49,720 --> 00:44:53,720
And that's true for a lot of the companies that monetize the information.

641
00:44:53,720 --> 00:45:10,720
If you're a Gmail user, you're only a customer in a very attenuated sense, because what funds your use of that service is actually the information that you volunteer to Google, that they then scrape to create their ad-based business model.

642
00:45:10,720 --> 00:45:21,720
And so, I think this is part of what Dr. Lewis is getting at, which is that people more and more these days are volunteering this information.

643
00:45:21,720 --> 00:45:24,720
But, you know, there's a way in which we've always done that.

644
00:45:24,720 --> 00:45:34,720
It's just now we have an internet to facilitate the exchange of the information, and so by necessity, so much of it resides with someone else.

645
00:45:34,720 --> 00:45:41,720
You know, we used to keep journals, now we just keep them in the cloud. And what does that say about who owns that information?

646
00:45:41,720 --> 00:45:47,720
Should technology change what we think of the ownership of that information? I don't think so.

647
00:45:47,720 --> 00:45:56,720
You know, right now the government takes the position throughout much of the country that it doesn't need a warrant to open email.

648
00:45:56,720 --> 00:46:09,720
But all of a sudden when we move to the digital analog, they don't. And that's a bizarre conception of privacy.

649
00:46:09,720 --> 00:46:16,720
The follow up with that too, about the kind of corporate surveillance and data gathering that happens when people do this.

650
00:46:16,720 --> 00:46:25,720
So one way is to, yeah, we used to write journals and put them in the cloud and we make it in a bit concerning sense open in public, right? And it's no longer ours.

651
00:46:25,720 --> 00:46:30,720
But what's happening with corporate surveillance is we give it over to another private entity, right?

652
00:46:30,720 --> 00:46:35,720
It then captures and re-organizes the data. And we do that when we sign those, we check mark those boxes, right?

653
00:46:35,720 --> 00:46:39,720
The terms of service agreements and user licensing agreements, the things that people tend to just like,

654
00:46:39,720 --> 00:46:50,720
yeah, I've got to get to my mail or I've got to get to this. I want to start this platform. But that's when this sort of, it's almost like the last bastion of this true, like, older form of sovereignty,

655
00:46:50,720 --> 00:46:57,720
where you give up your rights to your data because you've told them, you know, whatever you want to take from here, you can.

656
00:46:57,720 --> 00:47:07,720
And then some of the battles that are happening both legally but also culturally around how platforms like Facebook have to then respond to its users that say,

657
00:47:07,720 --> 00:47:14,720
well, you might want to alter those terms of service agreements before, you know, unless you want to alienate a lot of people.

658
00:47:14,720 --> 00:47:18,720
So there is a cultural dimension, I think, there too, as well as legal.

659
00:47:18,720 --> 00:47:36,720
Information has become a sellable item. So, you know, on your, if you write a check, I mean, there are so many public things we do.

660
00:47:36,720 --> 00:47:48,720
That get us into the private domain. I mean, Social Security secret is probably the most secret thing we have.

661
00:47:48,720 --> 00:47:56,720
I mean, if you have a package sent to you, they have your address, they have your zip code and commercial thinking of commercial.

662
00:47:56,720 --> 00:48:12,720
I mean, there's enormous you go on and buy something online. So people are harvesting bits of information which are now extraordinarily valuable, supposedly.

663
00:48:12,720 --> 00:48:22,720
Now, there's something interesting about it which is that the algorithms to analyze these bits are very primitive.

664
00:48:22,720 --> 00:48:28,720
And what really scares me is when they develop really good algorithms.

665
00:48:28,720 --> 00:48:46,720
And that's of course where a lot of our tax dollars are going as they try to figure out how to take in every phone message, every internet message and try to get information that's useful for whatever their purpose might be.

666
00:48:46,720 --> 00:48:52,720
It's not easy. There's an awful lot of information out there.

667
00:48:52,720 --> 00:49:01,720
And one of the things that may save us in some sense is that there's so much of it that's going to take a long time.

668
00:49:01,720 --> 00:49:08,720
They can't analyze sentences and phrases. And they can't really analyze emotional tone.

669
00:49:08,720 --> 00:49:24,720
They can't analyze words. So if you say president and you say the murder, the simple algorithm can put that together in a sentence and bring it up to some other level.

670
00:49:24,720 --> 00:49:39,720
So, but it would be nice to go back to the beginning when privacy was greater. But I think we're losing that.

671
00:49:39,720 --> 00:49:48,720
And while it would be nice to go back to the end of the 18th century, I don't think we can anymore.

672
00:49:48,720 --> 00:49:59,720
I think we have to confront that bits, these bits which make a part of our lives are just available. They're in the clouds.

673
00:49:59,720 --> 00:50:07,720
And there's no way around that except if we go back to writing letters and they can't open the letters.

674
00:50:07,720 --> 00:50:12,720
Of course, they can't probably. I don't tell us.

675
00:50:12,720 --> 00:50:26,720
So it just seems to me, you know, here's another etiquette. There used to be rules of behavior. You could keep your private, but you can act appropriately.

676
00:50:26,720 --> 00:50:36,720
Well, there's, you know, the 60s will let it all hang out. What does that mean? And simply mean that you don't have to have a private.

677
00:50:36,720 --> 00:50:46,720
If you didn't like something, you could say it instead of saying, which you could call a lie or deception. Thank you very much for the gift.

678
00:50:46,720 --> 00:50:55,720
In fact, you didn't like the gift at all. So I think lots of things are happening. Motion and expression.

679
00:50:55,720 --> 00:51:11,720
Pride. If you look at any sports, and by the way, it's all over the world, when people used to succeed in a sport, then baseball, they tipped their hat if they hit a home run with the bases loaded.

680
00:51:11,720 --> 00:51:27,720
Now you see the full display of the pride response as if it's perfectly okay to express yourself. So this public private thing is shifting.

681
00:51:27,720 --> 00:51:39,720
And where private is less valued. And now you said that. I said it more from my perspective than yours.

682
00:51:39,720 --> 00:51:51,720
But it's true. It's less valued now. So what we're confronted with is in fact governments, which tell us we're always a threat.

683
00:51:51,720 --> 00:51:59,720
There's always an enemy there. And the war on terrorism is endless. We don't declare war anymore.

684
00:51:59,720 --> 00:52:10,720
We haven't had who was the last president, Roosevelt, I suspect, who went before Congress and asked for a declaration of war, which is a requirement.

685
00:52:10,720 --> 00:52:27,720
So I mean, not only are we, our secrets being a private being invaded, but certainly the rule that the president needed a Congressional Act to declare war has disappeared too.

686
00:52:27,720 --> 00:52:42,720
So I don't think the solution, I mean I hope you find the solution, but I don't think it's going to be going back to the end of the 18th century.

687
00:52:42,720 --> 00:52:46,720
We just live in a totally different world.

688
00:52:46,720 --> 00:52:58,720
I think that's right as a practical matter. We're not going to go back to the modes of communication of the H3, but I think the principles are worth thinking about anyway.

689
00:52:58,720 --> 00:53:03,720
But there's always been this tension between private and public and what you share.

690
00:53:03,720 --> 00:53:09,720
And if you want to live a meaningfully social life, you construct your private sphere and then you choose what to share with people.

691
00:53:09,720 --> 00:53:14,720
Every time you have a conversation, you're sharing of your private life. And that's what it means to engage.

692
00:53:14,720 --> 00:53:20,720
And I don't know that people now value their privacy less.

693
00:53:20,720 --> 00:53:27,720
It just might mean that they share their private information in a different way as part of their social engagements.

694
00:53:27,720 --> 00:53:39,720
But there's a way in which you can think about this from the governmental perspective, which is that private people share information to engage meaningfully.

695
00:53:39,720 --> 00:53:44,720
But government share information often with the public anyway to maintain credibility.

696
00:53:44,720 --> 00:53:50,720
And there's a certain amount of sharing you have to do as a government in order to maintain legitimacy.

697
00:53:50,720 --> 00:53:58,720
Otherwise, you're viewed as a repressive government. You're viewed as one that isn't democratic, isn't governed by the people.

698
00:53:58,720 --> 00:54:06,720
But that credibility is only at stake when there is kind of an adversity between the government and some other strong actor.

699
00:54:06,720 --> 00:54:15,720
It used to be as you were saying investigative journalists who would hold the government's feet to the fire when they didn't share enough.

700
00:54:15,720 --> 00:54:21,720
Now, large media organizations don't fulfill that role as well, perhaps as they used to.

701
00:54:21,720 --> 00:54:25,720
There's a flat or business model when it comes to media.

702
00:54:25,720 --> 00:54:27,720
But that essence of adversity needs to be there.

703
00:54:27,720 --> 00:54:33,720
There needs to be someone who has an adverse interest to the government to force the government to maintain his credibility by sharing information.

704
00:54:33,720 --> 00:54:38,720
I mean, I can't an individual decide that. We've had two examples.

705
00:54:38,720 --> 00:54:43,720
One of the WikiLeaks and this last recent one where someone exposed the...

706
00:54:43,720 --> 00:54:44,720
Yes, nothing I'd say.

707
00:54:44,720 --> 00:54:49,720
So can an individual decide, this is immoral, I'm going to expose this.

708
00:54:49,720 --> 00:54:58,720
It's a very interesting question because one may feel that they've done a service, and yet do they have a right to do that?

709
00:54:58,720 --> 00:55:06,720
Suppose they decided that some Democratic principle was entirely wrong and should be exposed.

710
00:55:06,720 --> 00:55:10,720
I mean, there is a whole interesting argument we made about all of this.

711
00:55:10,720 --> 00:55:15,720
Yeah, it's a hard question about who should be making the decision when it comes to government secrecy.

712
00:55:15,720 --> 00:55:26,720
And it's difficult to tell the director of the NSA that one of his employees should be the one making the decision about what should be public and what shouldn't be.

713
00:55:26,720 --> 00:55:34,720
I think an equally intolerable answer is that the executive alone should decide what should be secret and what shouldn't be.

714
00:55:34,720 --> 00:55:40,720
And the practical answer is, you know, our constitution is one that sets up a system of checks and balances.

715
00:55:40,720 --> 00:55:41,720
It pits...

716
00:55:41,720 --> 00:55:43,720
Two or two secrets, wasn't it?

717
00:55:43,720 --> 00:55:44,720
Yeah.

718
00:55:44,720 --> 00:55:48,720
One, that they were doing it. And two, they're doing it to us.

719
00:55:48,720 --> 00:56:05,720
And that's kind of interesting because in a certain sense, he exposed they were doing it, which then allows us to be prepared to act in a certain way to protect our privacy.

720
00:56:05,720 --> 00:56:17,720
So maybe what we need is not that they do it, but that we know that they do it because it would be pretty stupid if you were plotting...

721
00:56:17,720 --> 00:56:26,720
to overthrow the government to do it in an email, you know, to a list of your colleagues who were going to blow up something.

722
00:56:26,720 --> 00:56:36,720
I mean, they know that, okay, the Germans didn't know we... I mean, you know, the Germans didn't know we broke their code.

723
00:56:36,720 --> 00:56:46,720
So the secret was not only getting their secrets where they were going to bomb and where they were going to be, but it was the secret that we could find out this.

724
00:56:46,720 --> 00:56:56,720
So maybe what we need to do is something around finding out what the secrets are.

725
00:56:56,720 --> 00:57:00,720
Aren't we protected in a certain sense now?

726
00:57:00,720 --> 00:57:03,720
There's also a notion of the legitimacy of the relationship, right?

727
00:57:03,720 --> 00:57:13,720
I mean, that any situation where you give somebody authority over you, you're saying, I know something about this person, they've represented themselves fairly, and there's a legitimacy to the authority that I'm giving you.

728
00:57:13,720 --> 00:57:24,720
And I think when people feel that legitimacy has been undermined, that they've been tricked or it's in some way, that that's when they start to react against it and feel that they have the right then to reveal or leak or something like that.

729
00:57:24,720 --> 00:57:39,720
Well, if you wanted to tell someone that you didn't want the government to know, you wouldn't send an email, you wouldn't go on the phone, you'd probably meet them in some public place to exchange.

730
00:57:39,720 --> 00:57:49,720
So we have some protection by no, surprisingly, by knowing that they can invade us.

731
00:57:49,720 --> 00:58:08,720
And so in a certain sense, I think the revelation of that, rather than anything particular secret, but that secret is what so infuriates the government, because now it prevents them from doing the one thing they hoped to do.

732
00:58:08,720 --> 00:58:15,720
At least they say they hoped to do is to catch these bad folks doing the thing.

733
00:58:15,720 --> 00:58:23,720
Now, if you're a smart, bad folk, you're not going to do it in a way that the government would find out.

734
00:58:23,720 --> 00:58:34,720
I'm not sure the revelations changed that, though, for, you know, if I think we all knew the government had the technological capability to do what's been revealed.

735
00:58:34,720 --> 00:58:42,720
And if you were a terrorist, you would certainly expect that there would be no barrier to the government using that capability against you.

736
00:58:42,720 --> 00:58:47,720
Any system that requires cause, you know, suspected terrorists will pass that threshold.

737
00:58:47,720 --> 00:58:49,720
They will be targetable under any system.

738
00:58:49,720 --> 00:58:54,720
The biggest surprise, I think, from our perspective, anyway, was that it was so untargeted.

739
00:58:54,720 --> 00:59:03,720
And so the only people I think who have been, who have learned information that they didn't already assume was happening, you know, who learned that the former surveillance officer was going to be able to do it,

740
00:59:03,720 --> 00:59:13,720
who learned that the former surveillance was happening and didn't already assume was happening, were the ordinary public, you know, innocent Americans who are not plotting terrorist attacks,

741
00:59:13,720 --> 00:59:18,720
and so don't think daily about how to disguise their communications.

742
00:59:18,720 --> 00:59:20,720
And that's our concern.

743
00:59:20,720 --> 00:59:27,720
You know, that it's difficult to quantify the cost of an invasion of privacy.

744
00:59:27,720 --> 00:59:40,720
You know, you might say that for going a particular international call with a controversial colleague or for going visiting a particular website is not much of a harm in isolation.

745
00:59:40,720 --> 00:59:46,720
But in the aggregate, I think it is, I mean, I think it isn't in isolation, but in the aggregate it certainly is, that those hesitations add up.

746
00:59:46,720 --> 01:00:00,720
You know, in a sense, it allows the government to break down our ability to construct the private life that you started out this conversation by describing.

747
01:00:00,720 --> 01:00:10,720
You know, it takes away from the private citizen agency, the ability to manage his private affairs.

748
01:00:10,720 --> 01:00:15,720
You've written about this as a battleground, right?

749
01:00:15,720 --> 01:00:20,720
Yeah, lots of battles in the battleground.

750
01:00:20,720 --> 01:00:22,720
I'm trying to think about the pluck out.

751
01:00:22,720 --> 01:00:25,720
One thing is about going back to the 1700s, we shouldn't go back there.

752
01:00:25,720 --> 01:00:37,720
Yes, again, technically speaking, but in terms of technology, the principles of democracy and a nation's foundation were forged there, so it's important to go back for that reason.

753
01:00:37,720 --> 01:00:44,720
Otherwise, we go back to the 60s in which it was said during the Vietnam War, we had it destroyed in order to save it, right?

754
01:00:44,720 --> 01:00:56,720
And I think if that logic starts appearing as part of what we exist in now, then we have brought that kind of war to the homeland, right?

755
01:00:56,720 --> 01:01:03,720
And so that concerns me in terms of the freedom and the spaces of dissent.

756
01:01:03,720 --> 01:01:15,720
So if everyone, if there's a blanket notion that citizens are potentially connected, not they're not a potentially terrorist, they're just potentially connected to someone who might be connected to a terrorist.

757
01:01:15,720 --> 01:01:28,720
Then dissenters who've already been determined to be terrorists in different kinds of discourses, the FBI considers lots of different kinds of protests to be actually forms of terrorism, domestic terrorism.

758
01:01:28,720 --> 01:01:38,720
Then we may have already gutted the foundation under which people can have a democratic life as well as a rich, interior, personal life.

759
01:01:38,720 --> 01:01:44,720
And so that's the battleground that concerns me, is also where dissent can take place.

760
01:01:44,720 --> 01:01:49,720
Both through surveillance, the counter surveillance, I was thinking about cop watch, right?

761
01:01:49,720 --> 01:01:55,720
So instead of these sort of also grand figures like Snowden who do these international things, how about very local, while politics is local,

762
01:01:55,720 --> 01:02:10,720
very local things like cop watch that are tracking police abuse on the streets, and then the ways those people who are documenting that might be kind of investigative journalists are being preemptively arrested, their equipment confiscated,

763
01:02:10,720 --> 01:02:21,720
all this sort of NYPD techniques against Occupy Wall Street attempts to visualize what was going on by the police, especially during the clearing out of Zuccotti.

764
01:02:21,720 --> 01:02:31,720
I mean, these are ways that there's a prevention, a preemptive actually intervention into the forms of transparency that can happen from bottom up from the people on the streets.

765
01:02:31,720 --> 01:02:35,720
So that to me concerns me, that's why I think it's a battleground.

766
01:02:35,720 --> 01:02:44,720
And so think about it more in terms of warfare, in which case law is very important to find curbs and to be part of that.

767
01:02:44,720 --> 01:02:59,720
But when I think of at least the stories I've heard about the NYPD having built into their budgets, the fact that they're going to predict these lawsuits because of their extra legal detention, preemptive detention of protesters on the streets of New York, right?

768
01:02:59,720 --> 01:03:06,720
So they figure that they're going to do this, it's going to be illegal, they're going to lose in court, and they're going to have to pay out a civil case, right?

769
01:03:06,720 --> 01:03:20,720
So that's already built into the way that the strategy works to manage dissent, and that to me is where, when you think about who gets to be, who gets to produce a kind of transparency upon whom and under what conditions so.

770
01:03:20,720 --> 01:03:47,720
Who would it be interesting to, if there was someone among us who was a high tech person, who could tell us what we would have to do to counter now, they're Google and some of the others are saying that in fact,

771
01:03:47,720 --> 01:03:52,720
they're going to build devices that will prevent them from doing it.

772
01:03:52,720 --> 01:04:03,720
Now, I mean, clearly one way is to try to prevent them by arguing from principles of law and justice and so on.

773
01:04:03,720 --> 01:04:16,720
Another way, of course, is as an evolution, you know, or something evolves, and then those that survive have to evolve new things to the people who are going to do it.

774
01:04:16,720 --> 01:04:27,720
And then there are some things to do those old things, and so that there is this kind of balance, there's this kind of always tension that exists in life.

775
01:04:27,720 --> 01:04:45,720
So I can't imagine, I mean, I cannot imagine, but I cannot not imagine that there are high tech folks who are figuring out ways that you might be able to scramble phone messages in some way or internet,

776
01:04:45,720 --> 01:05:02,720
and so on, and maybe what we need to equalize some of this injustice because it's happening to us, but we don't have much lee except through the law to try to counter it.

777
01:05:02,720 --> 01:05:16,720
Maybe we need to develop a new technology that I attach it to my phone for any phone call that I don't want anyone to be able to decipher.

778
01:05:16,720 --> 01:05:37,720
It turns out it's relatively simple to disguise the content of your internet communications, your emails. Most people won't take the extra steps necessary, and Gmail has no incentive to facilitate it because their business model is based upon access to your email.

779
01:05:37,720 --> 01:05:54,720
But there are ways to encrypt those communications. What's much more difficult is to disguise your metadata, and it turns out, and the reason is because you generally need to expose your metadata so that your communication service knows where to send your information.

780
01:05:54,720 --> 01:06:08,720
If you're sending an email, the two line needs to be exposed so that the next computer knows where to send it. There's a lot of research that's gone into how to disguise that originally funded by the Navy, because the government wanted a way of securing its internal communications,

781
01:06:08,720 --> 01:06:24,720
and so they invested heavily in a system of what's called onion routing, where you would sequentially encrypt your communications through a series of hops with the idea that no one hop knows the full path, and it's very difficult to reconstruct.

782
01:06:24,720 --> 01:06:32,720
But these are difficult technologies to use for most people, and I think they're part of the solution, but they're not the full solution.

783
01:06:32,720 --> 01:06:51,720
But if we have to do something like that, what kind of society are we living in? And what is the implication of having to protect ourselves, and what's the implication of the government having the power to find out information about a private individual

784
01:06:51,720 --> 01:07:06,720
without regard to the, you might say, the basic rights of that person for privacy? I mean, what's the implication? Have we undermined really the whole sense of democracy and the sense of trusting government?

785
01:07:06,720 --> 01:07:22,720
It seems like the whole world is beginning to shift in terms of what are our values, what are we dealing with here? Well, some of it's not new. We use envelopes when we send a mail generally.

786
01:07:22,720 --> 01:07:32,720
Most people don't send truly private pieces of mail on postcards. They don't write something very sensitive on a postcard. They put it in an envelope.

787
01:07:32,720 --> 01:07:46,720
And it turns out that just the way the phone systems developed, they're all like postcards. When you are talking over the phone, there is nothing to prevent anyone who has access to the line anywhere along it from listening to the conversation. There's no protection. It's a postcard.

788
01:07:46,720 --> 01:07:54,720
And we have kind of a system of blind trust for the intermediaries, but it's a trust that we're now learning is undressified.

789
01:07:54,720 --> 01:08:02,720
The same has largely been true of email. Email for most of its history has been communicated entirely unencrypted.

790
01:08:02,720 --> 01:08:11,720
If you send, you know, give me an example, if you send a Gmail from a Gmail account to another Gmail account, that's generally encrypted internally to Gmail.

791
01:08:11,720 --> 01:08:23,720
But if you send a Gmail email to someone on Yahoo's account, the connection between Gmail and Yahoo's is not encrypted. So anyone, and not just our government and not just our government, but even a relatively unsophisticated hacker.

792
01:08:23,720 --> 01:08:32,720
Could intercept that communication. So some of it, I think, is just common sense. It's putting the envelope on your communication.

793
01:08:32,720 --> 01:08:44,720
But I agree that we shouldn't have to. It's a sad commentary that people in the tech community are now thinking of their products from the No Trust model.

794
01:08:44,720 --> 01:08:50,720
They're assuming that nothing can be trusted. The server can't be trusted because the NSA has access to it.

795
01:08:50,720 --> 01:09:00,720
The telecoms can't be trusted because they'll roll over and give up the information to less design a system that is secure, even if you can't trust the servers, which is possible.

796
01:09:00,720 --> 01:09:03,720
You know, but it's a sad commentary.

797
01:09:03,720 --> 01:09:17,720
Following up, why would somebody want to have, you can hear it, a private conversation on their phone in a public place?

798
01:09:17,720 --> 01:09:28,720
I'm sure we all have experience. People having fights on the phone and other kinds of things, which are really private.

799
01:09:28,720 --> 01:09:39,720
Now, we couldn't have done that before, at best. We could be in an oboe for the street, but usually you were in your office or home where you did those.

800
01:09:39,720 --> 01:09:47,720
So we now have a device which promotes it.

801
01:09:47,720 --> 01:09:54,720
And I believe promotes the idea that there is in private.

802
01:09:54,720 --> 01:09:59,720
That is, I don't only see it as the government.

803
01:09:59,720 --> 01:10:09,720
But I guess I'm trying to think it through with something not just the government and us, but something is happening.

804
01:10:09,720 --> 01:10:15,720
Commercial, you were mentioning. I mean, there are some incredible things on online dating.

805
01:10:15,720 --> 01:10:24,720
Now, you can fill out a form to begin with, and you say you really want, you're interested in dating brunettes.

806
01:10:24,720 --> 01:10:31,720
And so the pictures and the information they send you are brunettes, but they're not all brunettes.

807
01:10:31,720 --> 01:10:45,720
Now, the devices now can monitor if you start hitting a not-on brunettes, but on blondes, let's say, or black-haired people.

808
01:10:45,720 --> 01:10:54,720
And they will now switch. They will take your action rather than what you stated.

809
01:10:54,720 --> 01:11:08,720
So we now have to end. We know that in terms of buying things, these devices, these algorithms can come up and present you with things that you've done before.

810
01:11:08,720 --> 01:11:18,720
So even commercially, they're keeping track of your behavior. So it's endemic, I guess, is what I'm saying.

811
01:11:18,720 --> 01:11:24,720
We've always had to protect ourselves from our government and our constitution.

812
01:11:24,720 --> 01:11:30,720
Our initial struggle was to try to figure out how to do that for this country.

813
01:11:30,720 --> 01:11:53,720
But it's not only it's ourselves allowing ourselves less privacy, and it is our commerce in our commercial worlds and our everyday lives in which people are keeping track of what we do, what we're buying, and so on.

814
01:11:53,720 --> 01:12:08,720
So I guess, I mean, you read some of Scalia, you know, he would like us back to, you can't go into your house.

815
01:12:08,720 --> 01:12:13,720
Well, that's what it says, you know, fundamental interpretation.

816
01:12:13,720 --> 01:12:19,720
You can't need a warrant to come into your home. Well, talking on a phone is not your home.

817
01:12:19,720 --> 01:12:24,720
So they could argue we're not going to take technology into account.

818
01:12:24,720 --> 01:12:32,720
Then of course, the more progressive arguments are that indeed it's the idea of privacy and protection.

819
01:12:32,720 --> 01:12:48,720
But it just seems to me that we need a kind of a balance, a kind of protection, one in terms of understanding why we are giving up privacy.

820
01:12:48,720 --> 01:13:09,720
And two, mechanisms by which those of us who want could reestablish it, learning how to encode or getting the tech folks to give us really very simple algorithms to make an arm of encoding and encrypting the easier to do.

821
01:13:09,720 --> 01:13:11,720
I'm going to butt in here in the interest of time.

822
01:13:11,720 --> 01:13:18,720
I'm going to sum up and say that we're very trusting and we want to be close to people and be known and we're very curious and therefore very prying.

823
01:13:18,720 --> 01:13:20,720
And we'll have to grapple with that in the area of secrecy.

824
01:13:20,720 --> 01:13:25,720
We're now open for questions for the next 20 minutes. Please come to the microphone if you'd like to ask.

825
01:13:25,720 --> 01:13:31,720
You didn't really discuss, and I'm interested in this because it's got to do with human nature.

826
01:13:31,720 --> 01:13:37,720
I think the mechanism by which the NSA obtains the data.

827
01:13:37,720 --> 01:13:43,720
For instance, I suspect they don't really hack like you were saying.

828
01:13:43,720 --> 01:13:46,720
I don't think they hacked the lines and servers and stuff.

829
01:13:46,720 --> 01:13:48,720
I think it works like this.

830
01:13:48,720 --> 01:13:50,720
I'm guessing.

831
01:13:50,720 --> 01:13:54,720
They get a court order and you could help me here.

832
01:13:54,720 --> 01:14:05,720
They get a court order that says, okay, you can go to Google or AOL and present it to the President and say,

833
01:14:05,720 --> 01:14:15,720
we want your permission to tap into your memory, your servers, et cetera.

834
01:14:15,720 --> 01:14:26,720
And the President of AOL thinks and says, hey, what if I said no?

835
01:14:26,720 --> 01:14:36,720
What would the government do if I refused to obey the court order?

836
01:14:36,720 --> 01:14:46,720
So, I would like to know really the mechanism by which they're doing all this stuff.

837
01:14:46,720 --> 01:14:49,720
And maybe someone could tell me.

838
01:14:49,720 --> 01:14:53,720
Yeah, there are a couple ways, a number of ways they do it.

839
01:14:53,720 --> 01:14:58,720
The vast majority of the information the NSA collects is not through hacking.

840
01:14:58,720 --> 01:14:59,720
They do do some hacking.

841
01:14:59,720 --> 01:15:10,720
There are documented cases of the FBI seeking hacking orders in order to remotely turn on the video camera on someone's laptop without that person knowing.

842
01:15:10,720 --> 01:15:11,720
So, hacking happens.

843
01:15:11,720 --> 01:15:14,720
But the vast majority of what the NSA does is not through hacking.

844
01:15:14,720 --> 01:15:18,720
It's either through court orders directing companies to turn over information.

845
01:15:18,720 --> 01:15:29,720
So, for example, the very first document that Edward Snowden revealed to the world was an order from the Secret Foreign Intelligence Surveillance Court compelling Verizon Business Network services,

846
01:15:29,720 --> 01:15:42,720
which is a subsidiary of Verizon, to turn over on an ongoing daily basis records of every single call that came across the network that either started in or ended in the United States.

847
01:15:42,720 --> 01:15:52,720
So, that is an example. There are similar types of orders that allow programs of surveillance directed at Google, Yahoo, Microsoft, etc.

848
01:15:52,720 --> 01:16:02,720
And then there's a lot of surveillance that takes place outside the United States, either through alliances with other countries, with their intelligence services,

849
01:16:02,720 --> 01:16:13,720
or through picking up information off the wire, kind of physical hacking or tapping, or picking it up out of the air using satellite and radio wave surveillance, which happens.

850
01:16:13,720 --> 01:16:16,720
But the vast majority of it is not through hacking.

851
01:16:16,720 --> 01:16:18,720
And you're absolutely right about that.

852
01:16:18,720 --> 01:16:21,720
What happens when the companies are talking to you?

853
01:16:21,720 --> 01:16:23,720
No, they know.

854
01:16:23,720 --> 01:16:25,720
Well, they're compelled by court order.

855
01:16:25,720 --> 01:16:27,720
They can challenge those orders.

856
01:16:27,720 --> 01:16:29,720
They don't have a great incentive to.

857
01:16:29,720 --> 01:16:41,720
It's interesting, you know, this type of order, the one that was directed to Verizon, there have been similar ones to all the major companies, and they've been going to those companies every 90 days since 2006.

858
01:16:41,720 --> 01:16:44,720
So, each one of them has received dozens of orders.

859
01:16:44,720 --> 01:16:47,720
Not a single one of the telecoms has challenged that order.

860
01:16:47,720 --> 01:16:50,720
Even though on its face it's extraordinary.

861
01:16:50,720 --> 01:16:57,720
Every single record of every single call that comes across your network, whether it's related to a terrorist or not, hand over.

862
01:16:57,720 --> 01:17:01,720
And not a single one has challenged them, but they could challenge them.

863
01:17:01,720 --> 01:17:05,720
The reason they don't have an incentive is because, A, they have a very close working relationship with the government.

864
01:17:05,720 --> 01:17:09,720
They have a lot of government contracts, so they have a financial incentive not to.

865
01:17:09,720 --> 01:17:12,720
B, the statute immunizes them, so they're not.

866
01:17:12,720 --> 01:17:15,720
They don't suffer any legal liability for turning over your call records.

867
01:17:15,720 --> 01:17:17,720
You can't sue them.

868
01:17:17,720 --> 01:17:21,720
And C, they're compensated at fair market value by the statute for their cooperation.

869
01:17:21,720 --> 01:17:22,720
So they're getting paid.

870
01:17:22,720 --> 01:17:24,720
So they don't have much of an incentive.

871
01:17:24,720 --> 01:17:34,720
And on the other side, on the side of the service providers, the internet, the tech companies, as opposed to the telecoms, the tech companies have occasionally pushed back.

872
01:17:34,720 --> 01:17:40,720
The last major one was by Yahoo back in 2007.

873
01:17:40,720 --> 01:17:47,720
Haven't really been any major challenges from the tech companies since although they're starting to ramp up their efforts now and they're pushing back a little bit.

874
01:17:47,720 --> 01:17:51,720
But they have the same relationships with the government.

875
01:17:51,720 --> 01:17:57,720
They have the same immunity provided by the statute and they have the same provision for fair market compensation.

876
01:17:57,720 --> 01:18:02,720
Please introduce yourself.

877
01:18:02,720 --> 01:18:11,720
Greg Burke, this usually we've heard that there has to be a tacit kind of balance between individuals and institutions.

878
01:18:11,720 --> 01:18:20,720
I mean, maybe governments or societies or businesses, things like that over time where there's an understanding that there's a certain amount of

879
01:18:20,720 --> 01:18:32,720
less than a free discussion, I don't know between each other in terms of what information we've heard about being polite.

880
01:18:32,720 --> 01:18:41,720
And we all passively never believe anything that we see on television, for instance, in terms of advertisements and stuff like that.

881
01:18:41,720 --> 01:18:43,720
And it's been acceptable over the years.

882
01:18:43,720 --> 01:18:45,720
It's been a certain balance.

883
01:18:45,720 --> 01:19:02,720
Now we have this huge amount of technology that has shifted and shift the balance away from the individual towards institutions with regard to how to manipulate this interaction of the dance of social commerce.

884
01:19:02,720 --> 01:19:14,720
And particularly I'm concerned now with how is the individual as a responsible in the democratic society supposed to get the information from the government.

885
01:19:14,720 --> 01:19:19,720
It's supposed to get the information that is necessary to make decisions about voting.

886
01:19:19,720 --> 01:19:31,720
The issue here is believing what you hear from politicians, from people who are actually in the government.

887
01:19:31,720 --> 01:19:43,720
It seems to me that we have in our society now created a paranoia about terrorism that requires a lot of people to want to give up certain rights.

888
01:19:43,720 --> 01:19:48,720
And this is an opportunity to manipulate.

889
01:19:48,720 --> 01:19:52,720
And I just use your imbalance now because of people.

890
01:19:52,720 --> 01:19:54,720
I can't figure out what's true.

891
01:19:54,720 --> 01:20:07,720
I mean, how can you all on the panel here figure out what's true in terms of being able to make a little perform decision as a citizen to vote both from what the government is telling you on what you see on the news.

892
01:20:07,720 --> 01:20:16,720
I read wiki and that's how I found out about the prison program and how Google collaborated with the government with the week.

893
01:20:16,720 --> 01:20:19,720
So that's my source of the shh.

894
01:20:19,720 --> 01:20:24,720
This is not just happening to us.

895
01:20:24,720 --> 01:20:26,720
We are participating in this.

896
01:20:26,720 --> 01:20:43,720
And I guess that's been the psychological point I've been trying to understand is to what degree have we psychologically moved away from this idea of the private.

897
01:20:43,720 --> 01:21:03,720
And I think that one could find many, many examples of a shift during the same period and maybe even a longer to that makes it easier for the government to, in fact, do some of these things.

898
01:21:03,720 --> 01:21:14,720
The shift to telephone conversations in public.

899
01:21:14,720 --> 01:21:16,720
Just simply that.

900
01:21:16,720 --> 01:21:19,720
Now my wife and I were in Paris.

901
01:21:19,720 --> 01:21:25,720
We've just come back and shockingly, there are not many people on the phone walking the streets.

902
01:21:25,720 --> 01:21:33,720
We've been walking the difference in the two major cities in the world in this city.

903
01:21:33,720 --> 01:21:38,720
You can't go a block without someone talking and hearing them.

904
01:21:38,720 --> 01:21:43,720
And that's not the case in a week of walking in Paris.

905
01:21:43,720 --> 01:21:50,720
So I think there's something happening psychologically.

906
01:21:50,720 --> 01:22:16,720
And I think we can even find it in some of our theories and that suggests that privacy in terms of not expressing what you really think or not doing such.

907
01:22:16,720 --> 01:22:20,720
I'll be brief.

908
01:22:20,720 --> 01:22:28,720
In fact, changing us and making us more susceptible to it.

909
01:22:28,720 --> 01:22:32,720
Now it was interesting because you asked what we could do.

910
01:22:32,720 --> 01:22:40,720
I mean there is some outrage and it's going on for a while, but it's shocking how little there is.

911
01:22:40,720 --> 01:22:51,720
I'd be very curious if people don't use their phone as much or don't use email as much knowing that potentially someone could be listening in and gathering data.

912
01:22:51,720 --> 01:22:58,720
So it's happening to us, but I think some other things are happening.

913
01:22:58,720 --> 01:23:05,720
There's a psychology that is happening at the same time, which is playing into this.

914
01:23:05,720 --> 01:23:13,720
Can I just suggest one thing because we've had this conversation a little bit today, but is it possible that most of that shift is at the margins?

915
01:23:13,720 --> 01:23:21,720
Because I suspect that if you ask the person who's on their phone walking down New York having what is usually in a conversation with a mom or dad.

916
01:23:21,720 --> 01:23:25,720
It's not usually the most private conversation in the world, but suppose even if it is.

917
01:23:25,720 --> 01:23:30,720
If you ask that person, would you share with me your Gmail password?

918
01:23:30,720 --> 01:23:35,720
Tell me your social security number, allow me to install video equipment in your home.

919
01:23:35,720 --> 01:23:37,720
They'll say no to those things.

920
01:23:37,720 --> 01:23:42,720
So I think there's a core of privacy that persists even if there is a shift in the margins.

921
01:23:42,720 --> 01:23:44,720
But is it just at the margins?

922
01:23:44,720 --> 01:23:47,720
Well the question is, is it at the...

923
01:23:47,720 --> 01:23:50,720
Well do you mean margin in terms of people or not?

924
01:23:50,720 --> 01:23:59,720
I mean I've been in restaurants where a young woman is talking to her mother and having an argument about something in which I'm privy to.

925
01:23:59,720 --> 01:24:02,720
And she does not.

926
01:24:02,720 --> 01:24:04,720
She is not.

927
01:24:04,720 --> 01:24:11,720
This doesn't seem to bother her that a complete stranger is listening to this going on.

928
01:24:11,720 --> 01:24:13,720
Now does everyone do it?

929
01:24:13,720 --> 01:24:15,720
No of course not.

930
01:24:15,720 --> 01:24:22,720
Lots of people don't talk on their phones unless they really need to or just small exchange of information.

931
01:24:22,720 --> 01:24:26,720
So in some sense it's not everyone doing it.

932
01:24:26,720 --> 01:24:38,720
But I do think, for example, etiquette which disappeared in the 60s when the idea was no you let it all hang out.

933
01:24:38,720 --> 01:24:48,720
There's a struggle etiquette books disappeared there actually have reappeared which is kind of an interesting phenomena.

934
01:24:48,720 --> 01:24:58,720
But it now is you can't have a private for it. You'll have to make it public if someone asks you a question.

935
01:24:58,720 --> 01:25:03,720
Good interpersonal life was you gave them the answer.

936
01:25:03,720 --> 01:25:06,720
So I don't know how marginal it is.

937
01:25:06,720 --> 01:25:17,720
But I would be concerned in this struggle with this, with government which is what are major concerns here.

938
01:25:17,720 --> 01:25:20,720
I would be concerned that it's not them just doing it to us.

939
01:25:20,720 --> 01:25:31,720
And if they are doing it to us, that we are becoming more compliant psychologically to it.

940
01:25:31,720 --> 01:25:34,720
Time for a couple more questions I hope.

941
01:25:34,720 --> 01:25:37,720
I think people waiting are.

942
01:25:37,720 --> 01:25:48,720
I just want to ask what you guys think is the role of the psychology of fear in the way that the policies have been crafted.

943
01:25:48,720 --> 01:25:57,720
Because it seems to me that the way that the NSA's role has grown sort of comports with what millions of years of evolution have forged in us.

944
01:25:57,720 --> 01:26:03,720
There's the classical example of you have an animal in Savannah and here's the glass gras rustling behind it.

945
01:26:03,720 --> 01:26:07,720
And it can make two types of mistakes in that situation.

946
01:26:07,720 --> 01:26:13,720
We're worried that there's a threat and turn around and there's nothing, which is a mistake.

947
01:26:13,720 --> 01:26:18,720
Or it could assume that it was just the wind, but it's actually a predator and a dead.

948
01:26:18,720 --> 01:26:24,720
And so of course evolution pushes us in the direction of leaping at a threat that may not be present.

949
01:26:24,720 --> 01:26:32,720
And it seems that our reaction to one very significant but relatively isolated terrorist event has had a very profound effect.

950
01:26:32,720 --> 01:26:40,720
On our government policy and the way that we all sort of accepted changes in our own security.

951
01:26:40,720 --> 01:26:47,720
And so because this is such a deep-seated, strong, evolutionarily formed predisposition,

952
01:26:47,720 --> 01:26:53,720
I wonder if there's a real plausible way of fighting against it because it's such a strong urge.

953
01:26:53,720 --> 01:26:57,720
I'll just add before the human nature component of this,

954
01:26:57,720 --> 01:27:04,720
we'll just add that there's a whole layer of mediated repetition of the trauma that happens that reactivates that moment too.

955
01:27:04,720 --> 01:27:18,720
So the annual replaying of the images, the bushes, the security codes and the color codes and the constant reminding of people that we're at war.

956
01:27:18,720 --> 01:27:27,720
So I think regardless of whether this, what it touches in terms of the basic dimensions of being a human,

957
01:27:27,720 --> 01:27:30,720
the cultural mediated dimensions are very political in terms of that can be changed.

958
01:27:30,720 --> 01:27:40,720
There's no reason why we have to say, okay, well, this is a defining moment of our entire senses of self as Americans because we've been attacked.

959
01:27:40,720 --> 01:27:45,720
Which is something that other countries around the world go through on a daily basis.

960
01:27:45,720 --> 01:27:52,720
Right, so this sort of American exceptionalism is something also I think that is a cultural dimension of this fear, that it's not just something that-

961
01:27:52,720 --> 01:28:00,720
We have been at war as a society. We haven't stopped from the Second World War. We have the Korean War.

962
01:28:00,720 --> 01:28:13,720
We have had an enemy. I mean, we could start looking at paranoia and judge on the stand or-

963
01:28:13,720 --> 01:28:21,720
An enemy for the last 75 years. There's all we spin in an enemy.

964
01:28:21,720 --> 01:28:30,720
That's true, but at the same time, the 9-11 acted like an acute trauma. In some sense was unexpected.

965
01:28:30,720 --> 01:28:37,720
I mean, we should have expected, but people didn't expect it because we've been fighting out there.

966
01:28:37,720 --> 01:28:50,720
We haven't had something hit us in quite this way. So it acts like a traumatic event to which it's almost like we're acting all of us in a post-traumatic neurosis.

967
01:28:50,720 --> 01:29:06,720
I mean, in the sense that we're always on guard against the repetition of the trauma, and that gets extended into policy and into the public arena in such a way as to cause a whole shift in our world.

968
01:29:06,720 --> 01:29:15,720
Since that happened, we've had a major shift in this country towards a long, a long signal reaction.

969
01:29:15,720 --> 01:29:24,720
And I don't think that's going to be easily reversed.

970
01:29:24,720 --> 01:29:37,720
I have a slightly different focus. I was struck by your analogy of the supernatural detective who could figure anything out retroactively.

971
01:29:37,720 --> 01:29:53,720
And the assumption of this broad data collection is essentially, I think, that if we have the data, somehow we can reconstruct the past and do so with great accuracy or with sufficient accuracy.

972
01:29:53,720 --> 01:30:06,720
And I'm thinking immediately a couple things. One is Google Books, where Google is going around scanning books.

973
01:30:06,720 --> 01:30:14,720
I don't know if any of you have ever had the experience of actually looking at these books. They're terrible.

974
01:30:14,720 --> 01:30:25,720
Nope, they scan them, but they don't proofread them. And I know, I mean, I've downloaded some of these books.

975
01:30:25,720 --> 01:30:39,720
Some of them are unreadable. They're gobbledygook. I notice, for example, even here we have to have technicians coming around constantly adjusting to get.

976
01:30:39,720 --> 01:30:58,720
I mean, this is open recording. There's no secrecy about it. And yet it takes apparently a great deal of tending to make sure that you get a good recording.

977
01:30:58,720 --> 01:31:15,720
And it takes a great deal of data who, obviously, if we record everybody's data, technically, if it were all listened to, it would mean that everybody would have to be working to listen to everybody's data.

978
01:31:15,720 --> 01:31:35,720
And it's a mathematical problem, almost. The existence of the data, I think, often gives people the idea that somehow the data is knowledge, whereas often I think it's not data but interpretation.

979
01:31:35,720 --> 01:31:49,720
And I'm also responding to the personal experience of having been the subject of a government investigation many years ago for political activity during the Vietnam War.

980
01:31:49,720 --> 01:32:09,720
And I got a copy of the investigative report. And I wrote a response to it, which began with the sentiment that as a taxpayer, I was greatly disappointed in the low quality of the investigation.

981
01:32:09,720 --> 01:32:21,720
I mean, I'm not going to bother to go. But it was, it was, I was appalled. This is what they came up with. How much did they spend for this?

982
01:32:21,720 --> 01:32:38,720
So I think in part, what I'm saying here is, yeah, there's an issue about the collection of data, but there's also an issue, I think, about the perception of almost, you know, supernatural.

983
01:32:38,720 --> 01:32:51,720
You know, supernatural knowledge and so forth, which I think is very, very questionable. But it, you know, knowledge or the perception of knowledge also is power.

984
01:32:51,720 --> 01:32:57,720
So I don't know if anybody wants to comment on those comments, but those are some of my.

985
01:32:57,720 --> 01:33:10,720
I would say, right, I mean, that can be very politically disabling to think that there's nothing that can be done. All this data is out there. The government is perfectly efficient as we're talking about efficiency, like creating the algorithms that can detect all these things.

986
01:33:10,720 --> 01:33:22,720
So, so the notion of that actually shifts a kind of power dynamic that says we can't do anything about it, but it's also, it's just so perfectly done. I mean, that when I study conspiracy theories, sometimes that's what it is.

987
01:33:22,720 --> 01:33:33,720
And the classic, the classic, the classic classic classic fantasy at the end of.

988
01:33:33,720 --> 01:33:43,720
He's Germany. And the efforts to reconstruct and understand what people thought they knew and also for to buy up.

989
01:33:43,720 --> 01:34:02,720
And the feeling of distrust and the ability to, to, to have a working democracy, basically, in, in a environment of such potential betrayal and distrust.

990
01:34:02,720 --> 01:34:18,720
The lesson of the, the 9-11 Commission report was not that we didn't have enough information, but, but that we didn't analyze it properly. But the response was to collect more out of a fear that, or, or a suspicion, I suppose, that if we just have more information, we'll solve it.

991
01:34:18,720 --> 01:34:34,720
You know, data can lie, which is one of the reasons why the government relies more heavily on metadata, because metadata is, doesn't lie as easily, or it's more easy, is more difficult to, to, you know, to obscure. But I think that's a good point.

992
01:34:34,720 --> 01:34:51,720
Carl Kleben from New York. I think the panel has covered a whole variety of different things and it's hard to be focused.

993
01:34:51,720 --> 01:35:11,720
Which is rather than the issue of national security and spying and the threat to civil liberties or the restrictions of civil liberties that have always happened when those things have been threatened and how much worse all of this could be down with and is with the explosion of information and the explosion of technology

994
01:35:11,720 --> 01:35:30,720
and the accessibility of information. What interests me more, in no way to strike the importance of all that, but what interests me more is the information that we give away so, and willingly, and the implications of that.

995
01:35:30,720 --> 01:35:47,720
And that is, you know, all given the accessibility on the Internet of all the data, including our financial data, our medical data, our legal data, our personal lives, what we do and what we don't do, how we spend our money, et cetera.

996
01:35:47,720 --> 01:36:06,720
I wonder if the panel could comment more about this psychological, social, and even political ramifications of that, leaving out for the moment the special issue of national security and national safety versus.

997
01:36:06,720 --> 01:36:20,720
But the whole change in privacy or the way in which, at least in America, and maybe in the rest of the world, we kind of willingly are, that's sort of gone or largely compromised.

998
01:36:20,720 --> 01:36:25,720
I'd like to start with that. So it's a question about the freely and willingly.

999
01:36:25,720 --> 01:36:30,720
I think there's Mauricio Lacerado who said, right, in the 60s, self-expression was a sign of freedom.

1000
01:36:30,720 --> 01:36:37,720
Today, it's an obligation and a compulsion, which is not just a psychological one, but an economic one.

1001
01:36:37,720 --> 01:36:41,720
It's an imperative that people create digital profiles.

1002
01:36:41,720 --> 01:36:45,720
Young people do, right, in order to be able to get jobs, right?

1003
01:36:45,720 --> 01:36:46,720
So that's one way.

1004
01:36:46,720 --> 01:36:51,720
It's like this constant need to express oneself, to show oneself digitally,

1005
01:36:51,720 --> 01:37:00,720
communicationally, in order to connect with the world. There's partially that. I think there's also, there is a change in what it means to share.

1006
01:37:00,720 --> 01:37:06,720
I don't think it means people just, you know, produce everything without thinking about it.

1007
01:37:06,720 --> 01:37:10,720
I mean, there are new words, I'd say, in our vocabulary.

1008
01:37:10,720 --> 01:37:11,720
Sorry?

1009
01:37:11,720 --> 01:37:14,720
Only adolescents texting to each other.

1010
01:37:14,720 --> 01:37:18,720
Sorry, adolescents texting to each other?

1011
01:37:18,720 --> 01:37:20,720
I mean, that's a particular form of communication.

1012
01:37:20,720 --> 01:37:21,720
Let me text you.

1013
01:37:21,720 --> 01:37:22,720
Sure.

1014
01:37:22,720 --> 01:37:23,720
I'm also doing that too.

1015
01:37:23,720 --> 01:37:24,720
Yeah.

1016
01:37:24,720 --> 01:37:30,720
So I'm thinking about words that have entered the vocabulary, like oversharing and TMI, which young people also use.

1017
01:37:30,720 --> 01:37:32,720
Too much information, right?

1018
01:37:32,720 --> 01:37:37,720
So it's not as though it's unfiltered.

1019
01:37:37,720 --> 01:37:44,720
I think the questions of etiquette, protocol, and what these filters are haven't been part of a discussion enough,

1020
01:37:44,720 --> 01:37:47,720
but we're not also asking young people enough.

1021
01:37:47,720 --> 01:37:49,720
How are they already managing this privacy?

1022
01:37:49,720 --> 01:37:53,720
They don't want parents looking. They don't want employers looking at everything, right?

1023
01:37:53,720 --> 01:37:58,720
They don't even want their partners, their romantic partners looking at all of their communication either.

1024
01:37:58,720 --> 01:38:05,720
So I would just want to complicate the notion that it's sort of this flood that almost is coming from individuals

1025
01:38:05,720 --> 01:38:11,720
rather than a social compulsion, again, around express yourself, or there's something questionable about you.

1026
01:38:11,720 --> 01:38:18,720
If you don't express yourself, if you're not on call 24-7, says the family, says the job,

1027
01:38:18,720 --> 01:38:21,720
says reality TV, right? You're hiding something.

1028
01:38:21,720 --> 01:38:26,720
So I think that what we've lost is a notion that, yeah, that we might want to say that there's something worth hiding,

1029
01:38:26,720 --> 01:38:32,720
which is a different one than just saying people are just giving freely and willingly, but what is it worth protecting?

1030
01:38:32,720 --> 01:38:36,720
Not just its privacy, but its secrecy, which might be a little different too.

1031
01:38:36,720 --> 01:38:41,720
And I also suggest that it's maybe more a difference in degree and not necessarily a difference in kind.

1032
01:38:41,720 --> 01:38:47,720
You've always had to share extraordinarily personal information in order to obtain certain services.

1033
01:38:47,720 --> 01:38:53,720
It is true now your medical records, for example, are often available online, but those medical records used to reside in your doctor's office.

1034
01:38:53,720 --> 01:39:02,720
The digitization makes them more easily accessible and brings with it complicated questions about informational security and privacy.

1035
01:39:02,720 --> 01:39:04,720
That's another con, though.

1036
01:39:04,720 --> 01:39:15,720
Again, a new audience has more gray hair than the population at large, but clearly,

1037
01:39:15,720 --> 01:39:27,720
I mean, you can go into a restaurant and someone's taking a picture of what they're eating to send to someone as they're eating it,

1038
01:39:27,720 --> 01:39:33,720
or they're describing what they're doing in the here and now.

1039
01:39:33,720 --> 01:39:39,720
And so these are another kind of information.

1040
01:39:39,720 --> 01:39:45,720
But it's hard to compare that to what would have happened 50 years ago when the technology didn't exist.

1041
01:39:45,720 --> 01:40:02,720
But the point is the technology allows for it, and I do believe there are social rules, psychological things that are changing in modern societies,

1042
01:40:02,720 --> 01:40:09,720
which are undoing this, loneliness.

1043
01:40:09,720 --> 01:40:17,720
I mean, silence, not having it light all the time, or being in the dark.

1044
01:40:17,720 --> 01:40:20,720
We evolved with light and dark.

1045
01:40:20,720 --> 01:40:24,720
We don't have to be in the dark anymore.

1046
01:40:24,720 --> 01:40:26,720
We can go on and on.

1047
01:40:26,720 --> 01:40:30,720
You can talk to someone in a continuous fashion.

1048
01:40:30,720 --> 01:40:34,720
We're texting in cars as we're driving.

1049
01:40:34,720 --> 01:40:42,720
I mean, there was a lot of insanity if you look at it from a cultural point of view that are going on.

1050
01:40:42,720 --> 01:40:49,720
I mean, yes, you have a nice dinner and you talk about it.

1051
01:40:49,720 --> 01:40:53,720
You say, gee, we went to this wonderful restaurant and you should go with it.

1052
01:40:53,720 --> 01:40:59,720
We had this dish and you'd describe it if you're a foodie, but now you can take a picture of it.

1053
01:40:59,720 --> 01:41:03,720
And not only show it later, but show it at the time you're eating.

1054
01:41:03,720 --> 01:41:05,720
You're not going to show it later. That's the thing.

1055
01:41:05,720 --> 01:41:07,720
So why not just think of it as temporality?

1056
01:41:07,720 --> 01:41:09,720
I'm doing it now. I don't have to deal with it later.

1057
01:41:09,720 --> 01:41:14,720
So it's a moment where it interrupts the present, but it just becomes a temporality.

1058
01:41:14,720 --> 01:41:17,720
It takes it out of the private.

1059
01:41:17,720 --> 01:41:24,720
It takes it out of eating the meal, at least either by yourself or with the person you're eating with.

1060
01:41:24,720 --> 01:41:32,720
And now you make it a public eating. This dessert is now a public act.

1061
01:41:32,720 --> 01:41:37,720
Now, if you think about it, okay, I mean, maybe that's a good thing.

1062
01:41:37,720 --> 01:41:39,720
I don't know, but it sure as hell is a new thing.

1063
01:41:39,720 --> 01:41:42,720
Well, it's always a public act. You're in a restaurant doing it.

1064
01:41:42,720 --> 01:41:44,720
The question is how big the audience is.

1065
01:41:44,720 --> 01:41:47,720
We're streaming this event, right? Or where it'll be streamed at some point.

1066
01:41:47,720 --> 01:41:51,720
Has the role of privacy changed in child development and families?

1067
01:41:51,720 --> 01:41:53,720
I think it has changed.

1068
01:41:53,720 --> 01:41:56,720
It's evolving things no longer.

1069
01:41:56,720 --> 01:42:00,720
Strict signs of demarcations that used to exist.

1070
01:42:00,720 --> 01:41:53,720
That's a

1071
01:42:00,720 --> 01:42:01,720
a subjective artifact.

1072
01:42:01,720 --> 01:42:04,720
My father was not my friend. He was my father.

1073
01:42:04,720 --> 01:42:07,720
And now fathers have to be friends.

1074
01:42:07,720 --> 01:42:13,720
And what does friendship require?

1075
01:42:13,720 --> 01:42:21,720
Well, friendship requires a reciprocality, which means giving up some of your privacy.

1076
01:42:21,720 --> 01:42:25,720
I actually have been trying to understand that.

1077
01:42:25,720 --> 01:42:27,720
I'm not sure it's a good thing.

1078
01:42:27,720 --> 01:42:31,720
I'm just not sure that it isn't leading.

1079
01:42:31,720 --> 01:42:39,720
Indeed, our parenting is not in fact leading a whole new set of generations.

1080
01:42:39,720 --> 01:42:46,720
Almost two now to in fact not to just to know everything.

1081
01:42:46,720 --> 01:42:51,720
And this is part of it. It's ongoing knowing everything.

1082
01:42:51,720 --> 01:42:57,720
Now, that's a setup when you say there's danger out there and we're going to protect you.

1083
01:42:57,720 --> 01:43:01,720
Because you used to it to begin with.

1084
01:43:01,720 --> 01:43:07,720
And now you've got to they're giving you some kind of what seems like a good reason,

1085
01:43:07,720 --> 01:43:11,720
which of course as you were saying isn't.

1086
01:43:11,720 --> 01:43:17,720
One, you have to analyze this huge amount that you're collecting, which is not easy to do.

1087
01:43:17,720 --> 01:43:22,720
But the fact is I think there is a psychological change.

1088
01:43:22,720 --> 01:43:28,720
And I think we as older people in fact are not as susceptible to it.

1089
01:43:28,720 --> 01:43:34,720
And I know as I the young and look at the public behavior,

1090
01:43:34,720 --> 01:43:42,720
I mean others in the room look at therapeutic sense and a different kind of behavior.

1091
01:43:42,720 --> 01:43:51,720
But in a public behavior, I see things which are just out of my experience.

1092
01:43:51,720 --> 01:43:54,720
I think we have time for one more question.

1093
01:43:54,720 --> 01:43:57,720
Two short ones.

1094
01:43:57,720 --> 01:43:59,720
Okay.

1095
01:43:59,720 --> 01:44:03,720
I'm a Lexi Kalaturak, a Summit Analyst here at New York Signaling.

1096
01:44:03,720 --> 01:44:09,720
So I had I think going through a few of the things I'm here, closer sorry.

1097
01:44:09,720 --> 01:44:16,720
Trust and mistrust, can you believe anything or can you know, certainly the governmental to private citizen level.

1098
01:44:16,720 --> 01:44:20,720
But I think it also comes up in the use of the technology and families.

1099
01:44:20,720 --> 01:44:23,720
I've been talking about child development.

1100
01:44:23,720 --> 01:44:31,720
Sometimes I'll have parents in my office who will say, you know, my kids' new phone has GPS on it.

1101
01:44:31,720 --> 01:44:34,720
I'll attract them.

1102
01:44:34,720 --> 01:44:37,720
Well, maybe.

1103
01:44:37,720 --> 01:44:43,720
And then hopefully we can get into a discussion of what would the purpose of tracking them be?

1104
01:44:43,720 --> 01:44:46,720
What information would they gain from that?

1105
01:44:46,720 --> 01:44:50,720
What would they do with that information?

1106
01:44:50,720 --> 01:44:57,720
And then it doesn't become an easy yes or no answer at that point, which I think is good.

1107
01:44:57,720 --> 01:45:01,720
So I think it can come up in other ways.

1108
01:45:01,720 --> 01:45:13,720
You know, is the government benevolently listening to everything or are they really just looking for terrorists or, you know, one of the, I think Obama's not finest moments was basically telling us, don't worry about what the NSA is doing, right?

1109
01:45:13,720 --> 01:45:16,720
But in a family, that might be okay.

1110
01:45:16,720 --> 01:45:24,720
In a family, a teenager might come home with alcohol in his breath and a parent may think, should I say something?

1111
01:45:24,720 --> 01:45:27,720
Well, that depends.

1112
01:45:27,720 --> 01:45:28,720
Have you talked about this before?

1113
01:45:28,720 --> 01:45:29,720
Is this the first time?

1114
01:45:29,720 --> 01:45:31,720
Is this the tenth time?

1115
01:45:31,720 --> 01:45:46,720
And hopefully what's there is a relationship in which you can maybe trust your teenager to a certain point, but not too much, but that it's established and that you can count on it in some way.

1116
01:45:46,720 --> 01:45:50,720
Obviously, when we're talking about private citizens and the government, we can't do that.

1117
01:45:50,720 --> 01:45:52,720
Some people feel you can't trust anything.

1118
01:45:52,720 --> 01:45:55,720
They say some people feel, well, you know, they probably know what they're doing.

1119
01:45:55,720 --> 01:46:04,720
So that's why we have rules and I'm very thankful we have people like you guarding our, but the balance of those things.

1120
01:46:04,720 --> 01:46:08,720
But I think it comes up in other ways too.

1121
01:46:08,720 --> 01:46:15,720
One other comment just on the issue of sort of groups and dissent.

1122
01:46:15,720 --> 01:46:22,720
One of the times op-ed columnist from many years ago wrote a column saying, never use your credit card, always pay cash.

1123
01:46:22,720 --> 01:46:25,720
It was the libertarian guy, I can't remember his name.

1124
01:46:25,720 --> 01:46:27,720
And I thought, well, why?

1125
01:46:27,720 --> 01:46:29,720
I mean, it's easy to use a credit card.

1126
01:46:29,720 --> 01:46:30,720
It's convenient.

1127
01:46:30,720 --> 01:46:36,720
Well, okay, but I'm not doing anything terribly embarrassing on my credit card or subversive.

1128
01:46:36,720 --> 01:46:44,720
But if I were a little bit, then I would probably want to be able to pay cash and not worry about it and not have myself targeted.

1129
01:46:44,720 --> 01:46:46,720
In some way.

1130
01:46:46,720 --> 01:46:58,720
So I think with texting and with every email being possibly recorded, we may be making a white bread society where it's really dangerous to say anything that's a little too provocative.

1131
01:46:58,720 --> 01:47:11,720
If you're a student who wants to go to college one day, God forbid somebody from the admissions office somewhere should see your Facebook where maybe you were holding a beer and you were underage, etc, etc.

1132
01:47:11,720 --> 01:47:19,720
I would say that in the 1980s, they had an album called Give Me Convenience or Give Me Death.

1133
01:47:19,720 --> 01:47:22,720
40 years ago to today, it still works.

1134
01:47:22,720 --> 01:47:29,720
Quick comment, I don't have a Facebook profile and I have experienced a certain amount of peer pressure to get one.

1135
01:47:29,720 --> 01:47:33,720
I've had friends say to me, come on, it's fun.

1136
01:47:33,720 --> 01:47:42,720
And I feel like an anomaly that I don't.

1137
01:47:42,720 --> 01:47:49,720
But my quick question is, were you recommending that individuals encrypt their email correspondence?

1138
01:47:49,720 --> 01:47:52,720
And if so, how do we get the information?

1139
01:47:52,720 --> 01:47:55,720
I'm happy to talk to the technology effort, if you like.

1140
01:47:55,720 --> 01:48:05,720
It turns out if you encrypt your emails, there are special rules that apply for the NSA's collection and they can collect and keep that information indefinitely under pretty much any one of their programs.

1141
01:48:05,720 --> 01:48:10,720
So the more you try to protect your privacy, the more susceptible you are to NSA surveillance.

1142
01:48:10,720 --> 01:48:17,720
Now they may not be able to decrypt it immediately, but they can keep it so long as they deem fit.

1143
01:48:17,720 --> 01:48:27,720
I think this is really one of the fundamental paradoxes of the situation, the hackers are the people that are involved in protecting our privacy, these kinds of dichotomies.

1144
01:48:27,720 --> 01:48:30,720
No, don't draw attention to yourself.

1145
01:48:30,720 --> 01:48:39,720
And there are interesting questions about the role of attention and direction of attention in terms of constructing the self when you have all of this information equally available.

1146
01:48:39,720 --> 01:48:48,720
The one thing that technology can do is make passive or pervasive or dragnet surveillance costly enough not to be effective.

1147
01:48:48,720 --> 01:48:52,720
It can force the government to engage in targeted rather than dragnet surveillance.

1148
01:48:52,720 --> 01:48:58,720
And the virtue of that is that the government then will use its limited resources where it should.

1149
01:48:58,720 --> 01:49:04,720
And it turns out even if you encrypt your communications, it's very easy to encrypt your communications in transit.

1150
01:49:04,720 --> 01:49:11,720
But it's very difficult to secure your endpoints. It's very difficult to secure your laptop or to secure your desktop.

1151
01:49:11,720 --> 01:49:17,720
And that's because Windows and Mac and all these operating systems are fundamentally insecure because they're too complicated to make secure.

1152
01:49:17,720 --> 01:49:20,720
And so if the NSA wants to get to you in a targeted way, they can.

1153
01:49:20,720 --> 01:49:28,720
And there's nothing to stop them. And that might be a good thing because they're generally when they use their resources in a targeted way or focusing on the right people.

1154
01:49:28,720 --> 01:49:35,720
And so technology can make untargeted surveillance difficult and force the government to engage in the type of targeted surveillance that should be engaging in.

1155
01:49:35,720 --> 01:49:42,720
So if I could say yes to you and 350 million other people I would, but if it's just, yeah, we can talk about it.

1156
01:49:42,720 --> 01:50:01,720
Thank you everybody for coming and for participating so thoroughly and thank our panelists very, very much.

