1
00:00:00,000 --> 00:00:07,680
I'm Ed Nurekseion, the Director of the Center for Today's Roundtable on Transhumanism was

2
00:00:07,680 --> 00:00:16,000
proposed by Bill Grasse, whom I will introduce and then he will introduce the rest of the

3
00:00:16,000 --> 00:00:17,600
participants.

4
00:00:17,600 --> 00:00:19,880
Just two words about future programs.

5
00:00:19,880 --> 00:00:28,320
We have a program on boredom on April 22, I believe, and you'll get the notice from

6
00:00:28,320 --> 00:00:37,160
our mailing list and then on May 12, we have a program on the completeness of physics.

7
00:00:37,160 --> 00:00:38,160
Thank you.

8
00:00:38,160 --> 00:00:41,840
We're interested in this topic of transhumanism.

9
00:00:41,840 --> 00:00:48,640
For me, it started in 2000 when I was involved in a conference at the University of Pennsylvania

10
00:00:48,640 --> 00:00:55,480
on life extension technology and then I started meeting all these people.

11
00:00:55,480 --> 00:01:01,560
The science itself is really interesting, but the proponents of it are also very interesting.

12
00:01:01,560 --> 00:01:07,880
The whole question of who we are as humans is something that engages me philosophically

13
00:01:07,880 --> 00:01:10,640
and otherwise.

14
00:01:10,640 --> 00:01:17,480
This question of how we're changing raises all kinds of really important questions.

15
00:01:17,480 --> 00:01:19,120
That's how I got into it.

16
00:01:19,120 --> 00:01:25,440
My formal training is in comparative religion, religious studies, and of course death is

17
00:01:25,440 --> 00:01:27,640
a big issue in that as well.

18
00:01:27,640 --> 00:01:33,280
These days I call myself more a big historian.

19
00:01:33,280 --> 00:01:38,400
So I'm really interested in the big story that science is telling and how we understand

20
00:01:38,400 --> 00:01:40,720
and interpret it today.

21
00:01:40,720 --> 00:01:42,960
How about you, Lee Silver?

22
00:01:42,960 --> 00:01:43,960
Hi.

23
00:01:43,960 --> 00:01:49,400
I'm Lee Silver, professor Princeton.

24
00:01:49,400 --> 00:01:53,680
Training is in biophysics, molecular biology.

25
00:01:53,680 --> 00:02:02,440
In the 1990s I wrote a book called Remaking Eden which talks about the technological advances

26
00:02:02,440 --> 00:02:08,000
that allow genetic engineering and the potential that genetic engineering could be used on

27
00:02:08,000 --> 00:02:10,600
humans in the future.

28
00:02:10,600 --> 00:02:16,040
The biggest negative response I got was from other scientists who said what you're talking

29
00:02:16,040 --> 00:02:19,040
about is never going to happen.

30
00:02:19,040 --> 00:02:27,560
20 years later, in fact, the technology is going faster than even I had predicted.

31
00:02:27,560 --> 00:02:29,000
So it's going to happen.

32
00:02:29,000 --> 00:02:31,000
We'll talk more.

33
00:02:31,000 --> 00:02:32,200
So I'm Father Sandelson.

34
00:02:32,200 --> 00:02:35,160
I'm from Arizona State University.

35
00:02:35,160 --> 00:02:37,160
I work on three areas.

36
00:02:37,160 --> 00:02:42,040
One is the relationship between philosophy and mysticism within the Jewish tradition.

37
00:02:42,040 --> 00:02:45,200
Another one is religion and ecology.

38
00:02:45,200 --> 00:02:48,400
And the third is religion science and technology.

39
00:02:48,400 --> 00:02:53,280
So my interest in transhumanism is through that angle.

40
00:02:53,280 --> 00:03:00,040
I actually got to know Billy Grassy through my work with the Templeton Foundation.

41
00:03:00,040 --> 00:03:01,520
So I've been in that conversation.

42
00:03:01,520 --> 00:03:07,760
I would say since 2003 but seriously since 2006.

43
00:03:07,760 --> 00:03:10,520
And I've been writing as a critic of transhumanism.

44
00:03:10,520 --> 00:03:14,640
So you're going to hear from me some of the criticism to the views that you're going to

45
00:03:14,640 --> 00:03:15,640
espouse.

46
00:03:15,640 --> 00:03:16,640
Hi everybody.

47
00:03:16,640 --> 00:03:19,640
I'm Francesca Rossi.

48
00:03:19,640 --> 00:03:26,080
I work at the IBM Research Center.

49
00:03:26,080 --> 00:03:31,800
I also have a professor of computer science at the University of Padawan, Italy.

50
00:03:31,800 --> 00:03:36,680
And I've been working in AI, artificial intelligence for many years.

51
00:03:36,680 --> 00:03:40,280
I think that my first conference was 30 years ago.

52
00:03:40,280 --> 00:03:43,760
So I've seen a lot of ups and downs in this technology.

53
00:03:43,760 --> 00:03:51,800
Now is a very high point for various reasons.

54
00:03:51,800 --> 00:03:58,120
And this also led me to being interested into the impact of this technology.

55
00:03:58,120 --> 00:04:04,560
Not on society or individuals or making us think of what we really want this technology

56
00:04:04,560 --> 00:04:09,880
to achieve by itself in autonomy or also together with us.

57
00:04:09,880 --> 00:04:16,800
So I think that that's what led me to being interested in the topic of this panel.

58
00:04:16,800 --> 00:04:23,880
Because I think that I'm optimistic that by understanding better how we want to shape

59
00:04:23,880 --> 00:04:30,480
the future of AI will also make us understand better what we have ourselves and how we want

60
00:04:30,480 --> 00:04:32,560
to relate to that technology.

61
00:04:32,560 --> 00:04:39,360
But having said that, I'm here I think much less expert than the other ones on the topic

62
00:04:39,360 --> 00:04:40,360
of the panel.

63
00:04:40,360 --> 00:04:46,080
So I'm here mostly to learn like everybody else.

64
00:04:46,080 --> 00:04:47,600
My name is Stephen Post.

65
00:04:47,600 --> 00:04:54,200
I direct the Center for Medical Humanities, Compassionate Care and Bioethics at Stony Brook

66
00:04:54,200 --> 00:04:56,640
University, Seattle Medicine.

67
00:04:56,640 --> 00:05:03,440
I've been there 10 years having migrated across Route 80 from Cleveland, Ohio in case

68
00:05:03,440 --> 00:05:05,560
med.

69
00:05:05,560 --> 00:05:13,040
I've been interested in pretty much all the great themes related to transhumanism.

70
00:05:13,040 --> 00:05:20,120
In the 1990s I was publishing a lot with Peter Whitehouse who had the patent on the colon

71
00:05:20,120 --> 00:05:23,640
S. race inhibitors in the treatment of Alzheimer's disease.

72
00:05:23,640 --> 00:05:30,000
And we were writing about cognitive enhancement using medications, not therapeutically, but

73
00:05:30,000 --> 00:05:35,600
actually to elevate certain kinds of fundamental capacities.

74
00:05:35,600 --> 00:05:41,920
I also went to that conference that Lee and Billy were at in 2000 at the University of

75
00:05:41,920 --> 00:05:47,880
Pennsylvania and we published a book called The Fountain of Youth which had every kind

76
00:05:47,880 --> 00:05:54,880
of scientific and philosophical and world religious and psychological perspective on

77
00:05:54,880 --> 00:06:00,360
the benefits and difficulties of anti-aging technologies.

78
00:06:00,360 --> 00:06:03,520
And that's certainly just rocketed now.

79
00:06:03,520 --> 00:06:08,040
I think way beyond whatever we thought might be possible.

80
00:06:08,040 --> 00:06:11,920
I've written a book, Why Good Things Happen to Good People, which is essentially a treat

81
00:06:11,920 --> 00:06:14,200
as on happiness.

82
00:06:14,200 --> 00:06:23,080
And thinking about whether certain forms of happiness can be technologically sustained,

83
00:06:23,080 --> 00:06:28,720
hedonic flow happiness, eudeministic happiness and so forth.

84
00:06:28,720 --> 00:06:35,240
Very interested in genetics and the perfect child over the many years in bioethics of

85
00:06:35,240 --> 00:06:37,240
course.

86
00:06:37,240 --> 00:06:45,440
So all of these aspects of transhumanism come together and they challenge us to think about

87
00:06:45,440 --> 00:06:53,440
how we want to recreate human nature going forward.

88
00:06:53,440 --> 00:06:56,120
So what do you think?

89
00:06:56,120 --> 00:06:57,120
Can we live forever?

90
00:06:57,120 --> 00:06:58,120
I don't know.

91
00:06:58,120 --> 00:06:59,120
It seems preposterous.

92
00:06:59,120 --> 00:07:01,120
But some of the...

93
00:07:01,120 --> 00:07:05,280
Lee, you're the biologist here.

94
00:07:05,280 --> 00:07:12,000
What are the technologies that are pushing us to the possibility of radical life extension?

95
00:07:12,000 --> 00:07:15,640
So when we say we, I assume you're talking about...

96
00:07:15,640 --> 00:07:18,560
The human species.

97
00:07:18,560 --> 00:07:19,680
And into the future.

98
00:07:19,680 --> 00:07:28,640
So I wrote a book that's 20 years ago talking about how technologies could be used to do

99
00:07:28,640 --> 00:07:31,360
everything basically that we could.

100
00:07:31,360 --> 00:07:35,720
Another DNA, it's not called genome editing because that sounds better than genetic engineering.

101
00:07:35,720 --> 00:07:44,400
And we could put in characteristics that would allow humans to avoid disease, live longer,

102
00:07:44,400 --> 00:07:46,600
be healthier.

103
00:07:46,600 --> 00:07:53,240
And what I've seen over the last 20 years in the field is that these technologies are

104
00:07:53,240 --> 00:07:57,640
happening faster than even anybody expected.

105
00:07:57,640 --> 00:07:59,440
It's going to happen.

106
00:07:59,440 --> 00:08:04,440
So I would say, I don't know, I mean, I don't believe in infinity, but will people be able

107
00:08:04,440 --> 00:08:07,760
to... will you be able to have children who live longer?

108
00:08:07,760 --> 00:08:08,760
Absolutely.

109
00:08:08,760 --> 00:08:12,880
I mean, will we already know how you could do that?

110
00:08:12,880 --> 00:08:16,200
Can you do it to yourself?

111
00:08:16,200 --> 00:08:18,640
That's not quite as easy.

112
00:08:18,640 --> 00:08:20,960
I don't have a response on that.

113
00:08:20,960 --> 00:08:22,680
So we're all familiar.

114
00:08:22,680 --> 00:08:28,480
Ray Kurzweil's book, The Singularity, it's what now, five, six, seven years old.

115
00:08:28,480 --> 00:08:34,560
He argues that there's all these exponential trends and we're moving towards this place

116
00:08:34,560 --> 00:08:44,320
where by 2045, we will pass a threshold in which we're going to be able to replenish

117
00:08:44,320 --> 00:08:51,280
the telomeres and our chromosomes, replace our organs with genetically cultivated organs

118
00:08:51,280 --> 00:08:55,840
out of pigs or other animals.

119
00:08:55,840 --> 00:09:06,200
And you know, it's, I guess, the natural human, maximum human lifespan is 125, somewhere

120
00:09:06,200 --> 00:09:07,200
around there.

121
00:09:07,200 --> 00:09:08,200
It's a current genome.

122
00:09:08,200 --> 00:09:11,960
Pardon, for the current genome, right.

123
00:09:11,960 --> 00:09:17,000
There are some species that live really long time.

124
00:09:17,000 --> 00:09:18,000
Turtles?

125
00:09:18,000 --> 00:09:19,000
Turtles.

126
00:09:19,000 --> 00:09:27,040
Anyway, that's only one aspect of the Transhumanist project.

127
00:09:27,040 --> 00:09:30,720
There's also the enhancement issues that are going on.

128
00:09:30,720 --> 00:09:32,520
You're jumping a little bit too much here.

129
00:09:32,520 --> 00:09:33,520
Okay, go ahead.

130
00:09:33,520 --> 00:09:37,160
You started with living, radical life extension is just one aspect.

131
00:09:37,160 --> 00:09:42,280
I would like to suggest that there's a difference between living longer and living forever.

132
00:09:42,280 --> 00:09:47,560
But over the gray and other proponents of radical life extension want is basically the postponement

133
00:09:47,560 --> 00:09:48,560
of death.

134
00:09:48,560 --> 00:09:51,000
There's a perpetual postponement of death.

135
00:09:51,000 --> 00:09:56,120
So the first task on the burden of proof is on you in this case to show us that this

136
00:09:56,120 --> 00:10:01,000
is a, doable, feasible, and b, is desirable.

137
00:10:01,000 --> 00:10:04,760
I would argue that it's definitely not desirable.

138
00:10:04,760 --> 00:10:05,760
I'm not a scientist.

139
00:10:05,760 --> 00:10:09,760
I cannot say if it's doable, but that's where the science should come in.

140
00:10:09,760 --> 00:10:14,400
So we can talk about radical life extension, but it's really one element in transhumanism

141
00:10:14,400 --> 00:10:17,640
that requires a lot more conversation and unpacking.

142
00:10:17,640 --> 00:10:19,560
Why is it not desirable?

143
00:10:19,560 --> 00:10:24,280
Okay, so, and what's the, you're playing into my field of that?

144
00:10:24,280 --> 00:10:25,280
Yeah, what's the option?

145
00:10:25,280 --> 00:10:31,400
So the option is, options, object, not desirable to who.

146
00:10:31,400 --> 00:10:32,560
Okay, fair enough.

147
00:10:32,560 --> 00:10:35,840
So let's talk about radical life extension.

148
00:10:35,840 --> 00:10:38,600
Why is it a good idea?

149
00:10:38,600 --> 00:10:40,800
And why is it not a good idea?

150
00:10:40,800 --> 00:10:45,240
So the argument is a title of the book suggests we all want to be young.

151
00:10:45,240 --> 00:10:50,680
I would first begin with a question, do we really, do I really want to be 20 years old?

152
00:10:50,680 --> 00:10:51,680
I definitely don't.

153
00:10:51,680 --> 00:10:55,560
Do I want to be a healthy person in my current stage?

154
00:10:55,560 --> 00:10:56,880
I mean my late 60s.

155
00:10:56,880 --> 00:10:58,440
Yes, I do.

156
00:10:58,440 --> 00:11:02,960
But between that and radical life extension, there's a big difference.

157
00:11:02,960 --> 00:11:08,160
So let's figure out what over the gray and other people who push for that agenda really

158
00:11:08,160 --> 00:11:09,160
want.

159
00:11:09,160 --> 00:11:11,920
They want, or reason.

160
00:11:11,920 --> 00:11:13,800
Everybody knows the name over the gray.

161
00:11:13,800 --> 00:11:15,680
No, you should know his work.

162
00:11:15,680 --> 00:11:19,440
He's a major proponent of radical, operate the gray.

163
00:11:19,440 --> 00:11:20,440
Radical life extension.

164
00:11:20,440 --> 00:11:21,760
He's a major prophet of that.

165
00:11:21,760 --> 00:11:22,760
What does he want?

166
00:11:22,760 --> 00:11:30,920
He wants us to be really living forever through perpetual remaking of the human body.

167
00:11:30,920 --> 00:11:33,240
The main metaphor is the car.

168
00:11:33,240 --> 00:11:35,880
The, the, yeah, well-built car.

169
00:11:35,880 --> 00:11:39,200
Are we really, is that a very good metaphor for a human?

170
00:11:39,200 --> 00:11:40,200
Not sure.

171
00:11:40,200 --> 00:11:41,520
I would argue that it's not.

172
00:11:41,520 --> 00:11:46,200
So, but you didn't say, I think, or I didn't hear why it's not there's arable.

173
00:11:46,200 --> 00:11:52,920
I mean, nobody wants to go back to being unexperienced, you know, like when we were 18 or so.

174
00:11:52,920 --> 00:11:58,240
But moving forward with more experience, more.

175
00:11:58,240 --> 00:11:59,680
And so does that.

176
00:11:59,680 --> 00:12:05,800
So can we avoid the degrade of our body so that not, not, I mean, so that, you know, we

177
00:12:05,800 --> 00:12:06,800
can.

178
00:12:06,800 --> 00:12:11,800
But with aging with some, some kind of you lose some, some capacity, yes, indeed.

179
00:12:11,800 --> 00:12:13,200
No, but they're not.

180
00:12:13,200 --> 00:12:14,200
What's wrong with that?

181
00:12:14,200 --> 00:12:16,120
What's wrong with being able to do that?

182
00:12:16,120 --> 00:12:17,120
Even though I don't know.

183
00:12:17,120 --> 00:12:18,760
There's nothing going on with my life.

184
00:12:18,760 --> 00:12:19,760
We will be healthier.

185
00:12:19,760 --> 00:12:21,800
The healthier we are, the better it is.

186
00:12:21,800 --> 00:12:23,520
But that's not what over is all about.

187
00:12:23,520 --> 00:12:27,440
And all the radical life extension project, as well as cryonics, don't forget that.

188
00:12:27,440 --> 00:12:29,080
That's also part of the story.

189
00:12:29,080 --> 00:12:31,800
What this is about is really immortality now.

190
00:12:31,800 --> 00:12:38,640
I would question the logic and the wisdom of this idea of immortality as they understand

191
00:12:38,640 --> 00:12:44,560
it in a very materialistic and especially machine based kind of approach to immortality.

192
00:12:44,560 --> 00:12:45,560
That's my art.

193
00:12:45,560 --> 00:12:47,560
But what would be the problem of it?

194
00:12:47,560 --> 00:12:50,560
So let's talk about the, okay.

195
00:12:50,560 --> 00:12:51,560
What would be the problem?

196
00:12:51,560 --> 00:12:56,080
Number one, what would people do if they live to be 500 years old?

197
00:12:56,080 --> 00:12:57,520
Learn, continuously learn.

198
00:12:57,520 --> 00:12:58,520
Learn the development.

199
00:12:58,520 --> 00:12:59,520
New science.

200
00:12:59,520 --> 00:13:02,080
I would make most people's worries.

201
00:13:02,080 --> 00:13:04,040
And what exactly would they do?

202
00:13:04,040 --> 00:13:10,000
And would the world as we know this planet, a very vulnerable planet can support all those

203
00:13:10,000 --> 00:13:13,360
billions of people who are going to live here forever?

204
00:13:13,360 --> 00:13:15,160
We can be doing planetary.

205
00:13:15,160 --> 00:13:17,880
So then we're going to space the colonization.

206
00:13:17,880 --> 00:13:22,160
So space colonization is the extension of radical life extension.

207
00:13:22,160 --> 00:13:24,160
That's exactly what's going on.

208
00:13:24,160 --> 00:13:26,160
So what's wrong with this debate?

209
00:13:26,160 --> 00:13:32,800
Is ancient, of course, and when Francis Bacon wrote, the New Atlantis, there were the waters

210
00:13:32,800 --> 00:13:35,560
of paradise, basically a fountain of youth.

211
00:13:35,560 --> 00:13:41,920
A hundred years later, when Jonathan Swift wrote Gulliver's Travels, Gulliver comes among

212
00:13:41,920 --> 00:13:49,520
a people called the Leagnagians and unusually, occasionally Leagnagians are born who are

213
00:13:49,520 --> 00:13:51,240
called Strollbrugs.

214
00:13:51,240 --> 00:13:53,280
That is to say, Immortals.

215
00:13:53,280 --> 00:13:56,560
And at first, Gulliver is excited about this.

216
00:13:56,560 --> 00:14:05,480
But then the chief of the Leagnagians tells him that they become forgetful, deeply forgetful,

217
00:14:05,480 --> 00:14:10,200
in their eighth decade of life and can no longer remember the words that were said immediately

218
00:14:10,200 --> 00:14:11,800
prior to the moment.

219
00:14:11,800 --> 00:14:16,440
And eventually, everyone hates them because they're incapable of communication.

220
00:14:16,440 --> 00:14:20,120
So he says to Gulliver, take a few of these Immortals back to your own people and tell

221
00:14:20,120 --> 00:14:21,920
them not to fear death.

222
00:14:21,920 --> 00:14:25,440
So there's always been the utopian and the dystopian aspect to this.

223
00:14:25,440 --> 00:14:30,040
And you bring it right up to the 20th century and you have JBS Haldane and then the reaction

224
00:14:30,040 --> 00:14:37,640
of Tolkien whose elves were immortal unless they were killed by a physical blow or in

225
00:14:37,640 --> 00:14:39,160
war.

226
00:14:39,160 --> 00:14:44,400
But there was a kind of malaise, a kind of listlessness about them, a kind of lack of

227
00:14:44,400 --> 00:14:46,520
intensity and purpose.

228
00:14:46,520 --> 00:14:51,280
Again, mortality being the mother of creativity, if you will.

229
00:14:51,280 --> 00:14:53,280
And so this debate-

230
00:14:53,280 --> 00:14:56,040
The mortality is good according to that, the way just back it's-

231
00:14:56,040 --> 00:15:02,680
Well, so that's well for Tolkien, for C.S. Lewis, for many others, mortality is a virtue.

232
00:15:02,680 --> 00:15:09,680
And Erwin, who is the great figure, gives up her immortality to marry Aragorn because

233
00:15:09,680 --> 00:15:12,520
she has a vision of a son.

234
00:15:12,520 --> 00:15:20,360
And she realizes that she can live a more fulfilled, eudaministic life by embracing

235
00:15:20,360 --> 00:15:24,640
a son as a mother than being an eternal elf.

236
00:15:24,640 --> 00:15:28,360
So it seems to me that this notion of mortality is good.

237
00:15:28,360 --> 00:15:33,920
When we were at this conference in 2000, Leon Katz, and there was another philosopher,

238
00:15:33,920 --> 00:15:37,560
the pro-death camp, is what I call them.

239
00:15:37,560 --> 00:15:40,080
And they were arguing against- No, it's a pro-life.

240
00:15:40,080 --> 00:15:41,240
I would say it's a pro-life.

241
00:15:41,240 --> 00:15:42,240
It's not pro-death.

242
00:15:42,240 --> 00:15:44,440
It's life and death at two sides of the same coin.

243
00:15:44,440 --> 00:15:47,800
So I mean, I think there's a religious component to this.

244
00:15:47,800 --> 00:15:49,880
It was, you know, until the 20th century.

245
00:15:49,880 --> 00:15:55,160
There was nothing we could do to expand, you know, make life longer.

246
00:15:55,160 --> 00:15:58,000
And it goes along the same lines with disease.

247
00:15:58,000 --> 00:16:02,040
There was nothing we could do to prevent most diseases.

248
00:16:02,040 --> 00:16:06,040
And so the religious response was, well, it's good to suffer.

249
00:16:06,040 --> 00:16:08,880
It sort of, you know, improves your soul in some way.

250
00:16:08,880 --> 00:16:12,640
And I think that's, I mean, I don't believe that nonsense.

251
00:16:12,640 --> 00:16:22,040
But I also think there's a difference between immortality versus life expansion, you know,

252
00:16:22,040 --> 00:16:24,400
in units, you know?

253
00:16:24,400 --> 00:16:25,400
Okay.

254
00:16:25,400 --> 00:16:28,120
So the human life, some people lived to 120.

255
00:16:28,120 --> 00:16:30,240
I think that's probably the maximum right now.

256
00:16:30,240 --> 00:16:34,600
And it goes up to 150, 200.

257
00:16:34,600 --> 00:16:36,720
I don't see the problem.

258
00:16:36,720 --> 00:16:38,640
I mean, the population is not a problem.

259
00:16:38,640 --> 00:16:44,720
The Japan, which has the longest lifespan is going down in population.

260
00:16:44,720 --> 00:16:46,480
I mean, they're very, very worried.

261
00:16:46,480 --> 00:16:50,360
They're not community people left because the women aren't having babies.

262
00:16:50,360 --> 00:16:58,880
But of their 120 million people, 14% of them have probable Alzheimer's disease because

263
00:16:58,880 --> 00:17:00,360
age is the main predictor.

264
00:17:00,360 --> 00:17:06,160
So the whole thing of life extension to me, I don't have an issue with it.

265
00:17:06,160 --> 00:17:11,600
And life extension, I mean, life span is the longest at any single representative of a

266
00:17:11,600 --> 00:17:13,520
species is known to have lived.

267
00:17:13,520 --> 00:17:20,080
Life expectancy is the average life, length of life in a given population in time.

268
00:17:20,080 --> 00:17:25,600
Japanese have an 88 year life expectancy.

269
00:17:25,600 --> 00:17:30,040
Huge problems with now, if you were to get to a point where we had people living longer,

270
00:17:30,040 --> 00:17:34,000
and I actually don't object to it terribly because I think aging is a disease, at least

271
00:17:34,000 --> 00:17:35,160
it's a syndrome.

272
00:17:35,160 --> 00:17:36,160
Okay?

273
00:17:36,160 --> 00:17:43,040
Our cluster of symptoms, it doesn't feel good to me.

274
00:17:43,040 --> 00:17:46,720
You have to include in that the compression of morbidity.

275
00:17:46,720 --> 00:17:52,280
So if you didn't have the compression of morbidity, for example, the eradication of

276
00:17:52,280 --> 00:17:59,160
these chronic diseases of old age, then you would have Swift's worst case scenario,

277
00:17:59,160 --> 00:18:04,160
which is extended life, but incredible dysfunction.

278
00:18:04,160 --> 00:18:07,160
Yeah, I don't think anybody wants that.

279
00:18:07,160 --> 00:18:08,320
For the purpose of what?

280
00:18:08,320 --> 00:18:10,880
That's the main issue that we need to put on the table.

281
00:18:10,880 --> 00:18:12,840
What's the purpose of being a human being?

282
00:18:12,840 --> 00:18:13,840
What do you want to accomplish?

283
00:18:13,840 --> 00:18:20,680
So tell us more, transhumanism gives us a purpose, a telos, the telos is posthumanism.

284
00:18:20,680 --> 00:18:24,240
So how do they envision the purpose of the whole operation?

285
00:18:24,240 --> 00:18:27,800
Ultimately, it's really the mechanization of the human.

286
00:18:27,800 --> 00:18:33,680
It's the total immersion and infusion and it's not an interface between the human and

287
00:18:33,680 --> 00:18:34,680
the machine.

288
00:18:34,680 --> 00:18:36,560
So if we ask, what is it all about?

289
00:18:36,560 --> 00:18:38,840
Why are we to live longer?

290
00:18:38,840 --> 00:18:42,280
It's ultimately in order to become a super intelligent machine.

291
00:18:42,280 --> 00:18:46,120
So that's the difference between transhumanism and posthumanism.

292
00:18:46,120 --> 00:18:52,280
Transhumanism is the process of enhancement of all the things that we are doing right

293
00:18:52,280 --> 00:18:53,280
now already.

294
00:18:53,280 --> 00:18:58,280
We are in a sense in a transhumanist position already or status already, but that's not

295
00:18:58,280 --> 00:18:59,440
where we're going to end.

296
00:18:59,440 --> 00:19:00,720
What it's all about?

297
00:19:00,720 --> 00:19:02,120
It's posthumanism.

298
00:19:02,120 --> 00:19:04,480
The total fusion between humans and computers.

299
00:19:04,480 --> 00:19:06,480
Are you for that?

300
00:19:06,480 --> 00:19:07,480
Are you for that?

301
00:19:07,480 --> 00:19:09,000
I'm definitely not for that.

302
00:19:09,000 --> 00:19:20,440
So I mean, we are already with technology and science and we are already, our capabilities

303
00:19:20,440 --> 00:19:25,080
are already extended by many technological things.

304
00:19:25,080 --> 00:19:28,520
And we also have the responsibility to critique what's happening.

305
00:19:28,520 --> 00:19:34,400
There's no doubt that transhumanism has impacted each and every aspect of human life today.

306
00:19:34,400 --> 00:19:35,400
No doubt about that.

307
00:19:35,400 --> 00:19:40,240
That's why I think it needs to be taken very seriously and not dismissed just a full-hardy

308
00:19:40,240 --> 00:19:41,240
thing.

309
00:19:41,240 --> 00:19:43,280
Some people have done in the beginning of the conversation.

310
00:19:43,280 --> 00:19:46,840
When I started working on these 15 years ago, people said, why are you spending time

311
00:19:46,840 --> 00:19:47,840
of that?

312
00:19:47,840 --> 00:19:48,840
It's all nonsense.

313
00:19:48,840 --> 00:19:49,840
I said, no, it's not nonsense.

314
00:19:49,840 --> 00:19:50,840
This is where we're going.

315
00:19:50,840 --> 00:19:51,840
We've got to critique it.

316
00:19:51,840 --> 00:19:53,320
We've got to look at it carefully.

317
00:19:53,320 --> 00:19:55,320
I think we're already there there.

318
00:19:55,320 --> 00:19:56,320
I think consulting is actually...

319
00:19:56,320 --> 00:19:59,640
It's actually better to...

320
00:19:59,640 --> 00:20:04,880
I'm back in the late 80s, a really unique thinker.

321
00:20:04,880 --> 00:20:09,520
We're Donna Haraway wrote a famous essay called Cyborg Manifesto.

322
00:20:09,520 --> 00:20:12,160
And I thought, wow, that's kind of cool.

323
00:20:12,160 --> 00:20:13,160
And it's true.

324
00:20:13,160 --> 00:20:18,640
And in some sense, we need to think about our technology as part of who we are and it changes

325
00:20:18,640 --> 00:20:20,400
who we are.

326
00:20:20,400 --> 00:20:23,280
And it's not just the glasses or the computers.

327
00:20:23,280 --> 00:20:26,280
Cars are part of nature.

328
00:20:26,280 --> 00:20:33,080
It's just a very complicated sex life that uses us as symbionts in their replication.

329
00:20:33,080 --> 00:20:34,080
And so...

330
00:20:34,080 --> 00:20:39,640
I'm saying this from an environmental perspective or what?

331
00:20:39,640 --> 00:20:45,920
I'm saying it in the sense that rather than think of our identity ending with our epidermis,

332
00:20:45,920 --> 00:20:51,600
sometimes it's inside our epidermis too, when we have transplants and so on and so forth.

333
00:20:51,600 --> 00:20:56,480
But rather than think of it that way, I think we should think of all these prosthetic devices

334
00:20:56,480 --> 00:20:59,800
as things that change who we are.

335
00:20:59,800 --> 00:21:04,680
And so whether you call us cyborgs or whether you call us transhumanists or when you have

336
00:21:04,680 --> 00:21:11,200
a different understanding of posthumanists, I think we're already radically changed species.

337
00:21:11,200 --> 00:21:19,040
And we don't experience this so much partly as individuals, but it's primarily a collective

338
00:21:19,040 --> 00:21:24,840
expression of who we are as a species and how we're evolving.

339
00:21:24,840 --> 00:21:28,520
And if you say, all right, so we're already post-homing, we're already transhumanists.

340
00:21:28,520 --> 00:21:33,840
That sort of takes away some of the utopic and dystopic dimensions of it.

341
00:21:33,840 --> 00:21:36,400
It's just a big muddle.

342
00:21:36,400 --> 00:21:41,800
And it's good things and bad things are both happening at the same time.

343
00:21:41,800 --> 00:21:44,160
I think it's great if we can improve health.

344
00:21:44,160 --> 00:21:47,200
I think it's great if we can improve longevity.

345
00:21:47,200 --> 00:21:50,360
I think it's very unlikely we're going to cure death.

346
00:21:50,360 --> 00:21:51,640
And I'll come back to that.

347
00:21:51,640 --> 00:21:59,360
I think it's unlikely that we're going to have the runaway artificial intelligence, which

348
00:21:59,360 --> 00:22:01,880
is going to take over the human species.

349
00:22:01,880 --> 00:22:08,640
But it is very likely and it's already happening that AI will augment all our capabilities.

350
00:22:08,640 --> 00:22:10,760
That's more likely than immortality.

351
00:22:10,760 --> 00:22:14,440
But the question is whether it's you welcoming it.

352
00:22:14,440 --> 00:22:18,480
You're not going to try to create friendly AI.

353
00:22:18,480 --> 00:22:28,200
Just before we turn it over to you, I just want to point out that there are people, Elon Musk,

354
00:22:28,200 --> 00:22:34,960
I mean Elon Musk, for instance, who thinks that the greatest danger that humanity faces

355
00:22:34,960 --> 00:22:42,400
is runaway artificial intelligence that's going to take over the planet and find us dispensable.

356
00:22:42,400 --> 00:22:45,800
And so I just want to point it out.

357
00:22:45,800 --> 00:22:50,800
And then do you think that's a realistic scenario or something to worry about?

358
00:22:50,800 --> 00:22:53,600
Some people worry about yes.

359
00:22:53,600 --> 00:22:56,120
Whether I think it's a realistic scenario.

360
00:22:56,120 --> 00:23:05,280
Usually it's a question that I don't work on a lot because I think that whether that's

361
00:23:05,280 --> 00:23:11,520
going to be realistic or not, I can be better prepared for whatever and whenever that moment

362
00:23:11,520 --> 00:23:12,520
will be.

363
00:23:12,520 --> 00:23:21,400
If I think about more current issues in augmenting our humanity with AI right now, there are

364
00:23:21,400 --> 00:23:23,960
issues to be considered right now.

365
00:23:23,960 --> 00:23:30,440
And there are very powerful algorithms and techniques even right now, even if we don't

366
00:23:30,440 --> 00:23:35,240
have general artificial intelligence, a very narrow one for the specific task.

367
00:23:35,240 --> 00:23:38,520
But still it is super intelligent on that task.

368
00:23:38,520 --> 00:23:45,480
So there are issues to be considered right now to work on, to find solutions, to guide

369
00:23:45,480 --> 00:23:53,960
these human intelligent augmentation via technology in a way that we can welcome this.

370
00:23:53,960 --> 00:23:58,360
Yes, I would like to welcome that.

371
00:23:58,360 --> 00:24:02,800
But not that you're asking if I welcome this fusion.

372
00:24:02,800 --> 00:24:07,840
If it's a fusion without even thinking about the implications and without thinking about

373
00:24:07,840 --> 00:24:11,120
the issues without working on that, maybe not.

374
00:24:11,120 --> 00:24:17,480
But if it's a fusion that is done day by day, improving and thinking about the issues and

375
00:24:17,480 --> 00:24:23,080
finding collective solutions and guiding it, then yes, I do welcome it.

376
00:24:23,080 --> 00:24:26,400
And you didn't tell me why I should not welcome it.

377
00:24:26,400 --> 00:24:27,400
Okay.

378
00:24:27,400 --> 00:24:32,880
So what I find problematic about the whole AI operation, we are obviously not talking

379
00:24:32,880 --> 00:24:38,840
the same language because you're talking about something very specific in the practice of

380
00:24:38,840 --> 00:24:40,240
artificial intelligence.

381
00:24:40,240 --> 00:24:44,600
I'm talking about transhumanism as basically as the way it should be understood, which

382
00:24:44,600 --> 00:24:46,000
is a social imaginary.

383
00:24:46,000 --> 00:24:47,000
It's a story.

384
00:24:47,000 --> 00:24:53,440
It's a story about humanity, about the destiny of humanity and what we need to do.

385
00:24:53,440 --> 00:24:55,520
And I critique it on that level.

386
00:24:55,520 --> 00:25:00,400
I critique it from a point of view of a political thinker and intellectually story and so forth.

387
00:25:00,400 --> 00:25:01,400
Okay.

388
00:25:01,400 --> 00:25:03,760
So we are not really talking the same language.

389
00:25:03,760 --> 00:25:04,760
Related.

390
00:25:04,760 --> 00:25:05,760
Yeah, definitely related.

391
00:25:05,760 --> 00:25:11,760
So my problem is, yes, we are going to live, we are living with machine next to machines,

392
00:25:11,760 --> 00:25:13,160
but I don't want to become a machine.

393
00:25:13,160 --> 00:25:14,480
I want them to understand.

394
00:25:14,480 --> 00:25:18,080
I want people like Nick Bostrom, the idea logs of transhumanism.

395
00:25:18,080 --> 00:25:24,280
I want them to understand why becoming a super intelligent machine is a problematic idea.

396
00:25:24,280 --> 00:25:26,960
I'll give you these three arguments.

397
00:25:26,960 --> 00:25:30,440
Number one, I'm first and foremost a body.

398
00:25:30,440 --> 00:25:35,240
I'm not just a pattern that is going to be instantiated in Silicon.

399
00:25:35,240 --> 00:25:43,800
I'm an embodied person and this embodied entity has mental cognitive as well as physical,

400
00:25:43,800 --> 00:25:45,640
emotional and all the rest.

401
00:25:45,640 --> 00:25:48,240
And I want to keep that integrity intact.

402
00:25:48,240 --> 00:25:54,760
But if you are going to upload me onto a computer, I'm not going to be the embodied thing that

403
00:25:54,760 --> 00:25:55,840
I am right now.

404
00:25:55,840 --> 00:25:57,200
I would call it an organism.

405
00:25:57,200 --> 00:26:02,880
I will use the organism element once I become instantiated in a computer.

406
00:26:02,880 --> 00:26:04,520
Okay, that's point number one.

407
00:26:04,520 --> 00:26:09,920
Point number two, the embodied aspect is very crucial because it has something to do with

408
00:26:09,920 --> 00:26:11,320
sex and with gender.

409
00:26:11,320 --> 00:26:15,600
The one thing that gets lost in this entire conversation is the gender dimension as if

410
00:26:15,600 --> 00:26:20,280
it doesn't exist, as if there is no sexuality, there is no reproduction, there is none of

411
00:26:20,280 --> 00:26:21,280
that exists.

412
00:26:21,280 --> 00:26:22,280
Why?

413
00:26:22,280 --> 00:26:25,280
Because all this business was created by men, excuse me.

414
00:26:25,280 --> 00:26:28,120
They are all AI operations.

415
00:26:28,120 --> 00:26:30,920
But anyway, I would like to protect embodiment.

416
00:26:30,920 --> 00:26:32,800
Okay, so that's a very important thing.

417
00:26:32,800 --> 00:26:35,240
Also the environmental dimension is very important to me.

418
00:26:35,240 --> 00:26:41,080
I don't think that what's going to happen with this AI taking over is good for the planet

419
00:26:41,080 --> 00:26:43,280
from environmental perspective.

420
00:26:43,280 --> 00:26:45,960
Unless you prove otherwise, let them prove otherwise.

421
00:26:45,960 --> 00:26:47,200
Right now, I think that they...

422
00:26:47,200 --> 00:26:49,440
What ways are not good for the planet?

423
00:26:49,440 --> 00:26:54,840
For example, extracting all those metals that you need in order to do what you're doing.

424
00:26:54,840 --> 00:26:57,960
Has anybody talked about what happens?

425
00:26:57,960 --> 00:27:00,000
Where do you get all these material from?

426
00:27:00,000 --> 00:27:01,720
It comes from the earth and some form...

427
00:27:01,720 --> 00:27:04,000
How are they more efficient than human beings?

428
00:27:04,000 --> 00:27:05,000
Yeah.

429
00:27:05,000 --> 00:27:06,000
So we co-sold from...

430
00:27:06,000 --> 00:27:07,000
So we co-sold from...

431
00:27:07,000 --> 00:27:08,000
How do you measure efficiency?

432
00:27:08,000 --> 00:27:13,480
As energy, human body consumes about 100 watts.

433
00:27:13,480 --> 00:27:14,480
Yeah, yeah, of course.

434
00:27:14,480 --> 00:27:18,480
I mean, out of being in our brain, it's about 20 watts.

435
00:27:18,480 --> 00:27:24,640
When the Europeans proposed some years ago to build a computer simulation of a brain,

436
00:27:24,640 --> 00:27:29,640
it was going to take nuclear power plants to do what...

437
00:27:29,640 --> 00:27:31,680
Yes, to just to run the computers.

438
00:27:31,680 --> 00:27:39,360
So, but then if you take out this uploading of whatever, so the disappearance of the embodiment,

439
00:27:39,360 --> 00:27:43,760
which is not something that I'm considering at all, but I mean, if you take those...

440
00:27:43,760 --> 00:27:45,320
That's true, that's fine.

441
00:27:45,320 --> 00:27:46,320
It's fine.

442
00:27:46,320 --> 00:27:49,760
So, I don't have to agree with everybody.

443
00:27:49,760 --> 00:27:51,320
But, yeah, there's one more.

444
00:27:51,320 --> 00:27:52,560
That's the third element.

445
00:27:52,560 --> 00:27:57,040
The real thing that bugs me is the reduction of the human to a pattern.

446
00:27:57,040 --> 00:27:59,040
You're dealing with patterns, right?

447
00:27:59,040 --> 00:28:00,040
When you're...

448
00:28:00,040 --> 00:28:02,040
You turn us into an algorithm.

449
00:28:02,040 --> 00:28:06,800
No, I'm trying to use algorithm to augment our capabilities.

450
00:28:06,800 --> 00:28:09,280
That's not a reduction to a pattern.

451
00:28:09,280 --> 00:28:14,000
Why don't you tell us a little bit about what your research is and what's going on in IBM

452
00:28:14,000 --> 00:28:15,000
once?

453
00:28:15,000 --> 00:28:17,000
Yeah, but I mean, not just in IBM.

454
00:28:17,000 --> 00:28:18,000
I mean, everywhere.

455
00:28:18,000 --> 00:28:25,400
I mean, so there are many people in AI that think that the purpose of AI is to augment

456
00:28:25,400 --> 00:28:26,400
our intelligence.

457
00:28:26,400 --> 00:28:27,400
When are called?

458
00:28:27,400 --> 00:28:30,720
So, that's why at IBM at some point, they didn't want to use artificial intelligence

459
00:28:30,720 --> 00:28:36,080
because it was kind of misleading as if you were creating another thing by itself and

460
00:28:36,080 --> 00:28:37,080
so on.

461
00:28:37,080 --> 00:28:41,400
So, at some point, they called it AI, but it was augmented intelligence.

462
00:28:41,400 --> 00:28:46,240
And now they said, okay, let's call it AI artificial intelligence, but what we mean is

463
00:28:46,240 --> 00:28:48,240
augmenting human intelligence.

464
00:28:48,240 --> 00:28:57,040
So, we have our own form of intelligence which brings intuition, asking the right questions,

465
00:28:57,040 --> 00:29:03,760
judgment, you know, and the AI has other things that you can do much better than us.

466
00:29:03,760 --> 00:29:09,440
So, this complementarity can bring the things together, but with the goal of not making

467
00:29:09,440 --> 00:29:15,960
us a machine, but using the machines to help us being better humans, whatever we want.

468
00:29:15,960 --> 00:29:17,680
We can define that.

469
00:29:17,680 --> 00:29:19,520
We're not going to disappear.

470
00:29:19,520 --> 00:29:21,040
We are going to be enhanced.

471
00:29:21,040 --> 00:29:23,600
So, I have this optimistic view.

472
00:29:23,600 --> 00:29:28,400
Of course, you have to be careful in doing this teaming, AI and humans.

473
00:29:28,400 --> 00:29:31,200
You have to be careful in the issues that can come out.

474
00:29:31,200 --> 00:29:36,200
You have to be careful in making the machines understand what humans want and not vice versa.

475
00:29:36,200 --> 00:29:41,360
You have to be careful in making the machine speak the language that we speak and not the

476
00:29:41,360 --> 00:29:43,160
opposite.

477
00:29:43,160 --> 00:29:50,160
And so, it really has to be an enhancement of ourselves and not us having to come to terms

478
00:29:50,160 --> 00:29:52,160
with the machines desire things.

479
00:29:52,160 --> 00:29:55,800
Do they want things to happen?

480
00:29:55,800 --> 00:29:58,600
If you tell, I mean, the machine has a goal.

481
00:29:58,600 --> 00:30:05,160
The question is, we know that natural selection is a natural process.

482
00:30:05,160 --> 00:30:10,560
So, this sounds very science-fiction-y and I wouldn't have thought, I mean, ten years

483
00:30:10,560 --> 00:30:13,160
ago, I would have thought this is a total nonsense.

484
00:30:13,160 --> 00:30:18,880
But machines that can learn how to replicate themselves or not the machines as much as

485
00:30:18,880 --> 00:30:24,720
the algorithm, the programs, if they can learn how to replicate themselves in the world,

486
00:30:24,720 --> 00:30:27,280
then natural selection takes over.

487
00:30:27,280 --> 00:30:35,040
Kevin Kelly, one of the founders of Wired Magazine wrote a book, What Technology Wants.

488
00:30:35,040 --> 00:30:40,400
And it's the old adage, you know, if all you have is a hammer, then every problem looks

489
00:30:40,400 --> 00:30:42,760
like a nail.

490
00:30:42,760 --> 00:30:49,560
And so, there is a way that, you know, the machines in our lives, whether it's a car,

491
00:30:49,560 --> 00:30:56,000
whether it's a computer or, you know, somewhat direct our behavior.

492
00:30:56,000 --> 00:31:05,400
Well, that's the autonomous technology thesis and it's certainly the case in medicine.

493
00:31:05,400 --> 00:31:09,000
The machinery becomes more elaborate.

494
00:31:09,000 --> 00:31:14,720
It takes a lot of effort, but it can be done to get a control over the utilization of those

495
00:31:14,720 --> 00:31:15,720
machines.

496
00:31:15,720 --> 00:31:22,880
I think, I mean, we keep talking about we as if that means something.

497
00:31:22,880 --> 00:31:23,880
Because I don't...

498
00:31:23,880 --> 00:31:24,880
You'll mind to...

499
00:31:24,880 --> 00:31:26,800
Well, but we're all individuals, right?

500
00:31:26,800 --> 00:31:27,800
There's a lot of individuals.

501
00:31:27,800 --> 00:31:32,240
I'm not going to, you know, leave my body and go into a machine.

502
00:31:32,240 --> 00:31:35,240
I'm not going to let somebody else make me into a machine.

503
00:31:35,240 --> 00:31:39,440
But people think that way, then, who is the we that's going to do it?

504
00:31:39,440 --> 00:31:40,440
So that's...

505
00:31:40,440 --> 00:31:42,080
So now we're on the same page here.

506
00:31:42,080 --> 00:31:48,680
I'm saying, and you seem to agree with that, that we need to look at those proposals and

507
00:31:48,680 --> 00:31:50,120
be critical of them.

508
00:31:50,120 --> 00:31:55,360
We shouldn't just add on this, kind of buy into it without any critical analysis.

509
00:31:55,360 --> 00:32:01,680
Let me remind all of us that when Nick Bostum started the whole conversation at least 15

510
00:32:01,680 --> 00:32:05,520
years ago, nobody spoke about the dangers of AI.

511
00:32:05,520 --> 00:32:13,120
In his most recent book, 2015, Super Intelligent Machines, only there he even he begins to

512
00:32:13,120 --> 00:32:18,400
doubt or begins to admit that there may be some things that we don't want to happen

513
00:32:18,400 --> 00:32:19,400
with artificial intelligence.

514
00:32:19,400 --> 00:32:26,280
But 15 years ago, he did not even entertain the possibility that there may be some malicious

515
00:32:26,280 --> 00:32:27,280
AI.

516
00:32:27,280 --> 00:32:29,800
So now they're busy doing friendly AI.

517
00:32:29,800 --> 00:32:34,000
If you read The New York Times just a few days ago, there is an essay precisely on that

518
00:32:34,000 --> 00:32:35,000
topic.

519
00:32:35,000 --> 00:32:36,920
I have it here.

520
00:32:36,920 --> 00:32:39,560
And just where is it?

521
00:32:39,560 --> 00:32:44,440
Yes, how to make a human friendly?

522
00:32:44,440 --> 00:32:51,920
So my problem is, why didn't they listen to the critics 15 years ago?

523
00:32:51,920 --> 00:32:54,440
Because 15 years ago, AI was not so pervasive.

524
00:32:54,440 --> 00:33:01,040
It was not so impactful on our life and society 15 years ago, we didn't have enough computing

525
00:33:01,040 --> 00:33:07,520
power and enough data to make AI learn and be aware of the environment and be used in

526
00:33:07,520 --> 00:33:09,400
the uncertainty of the world.

527
00:33:09,400 --> 00:33:17,080
So there was no issue because AI was not really giving was only used in very controlled environments.

528
00:33:17,080 --> 00:33:18,880
Now, instead, it's different.

529
00:33:18,880 --> 00:33:21,400
So that's why people think about these issues.

530
00:33:21,400 --> 00:33:22,400
Are we too late?

531
00:33:22,400 --> 00:33:28,600
So 15 years ago, I wrote an essay in which I said, our official intelligence that could

532
00:33:28,600 --> 00:33:34,920
drive a car across town and park it in the parking lot would pass the Turing test.

533
00:33:34,920 --> 00:33:41,480
That would be for me proof of its intelligence and the way that even more than the traditional

534
00:33:41,480 --> 00:33:42,480
Turing test.

535
00:33:42,480 --> 00:33:49,120
Of course, now that's doable.

536
00:33:49,120 --> 00:33:52,840
So one of the things that interests me is the limits, and I've written about this as

537
00:33:52,840 --> 00:34:01,200
well, the limits of computation and the possible limits of the genetic engineering as well,

538
00:34:01,200 --> 00:34:03,840
that these are complex systems.

539
00:34:03,840 --> 00:34:11,280
And the more complex the system gets, the more you start pushing against nonlinear dynamics

540
00:34:11,280 --> 00:34:13,000
and unpredictability.

541
00:34:13,000 --> 00:34:20,600
And so the genome, I understand to be a bureaucracy, not a bunch of all-off switches.

542
00:34:20,600 --> 00:34:26,160
And within computer science, I'm not an expert in this, there are problems that can't be

543
00:34:26,160 --> 00:34:27,160
solved.

544
00:34:27,160 --> 00:34:31,080
There are problems that, because you can't write an algorithm for it, and there are other

545
00:34:31,080 --> 00:34:34,520
problems that are just too hard to solve.

546
00:34:34,520 --> 00:34:41,240
And I just wonder whether in these two domains, we might be running up against a complexity

547
00:34:41,240 --> 00:34:42,240
horizon.

548
00:34:42,240 --> 00:34:45,040
And these two domains are joining.

549
00:34:45,040 --> 00:34:47,520
And the brain as well, I would say, is also another potentially.

550
00:34:47,520 --> 00:34:51,480
Yes, and those three domains are joining forces.

551
00:34:51,480 --> 00:34:55,280
I mean, deep learning now, nobody knows how these algorithms work.

552
00:34:55,280 --> 00:34:57,120
Well, that's not exactly true.

553
00:34:57,120 --> 00:34:58,680
I mean, nobody knows.

554
00:34:58,680 --> 00:34:59,680
No.

555
00:34:59,680 --> 00:35:03,760
There's a bit of a black box, but somebody had to set up the black box to work and they

556
00:35:03,760 --> 00:35:06,120
have data into it, and they have to...

557
00:35:06,120 --> 00:35:12,200
Yeah, but if you ask why you did something, why you made the second decision,

558
00:35:12,200 --> 00:35:14,880
which is still a bit opaque.

559
00:35:14,880 --> 00:35:20,360
I mean, in some cases, you can go back to the kind of training data that allowed to make

560
00:35:20,360 --> 00:35:27,160
that decision, but it's still a bit opaque, much more opaque than traditional logic-based

561
00:35:27,160 --> 00:35:33,160
AI systems, which however, which were much less accurate than deep learning.

562
00:35:33,160 --> 00:35:36,640
And now there's a reproducibility problem in some of the AI.

563
00:35:36,640 --> 00:35:37,640
Yeah.

564
00:35:37,640 --> 00:35:39,440
And there should be.

565
00:35:39,440 --> 00:35:40,440
Okay, why?

566
00:35:40,440 --> 00:35:46,840
I mean, well, first of all, if you're talking about deep learning, and there's many, many

567
00:35:46,840 --> 00:35:52,120
hidden layers in the process of the machine learning how to recognize things.

568
00:35:52,120 --> 00:35:55,680
So you've given a lot of data and you give it some algorithms and...

569
00:35:55,680 --> 00:35:57,120
Yeah, but you know, but it's...

570
00:35:57,120 --> 00:35:59,520
Do people know how deep learning works?

571
00:35:59,520 --> 00:36:00,520
There is a...

572
00:36:00,520 --> 00:36:01,520
Go ahead, play it.

573
00:36:01,520 --> 00:36:03,200
No, I mean, it's very simple.

574
00:36:03,200 --> 00:36:07,520
I mean, it's not in the details of the algorithm, but it's very, very simple.

575
00:36:07,520 --> 00:36:16,160
So if you want an algorithm that can recognize whether in a picture that is a cat or a dog,

576
00:36:16,160 --> 00:36:17,160
okay?

577
00:36:17,160 --> 00:36:21,800
So you give it a picture and you want this algorithm to say whether it's a cat or a dog,

578
00:36:21,800 --> 00:36:22,800
okay?

579
00:36:22,800 --> 00:36:28,320
So what you do, you start with an algorithm that is behaving very randomly, okay?

580
00:36:28,320 --> 00:36:30,200
It doesn't know anything.

581
00:36:30,200 --> 00:36:33,600
And you start giving a lot of examples.

582
00:36:33,600 --> 00:36:34,600
What is one example?

583
00:36:34,600 --> 00:36:41,080
An example is one picture and the information that I give to the algorithm that I say whether

584
00:36:41,080 --> 00:36:43,360
in that picture there is a cat or a dog, okay?

585
00:36:43,360 --> 00:36:47,080
So I say, there's a picture, there's a cat, there's a picture, there's a dog, there's

586
00:36:47,080 --> 00:36:48,080
a picture, there's a cat.

587
00:36:48,080 --> 00:36:50,320
And I give a lot of these examples.

588
00:36:50,320 --> 00:36:55,800
And then the algorithm, by looking at one example after the other one in sequence, the

589
00:36:55,800 --> 00:37:02,040
beginning is kind of random and then it starts tuning its parameters in a way that it learns

590
00:37:02,040 --> 00:37:06,200
this relationship between pictures and whether there is a cat or a dog there.

591
00:37:06,200 --> 00:37:11,680
At the end of this, that we call the training phase, you have an algorithm that has some

592
00:37:11,680 --> 00:37:16,800
parameters tuned in a certain way that hopefully has learned the relationship between pictures

593
00:37:16,800 --> 00:37:22,160
and these two animals and the can able, is able to generalize it to picture that he has

594
00:37:22,160 --> 00:37:23,680
never seen before.

595
00:37:23,680 --> 00:37:28,600
So now you give it another picture, you don't tell if it's a cat or a dog and possibly the

596
00:37:28,600 --> 00:37:33,040
algorithm will be able to recognize whether there is a cat or a dog.

597
00:37:33,040 --> 00:37:37,600
And usually with this algorithm, you give it enough data, but not just enough data.

598
00:37:37,600 --> 00:37:42,920
If you give it data which is diverse enough, inclusive enough of all the possible situation,

599
00:37:42,920 --> 00:37:45,480
then the algorithm is able to generalize.

600
00:37:45,480 --> 00:37:51,600
And many deep learning approaches to, for example, automatic vision, like recognizing

601
00:37:51,600 --> 00:37:53,320
what is in a picture.

602
00:37:53,320 --> 00:37:56,040
Right now a very small percentage of error.

603
00:37:56,040 --> 00:37:59,600
An error is always there, but the percentage value is very, very small.

604
00:37:59,600 --> 00:38:01,760
So they are very, very good at generalizing.

605
00:38:01,760 --> 00:38:07,280
But you need a lot of data for training these examples and you need a lot of computing power

606
00:38:07,280 --> 00:38:10,320
because of this amount of data that you have to deal with.

607
00:38:10,320 --> 00:38:11,680
So the idea is very simple.

608
00:38:11,680 --> 00:38:16,960
You learn by example, which by the way, is something that AI people are trying to move

609
00:38:16,960 --> 00:38:18,920
forward from that.

610
00:38:18,920 --> 00:38:25,000
Because if you learn only from examples, the relationship between input and output, then

611
00:38:25,000 --> 00:38:29,600
if you give it another task, which is very similar to that, but not exactly the same,

612
00:38:29,600 --> 00:38:32,720
you have to start this whole process from scratch.

613
00:38:32,720 --> 00:38:38,520
Because the algorithm is actually not lamp general idea what is a cat or what is a dog.

614
00:38:38,520 --> 00:38:40,760
But it's just lamp this input-output relation.

615
00:38:40,760 --> 00:38:42,560
We're just still stupid.

616
00:38:42,560 --> 00:38:43,560
Yeah.

617
00:38:43,560 --> 00:38:47,920
Of course a two-year-old could do that.

618
00:38:47,920 --> 00:38:48,920
Yeah.

619
00:38:48,920 --> 00:38:49,920
So what's the utility?

620
00:38:49,920 --> 00:38:52,920
And why should we be worried that that's going to take over the world?

621
00:38:52,920 --> 00:38:55,760
That's not going to take over the world.

622
00:38:55,760 --> 00:38:59,800
That's just got to stop.

623
00:38:59,800 --> 00:39:03,480
So basically, this is analogous to the genome.

624
00:39:03,480 --> 00:39:08,880
The genome, each of us came from a single cell.

625
00:39:08,880 --> 00:39:16,240
And that single cell had information equivalent to 6 billion bits of digital data.

626
00:39:16,240 --> 00:39:17,240
That's it.

627
00:39:17,240 --> 00:39:24,840
Well, that cell with 6 billion bits of information grew into each human being.

628
00:39:24,840 --> 00:39:31,480
Can you change some of those bits and become a mouse, not a human being?

629
00:39:31,480 --> 00:39:33,200
And that's an algorithm.

630
00:39:33,200 --> 00:39:37,040
We don't know how that works.

631
00:39:37,040 --> 00:39:39,320
Evolution created this thing.

632
00:39:39,320 --> 00:39:47,200
In a sense, machine learning, the process, there are lots of hidden layers that are

633
00:39:47,200 --> 00:39:49,800
not telling the machine how to learn.

634
00:39:49,800 --> 00:39:52,680
It's just learning from what you're giving it.

635
00:39:52,680 --> 00:39:58,280
And what's happening with the genome and what's going to happen is using the computational

636
00:39:58,280 --> 00:40:06,800
tools to be able to see perturbations, how perturbations in the genome affect the output,

637
00:40:06,800 --> 00:40:07,800
the final output.

638
00:40:07,800 --> 00:40:13,280
How many human genetic diseases have been identified?

639
00:40:13,280 --> 00:40:14,280
Simple genetic diseases?

640
00:40:14,280 --> 00:40:15,280
Well, you could do 6,000.

641
00:40:15,280 --> 00:40:20,160
Well, diseases are not a good categorization.

642
00:40:20,160 --> 00:40:30,400
I mean, there are hundreds of thousands of mutations that can cause various diseases.

643
00:40:30,400 --> 00:40:33,960
Least started a company called Gene Peake.

644
00:40:33,960 --> 00:40:34,960
Gene Peake.

645
00:40:34,960 --> 00:40:35,960
Gene Peake.

646
00:40:35,960 --> 00:40:44,600
That does pre-pregnancy screening to identify what would be a good match.

647
00:40:44,600 --> 00:40:53,520
How reliable, how many traits are you screening for and how reliably.

648
00:40:53,520 --> 00:41:04,680
So, you know, I know this is used extensively in acidic Orthodox Jewish communities for certain

649
00:41:04,680 --> 00:41:06,200
recessive diseases.

650
00:41:06,200 --> 00:41:10,240
But tell us a little bit about where this is leading.

651
00:41:10,240 --> 00:41:13,960
The technology has exploded.

652
00:41:13,960 --> 00:41:19,880
So just a couple of years ago, you could look at, you know, 20 diseases first, 100.

653
00:41:19,880 --> 00:41:22,200
We now look at 7,000.

654
00:41:22,200 --> 00:41:29,680
And we look at, we create, we take a presumptive mother and people want to be a mother and

655
00:41:29,680 --> 00:41:35,160
a father, we take their DNA, it's digital information, then we create virtual babies,

656
00:41:35,160 --> 00:41:36,160
virtual genomes.

657
00:41:36,160 --> 00:41:40,920
You don't really know who's contributing the genes.

658
00:41:40,920 --> 00:41:41,920
It's 50, 50.

659
00:41:41,920 --> 00:41:46,200
Well, we create lots of virtual babies from a couple.

660
00:41:46,200 --> 00:41:49,880
And what do the virtual babies look like?

661
00:41:49,880 --> 00:41:54,040
Well, so we see whether they have disease right now.

662
00:41:54,040 --> 00:41:57,160
But the question is, okay, are there any limits?

663
00:41:57,160 --> 00:42:02,000
I'm talking primarily about ethical limits to this kind of screening.

664
00:42:02,000 --> 00:42:03,000
Wow.

665
00:42:03,000 --> 00:42:09,280
So in my view, people should be able to do what they want with their own data.

666
00:42:09,280 --> 00:42:12,720
But what's the goal of this analysis?

667
00:42:12,720 --> 00:42:18,800
So you take two people, the DNA of two people, you understand what are most of the many possibilities

668
00:42:18,800 --> 00:42:19,720
that are offspring.

669
00:42:19,720 --> 00:42:25,240
And then they can, if they want, they can use that information to avoid disease by picking

670
00:42:25,240 --> 00:42:26,240
an embryo.

671
00:42:26,240 --> 00:42:33,080
If you can embryo that doesn't have the genes causing disease, those in vitro fertilization,

672
00:42:33,080 --> 00:42:34,800
they start with a bunch of embryos.

673
00:42:34,800 --> 00:42:41,600
So you generate these offsprings, not just virtually, not just on a computer saying,

674
00:42:41,600 --> 00:42:48,240
this is the DNA that can come out, but as embryos, that's for them to decide.

675
00:42:48,240 --> 00:42:52,480
We give them the information so they can do that.

676
00:42:52,480 --> 00:43:00,320
How do you encounter this in hospitals today, Steven?

677
00:43:00,320 --> 00:43:12,920
Actually, there is a counter-cultural critique of the perfect baby notion, which is subjectively

678
00:43:12,920 --> 00:43:15,280
defined.

679
00:43:15,280 --> 00:43:17,600
And that's really the disability model.

680
00:43:17,600 --> 00:43:25,760
So people like Adrian Ash, for example, but many others have claimed that with our constant

681
00:43:25,760 --> 00:43:39,800
urge to enhance the lives and the lifespans that we bring into the world, that we eliminate

682
00:43:39,800 --> 00:43:49,600
the virtue of tolerance, of inclusion of a common or a shared humanity.

683
00:43:49,600 --> 00:43:58,280
And we begin as a culture to select certain traits which become definitive of a human being,

684
00:43:58,280 --> 00:44:02,720
having any kind of moral considerably.

685
00:44:02,720 --> 00:44:05,960
It becomes, if you will, sort of neo-ugenic.

686
00:44:05,960 --> 00:44:12,720
Now, that may not be the case, but that's the disability critique is that we benefit when

687
00:44:12,720 --> 00:44:19,760
we have, quote unquote, imperfection in the world because it teaches us to be more inclusive

688
00:44:19,760 --> 00:44:25,640
and to be understanding that in fact there are deeper things in community than hypercognitive

689
00:44:25,640 --> 00:44:29,080
values, hypercognitive achievements, and so forth.

690
00:44:29,080 --> 00:44:37,000
So the first thing I think it's very important to point out is that the perfect baby is fiction.

691
00:44:37,000 --> 00:44:39,320
There's no such thing as a perfect baby.

692
00:44:39,320 --> 00:44:41,320
My mom thought I was.

693
00:44:41,320 --> 00:44:47,720
I mean, that's not what people are doing right now.

694
00:44:47,720 --> 00:44:53,560
What they're trying to do is, you know, prevent a disease or, you know, provide resistance

695
00:44:53,560 --> 00:44:54,560
to a disease.

696
00:44:54,560 --> 00:44:56,560
I mean, that's not perfection.

697
00:44:56,560 --> 00:45:00,600
And I think it's really, I think people who are against the technology, they use this

698
00:45:00,600 --> 00:45:01,880
and it's a red herring.

699
00:45:01,880 --> 00:45:06,440
I mean, it's not what we're doing and there never will be a perfect baby.

700
00:45:06,440 --> 00:45:10,760
So can I comment on this though that so I actually agree with what you just said because

701
00:45:10,760 --> 00:45:21,480
in my view, I don't really engage in the transhumanist conversation at high philosophical levels.

702
00:45:21,480 --> 00:45:23,960
To me, it's all incremental.

703
00:45:23,960 --> 00:45:26,960
So we're going to have successful anti-aging.

704
00:45:26,960 --> 00:45:33,840
I was looking at a remarkable piece of work going on now as of, well, 2016 at the Mayo

705
00:45:33,840 --> 00:45:40,200
Clinic and its scripts where they're using an agent which combines chemotherapy and a

706
00:45:40,200 --> 00:45:42,440
plant dye, believe it or not.

707
00:45:42,440 --> 00:45:49,720
And they're actually eliminating senile cells from the kidneys of individuals with failing

708
00:45:49,720 --> 00:45:50,720
kidneys.

709
00:45:50,720 --> 00:45:53,480
They've been incredibly successful in mice.

710
00:45:53,480 --> 00:45:58,680
Now, this is therapeutic.

711
00:45:58,680 --> 00:46:04,600
We're doing things therapeutically but then we'll look pretty good as generalized enhancements.

712
00:46:04,600 --> 00:46:10,600
We'll all want to keep our organs from becoming senile, if you will.

713
00:46:10,600 --> 00:46:17,440
And similarly, the National Institute on Aging is devoting 55% right now but it's budget

714
00:46:17,440 --> 00:46:21,200
strictly to the science of anti-aging.

715
00:46:21,200 --> 00:46:27,920
Now that's why because they've given up, frankly, on finding solutions to many of the chronic

716
00:46:27,920 --> 00:46:34,640
illnesses, the main precipitating or risk factor for which is age itself.

717
00:46:34,640 --> 00:46:40,440
I mean, a hundred years ago, people got dementia because of syphilis, right?

718
00:46:40,440 --> 00:46:48,240
Now that they're living into their 70s and 80s, it's age itself and of course Alzheimer

719
00:46:48,240 --> 00:46:51,080
himself wasn't sure if he discovered a disease.

720
00:46:51,080 --> 00:46:57,560
He actually thought that he discovered a natural part of human senile aging.

721
00:46:57,560 --> 00:47:03,920
So because we've got to this kind of midway point where we're all living longer on average,

722
00:47:03,920 --> 00:47:10,280
we're so much more subject to these many diseases of old age, including cancer and so forth,

723
00:47:10,280 --> 00:47:18,080
that maybe the solution so the NIA thinks is to actually figure out, you know, telemirrically

724
00:47:18,080 --> 00:47:21,080
and in other ways what the process of senescence is.

725
00:47:21,080 --> 00:47:26,800
And if we can turn that around, okay, maybe we'll be living to be on average 112 years

726
00:47:26,800 --> 00:47:33,680
old or maybe 110 depending on who you talk with, you know, people do discuss these things.

727
00:47:33,680 --> 00:47:38,640
But if we can compress morbidity and we can get rid of these chronic illnesses, that would

728
00:47:38,640 --> 00:47:41,640
become an enhancement but a valuable enhancement.

729
00:47:41,640 --> 00:47:49,840
So in many of these areas, genetics, anti-aging, even psychiatry and so forth, I think that

730
00:47:49,840 --> 00:47:55,960
we start trying with the attempt to address a therapeutic need and then it just spills

731
00:47:55,960 --> 00:47:58,040
over because it's a good thing.

732
00:47:58,040 --> 00:47:59,400
I don't know if that makes any sense.

733
00:47:59,400 --> 00:48:00,400
No, it makes sense.

734
00:48:00,400 --> 00:48:01,400
That's a lot of sense.

735
00:48:01,400 --> 00:48:07,600
But usually the boundary between therapy and enhancement is very blurred and it's very

736
00:48:07,600 --> 00:48:08,600
fluid.

737
00:48:08,600 --> 00:48:09,600
It is.

738
00:48:09,600 --> 00:48:13,400
You're saying that it's kind of a natural growth that once we figure out the therapeutic

739
00:48:13,400 --> 00:48:15,320
stuff, it becomes enhancement.

740
00:48:15,320 --> 00:48:18,280
I don't think that that's where it comes to attention.

741
00:48:18,280 --> 00:48:19,280
It tends to.

742
00:48:19,280 --> 00:48:20,280
Well, maybe.

743
00:48:20,280 --> 00:48:25,000
But a lot of time it's this enhancement that I would call it an ideology.

744
00:48:25,000 --> 00:48:26,760
It's ideological thrust.

745
00:48:26,760 --> 00:48:29,240
It's kind of, we've got to be enhanced.

746
00:48:29,240 --> 00:48:31,600
If you're not enhanced, there's something wrong with you.

747
00:48:31,600 --> 00:48:33,680
And don't you dare stand in the process.

748
00:48:33,680 --> 00:48:40,040
I'm quoting actually Hugo DeGarris, the Australian transhumanist who said, don't you stand in

749
00:48:40,040 --> 00:48:43,160
the middle of this process of becoming perfect?

750
00:48:43,160 --> 00:48:46,120
So they use the concept of perfection.

751
00:48:46,120 --> 00:48:47,120
That's what they want.

752
00:48:47,120 --> 00:48:48,120
But they're just idiots.

753
00:48:48,120 --> 00:48:49,120
They're just idiots.

754
00:48:49,120 --> 00:48:50,120
I mean, Aubrey's degrade.

755
00:48:50,120 --> 00:48:52,680
You know, by the way, it doesn't have a position.

756
00:48:52,680 --> 00:48:55,200
He's not even a scientist.

757
00:48:55,200 --> 00:48:57,200
He's just a guy who hangs out in coffee shops in Cambridge.

758
00:48:57,200 --> 00:48:58,200
He's never met him.

759
00:48:58,200 --> 00:49:03,600
No, he's in the United States and he's now funded by the Metuzela.

760
00:49:03,600 --> 00:49:05,240
Well, that's true.

761
00:49:05,240 --> 00:49:08,120
But I mean, I won't say anything more about Aubrey.

762
00:49:08,120 --> 00:49:09,600
I mean, I don't know him too.

763
00:49:09,600 --> 00:49:12,000
I've encountered him a number of times.

764
00:49:12,000 --> 00:49:15,520
I think these people are sort of, what should I say?

765
00:49:15,520 --> 00:49:23,440
They're like adolescents in a science class at age 14 and some crazy idea comes along.

766
00:49:23,440 --> 00:49:25,680
And they think, oh my God, that's it.

767
00:49:25,680 --> 00:49:32,720
And they have no connection with the narrative of the human experience to be able to think

768
00:49:32,720 --> 00:49:34,480
critically about it, which is your point.

769
00:49:34,480 --> 00:49:40,760
So just to develop this conversation one more bit here, I wrote the other day to a friend

770
00:49:40,760 --> 00:49:46,440
of mine, Francisco Cardeso Comes de Matos, who's the world's leading south.

771
00:49:46,440 --> 00:49:47,440
He's a linguist.

772
00:49:47,440 --> 00:49:50,240
He's incredibly well known.

773
00:49:50,240 --> 00:49:52,520
And I asked him to reflect on France's humanism.

774
00:49:52,520 --> 00:49:55,200
Here's what he wrote back in an email.

775
00:49:55,200 --> 00:49:59,240
This is from Ressif Brazil.

776
00:49:59,240 --> 00:50:02,080
Human health enhancing question mark.

777
00:50:02,080 --> 00:50:05,840
Human dignity elevating question mark.

778
00:50:05,840 --> 00:50:09,960
Human mind expanding question mark.

779
00:50:09,960 --> 00:50:14,080
You know, jobs slept on my floor at Reed College, and he never let his kids play with computers

780
00:50:14,080 --> 00:50:16,400
growing up.

781
00:50:16,400 --> 00:50:19,960
Human spirituality probing question mark.

782
00:50:19,960 --> 00:50:22,800
Human creativity amplifying question mark.

783
00:50:22,800 --> 00:50:29,360
You should take a look at Delaney Rustin's great video screenagers, the movie, about

784
00:50:29,360 --> 00:50:35,360
how we're struggling with creativity and somatic learning and memory and knowledge and

785
00:50:35,360 --> 00:50:36,360
so forth.

786
00:50:36,360 --> 00:50:37,660
It's very interesting.

787
00:50:37,660 --> 00:50:44,480
With peaceful nature strengthening, human interaction facilitating, human science is

788
00:50:44,480 --> 00:50:46,480
integrating.

789
00:50:46,480 --> 00:50:49,840
So those are just reflections from someone down in the south.

790
00:50:49,840 --> 00:50:52,000
Yeah, but what does this question mark?

791
00:50:52,000 --> 00:50:53,000
Yes, it's what does it mean?

792
00:50:53,000 --> 00:50:54,480
Well, he's asking me to think about that.

793
00:50:54,480 --> 00:50:59,400
Well, the questions that you're asking, Hana, that we need to think carefully about these

794
00:50:59,400 --> 00:51:00,400
possibilities.

795
00:51:00,400 --> 00:51:04,120
But are those things that are, you know, that would be welcome or not?

796
00:51:04,120 --> 00:51:06,160
Well, he's wanting us to question them.

797
00:51:06,160 --> 00:51:10,280
Well, I think the implication is that those would all be positive.

798
00:51:10,280 --> 00:51:11,280
Yeah.

799
00:51:11,280 --> 00:51:14,120
And the question is, does transhumanism work?

800
00:51:14,120 --> 00:51:15,120
Right.

801
00:51:15,120 --> 00:51:17,880
Well, he's raising an ideology or his actual technology.

802
00:51:17,880 --> 00:51:22,800
He's raising many doubts about the transhumanist ideology because he doesn't think that it's

803
00:51:22,800 --> 00:51:24,880
attending necessarily.

804
00:51:24,880 --> 00:51:29,840
Now I'm not ruling it out, but he's saying his sense is that it's not attending to the

805
00:51:29,840 --> 00:51:35,920
most important elements of, shall we say, human enhancement in human, if you will, perfect

806
00:51:35,920 --> 00:51:46,120
ability, which has to do with character traits, dignity, empathy.

807
00:51:46,120 --> 00:51:48,160
You know, I have a close friend.

808
00:51:48,160 --> 00:51:49,160
He's in intelligence.

809
00:51:49,160 --> 00:51:50,160
Right.

810
00:51:50,160 --> 00:51:53,680
Well, let me just say that I have a close friend who's 90 years old and she was my

811
00:51:53,680 --> 00:51:56,720
dorm mother at a high school in New Hampshire.

812
00:51:56,720 --> 00:51:58,440
She lives in Martha's Vineyard.

813
00:51:58,440 --> 00:52:00,840
She's got one grandson.

814
00:52:00,840 --> 00:52:06,920
His son's 12 years old, he was literally raised on screens, okay, unfortunately.

815
00:52:06,920 --> 00:52:13,200
And she took her daughter and her grandson to the Swiss Alps for a vacation.

816
00:52:13,200 --> 00:52:17,640
And she came back and she called me just this December and she was in tears because she

817
00:52:17,640 --> 00:52:21,480
said her grandson has absolutely no empathic qualities.

818
00:52:21,480 --> 00:52:23,200
Cannot interact.

819
00:52:23,200 --> 00:52:26,080
Cannot even make eye contact.

820
00:52:26,080 --> 00:52:28,000
And she was so frustrated.

821
00:52:28,000 --> 00:52:31,680
She said it was the worst nightmare of her entire life.

822
00:52:31,680 --> 00:52:33,240
But that's why.

823
00:52:33,240 --> 00:52:37,400
So that's what we're asking is whatever it is, is it dignity and enhancing?

824
00:52:37,400 --> 00:52:41,840
Now some of these things, including, you know, by the way, some of the genetic ideas, are

825
00:52:41,840 --> 00:52:44,320
not contrary to human dignity by any means.

826
00:52:44,320 --> 00:52:45,760
I don't want to say they are.

827
00:52:45,760 --> 00:52:49,040
But these are the deeper questions that you need to be asking.

828
00:52:49,040 --> 00:52:50,040
Yeah.

829
00:52:50,040 --> 00:52:55,840
The point is, I think the discussion is at many levels and I'm not clear because we talk

830
00:52:55,840 --> 00:53:03,560
about increasing lifespan, which is one thing, creating immortality, it's another thing.

831
00:53:03,560 --> 00:53:08,480
Becoming all of us just computers and algorithms is yet another thing.

832
00:53:08,480 --> 00:53:14,080
But the thing that you are talking about, Francesca, has to do more with AI and the dangers of

833
00:53:14,080 --> 00:53:15,080
AI.

834
00:53:15,080 --> 00:53:16,080
It's specifically...

835
00:53:16,080 --> 00:53:17,080
I'm not the dangers.

836
00:53:17,080 --> 00:53:18,080
I will not put it that way.

837
00:53:18,080 --> 00:53:27,640
But I will put AI helping us to enhance our own traits and all those things with the question

838
00:53:27,640 --> 00:53:28,640
marks.

839
00:53:28,640 --> 00:53:30,480
I will say, yes, we can do that.

840
00:53:30,480 --> 00:53:33,280
If you are careful enough, we can do all of those.

841
00:53:33,280 --> 00:53:34,280
But what helps really?

842
00:53:34,280 --> 00:53:35,280
And they should be welcome.

843
00:53:35,280 --> 00:53:36,880
But I thought what works.

844
00:53:36,880 --> 00:53:44,480
Because with our creativity, enhance our traits and answer, we can do much more scientific

845
00:53:44,480 --> 00:53:45,480
discoveries.

846
00:53:45,480 --> 00:53:48,080
And discover fewer for money more diseases.

847
00:53:48,080 --> 00:53:49,520
No, we know that.

848
00:53:49,520 --> 00:53:56,200
But what you were saying also, I thought, was that we need to be aware of the potential

849
00:53:56,200 --> 00:54:02,440
problems and dangers in order to not end up in a place we don't want to end up.

850
00:54:02,440 --> 00:54:08,640
The fact is that the grandson of your friend has already ended up there.

851
00:54:08,640 --> 00:54:12,040
So there was no way to prevent it.

852
00:54:12,040 --> 00:54:17,160
And part of the issue seems to me in trying to say we can, in fact, figure out what the

853
00:54:17,160 --> 00:54:22,640
problems would be, we may not be able to figure out, just as we didn't know what the

854
00:54:22,640 --> 00:54:31,640
effect of social media on us would be, what the effect of the iPhone or the telephone

855
00:54:31,640 --> 00:54:32,960
would be on us.

856
00:54:32,960 --> 00:54:35,400
So only after the fact we are saying.

857
00:54:35,400 --> 00:54:36,920
So it's retrospective.

858
00:54:36,920 --> 00:54:42,000
And now people are saying, well, maybe we shouldn't buy into the MacArthur ideology of

859
00:54:42,000 --> 00:54:46,520
doing away with teachers in the grade schools and just having people on iPads.

860
00:54:46,520 --> 00:54:48,160
Maybe we should be more thoughtful about it.

861
00:54:48,160 --> 00:54:54,400
The American Journal of Pediatrics had an entire volume devoted in December to the problem

862
00:54:54,400 --> 00:54:59,880
of raising children and getting them into basic social skills.

863
00:54:59,880 --> 00:55:04,720
And so now the suggestion is, well, maybe we should hold off until they're 10 or 11

864
00:55:04,720 --> 00:55:05,720
before they're.

865
00:55:05,720 --> 00:55:06,720
Look at this.

866
00:55:06,720 --> 00:55:09,880
Now we are going to have driverless cars.

867
00:55:09,880 --> 00:55:14,080
It seems to be from everything I hear around the corner.

868
00:55:14,080 --> 00:55:15,080
So to be.

869
00:55:15,080 --> 00:55:28,000
What are going to be the consequences of having driverless cars?

870
00:55:28,000 --> 00:55:32,680
What is going to be the consequence on our ability to drive or on our ability to do other

871
00:55:32,680 --> 00:55:34,040
physical things?

872
00:55:34,040 --> 00:55:37,840
What is going to be the consequence in terms of traffic situations?

873
00:55:37,840 --> 00:55:40,320
What is going to be the consequence economically?

874
00:55:40,320 --> 00:55:44,520
What's going to be the consequence in terms of insurance companies?

875
00:55:44,520 --> 00:55:46,680
And some of these things you can think ahead.

876
00:55:46,680 --> 00:55:52,000
But it seems to me that really parts of it, you cannot think until you're dealing with

877
00:55:52,000 --> 00:55:53,000
it already.

878
00:55:53,000 --> 00:55:54,000
That's too late.

879
00:55:54,000 --> 00:55:59,320
Well, it's not too late, but you can't think about it until you have some experience.

880
00:55:59,320 --> 00:56:06,720
But the whole conversation has various themes or strengths in it.

881
00:56:06,720 --> 00:56:10,280
And it's very hard to really differentiate between them.

882
00:56:10,280 --> 00:56:15,760
And it's very hard also to debate with transhumanists because when I asked, you know, I attacked

883
00:56:15,760 --> 00:56:20,280
them on one issue and said, oh, no, I didn't say, hey, I actually believe in B. So, okay,

884
00:56:20,280 --> 00:56:24,400
you go to B, oh, I don't believe in B, I believe in C. So it's kind of a moving target.

885
00:56:24,400 --> 00:56:27,040
It's very, very hard to pin down what's going on.

886
00:56:27,040 --> 00:56:28,040
In other words.

887
00:56:28,040 --> 00:56:35,360
They don't really take ownership of the consequences that will come about if they're dream, if

888
00:56:35,360 --> 00:56:37,760
that social imaginary will come to be a reality.

889
00:56:37,760 --> 00:56:38,760
I don't know.

890
00:56:38,760 --> 00:56:42,520
I mean, there is this book I was talking to, and I mentioned to you and I was talking

891
00:56:42,520 --> 00:56:47,360
to Francesca, but by Max Tech Markets focused on how we can prevent.

892
00:56:47,360 --> 00:56:48,360
Life 3.0.

893
00:56:48,360 --> 00:56:49,360
Yeah.

894
00:56:49,360 --> 00:56:51,520
How we can prevent some of the complications.

895
00:56:51,520 --> 00:56:55,160
So, I mean, there is an effort to think about it.

896
00:56:55,160 --> 00:56:56,920
And I think there is more of a focus.

897
00:56:56,920 --> 00:57:02,720
But you're right that people now think a lot about these issues and how to resolve them

898
00:57:02,720 --> 00:57:07,080
because there have been issues that have been shown in some examples.

899
00:57:07,080 --> 00:57:12,280
So it was like a trial and, you know, and you were right.

900
00:57:12,280 --> 00:57:17,040
Maybe one should have thought that at the beginning, but it was impossible to predict

901
00:57:17,040 --> 00:57:18,600
or very, very difficult.

902
00:57:18,600 --> 00:57:23,280
So, we saw some issues and I was like, oh, okay.

903
00:57:23,280 --> 00:57:24,960
So, let's rethink from scratch.

904
00:57:24,960 --> 00:57:27,160
You know, how are we going to develop AI?

905
00:57:27,160 --> 00:57:33,880
How are we going to design the AI and embed in the design itself, not just at the end

906
00:57:33,880 --> 00:57:35,120
and check how it behaves.

907
00:57:35,120 --> 00:57:41,760
And yeah, but embed, since the design phase, these issues that it should not be by as they

908
00:57:41,760 --> 00:57:46,000
should be fair, they should be explainable that they should all these things that you

909
00:57:46,000 --> 00:57:52,320
want the AI to have in order to not have these collateral negative aspects.

910
00:57:52,320 --> 00:57:58,000
But remember, the people who raised those critiques where dismissed as bio-conservatives

911
00:57:58,000 --> 00:58:01,120
way back at least 15 years ago, you were bio-conficertives.

912
00:58:01,120 --> 00:58:03,640
How can you raise those problems?

913
00:58:03,640 --> 00:58:07,960
All the people who spoke about the perils of technology were dismissed off-end.

914
00:58:07,960 --> 00:58:11,920
Now it turns out that they were closer to the truth probably than they were.

915
00:58:11,920 --> 00:58:18,000
And then they all, it's a tough balance because you don't want to say, okay, no more technology,

916
00:58:18,000 --> 00:58:19,160
no more advancement.

917
00:58:19,160 --> 00:58:21,040
We are happy the way it is.

918
00:58:21,040 --> 00:58:22,040
Let's stay here.

919
00:58:22,040 --> 00:58:28,120
On the other hand, there are things that are constant improvements, what you were talking

920
00:58:28,120 --> 00:58:33,560
about and what they're doing now, for example, in psychiatry where somebody gives blood and

921
00:58:33,560 --> 00:58:38,920
you can tell from the blood what medications will be working properly or better than others,

922
00:58:38,920 --> 00:58:45,080
what they would metabolize better than others, so that these are useful.

923
00:58:45,080 --> 00:58:52,240
But there is, however, the issue that I think you were alluding to is are there other things

924
00:58:52,240 --> 00:58:56,600
that could be dangerous and is it possible to prevent those?

925
00:58:56,600 --> 00:59:00,960
And I think some may be so and some may be hard to tell.

926
00:59:00,960 --> 00:59:07,360
But I think that now we are in a better position than 15 years ago because 15 years ago, the

927
00:59:07,360 --> 00:59:12,280
silos of different disciplines were much more well-defined.

928
00:59:12,280 --> 00:59:17,040
You know, technologies to one side, sociologists on the other side, economists on the other

929
00:59:17,040 --> 00:59:20,840
side, and they were rarely talking to each other.

930
00:59:20,840 --> 00:59:26,120
Now I regularly work and organize events with all these people.

931
00:59:26,120 --> 00:59:32,960
And the high people together with psychologists, sociologists, you know, and see all of these.

932
00:59:32,960 --> 00:59:35,960
How many humanists do you include in the interdisciplinary conversation?

933
00:59:35,960 --> 00:59:37,360
Maybe we should have more.

934
00:59:37,360 --> 00:59:40,800
So what I'm saying is that I really see a change in that.

935
00:59:40,800 --> 00:59:46,120
And that's what makes me optimistic that we can solve because the high people can find

936
00:59:46,120 --> 00:59:51,760
maybe technical solutions to issues, but the definition and the understanding and identification

937
00:59:51,760 --> 00:59:54,240
of the issues should be done together with everybody else.

938
00:59:54,240 --> 00:59:58,640
Okay, so if you bring the humanist perspective into, and I'm very happy to hear what you

939
00:59:58,640 --> 01:00:03,840
just said, but if you bring the humanist perspective, the humanist scholar into this conversation,

940
01:00:03,840 --> 01:00:06,360
into the mix, and the humanist would say, you know what?

941
01:00:06,360 --> 01:00:15,560
There is a dimension which is nonmeasurable, non-allegrizable, non-reductionable or nonreducable.

942
01:00:15,560 --> 01:00:17,160
What do you do with that?

943
01:00:17,160 --> 01:00:18,840
Are you saying to this guy, you know what?

944
01:00:18,840 --> 01:00:19,840
This is nonsense.

945
01:00:19,840 --> 01:00:24,480
Or are you saying, no, I really have to take you seriously.

946
01:00:24,480 --> 01:00:31,440
Let me figure out how the non-observable, the non-reducable, how do I fit it into my analysis?

947
01:00:31,440 --> 01:00:32,440
That's a challenge.

948
01:00:32,440 --> 01:00:35,240
Go ahead and tell us what you think that is.

949
01:00:35,240 --> 01:00:36,240
Yeah, one example.

950
01:00:36,240 --> 01:00:39,640
Well, you wrote a book called that by whatever names, right?

951
01:00:39,640 --> 01:00:45,160
So, I'm happy to talk about a soul as an emergent phenomenon.

952
01:00:45,160 --> 01:00:47,240
Okay, so talk about the soul.

953
01:00:47,240 --> 01:00:48,240
That's fine.

954
01:00:48,240 --> 01:00:49,240
I don't know how you call it.

955
01:00:49,240 --> 01:00:50,240
You can call it soul.

956
01:00:50,240 --> 01:00:51,240
You can call it psyche.

957
01:00:51,240 --> 01:00:55,920
You can call it all sorts of things, which will be problematic for the people who are doing

958
01:00:55,920 --> 01:00:57,360
this kind of work.

959
01:00:57,360 --> 01:01:01,480
And all I'm saying that I would like to see these people at least admit the possibility

960
01:01:01,480 --> 01:01:04,520
of the nonreducable dimension.

961
01:01:04,520 --> 01:01:08,120
The non-materialistic, the non-allegory ziable, the non-mathematical.

962
01:01:08,120 --> 01:01:09,600
You cannot mathesize that.

963
01:01:09,600 --> 01:01:29,200
Why are you saying non-mat

964
01:01:29,200 --> 01:01:35,440
of the non-mathematical.

965
01:01:35,440 --> 01:01:36,440
You see that there's no contribution.

966
01:01:36,440 --> 01:01:38,840
One book on AI today, almost.

967
01:01:38,840 --> 01:01:40,280
Yeah, I believe you.

968
01:01:40,280 --> 01:01:45,720
So this particular book, he's arguing that the AI project has been mistaken because it

969
01:01:45,720 --> 01:01:48,880
was basically based on mind-body dualism.

970
01:01:48,880 --> 01:01:52,900
And what he's telling us that we need to go back from a platonic position which under

971
01:01:52,900 --> 01:01:58,640
Giro did the entire project to a Aristotelian position that takes the body into account

972
01:01:58,640 --> 01:01:59,640
from the very beginning.

973
01:01:59,640 --> 01:02:04,120
In other words, the AI, the artificial intelligence has to think through the body or like the

974
01:02:04,120 --> 01:02:10,240
body thinks, not like the mind thinks as we think the mind thinks, but as the body thinks.

975
01:02:10,240 --> 01:02:12,040
It's a very interesting argument.

976
01:02:12,040 --> 01:02:15,880
But if that's the case, it goes against what transhumanism is trying to accomplish.

977
01:02:15,880 --> 01:02:18,760
Two things I want to say.

978
01:02:18,760 --> 01:02:28,000
Next is that what AI was conceived of 30 or 40 years ago was the AI project, as you call

979
01:02:28,000 --> 01:02:29,000
it.

980
01:02:29,000 --> 01:02:31,240
I think it's different than what signed.

981
01:02:31,240 --> 01:02:35,840
I mean, scientists are just basically, it doesn't matter if the artificial intelligence

982
01:02:35,840 --> 01:02:39,440
algorithms match what actually happens in the brain.

983
01:02:39,440 --> 01:02:42,920
It's whatever works to be able to solve problems.

984
01:02:42,920 --> 01:02:43,920
Problem solving, that's all it is.

985
01:02:43,920 --> 01:02:51,720
The typical analogy that is at the beginning of every AI textbook is that planes fly,

986
01:02:51,720 --> 01:02:54,680
but they don't fly like bears fly.

987
01:02:54,680 --> 01:02:56,960
They don't fall fair.

988
01:02:56,960 --> 01:02:58,960
But still, they do fly.

989
01:02:58,960 --> 01:03:07,080
So, it's not necessary to have some form of intelligence or to be able to help ourselves

990
01:03:07,080 --> 01:03:11,600
to be monitored that they have to replicate our way of doing it.

991
01:03:11,600 --> 01:03:16,240
I think the point is, we use AI now.

992
01:03:16,240 --> 01:03:22,440
We're not trying to replicate and challenge, to solve problems.

993
01:03:22,440 --> 01:03:26,120
The second thing I wanted to come back to is the transhumanism.

994
01:03:26,120 --> 01:03:34,560
And just the notion of what Billy said that we're always transitioning human-wise.

995
01:03:34,560 --> 01:03:40,200
I mean, 100,000 years ago, people didn't have writing yet.

996
01:03:40,200 --> 01:03:47,720
They didn't have the brains that were able to produce novels or anything like that.

997
01:03:47,720 --> 01:03:52,520
I mean, they didn't wear clothes until 70 or 80,000 years ago.

998
01:03:52,520 --> 01:03:55,080
And this evolution took place.

999
01:03:55,080 --> 01:03:58,080
And so, humans have been transitioning.

1000
01:03:58,080 --> 01:04:06,320
The difference is that transhumanism is for what they call a directed evolution.

1001
01:04:06,320 --> 01:04:07,320
It's not evolution.

1002
01:04:07,320 --> 01:04:08,480
It's directed evolution.

1003
01:04:08,480 --> 01:04:09,800
It's controlled evolution.

1004
01:04:09,800 --> 01:04:12,560
It's actually accelerated evolution.

1005
01:04:12,560 --> 01:04:14,760
That's the whole point of evolution.

1006
01:04:14,760 --> 01:04:16,400
The question is, who's directing it?

1007
01:04:16,400 --> 01:04:17,400
So, I have a question.

1008
01:04:17,400 --> 01:04:21,680
Tenshu read the books by Tenshu, if you know Tenshu's work.

1009
01:04:21,680 --> 01:04:25,360
And he's going to create what he calls Kobe's.

1010
01:04:25,360 --> 01:04:26,360
Kobe's art.

1011
01:04:26,360 --> 01:04:32,040
But I think it's really critical to ask the question when you say it's directed evolution.

1012
01:04:32,040 --> 01:04:35,320
I think directed by a society, like in the...

1013
01:04:35,320 --> 01:04:36,320
Actually, that's...

1014
01:04:36,320 --> 01:04:42,080
Well, I would say it's directed by those who take control of the process of engineering.

1015
01:04:42,080 --> 01:04:43,280
It's the engineer.

1016
01:04:43,280 --> 01:04:46,960
Those who will engineer the process, they will be in the position of directing it.

1017
01:04:46,960 --> 01:04:51,880
So, I want to take a big history perspective on this.

1018
01:04:51,880 --> 01:04:55,320
The sun is going to be a reliable partner on...

1019
01:04:55,320 --> 01:04:59,480
For complexity on Earth for about 2 billion years.

1020
01:04:59,480 --> 01:05:02,040
And that's a really big future.

1021
01:05:02,040 --> 01:05:08,040
There's no reason, based on what we know about the past, to think that our species unchanged

1022
01:05:08,040 --> 01:05:09,800
will be here forever.

1023
01:05:09,800 --> 01:05:12,200
I mean, we might be here for...

1024
01:05:12,200 --> 01:05:15,240
Our descendants might be here for 2 billion years.

1025
01:05:15,240 --> 01:05:21,440
They have a possibility of living on this planet without science fiction traveling late

1026
01:05:21,440 --> 01:05:24,040
years with technology that we don't have.

1027
01:05:24,040 --> 01:05:25,680
And they never have.

1028
01:05:25,680 --> 01:05:28,480
So the planet is a pretty amazing place.

1029
01:05:28,480 --> 01:05:33,120
And it's already had a really amazing journey in our species, but it's problems.

1030
01:05:33,120 --> 01:05:34,120
But what...

1031
01:05:34,120 --> 01:05:40,440
I mean, isn't it just take it for granted that we're going to evolve into something different?

1032
01:05:40,440 --> 01:05:45,840
And they may or may not call themselves humans.

1033
01:05:45,840 --> 01:05:52,880
We're already a domesticated species compared to 100,000 years ago.

1034
01:05:52,880 --> 01:05:55,200
And how it happens, even if it's...

1035
01:05:55,200 --> 01:05:59,160
It's still has to work.

1036
01:05:59,160 --> 01:06:01,600
It has to be selected.

1037
01:06:01,600 --> 01:06:04,640
It has to have function.

1038
01:06:04,640 --> 01:06:08,520
A certain amount of dysfunction will be in there too, for sure.

1039
01:06:08,520 --> 01:06:10,560
And there'll be unintended consequences.

1040
01:06:10,560 --> 01:06:19,040
But I don't want to see it through the lens of utopia or dystopia, because I think those

1041
01:06:19,040 --> 01:06:23,280
are dangerous tropes to get into.

1042
01:06:23,280 --> 01:06:29,680
But the point is that, of course, we're going to evolve.

1043
01:06:29,680 --> 01:06:31,200
But there's a difference.

1044
01:06:31,200 --> 01:06:32,760
Evolution is a process.

1045
01:06:32,760 --> 01:06:34,080
It's a very slow process.

1046
01:06:34,080 --> 01:06:35,080
It's a process that...

1047
01:06:35,080 --> 01:06:40,080
Nobody knows really what's going on, but they claim that they can control the process.

1048
01:06:40,080 --> 01:06:41,480
I don't want them to control.

1049
01:06:41,480 --> 01:06:43,480
No, but that's not a really professional.

1050
01:06:43,480 --> 01:06:46,480
It's totally a crucial, more generally.

1051
01:06:46,480 --> 01:06:49,400
The process is based on reproduction.

1052
01:06:49,400 --> 01:06:51,680
Who controls reproduction?

1053
01:06:51,680 --> 01:06:53,960
The women will reproduce, right?

1054
01:06:53,960 --> 01:06:55,960
As long as the women have the freedom to reproduce.

1055
01:06:55,960 --> 01:06:56,960
That could be true.

1056
01:06:56,960 --> 01:06:57,960
It's not the engineers.

1057
01:06:57,960 --> 01:06:59,920
I mean, I'm in this field, right?

1058
01:06:59,920 --> 01:07:02,560
I know it's women controlling.

1059
01:07:02,560 --> 01:07:07,120
If they want to give their child a gene which prevents disease, or they want to avoid...

1060
01:07:07,120 --> 01:07:08,600
It means the women are doing it.

1061
01:07:08,600 --> 01:07:10,480
Now, that's one way, right?

1062
01:07:10,480 --> 01:07:16,200
So, you know, directed evolution could be each individual woman or couple.

1063
01:07:16,200 --> 01:07:19,480
Culture has been directing human mating for a long time.

1064
01:07:19,480 --> 01:07:21,480
Yeah, but I think you're talking about...

1065
01:07:21,480 --> 01:07:23,480
You're talking about a societal...

1066
01:07:23,480 --> 01:07:27,760
But on this point, I mean, just because this is a great immediate segue and then please

1067
01:07:27,760 --> 01:07:34,840
go on, but that's where Leon Cass and Hans Jonas come in.

1068
01:07:34,840 --> 01:07:37,680
Because they would tell you that...

1069
01:07:37,680 --> 01:07:39,760
I mean, Leon Cass wrote an interesting book.

1070
01:07:39,760 --> 01:07:40,760
I don't agree.

1071
01:07:40,760 --> 01:07:44,880
I was his TA for a while, but it's called Toward a More Natural Science.

1072
01:07:44,880 --> 01:07:51,160
And his argument is that all of the things we really think matter in the quality of our

1073
01:07:51,160 --> 01:07:57,360
human experience at the family and at the level of community, all of it somehow has to

1074
01:07:57,360 --> 01:08:00,840
do with the evolution of empathy.

1075
01:08:00,840 --> 01:08:06,120
As Aldous Huxley said at the end of his life, when someone asked him for some advice for

1076
01:08:06,120 --> 01:08:09,720
the younger generation, he said, well, it's a little embarrassing because I've written

1077
01:08:09,720 --> 01:08:15,240
so many books, but I would say try to be a little kinder.

1078
01:08:15,240 --> 01:08:23,440
His compassion, empathy, attentive listening, all these kinds of virtues are really critical.

1079
01:08:23,440 --> 01:08:25,120
And where do they emerge from?

1080
01:08:25,120 --> 01:08:31,040
Well, evolutionary biologists and evolutionary psychologists will spin this a little differently,

1081
01:08:31,040 --> 01:08:40,440
but fundamentally from the fact that we are a reproductive species, that humans invest

1082
01:08:40,440 --> 01:08:47,000
greatly in their offspring, who are dependent for very protracted periods of time compared

1083
01:08:47,000 --> 01:08:48,800
to any other species.

1084
01:08:48,800 --> 01:08:54,440
And so a lot of these things that we view, you know, altruism and so forth, all the noble

1085
01:08:54,440 --> 01:09:02,640
aspects of the human endeavor really emerge from this kind of turning over of the generations.

1086
01:09:02,640 --> 01:09:09,920
So if you move, he would say, toward something that is strongly anti-aging.

1087
01:09:09,920 --> 01:09:15,160
For example, in Japan, by the way, they are living really long, but they're not reproducing

1088
01:09:15,160 --> 01:09:16,160
much.

1089
01:09:16,160 --> 01:09:20,320
Well, that's why the population is going to be opposite of what people thought.

1090
01:09:20,320 --> 01:09:22,160
Yeah, and so the investment, correct?

1091
01:09:22,160 --> 01:09:23,160
But because of that?

1092
01:09:23,160 --> 01:09:26,400
Well, parental investment, well, I won't go.

1093
01:09:26,400 --> 01:09:27,400
That's not quite the question.

1094
01:09:27,400 --> 01:09:33,640
I mean, the issue is that so much of this is what Tolkien pointed out.

1095
01:09:33,640 --> 01:09:42,840
This is why, you know, Erwin separates herself from the immortality of the elves so that

1096
01:09:42,840 --> 01:09:46,080
she can experience the love of a child and so forth.

1097
01:09:46,080 --> 01:09:47,320
So that's really important.

1098
01:09:47,320 --> 01:09:49,920
That's a natural evolutionary dimension.

1099
01:09:49,920 --> 01:09:57,200
It's not, and for Cass and for Hans Jonas, that dimension is something that is imperiled

1100
01:09:57,200 --> 01:10:03,360
by anti-aging technologies, which would be humanly directed.

1101
01:10:03,360 --> 01:10:09,320
Now, I'm not a, but understand, I'm so concerned about these chronic illnesses associated with

1102
01:10:09,320 --> 01:10:13,000
old age, that I can actually tolerate a little transhumanism.

1103
01:10:13,000 --> 01:10:17,160
I see that you're going with it, but you forget what they really want.

1104
01:10:17,160 --> 01:10:18,840
Let's go back to the telos.

1105
01:10:18,840 --> 01:10:27,080
The telos is, I'll put it very bluntly, it's to make the human biological species obsolete.

1106
01:10:27,080 --> 01:10:28,080
That's what it's about.

1107
01:10:28,080 --> 01:10:32,720
It's about the planned obsolescence of the human species.

1108
01:10:32,720 --> 01:10:37,600
What it is for Ted Shoe, for Giulio Prisco, for Nick Bostrom, for all the people who write

1109
01:10:37,600 --> 01:10:39,400
in that genre.

1110
01:10:39,400 --> 01:10:42,720
You can say, okay, they don't really matter because they don't really do the science.

1111
01:10:42,720 --> 01:10:46,000
They just popularize a certain kind of myth.

1112
01:10:46,000 --> 01:10:47,000
What difference does it make?

1113
01:10:47,000 --> 01:10:51,040
It makes a lot of difference because it affects the culture.

1114
01:10:51,040 --> 01:10:52,040
Even if ideologies are true.

1115
01:10:52,040 --> 01:10:53,040
It makes a lot of difference.

1116
01:10:53,040 --> 01:10:54,040
I don't read those people.

1117
01:10:54,040 --> 01:10:55,040
I have to say, I know that these forces were made out of.

1118
01:10:55,040 --> 01:10:56,040
Why?

1119
01:10:56,040 --> 01:10:57,040
Why do you think?

1120
01:10:57,040 --> 01:11:02,320
I don't read those people.

1121
01:11:02,320 --> 01:11:05,600
Why do you think humans as currently defined?

1122
01:11:05,600 --> 01:11:09,720
Why do you think it's so essential that we stay the way that we live?

1123
01:11:09,720 --> 01:11:11,200
That's a bad idea.

1124
01:11:11,200 --> 01:11:15,360
But why does the species have to stay the way that we open up for questions?

1125
01:11:15,360 --> 01:11:16,880
Then we open up for questions.

1126
01:11:16,880 --> 01:11:21,000
Here is, I guess, it depends kind of where you come from.

1127
01:11:21,000 --> 01:11:22,680
I come from the Jewish tradition.

1128
01:11:22,680 --> 01:11:26,520
In the Jewish tradition, as well as in the Islamic and the Christian traditions, we do

1129
01:11:26,520 --> 01:11:28,760
talk about creation in the image of God.

1130
01:11:28,760 --> 01:11:32,320
One argument you can dismiss it, you can say, who cares about what religions say.

1131
01:11:32,320 --> 01:11:35,840
But if you do care about what religions say, you can say, well, there's something really

1132
01:11:35,840 --> 01:11:37,640
precious about being human.

1133
01:11:37,640 --> 01:11:43,640
Being human comes with all those vulnerabilities and limitations and kind of words and all.

1134
01:11:43,640 --> 01:11:45,000
We are not perfect.

1135
01:11:45,000 --> 01:11:46,000
It's not about perfection.

1136
01:11:46,000 --> 01:11:47,000
We never will be perfect.

1137
01:11:47,000 --> 01:11:49,000
Thank God we are not going to be perfect.

1138
01:11:49,000 --> 01:11:51,680
We should aspire for that kind of perfection.

1139
01:11:51,680 --> 01:11:55,520
We should aspire more to perfection along the lines that Steve proposed, which is the

1140
01:11:55,520 --> 01:11:58,480
tradition of Aristotelian and religious tradition.

1141
01:11:58,480 --> 01:12:02,240
Talk about virtue, talk about character, talk about morality.

1142
01:12:02,240 --> 01:12:04,160
In that sense, there's an offering.

1143
01:12:04,160 --> 01:12:07,760
There's a way religion has always been against the kind of progress that we have.

1144
01:12:07,760 --> 01:12:09,440
No, that's absolutely not true.

1145
01:12:09,440 --> 01:12:11,400
Not in the middle ages, for sure.

1146
01:12:11,400 --> 01:12:14,640
I will have to take issue with you here.

1147
01:12:14,640 --> 01:12:18,920
Actually, many of the people who were doing the forefront of science in the 13th and 14th

1148
01:12:18,920 --> 01:12:20,520
century were all religious people.

1149
01:12:20,520 --> 01:12:22,520
How were they handled?

1150
01:12:22,520 --> 01:12:26,520
I know they were religious, but how were they accepted?

1151
01:12:26,520 --> 01:12:29,520
The most fantastic thing.

1152
01:12:29,520 --> 01:12:31,320
Yes, how was Gaggilio?

1153
01:12:31,320 --> 01:12:34,680
And he was doing the same thing.

1154
01:12:34,680 --> 01:12:36,000
My manna is the same thing.

1155
01:12:36,000 --> 01:12:39,520
It's not the case that the religious person was always dismissed by...

1156
01:12:39,520 --> 01:12:40,520
No, no, no.

1157
01:12:40,520 --> 01:12:42,520
But that was before Darwin.

1158
01:12:42,520 --> 01:12:44,520
That was before Darwin.

1159
01:12:44,520 --> 01:12:49,520
Fair enough, Darwinism is an important turning point.

1160
01:12:49,520 --> 01:12:55,520
The realization that 100,000 years ago, the human species was different than it is today.

1161
01:12:55,520 --> 01:12:57,520
That didn't come in...

1162
01:12:57,520 --> 01:12:59,520
Notion did not come to existence.

1163
01:12:59,520 --> 01:13:04,400
What I was saying, by the way, doesn't have much to do with religion.

1164
01:13:04,400 --> 01:13:09,960
I remember looking back now, 30 years, when my wife and I had our first child, we went

1165
01:13:09,960 --> 01:13:10,960
home.

1166
01:13:10,960 --> 01:13:15,280
My older sister observed me for a while.

1167
01:13:15,280 --> 01:13:19,480
A couple of days into this, she said, you know, Stevie, I never really liked you.

1168
01:13:19,480 --> 01:13:21,480
Until you became a father.

1169
01:13:21,480 --> 01:13:23,680
Which is a great compliment.

1170
01:13:23,680 --> 01:13:31,240
But what she saw was that somehow my being had expanded in the care of a child.

1171
01:13:31,240 --> 01:13:32,240
And there was a growth.

1172
01:13:32,240 --> 01:13:34,760
Now I don't think that's everybody's journey by any means.

1173
01:13:34,760 --> 01:13:37,000
Don't get me wrong.

1174
01:13:37,000 --> 01:13:45,280
But simply stated, many of the key assets that we feel make a human life meaningful and

1175
01:13:45,280 --> 01:13:51,720
not narcissistic, not I-it, but I-vow, are related to the natural evolutionary trajectory.

1176
01:13:51,720 --> 01:13:55,520
Now when you get an Aldous Huxley talking about a brave new world, where you sort of

1177
01:13:55,520 --> 01:14:01,880
engineered that it is dystopian out of the picture, then what do you have left?

1178
01:14:01,880 --> 01:14:04,880
And that was the question.

1179
01:14:04,880 --> 01:14:10,440
So again, but I don't want to go any further with it, but I do think it doesn't hang on

1180
01:14:10,440 --> 01:14:11,440
a religious argument.

1181
01:14:11,440 --> 01:14:13,280
Let's take some questions from the audience.

1182
01:14:13,280 --> 01:14:14,280
Wonderful.

1183
01:14:14,280 --> 01:14:15,280
I'll go around.

1184
01:14:15,280 --> 01:14:16,280
I'm going to start on this side.

1185
01:14:16,280 --> 01:14:17,280
I ask you to be brief.

1186
01:14:17,280 --> 01:14:19,280
If you're going to make a comment, make it short.

1187
01:14:19,280 --> 01:14:21,880
If you're going to make a question.

1188
01:14:21,880 --> 01:14:24,280
And then I'll go-yes, I'm sorry.

1189
01:14:24,280 --> 01:14:25,280
Go ahead, please.

1190
01:14:25,280 --> 01:14:26,280
That's right.

1191
01:14:26,280 --> 01:14:27,280
Go ahead, Steve.

1192
01:14:27,280 --> 01:14:28,280
Go ahead, Steve.

1193
01:14:28,280 --> 01:14:29,280
I'll post your name.

1194
01:14:29,280 --> 01:14:30,280
Stuart Danbrought.

1195
01:14:30,280 --> 01:14:32,280
Just brief background.

1196
01:14:32,280 --> 01:14:33,280
Brief.

1197
01:14:33,280 --> 01:14:38,960
Research Fellow at the Artificial General Intelligence Society Research Fellow at Brain Machine Interface

1198
01:14:38,960 --> 01:14:39,960
Consortium.

1199
01:14:39,960 --> 01:14:45,720
I do work with autonomous systems at IEEE.

1200
01:14:45,720 --> 01:14:51,840
I'm ridiculously interdisciplinary and I also focus a lot on ethics.

1201
01:14:51,840 --> 01:14:57,800
So it's kind of a basic question, really, because we don't have time to go into the

1202
01:14:57,800 --> 01:14:59,640
technical details.

1203
01:14:59,640 --> 01:15:08,000
But if, for example, Haba, you had a young person in your family or someone who was still

1204
01:15:08,000 --> 01:15:15,160
in utero and it was known that they were going to express with, let's say, a fatal heart

1205
01:15:15,160 --> 01:15:16,160
condition.

1206
01:15:16,160 --> 01:15:21,320
But CRISPR-Cas9 could be used to prevent that.

1207
01:15:21,320 --> 01:15:22,960
Would that be something you'd be comfortable with?

1208
01:15:22,960 --> 01:15:26,000
Yeah, actually I'm going to speak precisely on that issue.

1209
01:15:26,000 --> 01:15:33,120
Okay, so now how is that different when writ large of the transhumanism discussion we're

1210
01:15:33,120 --> 01:15:34,120
having?

1211
01:15:34,120 --> 01:15:35,120
Yeah.

1212
01:15:35,120 --> 01:15:42,280
It's already been done, a paper about two months ago in utero child had a genetic anomaly

1213
01:15:42,280 --> 01:15:45,120
that was going to express with heart failure.

1214
01:15:45,120 --> 01:15:51,440
At some point in his or her, I forget the sex of the fetus, young life.

1215
01:15:51,440 --> 01:15:59,560
So why is that not extensible to the major objections that you've been raising in this

1216
01:15:59,560 --> 01:16:00,560
discussion?

1217
01:16:00,560 --> 01:16:07,480
That goes back to the issue of the separation or at least raising the awareness.

1218
01:16:07,480 --> 01:16:09,560
There's a difference between therapy and enhancement.

1219
01:16:09,560 --> 01:16:14,520
So by the way, from the Jewish perspective, Jewish perspective is very pro biotechnology,

1220
01:16:14,520 --> 01:16:19,440
very pro-crispuh, including the very orthodox and even ultra orthodox.

1221
01:16:19,440 --> 01:16:25,200
They're very much for using gene editing or genome editing in order to solve that particular

1222
01:16:25,200 --> 01:16:27,200
kind of problem.

1223
01:16:27,200 --> 01:16:32,200
I think there's still a difference between that and enhancement, the way that Nick Bostrom

1224
01:16:32,200 --> 01:16:33,560
and the rest of them speak about.

1225
01:16:33,560 --> 01:16:35,560
So let me ask, so I...

1226
01:16:35,560 --> 01:16:40,960
So but there must be a threshold, I mean, because otherwise if you say, oh, we are human

1227
01:16:40,960 --> 01:16:46,680
also because our vulnerabilities are limits and so on, then you will not even cure any

1228
01:16:46,680 --> 01:16:48,840
disease because it's part of you.

1229
01:16:48,840 --> 01:16:52,480
So there must be a threshold that where do you put that threshold?

1230
01:16:52,480 --> 01:16:54,160
Well, so here's the problem.

1231
01:16:54,160 --> 01:17:01,920
The problem is that, you know, taking off of this crisper, it may be possible to change

1232
01:17:01,920 --> 01:17:07,920
the genes of a human being such that a child would be born with a heart that was stronger

1233
01:17:07,920 --> 01:17:11,600
and longer lasting than a normal human heart.

1234
01:17:11,600 --> 01:17:13,720
Same thing with every other organ.

1235
01:17:13,720 --> 01:17:17,000
Now that's enhancement, right?

1236
01:17:17,000 --> 01:17:19,720
So that, I agree with you that it's tricky here.

1237
01:17:19,720 --> 01:17:27,360
But I don't have a clear thing, but I don't like is the narrative, the enhancement narrative,

1238
01:17:27,360 --> 01:17:36,000
the myth that we've been fed by those shall we say, prophets of transhumanism as if that

1239
01:17:36,000 --> 01:17:38,200
would be the solution to all the problems.

1240
01:17:38,200 --> 01:17:44,680
That's all, I'll say, we have to go much more, I'll try an error and much more one at a time.

1241
01:17:44,680 --> 01:17:48,320
Here's an example of something that, yes, the position would be positive.

1242
01:17:48,320 --> 01:17:50,320
The unbelievable.

1243
01:17:50,320 --> 01:17:51,320
Right.

1244
01:17:51,320 --> 01:17:53,320
And we could prevent it.

1245
01:17:53,320 --> 01:17:55,320
We could change that.

1246
01:17:55,320 --> 01:17:56,320
Yeah.

1247
01:17:56,320 --> 01:17:57,320
Yeah.

1248
01:17:57,320 --> 01:18:06,320
But, but, you know, Nick Prostam used to say, let's try to self-stoip it in the end.

1249
01:18:06,320 --> 01:18:09,320
We have about six or seven hands up to working, moving.

1250
01:18:09,320 --> 01:18:10,320
Yes.

1251
01:18:10,320 --> 01:18:11,320
Stand up.

1252
01:18:11,320 --> 01:18:12,920
My name is Todd Esig.

1253
01:18:12,920 --> 01:18:15,800
I'm a psychoanalyst here in the city.

1254
01:18:15,800 --> 01:18:21,880
And I really appreciated the comments about incrementalism and making things work.

1255
01:18:21,880 --> 01:18:28,440
And I want to bring up an issue of the fact that every enhancement always has a downside.

1256
01:18:28,440 --> 01:18:33,480
It's impossible to have a technological advance that doesn't have a loss as well as a gain.

1257
01:18:33,480 --> 01:18:38,200
And that one of the front lines for seeing the losses of technologies are our practices.

1258
01:18:38,200 --> 01:18:45,640
And for example, I see many people who are using Adderall for enhancement purposes and

1259
01:18:45,640 --> 01:18:49,040
their careers and relationships are being destroyed.

1260
01:18:49,040 --> 01:18:54,040
Like your grandson, we see many people who are having their communications kind of enhanced

1261
01:18:54,040 --> 01:18:59,520
by communications technology and their capacity for intimacy is being destroyed.

1262
01:18:59,520 --> 01:19:03,960
So I'd like the people who are kind of involved in the technological front lines to comment

1263
01:19:03,960 --> 01:19:17,760
on how we can better communicate with you.

1264
01:19:17,760 --> 01:19:23,240
So much of what medical ethics has been about, you know, there was a time when it was a horrible

1265
01:19:23,240 --> 01:19:27,840
idea to think that someone didn't have to die with a tube in every orifice, natural

1266
01:19:27,840 --> 01:19:29,400
and unnatural.

1267
01:19:29,400 --> 01:19:35,400
And now 70% of people in hospitals die after treatment has been tried and withdrawn.

1268
01:19:35,400 --> 01:19:40,880
And so we make progress and we have committees and we do case consultations.

1269
01:19:40,880 --> 01:19:48,840
And so you begin to get a handle through experience and no, nobody can anticipate all of these

1270
01:19:48,840 --> 01:19:52,280
possibilities coming on down the truck, the track.

1271
01:19:52,280 --> 01:19:57,400
But as you deal with them as an active agent, you can make successes.

1272
01:19:57,400 --> 01:20:03,440
But certainly, I mean, I really recommend Delaney Rustin's movie Screenagers, which

1273
01:20:03,440 --> 01:20:10,120
has been all over the country, just helping families begin to deal process-wise and psychologically

1274
01:20:10,120 --> 01:20:15,400
with the struggle of getting some control over the kid's screen time, you know, which

1275
01:20:15,400 --> 01:20:20,960
I guess was a problem when I was a boy because we wanted to watch TV all the time.

1276
01:20:20,960 --> 01:20:21,960
Right.

1277
01:20:21,960 --> 01:20:29,680
So throw in a comment, a lot of this stuff, the profit that drives it, is designed to

1278
01:20:29,680 --> 01:20:33,800
appeal to like our brain stem, our hunter-gatherer brain.

1279
01:20:33,800 --> 01:20:37,400
It's meant to be addictive intentionally.

1280
01:20:37,400 --> 01:20:46,920
And so it's partly in the design and the profit motive of Google or Facebook or whatever,

1281
01:20:46,920 --> 01:20:50,200
to make these devices and their utilities.

1282
01:20:50,200 --> 01:20:51,760
That's Ken's son of Brookhaven.

1283
01:20:51,760 --> 01:20:53,800
He wanted to finish up one comment.

1284
01:20:53,800 --> 01:20:59,600
Yeah, in the medical area, there are M&M conferences and processes are built in.

1285
01:20:59,600 --> 01:21:04,480
I'm not so sure those processes are currently built into AI development, into genetics

1286
01:21:04,480 --> 01:21:05,480
development.

1287
01:21:05,480 --> 01:21:09,840
And so the question is how can those processes be built in with those of us on the front

1288
01:21:09,840 --> 01:21:14,640
lines, being the downside of technology, be part of your creative process?

1289
01:21:14,640 --> 01:21:15,640
Yeah.

1290
01:21:15,640 --> 01:21:21,480
I agree that in all this multidisciplinary initiative that I'm involved, I didn't see

1291
01:21:21,480 --> 01:21:29,520
much presence of those like you that see the fact, may see some effects of the technology.

1292
01:21:29,520 --> 01:21:34,960
So definitely I think that that should be more present.

1293
01:21:34,960 --> 01:21:40,520
And like we, for example, we have put together something which is called the partnership on

1294
01:21:40,520 --> 01:21:46,080
AI that started from six companies, but now as more than 60 partners of which only 30

1295
01:21:46,080 --> 01:21:47,080
percent are companies.

1296
01:21:47,080 --> 01:21:50,160
And then everybody else, you know, from various disciplines.

1297
01:21:50,160 --> 01:21:56,560
But I don't think there is any initiative or entity or organization that has to do with

1298
01:21:56,560 --> 01:22:00,360
SAC one analysis and the effect of the technology.

1299
01:22:00,360 --> 01:22:02,880
So that definitely is something that should be there.

1300
01:22:02,880 --> 01:22:08,680
And the goal is to build together best practices in designing and developing AI.

1301
01:22:08,680 --> 01:22:12,200
So that, you know, these negative effects are not there.

1302
01:22:12,200 --> 01:22:13,840
And drawing the right boundaries.

1303
01:22:13,840 --> 01:22:15,560
Boundary creation is key.

1304
01:22:15,560 --> 01:22:21,360
You know, I mean, the reason why positions have the highest suicide rate of any profession

1305
01:22:21,360 --> 01:22:27,480
per capita in the country right now and why 50 percent of them respond from coast to coast

1306
01:22:27,480 --> 01:22:32,520
to surveys about satisfaction saying they quit if they could afford to.

1307
01:22:32,520 --> 01:22:35,480
I mean, this is serious stuff.

1308
01:22:35,480 --> 01:22:40,560
You know, a lot of it is because, you know, let's take email, just take email, let alone

1309
01:22:40,560 --> 01:22:42,360
electronic medical records.

1310
01:22:42,360 --> 01:22:46,120
They're not connecting empathically with their patients so they lose meaning.

1311
01:22:46,120 --> 01:22:48,040
It becomes depersonalized.

1312
01:22:48,040 --> 01:22:50,640
And also they can't get away from it.

1313
01:22:50,640 --> 01:22:53,480
The email is a machine, that's a machine.

1314
01:22:53,480 --> 01:22:54,720
They're a human being.

1315
01:22:54,720 --> 01:22:55,960
They need time away.

1316
01:22:55,960 --> 01:22:57,640
They need to balance, but they can't get it.

1317
01:22:57,640 --> 01:23:03,640
And so in that sense, boundary drawing in all these different areas is what's really

1318
01:23:03,640 --> 01:23:07,080
important and we're not very good at it sometimes.

1319
01:23:07,080 --> 01:23:10,880
First, I'm going to remove the other side.

1320
01:23:10,880 --> 01:23:18,600
One thing I'm surprised that I didn't get into is the coming deep integration of DNA

1321
01:23:18,600 --> 01:23:23,720
from among different organisms, which is already happening.

1322
01:23:23,720 --> 01:23:26,320
You know, chimeric organisms are already here.

1323
01:23:26,320 --> 01:23:30,440
But we're now getting to the point where we're going to have deeper integration of human

1324
01:23:30,440 --> 01:23:35,960
DNA with that of other, you know, not just pigs in order to grow kidneys, which Bob

1325
01:23:35,960 --> 01:23:40,600
Langer is doing up at MIT, but also, you know, we're going to get to the creation of new

1326
01:23:40,600 --> 01:23:46,480
beings, the integration of human DNA with that of other non-humans.

1327
01:23:46,480 --> 01:23:51,200
And we will begin to see things that haven't existed before that have various characteristics

1328
01:23:51,200 --> 01:23:52,440
good and bad.

1329
01:23:52,440 --> 01:23:54,560
So sort of curious.

1330
01:23:54,560 --> 01:23:57,800
That goes back to their environmental argument that I made.

1331
01:23:57,800 --> 01:23:58,800
Right?

1332
01:23:58,800 --> 01:24:06,840
So environmentalist are really against that kind of, that's a whole debate about GMOs,

1333
01:24:06,840 --> 01:24:07,840
right?

1334
01:24:07,840 --> 01:24:11,080
So from environmental perspective, at least it's, I would go against it.

1335
01:24:11,080 --> 01:24:17,800
So I mean, I think there's a species to reference there.

1336
01:24:17,800 --> 01:24:24,120
I mean, if you look, actually look at chimpanzee DNA, most genes are identical to human genes.

1337
01:24:24,120 --> 01:24:25,120
They're not human DNA.

1338
01:24:25,120 --> 01:24:29,880
I think it's still 1% or what's a hunch?

1339
01:24:29,880 --> 01:24:30,880
Chimpanzees.

1340
01:24:30,880 --> 01:24:36,480
In the genes, less than 0.1% difference.

1341
01:24:36,480 --> 01:24:38,480
And the conclusion, the philosophical...

1342
01:24:38,480 --> 01:24:43,280
Well, I mean, what you said was what you're saying is we're going to take information.

1343
01:24:43,280 --> 01:24:47,560
I mean, every organ is, every living species is connected to other species.

1344
01:24:47,560 --> 01:24:52,720
So, so these I already showed that there's something wrong with a completely geneticist

1345
01:24:52,720 --> 01:24:58,680
bottom-up approach because we're not, we're significantly different than chimpanzees.

1346
01:24:58,680 --> 01:25:01,360
Ah, 0.1% just 3,000,000.

1347
01:25:01,360 --> 01:25:09,160
But you know, I mean, if you go back to the new Atlantis, which is the definitive initial

1348
01:25:09,160 --> 01:25:15,680
statement of the biological revolution, I mean, Bacon argued not only for fountains of

1349
01:25:15,680 --> 01:25:19,920
youth, the waters of youth, but he argued for chimeras.

1350
01:25:19,920 --> 01:25:20,920
And it's all there.

1351
01:25:20,920 --> 01:25:22,600
It's all part of the vision.

1352
01:25:22,600 --> 01:25:25,560
And I don't know that you can get...

1353
01:25:25,560 --> 01:25:28,760
I think it's inevitable that we'll move in that direction personally.

1354
01:25:28,760 --> 01:25:34,760
I think we're already incrementally halfway there, don't you?

1355
01:25:34,760 --> 01:25:37,760
Oh, it's already happened.

1356
01:25:37,760 --> 01:25:38,760
No.

1357
01:25:38,760 --> 01:25:46,760
And if it can benefit, if it seems to be benefiting human beings, then mothers will do it.

1358
01:25:46,760 --> 01:25:49,520
Oh, yeah, we'll do it.

1359
01:25:49,520 --> 01:25:50,520
Thank you.

1360
01:25:50,520 --> 01:25:53,040
Thank you very much.

1361
01:25:53,040 --> 01:25:54,040
And it's a great conversation.

1362
01:25:54,040 --> 01:25:56,240
Thank you very much for the one of the best here.

1363
01:25:56,240 --> 01:26:03,800
And I just would like to point, I recently read an article about edging.

1364
01:26:03,800 --> 01:26:09,880
Physics makes edging inevitable, not biology.

1365
01:26:09,880 --> 01:26:15,520
It's in now tillus, a candidate that was published in, in, in, uh, laboratory.

1366
01:26:15,520 --> 01:26:23,680
And on a scale, thermal physics guarantees our decline, no matter how many diseases we

1367
01:26:23,680 --> 01:26:24,680
cure.

1368
01:26:24,680 --> 01:26:26,680
It's not my opinion.

1369
01:26:26,680 --> 01:26:37,840
So that are the, are the article in St. David's, you're a comedian about fake shortages.

1370
01:26:37,840 --> 01:26:46,560
They, they, um, um, um, make people see and tell them they put, for example, a stand in

1371
01:26:46,560 --> 01:26:47,960
the heart.

1372
01:26:47,960 --> 01:26:55,200
And these actually make them as good as people with stance with real state stance.

1373
01:26:55,200 --> 01:27:03,000
So even surgeries could help like, see, but if so, and, and so our integrations with machines

1374
01:27:03,000 --> 01:27:06,280
also very questionable in this matter.

1375
01:27:06,280 --> 01:27:12,880
And even, um, hip replacement also exercises and weight loss.

1376
01:27:12,880 --> 01:27:20,880
In many experiments had much more effect and much, but effect there's hip replacement.

1377
01:27:20,880 --> 01:27:29,560
And, and yes, and about AI and about our integration, migration about it.

1378
01:27:29,560 --> 01:27:37,480
We don't discuss problem with, uh, phantom pain, for example, when people get those

1379
01:27:37,480 --> 01:27:40,280
legs, then still have pain.

1380
01:27:40,280 --> 01:27:47,840
Or do you, uh, understand when we, uh, we, uh, it's a question about embodiment, uh, when

1381
01:27:47,840 --> 01:27:53,880
we get out our voice, uploaded some machine and we want to eat, we want to have sex or

1382
01:27:53,880 --> 01:27:56,680
we want not just something to see.

1383
01:27:56,680 --> 01:28:02,720
Don't you think that it will be the perfect reason when we can upload someone there and

1384
01:28:02,720 --> 01:28:04,720
force leave them forever?

1385
01:28:04,720 --> 01:28:05,720
Okay.

1386
01:28:05,720 --> 01:28:07,720
Uh, the honest is on you.

1387
01:28:07,720 --> 01:28:14,160
There are some cognitive, uh, capabilities that we don't have.

1388
01:28:14,160 --> 01:28:20,560
So if you surround somebody with an incredible amount of data, our brain cannot handle that.

1389
01:28:20,560 --> 01:28:25,960
Our brain, as we said, is very efficient, you know, but cannot handle that.

1390
01:28:25,960 --> 01:28:31,720
So if you have to make a decision and you have a lot of data available to you, but you cannot

1391
01:28:31,720 --> 01:28:37,200
handle it, you're going to make a decision with just a very small subset of, of information.

1392
01:28:37,200 --> 01:28:42,400
And so that you're going to make a decision, but probably will not be as good or whatever

1393
01:28:42,400 --> 01:28:48,680
good means in that context as if you could find patterns and information and knowledge

1394
01:28:48,680 --> 01:28:50,600
from all that information.

1395
01:28:50,600 --> 01:28:56,120
So that's something that is not placebo, I think, is something that cannot be replaced

1396
01:28:56,120 --> 01:29:02,040
by something that we can do by ourselves.

1397
01:29:02,040 --> 01:29:04,880
We have time now.

1398
01:29:04,880 --> 01:29:05,880
Okay.

1399
01:29:05,880 --> 01:29:09,760
I'm Giro, neuroscientist by training.

1400
01:29:09,760 --> 01:29:17,680
Um, so in the face of the inevitability of the evolution of technology and the evolution

1401
01:29:17,680 --> 01:29:26,680
of our species, which I think is a fact, um, I do see three major problem with the transhumanist

1402
01:29:26,680 --> 01:29:36,240
movement, which is that overall it is rooted in low complexity thinking.

1403
01:29:36,240 --> 01:29:38,800
And by low complexity thinking, I'm saying two things.

1404
01:29:38,800 --> 01:29:46,240
First, it is reducing human reality to its biological component and its cognitive component,

1405
01:29:46,240 --> 01:29:47,640
ignoring everything else.

1406
01:29:47,640 --> 01:29:51,440
Which is really considering very little about what we are.

1407
01:29:51,440 --> 01:30:00,400
The second also is like it's really classically, uh, offering simple, uh, causalities.

1408
01:30:00,400 --> 01:30:05,440
And it's having a really bad time to understand every, every single action in a much more

1409
01:30:05,440 --> 01:30:12,000
complex environment, which means that in the end, we have always this like, uh, conversation

1410
01:30:12,000 --> 01:30:16,720
about what are the consequences of the thing you, you think is so great.

1411
01:30:16,720 --> 01:30:23,560
And the step thing is that it's completely ignoring the own emotional development of

1412
01:30:23,560 --> 01:30:25,200
human beings.

1413
01:30:25,200 --> 01:30:32,200
And if you have, you know, average human beings, all they want is to be stronger to have a,

1414
01:30:32,200 --> 01:30:37,560
uh, a body that's going to attract more mates, to have a bigger penis, bigger, bigger boobs.

1415
01:30:37,560 --> 01:30:39,000
So we know human beings, right?

1416
01:30:39,000 --> 01:30:42,640
We know what the kind of things we like when we want to be enhanced.

1417
01:30:42,640 --> 01:30:45,800
So my question, my question is this.

1418
01:30:45,800 --> 01:30:55,880
Is that is there any hope of, of sort of like, uh, a neo trans, you managed movement that

1419
01:30:55,880 --> 01:31:04,760
would, yeah, yeah, whatever I said, um, that would actually do the things that it took

1420
01:31:04,760 --> 01:31:10,880
you 15 years to understand, which is that you cannot, and I'm really not taking it personally,

1421
01:31:10,880 --> 01:31:11,880
right?

1422
01:31:11,880 --> 01:31:20,640
It's not, um, which is that we know that any, uh, technological improvement at that scale

1423
01:31:20,640 --> 01:31:23,160
is going to have negative impact.

1424
01:31:23,160 --> 01:31:28,680
So let's put together a transdisciplinary approach where we have like philosophers and

1425
01:31:28,680 --> 01:31:31,480
psychologists and everything, but is that okay?

1426
01:31:31,480 --> 01:31:32,480
Okay.

1427
01:31:32,480 --> 01:31:34,560
So maybe you're going to say, yeah, it's a good idea, but do you think there is a real

1428
01:31:34,560 --> 01:31:36,720
commitment in the world for that?

1429
01:31:36,720 --> 01:31:38,640
Not just like a good intention in this room.

1430
01:31:38,640 --> 01:31:44,560
But, I mean, there is commitment because there are initiatives like the one, like the

1431
01:31:44,560 --> 01:31:48,440
partnership where, yeah, it's one, but there are many initiatives when there in the last

1432
01:31:48,440 --> 01:31:55,440
two years, I think I've seen, I don't know, at least 10 or 20 Institute centers that are

1433
01:31:55,440 --> 01:32:02,840
multidisciplinary and they study exactly this, you know, um, the fact that AI can, uh, has

1434
01:32:02,840 --> 01:32:11,000
this goal you say of enhancing just our cognitive abilities and our, um, um, physical abilities.

1435
01:32:11,000 --> 01:32:19,800
I mean, that's, that's, I mean, you are, you are speaking as if AI is what has been done

1436
01:32:19,800 --> 01:32:21,920
for the last 50 years.

1437
01:32:21,920 --> 01:32:23,880
And now it's done.

1438
01:32:23,880 --> 01:32:24,880
It's not done.

1439
01:32:24,880 --> 01:32:27,560
It's a, it continues revolving technology.

1440
01:32:27,560 --> 01:32:36,040
Uh, after all is only 50 years old and it's really evolving and trying to understand how

1441
01:32:36,040 --> 01:32:43,120
better and better and we're considering the consequences as well can help us.

1442
01:32:43,120 --> 01:32:50,960
So I'm not, uh, it could be that the emphasis right until now was into announcing, uh, physical

1443
01:32:50,960 --> 01:32:58,400
and cognitive capabilities, but I don't think that that's, uh, that the AI people would

1444
01:32:58,400 --> 01:33:06,520
say, oh, we only want to help that, you know, that that's more and the more you are multidisciplinary,

1445
01:33:06,520 --> 01:33:09,640
the more you understand these other dimensions.

1446
01:33:09,640 --> 01:33:14,800
And then the AI people can also understand how to, you know, relate to those other dimensions,

1447
01:33:14,800 --> 01:33:15,800
I think.

1448
01:33:15,800 --> 01:33:21,280
So I think somebody like you engage Ben Gertzel, one of the major transhumanist AI who works

1449
01:33:21,280 --> 01:33:22,280
in.

1450
01:33:22,280 --> 01:33:23,280
I will engage with anybody.

1451
01:33:23,280 --> 01:33:24,280
Yes.

1452
01:33:24,280 --> 01:33:25,280
Yeah.

1453
01:33:25,280 --> 01:33:26,280
But not the preclude.

1454
01:33:26,280 --> 01:33:27,280
Yeah.

1455
01:33:27,280 --> 01:33:28,280
But I want this into this.

1456
01:33:28,280 --> 01:33:33,400
I saw with Nick bus from to, to events all the time and they speak with him all the time.

1457
01:33:33,400 --> 01:33:35,360
So, so get him.

1458
01:33:35,360 --> 01:33:38,920
Yeah, the move in your direction and we'll be in better shape.

1459
01:33:38,920 --> 01:33:45,160
So I would just have a real quick, um, response in terms of actual transhumanism in terms

1460
01:33:45,160 --> 01:33:48,400
of our species, um, evolving.

1461
01:33:48,400 --> 01:33:54,240
Uh, it's, it seems to me that we can't, we can't stop it or let it go.

1462
01:33:54,240 --> 01:33:56,160
We have no idea where it's going to go.

1463
01:33:56,160 --> 01:33:59,880
I mean, do I disagree with this notion of directed evolution?

1464
01:33:59,880 --> 01:34:02,960
No, but nobody's going to direct the evolution of the human species.

1465
01:34:02,960 --> 01:34:07,040
It's just going to happen on our hands.

1466
01:34:07,040 --> 01:34:12,120
I mean, I'm into that, but just tell this to Julia Pritzko, who has a whole conversation

1467
01:34:12,120 --> 01:34:13,280
against what you do.

1468
01:34:13,280 --> 01:34:17,760
They have no power to, they're, they're, they're like utilitarian somewhere.

1469
01:34:17,760 --> 01:34:18,760
Correct.

1470
01:34:18,760 --> 01:34:24,160
It's, you know, creating some sort of idea that does not apply to reality in a democratic

1471
01:34:24,160 --> 01:34:25,160
society.

1472
01:34:25,160 --> 01:34:26,160
I'm sorry.

1473
01:34:26,160 --> 01:34:28,440
If you want to minimize their impact, go ahead and write against them.

1474
01:34:28,440 --> 01:34:30,160
That's exactly what I've been doing.

1475
01:34:30,160 --> 01:34:31,360
So we're running out of time.

1476
01:34:31,360 --> 01:34:37,960
I'm going to ask that the last show of hands, one, two, three, four, that's not going to

1477
01:34:37,960 --> 01:34:38,960
get it right there.

1478
01:34:38,960 --> 01:34:39,960
Sorry.

1479
01:34:39,960 --> 01:34:44,360
I just want to ask you each to make it really quick and you guys hold your, hold yours so

1480
01:34:44,360 --> 01:34:46,000
you can write notes or whatever.

1481
01:34:46,000 --> 01:34:49,360
Put it out there and then we'll give you one last chance to respond.

1482
01:34:49,360 --> 01:34:53,640
So very quickly, please.

1483
01:34:53,640 --> 01:34:55,920
My name is Bernard Starr.

1484
01:34:55,920 --> 01:34:58,360
I'm a recovering academic.

1485
01:34:58,360 --> 01:35:05,720
Several, several months ago, I published an article titled on the verge of immortality

1486
01:35:05,720 --> 01:35:13,040
or are we stuck with death based on a, I'd say a four hour interview with a cellular

1487
01:35:13,040 --> 01:35:14,840
biologist, Leonard Haeflick.

1488
01:35:14,840 --> 01:35:15,840
Oh yeah.

1489
01:35:15,840 --> 01:35:16,840
I'm sure many of you know.

1490
01:35:16,840 --> 01:35:17,840
Yeah.

1491
01:35:17,840 --> 01:35:23,240
Known for the Haeflick limit who set lifespan at about 120 years.

1492
01:35:23,240 --> 01:35:25,840
And he has a radically different position.

1493
01:35:25,840 --> 01:35:35,080
He's more or less a naysayer on the prospects of extensive life, life span.

1494
01:35:35,080 --> 01:35:41,720
And it's a position that I haven't heard discussed here today.

1495
01:35:41,720 --> 01:35:50,520
He believes that more than believes, he asserts that we know virtually zero about cellular

1496
01:35:50,520 --> 01:35:57,480
aging and that most of the gains in longevity aside from public health measures that were

1497
01:35:57,480 --> 01:36:04,280
introduced in the turn of the last century are based primarily on the cure of diseases.

1498
01:36:04,280 --> 01:36:09,000
And he says, well, that's welcome and most of the funding is in that direction because

1499
01:36:09,000 --> 01:36:16,880
that's what people demand and that's what the NIH and other funding agencies tend to

1500
01:36:16,880 --> 01:36:23,160
fund that virtually nothing is spent on cellular aging at all.

1501
01:36:23,160 --> 01:36:28,800
And that although we talk about diseases of old age, nobody really seriously addresses

1502
01:36:28,800 --> 01:36:33,640
the question of why do cells age?

1503
01:36:33,640 --> 01:36:40,760
That he believes that there's a common factor at the cellular level that would lead to a

1504
01:36:40,760 --> 01:36:43,240
cure of all the diseases.

1505
01:36:43,240 --> 01:36:50,760
All your comments, we're going to get all the questions out there.

1506
01:36:50,760 --> 01:36:51,760
Please.

1507
01:36:51,760 --> 01:36:52,920
My name is Moshe.

1508
01:36:52,920 --> 01:36:54,400
I'm a psychologist.

1509
01:36:54,400 --> 01:37:02,960
I'm trying to build on the point that Jill mentioned about the idea of artificial intelligence,

1510
01:37:02,960 --> 01:37:07,600
technology and basically happiness.

1511
01:37:07,600 --> 01:37:12,200
It's hard to imagine, we're just the beginning of that technology and expansion if it's

1512
01:37:12,200 --> 01:37:19,920
cyber technology inside, outside and realizing how much of the people around us because of

1513
01:37:19,920 --> 01:37:27,920
technology become addicted, anxiety exposed to that and so on and so forth, all the problems.

1514
01:37:27,920 --> 01:37:32,920
And it has a spirit in itself, by the way, that's so hard to control and to imagine what

1515
01:37:32,920 --> 01:37:36,640
is going to be, especially with the old stewards of singularity.

1516
01:37:36,640 --> 01:37:41,560
I'm just curious to know about your perspective about that future that it almost has a spirit

1517
01:37:41,560 --> 01:37:48,160
in itself that how is that going to impact us if it's at all impossible to predict about

1518
01:37:48,160 --> 01:37:50,920
our own happiness or maybe spirituality?

1519
01:37:50,920 --> 01:37:51,920
Thank you.

1520
01:37:51,920 --> 01:37:54,920
And there was one over two.

1521
01:37:54,920 --> 01:38:03,040
Very Paul, physics teacher, I was wondering, there was a artificial intelligence program

1522
01:38:03,040 --> 01:38:13,040
that detected an exoplanet and the way it worked was that this exoplanet had been missed

1523
01:38:13,040 --> 01:38:14,440
by humans.

1524
01:38:14,440 --> 01:38:22,640
But we had lots of examples of planets that had been detected by humans and so the program

1525
01:38:22,640 --> 01:38:28,200
was taught to recognize the planet and it recognized the planet better than humans could.

1526
01:38:28,200 --> 01:38:30,360
So obviously that's a good thing.

1527
01:38:30,360 --> 01:38:37,600
But I was wondering how can we defend against artificial intelligence when it might encroach

1528
01:38:37,600 --> 01:38:46,600
on our privacy and be able to find, let's say it's some kind of disability, say somebody

1529
01:38:46,600 --> 01:38:54,400
has a disability that isn't really something that is, you know, we should hold against

1530
01:38:54,400 --> 01:38:55,400
them.

1531
01:38:55,400 --> 01:38:59,400
And yet this artificial intelligence would be able to detect that.

1532
01:38:59,400 --> 01:39:06,960
How do we defend against that?

1533
01:39:06,960 --> 01:39:08,600
My name is Domisio Coutinho.

1534
01:39:08,600 --> 01:39:10,880
I'm from Brazil.

1535
01:39:10,880 --> 01:39:20,440
And I'd like to have a few questions, so if it's more common to what I heard, I really

1536
01:39:20,440 --> 01:39:33,360
understand that it's possible and really admirable in there that it's able to have, you know,

1537
01:39:33,360 --> 01:39:40,240
looking for artificial intelligence and no question about it, something desirable.

1538
01:39:40,240 --> 01:39:49,760
But the question is, is there anything other than these that we should look for?

1539
01:39:49,760 --> 01:39:58,440
We have from Plato a very important saying, nor should you yourself get to know yourself,

1540
01:39:58,440 --> 01:40:00,080
nor is it an option.

1541
01:40:00,080 --> 01:40:04,880
Do you know ourself enough?

1542
01:40:04,880 --> 01:40:09,960
We know you're made from about 70 billions atoms in our body.

1543
01:40:09,960 --> 01:40:18,120
Everything he has said about the importance of this extraordinary element in our life.

1544
01:40:18,120 --> 01:40:23,400
Everything that moves, everything that's visible, everything that's around here is made and

1545
01:40:23,400 --> 01:40:27,000
composed by atoms.

1546
01:40:27,000 --> 01:40:31,480
What are these little ants, little ants in ourself?

1547
01:40:31,480 --> 01:40:45,800
They see the potential opportunities made us here to be what we are.

1548
01:40:45,800 --> 01:40:57,600
They taught us everything we are, how to walk, how to talk, how to speak, how to speak.

1549
01:40:57,600 --> 01:41:03,720
When we try to improve ourself, improve humanity, do we have to look for somebody else, any

1550
01:41:03,720 --> 01:41:10,560
place else, other than those who are responsible, or are existence youngers.

1551
01:41:10,560 --> 01:41:17,400
And then if you do that, sister of the great-to-rate in rooms, you say, who is going to benefit

1552
01:41:17,400 --> 01:41:19,120
our audience first?

1553
01:41:19,120 --> 01:41:24,680
The regular human being, the street staff is going to benefit from artificial intelligence

1554
01:41:24,680 --> 01:41:29,200
primarily, it's up to us to answer that question.

1555
01:41:29,200 --> 01:41:34,040
However, I will say that these...

1556
01:41:34,040 --> 01:41:40,000
There's a lot of things that we...

1557
01:41:40,000 --> 01:41:42,120
I'm an ex-marathon runner.

1558
01:41:42,120 --> 01:41:47,120
Before I run 1,000 miles, I learned how to run 100 miles.

1559
01:41:47,120 --> 01:41:53,600
If you had to try to get artificial intelligence, I would ask you, do you know how to deal

1560
01:41:53,600 --> 01:41:55,360
with the cancer?

1561
01:41:55,360 --> 01:41:59,680
Do you know how to extinguish your fire that every year starts, thousands of thousands

1562
01:41:59,680 --> 01:42:01,800
of miles, billions of billions of miles, are out of the United States?

1563
01:42:01,800 --> 01:42:04,640
And do you cross the army and do not think about it?

1564
01:42:04,640 --> 01:42:05,640
How about this storm?

1565
01:42:05,640 --> 01:42:07,640
Thank you very much.

1566
01:42:07,640 --> 01:42:08,640
Thank you.

1567
01:42:08,640 --> 01:42:14,080
So, last round for everybody here.

1568
01:42:14,080 --> 01:42:17,760
You can pick up on any of those pieces.

1569
01:42:17,760 --> 01:42:25,640
So, I'd like to respond to the hay flick effect.

1570
01:42:25,640 --> 01:42:32,280
And that idea that there was a limit to the number of cell divisions, and so there was

1571
01:42:32,280 --> 01:42:42,560
this limit and cells couldn't go beyond the limit and they died and that consequence

1572
01:42:42,560 --> 01:42:45,840
is that it would mean that there's a limit to human life.

1573
01:42:45,840 --> 01:42:46,840
Sorry?

1574
01:42:46,840 --> 01:42:47,840
Right.

1575
01:42:47,840 --> 01:42:55,440
Now, the several problems with that, that was all done before the realization of what

1576
01:42:55,440 --> 01:42:58,360
genetic engineering could do.

1577
01:42:58,360 --> 01:43:03,000
And genetic engineering can make cells, I mean, you don't even need genetic engineering,

1578
01:43:03,000 --> 01:43:04,240
just cancer.

1579
01:43:04,240 --> 01:43:08,960
Cancer cells, they go dividing and dividing and dividing forever.

1580
01:43:08,960 --> 01:43:12,040
We understand how telomeres are lengthened.

1581
01:43:12,040 --> 01:43:18,920
And the important thing to think about is that if you look at my cells, it goes back

1582
01:43:18,920 --> 01:43:21,960
in a continuous line back for 4 billion years.

1583
01:43:21,960 --> 01:43:32,560
I mean, so, you know, it's, the organisms have figured out how to maintain continuity

1584
01:43:32,560 --> 01:43:36,360
of life, even if the individual organism dies.

1585
01:43:36,360 --> 01:43:43,120
And I think, I mean, I think hay flick is just, you know, he's of the generation prior to

1586
01:43:43,120 --> 01:43:44,120
genetic engineering.

1587
01:43:44,120 --> 01:43:50,360
I think you didn't know the power that was actually possible with that.

1588
01:43:50,360 --> 01:43:56,720
I tend to agree with that assessment, but I think that's part of the way it is.

1589
01:43:56,720 --> 01:44:06,320
Well, the second law of thermodynamics doesn't work because, I mean, there's a, there's a

1590
01:44:06,320 --> 01:44:07,320
energy input, right?

1591
01:44:07,320 --> 01:44:13,840
I mean, a single thermodynamics, everything's going to degrade if there's no energy input.

1592
01:44:13,840 --> 01:44:18,440
For next 2 billion years, there's plenty of energy input and, and...

1593
01:44:18,440 --> 01:44:21,440
At least on this planet.

1594
01:44:21,440 --> 01:44:22,440
Yeah.

1595
01:44:22,440 --> 01:44:23,440
Yeah.

1596
01:44:23,440 --> 01:44:24,440
Okay.

1597
01:44:24,440 --> 01:44:26,800
So, since there were a lot of questions about the AI, so maybe I'll try to respond to some

1598
01:44:26,800 --> 01:44:27,800
of them.

1599
01:44:27,800 --> 01:44:34,000
So, I remember, okay, the last one was about, you know, who is going to benefit for AI,

1600
01:44:34,000 --> 01:44:37,920
you know, if the people in the street are going to benefit and so on.

1601
01:44:37,920 --> 01:44:45,200
And I think that actually, you know, the, we can make, for example, healthcare is one

1602
01:44:45,200 --> 01:44:53,200
obvious and current sector where AI is being used or trying to be used to, and to help

1603
01:44:53,200 --> 01:44:58,080
doctors to find better cures, diagnosis, therapies and so on.

1604
01:44:58,080 --> 01:45:05,080
And of course, this will improve the quality of healthcare in our first world, the country,

1605
01:45:05,080 --> 01:45:06,080
world.

1606
01:45:06,080 --> 01:45:12,040
But, you know, the main impact will be in developing countries where doctors do not see

1607
01:45:12,040 --> 01:45:17,920
as many cases, they do not have, you know, the same education and kind of experience.

1608
01:45:17,920 --> 01:45:22,080
So that's really what the impact is being done, actually.

1609
01:45:22,080 --> 01:45:28,000
And more generally from that, I mean, again, the multidisciplinary is important because

1610
01:45:28,000 --> 01:45:36,880
AI people and people in that know the problems that our planet has, like for example, the

1611
01:45:36,880 --> 01:45:44,000
UN are regularly getting together and studying our AI can help towards the 17 sustainable

1612
01:45:44,000 --> 01:45:45,000
development goals.

1613
01:45:45,000 --> 01:45:48,080
For each one of them, what are the issues?

1614
01:45:48,080 --> 01:45:49,080
What are the problems?

1615
01:45:49,080 --> 01:45:54,160
How they can be framed and solved by AI in part or totally or whatever.

1616
01:45:54,160 --> 01:46:01,600
So really, there is an emphasis on the represent the communities developing countries, you know,

1617
01:46:01,600 --> 01:46:09,040
and what AI can do for that and not just for our first world world.

1618
01:46:09,040 --> 01:46:12,160
The second one is about happiness and well-being.

1619
01:46:12,160 --> 01:46:18,440
Again, I think that there are efforts in that direction, you know, to understand how to use

1620
01:46:18,440 --> 01:46:25,400
AI not just to make us more efficient, but also to improve our well-being.

1621
01:46:25,400 --> 01:46:33,280
So one example is that IEEE, which is the World Wide Association for Engineers, has

1622
01:46:33,280 --> 01:46:42,600
put together a very interesting document, more than 200 pages, is called EthicaliAligned

1623
01:46:42,600 --> 01:46:51,320
Design, which is about all the issues, ethics, issues that can come up when you are developing

1624
01:46:51,320 --> 01:46:58,320
and deploying AI into the real world and for each one of them possible solutions.

1625
01:46:58,320 --> 01:47:03,800
And one chapter of this big book is about well-being.

1626
01:47:03,800 --> 01:47:06,120
So that is something to go.

1627
01:47:06,120 --> 01:47:12,400
Another thing is that I, for example, am discussing with people that know about well-being and

1628
01:47:12,400 --> 01:47:20,680
maybe they don't know about it, yeah, to understand really what it means for AI to help improve

1629
01:47:20,680 --> 01:47:25,320
our well-being, whether it's an individual or a collective well-being and so on.

1630
01:47:25,320 --> 01:47:33,000
So for example, there is a person, the venerable Tenzin Priyadarshi that I work with.

1631
01:47:33,000 --> 01:47:39,320
He is an MIT, he is the Director of the Ethics Institute at MIT, and he is an expert of well-being,

1632
01:47:39,320 --> 01:47:42,960
empathy, and these topics.

1633
01:47:42,960 --> 01:47:48,200
And he is interested in really working to understand how technology and AI specifically

1634
01:47:48,200 --> 01:47:58,960
can make us not less profitable, less efficient, but more, you know, and enhancing our well-being

1635
01:47:58,960 --> 01:48:06,480
and empathy and these traits that are typical than we might want to maintain and even enhance.

1636
01:48:06,480 --> 01:48:12,800
So I see a lot of things going on, but you have to understand that things are not, I mean,

1637
01:48:12,800 --> 01:48:17,600
we are not at the point that, you know, everything is concluded and then you can judge AI right

1638
01:48:17,600 --> 01:48:18,600
now.

1639
01:48:18,600 --> 01:48:19,760
You know, things are evolving.

1640
01:48:19,760 --> 01:48:25,560
People are understanding more and more and by talking to each other, especially in a place

1641
01:48:25,560 --> 01:48:30,840
like this with people that have different point of view, different experience, you know.

1642
01:48:30,840 --> 01:48:37,840
And then this brings along, you know, a better understanding of what to do.

1643
01:48:37,840 --> 01:48:39,840
I'm doing it.

1644
01:48:39,840 --> 01:48:42,840
Well, did you want to?

1645
01:48:42,840 --> 01:48:44,840
I just want to take a moment.

1646
01:48:44,840 --> 01:48:48,840
I'm smacked by how much things have changed in our lifetime.

1647
01:48:48,840 --> 01:48:49,840
We should just be blown.

1648
01:48:49,840 --> 01:48:54,480
I mean, we live day to day and we get used to this stuff very quickly, but these little

1649
01:48:54,480 --> 01:48:59,840
devices here and the way we live today, the flying around the world, and we should just

1650
01:48:59,840 --> 01:49:05,640
sort of take a moment and take a deep breath and say, my, how things have changed in a

1651
01:49:05,640 --> 01:49:07,680
very short period of time.

1652
01:49:07,680 --> 01:49:14,280
And remember that they could stop changing or could plateau or it could continue.

1653
01:49:14,280 --> 01:49:20,960
And if it continues at the pace that it has been, then I think we will be a much transformed

1654
01:49:20,960 --> 01:49:25,360
species on a very transformed planet for better and for ill.

1655
01:49:25,360 --> 01:49:26,600
That's how I would frame it.

1656
01:49:26,600 --> 01:49:28,960
So I think we could stop on that, Douglas.

1657
01:49:28,960 --> 01:49:29,960
Yeah, that sounds good.

1658
01:49:29,960 --> 01:49:35,680
I would also say on that little device in front of you, a certain point 20 years ago,

1659
01:49:35,680 --> 01:49:37,800
very few people had them.

1660
01:49:37,800 --> 01:49:39,360
Now everybody does.

1661
01:49:39,360 --> 01:49:44,000
And so every technological development begins with a certain kind of, if you will, an

1662
01:49:44,000 --> 01:49:46,240
imbalancer and injustice.

1663
01:49:46,240 --> 01:49:53,080
And if we took that as our sole determinant, then we would have no technological development

1664
01:49:53,080 --> 01:49:54,080
whatsoever.

1665
01:49:54,080 --> 01:50:02,240
So I will try to have the last word here by saying that as long as we keep the critical

1666
01:50:02,240 --> 01:50:08,400
perspective on those developments and not take them as inevitable and as necessary and

1667
01:50:08,400 --> 01:50:11,960
as determined, then very good shape.

1668
01:50:11,960 --> 01:50:18,200
But if we buy into the transhumanist myth that it has to happen the way, let's say, tells

1669
01:50:18,200 --> 01:50:22,320
us it's going to happen, then we have a problem.

1670
01:50:22,320 --> 01:50:23,320
Thank you.

1671
01:50:23,320 --> 01:50:24,320
Thanks, Douglas.

1672
01:50:24,320 --> 01:50:25,320
Thank you.

1673
01:50:25,320 --> 01:50:26,320
Thanks.

1674
01:50:26,320 --> 01:50:27,320
Thanks, darling.

1675
01:50:27,320 --> 01:50:28,320
Bye.

1676
01:50:28,320 --> 01:50:29,320
Bye.

1677
01:50:29,320 --> 01:50:30,320
Bye.

1678
01:50:30,320 --> 01:50:31,320
Bye.

1679
01:50:31,320 --> 01:50:32,320
Bye.

1680
01:50:32,320 --> 01:50:57,320
Bye.

