I'm just going to go through my
Oh, you're still going to start periods.
And...
But you still need a serious attempt.
You still need a serious attempt.
Well, sometimes I'll be yes.
All right, I hope you don't.
Okay, good morning everyone.
Hi, good morning everyone.
Welcome back to Hillock Center.
This is round table number four in our series
on coding in the new human phenotype.
The topic for today's talk is our natural language
generators for real.
We've got to have a wonderful panel of experts
that talk to us about this and talk us through this.
If you didn't see any of the three talks from yesterday,
I really do recommend you take a look at them on YouTube
where if you look up Hillock Center, you'll find them.
They really were terrific.
So I want to, without further ado, go on and give a brief
bio of each of our esteemed panelists.
First, Francesca Rossi is an IBM fellow
and the IBM AI ethics global leader.
She is based at the TJ Watson IBM research lab in New York
where she leads AI research projects.
She co-chairs the IBM AI ethics board
and she participates in many global multi-stakeholder
initiatives on AI ethics such as the partnership on AI,
the World Economic Forum, the United Nations ITU AI
for Good Summit and the Global Partnership on AI.
She is the president of AAI, the worldwide association
of AI researchers.
Dennis Yitenen is an associate professor of English
and comparative literature at Columbia University.
His teaching of research happened at the intersection
of people, texts and technologies.
A long time affiliate of Columbia's Data Science Institute
and formerly a Microsoft engineer and a Berkman Center
for Internet and Society fellow, his code runs on millions
of personal computers worldwide.
Tenon received his doctorate in comparative literature
at Harvard University under the advisement
of Professor Elaine Skari and William Todd,
a co-founder of Columbia's group for experimental methods
and UNS that research and the editor
of the On Method book series at Columbia University Press.
He is the author of plain text, The Poetics
of Computation 2017.
Kyung Young Cho is an associate professor of computer science
and data science at New York University and CIFAR
fellow of learning in machines and brains.
He is also a senior director of frontier research
at the prescient design team within Genentech research
and early development.
He was a research scientist at Facebook AI research
from June 2017 to May 2020 and a postdoctoral fellow
at the University of Montreal until the summer of 2015
under the supervision of Professor Yashua Benjio
after receiving his PhD in MSC degrees from Alto University
in April 2011 and April 2014 respectively
under the supervision of Professor Yuhua Karhone.
Catherine Elkins has written over a dozen articles
on memory, consciousness, and embodied aesthetic experience
in a wide range of writers from Plato and Sappho
to Wordsworth and Wolf.
In proofs in search of lost time, philosophical perspectives,
she refrained proofs exploration of consciousness
and light of integrated information theory.
In the shape of stories, she used the AI software,
Sentiment ARCs, to develop the first robust methodology
for exploring the emotional arcs of stories.
Her audible.com lectures on the giants of French literature
and the modern novel have won her an international audience.
Noah Jean-Sirakuso, PhD in math from Brown University,
is an assistant professor of mathematics
and data science at Bentley University.
Noah's research interests include algebraic geometry,
the abstract study of systems of polynomial equations
in their solutions, machine learning,
especially topological and geometric data analysis,
artificial intelligence, empirical legal studies,
phylogenetics, and misinformation.
Ned Block is silver professor of philosophy,
psychology, and neural science.
Came to NYU in 1996 from MIT,
where he was chair of the philosophy program.
He works in philosophy of mind and foundations
of neuroscience and cognitive science
and is currently writing a book on attention.
He is a fellow of the American Academy of Arts and Sciences,
a fellow of the Cognitive Science Society,
has been a Guggenheim fellow, a senior fellow in the center
for the study of language and information,
a Sloan Foundation fellow, a faculty member
at two National Endowment for the Humanities Summer
Institutes and two Summer Seminars.
The recipient of fellowships from the National Endowment
for the Humanities, the American Council on Learning
Societies and the National Science Foundation,
and a recipient of the Robert A. Mu Alumni Award
in Humanities and Social Science from MIT.
He is a past president of the Society for Philosophy
and Psychology, a past chair of the MIT Press Cognitive Science
Board, and past president of the Association
for the Scientific Study of Consciousness.
It's quite a mouthful.
Anyway, speaking of mouthful, so we're
going to be talking today about language
and artificial language generator
and what that means for us in our near and more distant future.
So welcome, everyone.
OK, so apparently all I need to do
is start to say a few words, and you're
all going to fill in the rest.
The way that the natural language generators work,
as some of you may know, is that after giving a prompt
of a few words, an entire story will
come out like in search of long time.
So I wonder if any of you want to take a shot
at giving a general orientation to our general audience
about what, for example, the GPT-3 generator is
and what we might expect from it in its future iterations.
I can give you the start and then people can chime in.
So until a few years ago, artificial intelligence
was becoming very good at interpreting content
that we were producing, like images and text
and other content.
So natural language generators are,
instead, the most recent advances of AI,
where AI is becoming good, also at generating new content,
rather than just interpreting the content
that we are producing.
And in order to do that, it is, and I'm saying this,
content is not text because it's not limited to text,
but it can also be videos, it can also be images, for example,
so different kinds of data that is content that is generated,
so-called generative AI.
So AI that can generate new content
besides being able to interpret content that we produce.
And the way, I mean, I live out a lot of details,
but the way it can do that is by being trained
on vast amounts of data, unlabeled data is called,
so data that is found on the web without any curation
from human beings, any labeling is called from human beings,
so this data that is found on the web in vast amounts,
that is used to train these AI systems,
and for example, text that is found on the web,
that is trained to use these AI systems that then can,
again, as you said, respond to a prompt in an appropriate way.
This data is the source of knowledge,
if you wanted knowledge, of the system,
but there is also knowledge that is given in the prompt itself,
so there is an area called prompt engineering,
because writing a prompt, also the way you write the prompt
of how long the prompt is, what you put in the prompt,
can also trigger a different, more informative
or less informative response from the natural language
generator.
And if I now put the heart of a company like IBM or others,
they may want to use this for some applications.
One possibility to use them is to take them
as trained by these vast amounts of data,
and then further tune them with supervised and labeled data
on a specific task to, but very little amount of data
to solve a specific task.
But with the general knowledge that is given by this initial phase
of training over vast amounts of data,
and this allows you to have specialized solver
for one particular problem, but with this more general knowledge,
which is needed usually to solve well, even a specific task,
and especially when the kind of labeled data
that you need for a specific data is kind of limited.
So you just need a little bit of this labeled data
for the specific task, because you
rely on this vast amount of general data.
So that's some initial thing, but feel free to chime in.
I would add that I think the probably most important thing
for many listeners to realize is just how amazing the production
of some of these programs have been.
I think they've been so incredible that two years ago,
nobody would have predicted that they could, or three years ago.
So GPT-3 came out of 2020.
So a little bit.
So around three years ago.
So at the top of GPT-2, I don't think anybody would
have predicted what they could do.
If you haven't seen any of these,
there's been a couple articles in the New York Times, etc.
It's kind of amazing.
But it's important to realize what they're good at,
what they're not good at.
So what they're good at is open-ended contexts
where there's no very specific right answer,
where what counts as style and creativity.
That's what's kind of amazing.
Creativity.
These things are good.
They're the opposite of what everybody would have expected.
So for example, in New York, they're published a poem.
The style of Phil Plarkin was actually pretty good.
And people could get them to write news articles
and get interviews in the style of a particular person
with the right fine tuning that Francesca mentioned.
So that's kind of amazing.
I think that's the first thing to realize is that it's just
how astonishing they really are.
But the second thing is the severe difficulties.
So the main, but one really bad, severe difficulty is
that they have been pointed out by many people,
no world love.
So they just continue a style in a certain way
and with ignoring actual facts.
So they'll spin a web of text along a certain line,
but then it can just completely contradict what's really true.
Even though you could get the real information from Siri
or Google Search, importantly, and Francesca mentioned this,
but they don't actually, I mean, you
can add a Google Search to one of these large language models.
But then you have a problem with the interface.
So in just the operation of the large language models,
they don't have access to the internet.
They're trained on the internet with enough electricity
to power a small city.
And then you can run the trained model on a smaller computer.
But they don't actually have the ability to look things up.
So you'll get a better answer from Siri
than you will from that.
Now, with all their failings, people
have tried to hook up more standard systems to them.
And then there's the issues of how that interface is supposed
to work.
So the big negatives are no world model.
In the generation of language, no understanding
of long-range dependencies.
So some of their critics have been fond of pointing out
that they treat words as just like a stream
without getting the hierarchical structure.
And there was a paper that came out, I believe, just yesterday,
by Stan de Hans laboratory looking
at short-range dependencies and long-range dependencies
with relative clauses.
So the case, the example they used,
I think, was the key, the short-range would be the key is green.
Or the key, the man had its green.
And then you keep adding relative clauses, like the key,
that the man in the corner had was green.
And as soon as you get to a fairly long, relative clauses,
forget it.
They don't know which names did what to who.
So that is a general feature of their not understanding
the structure of language.
And the language is what they're made for.
But they keep getting things wrong.
And then the real, I guess, the most significant issue is,
some people think, OK, you just need bigger ones.
And then others think, no, there's something really
principled missing.
And I think that's the key to bait.
Another thing they're really bad at
is logic and arithmetic.
Again, like the opposite of what people think.
So I think people who aren't familiar with these things
have to reel off.
They're quite different from their strengths.
They're different from what everybody would have expected.
And their weaknesses are especially different
from what everybody would have expected.
So it's a pretty peculiar thing.
And for me, I'm more interested in the mind than I am
in lots of other things.
I want to know, what is the tells you about the mind?
I can actually put something on top of this.
So one of the issues I see, the us
looking at all these amazing language generators,
like the GPTs and whatnot, is the fact
that they are doing something amazing, which is very obvious,
because we can just see them doing amazing stuff.
But we actually don't know exactly how those amazing things
are happening.
So that just pointed out the creativity, which is amazing.
So these things are actually creating something new
that it has not seen before during training time.
And then in the term of the machine learning,
there's called generalization.
So how can these models be able to do something
that it was not trained on explicitly?
And then how it happens in the statistical sense
is that it does all those counting of all the parent
CC during training.
And then what it does is that because it
has a limited capacity, it needs
to compress all the things that it has seen.
And then while doing so, it loses some information.
But the information loss is information gained
on the things that it has not seen before.
And what are these new patterns that this compression
mechanism or the training algorithm
are actually focusing on that we find really amazing.
So the process of, let's say, generalization
or the creativity by compression
is a complete mystery at the moment.
A lot of people in a lot of theoreticians
actually do work on it.
From the perspective of, well, is there
some kind of implicit regularization happening?
That is, your regular is how learning happens.
And thereby increasing these models
to do something that it has never seen before.
And how does that actually connect
to the amazing nature of the generalization
of the creativity that we see?
And then this lack of our understanding
of these very simple fundamental things.
Essentially, we're saying that they will,
these models are counting and compressing.
And somehow magically, thereby, it
does generalize to a completely unseen or the new things
that look amazing to us.
Well, what is this?
It's a very simple thing, right?
The question is out there.
But we have absolutely no idea what
is the even right way to approach answering the question.
Now, let's stop for a second about the word creativity.
I mean, there's so many different branches
to look into and will hopefully do that.
It's amazing.
I want to stop about the word creativity,
because let's stop and imagine that.
I would prefer we use creativity as demonstrated
by these as creativity with an asterisk.
And not assume right off the bat that it's creativity.
It's the same property that we have.
We instantiate when we're being creative.
Is it just a very advanced way of being a dancing bear?
Are these very, very clever things that do wow us
because I didn't think a computer could write something
inventive like that?
And how deeply does it go?
And as you were saying that in some ways,
what does it mean about the mind and creativity in humans?
So does anyone want to take a little whack at that?
I think it's helpful to zoom in for a second
on the training process itself.
So we talked about how it scans this text.
It's actually a very simple process
that it's undertaking while it does this.
It doesn't read the text.
But as it processes the text, the computer algorithm
just masks or it sort of hides random words.
And it asks the neural network to try
to predict the missing words.
And that's the whole process.
It just reads along.
So imagine I'm talking to you and I say,
my dog likes to blink.
Everyone in this room, I'm sure it's on your head.
You heard sort of an auto completion.
Maybe my dog likes to play, my dog likes to walk.
And that's all we're asking the computer to do.
We give it some text, it reads some text.
And then it tries to predict the next word,
the missing words.
And as it does that, it just develops this process
of being able to predict the next word.
And I think, talking into some of the earlier things
as far as how it's surprisingly bad
at things like arithmetic and logic,
when you think about that process, of course it is.
It'll do well at things like what is,
you can prompt it and ask it what's two plus three.
Well because it's seen that in training text
so it can predict the answer is five.
But if you give it more complicated numbers
than it hasn't seen, it's a little bit, I think,
like some children learn to spell
by memorizing the spelling of words rather than phonetics.
And this algorithm is very much the non-phonetic version.
It's just memorizing a bunch of correlations and patterns
without developing that understanding.
What's surprising, I think, is that it does have
some hints of understanding that it shouldn't
from such a basic process.
And going back to your question about creativity,
I think one thing that's helpful to think is,
you know, if you have your phone
and you start typing text message in it suggests words,
you can just keep clicking those buttons.
You're basically running something like GPT-3
based on the text that's there.
It'll just sort of randomly predict the next word.
It's very improvisatory, so I think it helps
to think a little bit maybe like jazz,
where by nature it's just kind of rambling
and improvising and making up as it goes,
which actually can make it seem more creative
than a very structured, rigid thing
where it's trying to articulate and express an idea.
It doesn't have the idea, but it can just kind of wing it
and improvise words, which I think gives it
a kind of like local aspect of creativity,
but not the global, there's not a creative idea
that's expressing, but the wordings are creative
for the lack of idea that it has.
Well, this may be a little bit of what you said earlier,
and if you follow it long enough,
it sort of seems to go off the track a little bit, right?
Yeah, well, a complete change persona, et cetera,
but your question suggested that we're amazed
at how creative it is for a machine.
I don't think that's right.
I can't write a poem in the style of Philip Larkin.
I can draw the amazing pictures it draws,
and the other things it does also can be well beyond
what many people could do.
I mean, it explains jokes, for example.
It looks, the economist published a series of little bits
of where they showed the economist covers.
They explained the covers.
I mean, it did a really good job.
I don't remember which system it was,
but it did an amazing job.
At the same time, that same issue,
the economist had a wonderful little article
about Douglas Hofstadter, where it asked questions like,
the last time, when will the next time
that Egypt be transported over the Golden Gate Bridge?
And it gave an answer, assuming that Egypt
could be transported over the Golden Gate Bridge.
So that's the lack of a world.
So creativity, I think it's, I mean, look,
I don't know how to define creativity.
It isn't what we do probably, but it's very, very impressive.
Combined with these other lacks of logic
resitting in a world level.
Can I kind of push back on this word amazing
that we keep using?
Is that, so my work specifically with language generating
is historical, and some of the earliest materials
that I found were Ramon Lully, or Ramon Yui,
was a medieval, medieval, Mayorkan theologian
who created these paper machines
that were prototype language,
simple combinatorial language generators,
where you rotate the circles and combine
all possible truths about God.
And that work and those devices were so amazing,
in a sense, and so kind of unreasonably effective,
that it spawned a number of cults all across Europe
who are Lully and sort of theologians,
Lully and poets that persisted for centuries.
And I think they asked some of the same questions
that we are asking of these somewhat more sophisticated
tools, but the thing is this kind of creativity
that's combinatorial, that's mathematical,
that's statistically driven, has been with us
for a very, very long time.
It's just we tend to kind of forget that history
and then rediscover those devices
and be amazed again and kind of be discomforted again
by their presence in our midst.
But that's where I would a little bit say,
you know, is this a new kind of phenomenon?
Is this something that we're continually struggling with?
I will say we were teaching earlier
from a deep learning that would generate text,
and it worked pretty poorly.
It was somewhat word salad.
It seemed creative, but it didn't really make sense
all the time.
And I still remember the day that GPT-2 came out,
and we started working with students,
and we taught it to write like Oscar Wilde
and like check off and like all these writers.
And do things exactly, as you said,
that my students have trouble with.
I asked them, okay, you read Virginia Woolf,
write a passage like Virginia Woolf, you know,
they can't, right?
And so that's more of a paradox, right?
That it can do things that are very difficult for us,
but it can't do things that are easy, right, for us.
And so people get very confused about it
because they say, oh, it can't do this, right?
And therefore it's dumb, but it's part of that paradox.
And even in terms of counting,
people have found that it counts
like little children count, right,
as they're learning to count.
So if you haven't worked with it,
it can be kind of hard to understand
because you think how can this work?
The other thing that I would say is for decades,
people wanted to teach computers how to process language
and generate language based on rules, right?
This was Chomsky's universal grammar.
And we thought if we just gave it enough rules
and enough edge cases, somehow that would work.
And it really hasn't worked well at all.
And no one really expected that if we just gave it
a massive amount of language,
it would be able to do such a phenomenal job.
So we're all still trying to figure out
what does that mean?
And what does that mean about how language works?
And what does that mean about the nature of meaning?
And what does it mean about the nature of our own mind
and creativity?
So it has a lot to teach us,
but it doesn't fit neatly into human categories.
And that's kind of what we're experimenting with right now
to try to figure out how does it work?
And what does this mean?
In my view also, I mean, connecting also to what was said earlier,
many of these behaviors that we see as amazing,
historically, you say that this happened many times,
are a bit cherry-picked.
Because then you generate a lot of different texts
from prompt, and some of them are amazing,
and some of them are amazing in the negative and wrong way,
because it's completely out of track.
So we have to be aware that there is no real reliability
there.
So there is a problem with reliability.
And then there is also a problem connected,
going back to creativity, is how do we
want to use these systems to replace human creativity,
or to augment and support and expand human creativity?
Like one of my recent talks I did this PowerPoint slides,
all the pictures from PowerPoint slides
were generated with Dalib, which is not a text generator,
but an image generator from a text while description of a scene.
So all my images were generated using this algorithm,
and they were beautiful.
And I was, oh, my god, you know, a lot.
But then at the end, I said, OK, wait a minute.
I didn't use any graphic designer.
I didn't pay any copyright, because you
own the images that you generate with Dalib.
So what does that mean if everybody would do like that?
Then graphic designer would be out of a job.
That would be possible consequences.
And that's maybe economically in their business model
is going to be a damage for them.
But most importantly, if everybody would do like that,
what would happen to the creative process, not the outcome,
but the process of creation that society needs to have.
And people need to have, if you have a society where
nobody follows the creative process anymore,
then what kind of society is that going to become?
So really, the question about how we
want to use these new techniques and within our society.
So we have to be more conscious about, besides being
excited about the novelty of the thing, which maybe is not
really a novelty, as you said, but also more conscious
about how we want them to be embedded in our job
or in our interaction with each other,
in our creative process and so on.
So what is the role of this technological progress,
for example, by these techniques, in the progress of society?
So let me also give you another historical anecdote
that may answer a little bit.
We can look back and see how did we deal with this before.
So in my book that's upcoming, I talk
a lot about the previous flourishing of text generators,
which may surprise you was late 19th, early 20th century,
which were exactly the template-based generators,
which work like this mad libs, something blank,
fill in the likely bent blank, and there were devices
and charts and tables that were used on a massive scale
to generate movie plots, to generate theater plays.
Today, when you're looking at the Netflix show,
it's the practice of using template generators
of particular lineage that go back to the plot genie.
And these were like best-selling writer-aids books
that are completely integrated in contemporary writing,
creative writing programs.
And what they have contributed to the mass production of art.
So if we think about the flourishing of art
in the 20th century and the massive popular art,
it's kind of completely integrated
the particular form of writing.
When you see it early on, people are actually saying,
OK, I need to write more.
I need to sell more little, like,
pulpy plots to the magazines.
I need to sell more pulpy plots to the Hollywood studio.
And we are living in a kind of cultural environment
where this algorithmic combinatorial template-based,
a little bit maybe statistical to a lesser extent,
but it's very well integrated to our processes.
But to the extent that we don't normally see it, right,
when you see that those credits roll
at the end of a Netflix show, you're not thinking, like,
oh, wait a second.
They didn't actually invent this.
Did they use a particular writer's aid that
helped them in some of the unfair and creative ways?
But there are complaints that a plot seems formulaic,
or it seems you could people can perceive that from time
and time.
Like this plot seems to have been written by a computer,
for example.
I want to follow, again, along this idea
of what sorts of creativity are these different sort
of properties or categories.
Amazement, and it has been mentioned a lot.
So when you look at art, visual art,
and maybe it's easier in visual art to come up with this,
we have a history of being amazed by certain masters of art.
So Da Vinci draws something, and it looks amazing.
Part of our amazement is how did he do that?
What was the manual, the dexterity,
and whatever else goes into it?
I'm not equipped to say all the things that go into it,
but I do know that the human factor is absolutely
a part of what amazes me and most of us.
Now when a computer generates a Da Vinci-like sketch,
it's not doing the same thing.
And we might say, oh, it's amazing,
because, wow, what an incredible facsimile,
but it didn't do the steps that amazed us historically.
What historically accounted for our amazement and seeing art.
So that, I think, expresses the difference between
sort of formulaic and algorithmic generated creativity,
and the creativity that we're accustomed to,
and is that something that's going to be bridged some point?
Let me push back a bit on the aesthetic.
Yes, some of this is a very basic principle.
On top of which, we have built these large-scale language
models, have been here for centuries.
And in particular, if you read the book from 1950
by Paul Shetter, in fact, we actually do see the perplexity,
as well as the low probability, and how we want to actually
maximize the low probability of the correct following words.
Given all the context, in order to infect model,
the distribution is all written there.
Yes, that's true.
But that doesn't necessarily imply that we are only relying
on them in order to build these large-scale language models,
to build them.
And in fact, I guess, one of those people
who actually built a very large one was in the first place.
And it takes a lot more so than simple principle
of counting and compression.
Unfortunately, it's not that simple.
There are a lot of ways in which we can parametrize.
So the idea why everyone is going crazy about transformer,
which was only proposed about five years ago,
is because it took us half a century to get to that point.
In order to come up with all those simple,
but our algorithmic techniques and tricks
that we had to develop.
And then all these optimization algorithms that we use.
Yes, we can go all the way back to, again, 52 or so
to talk about the stochastic gradient descent and so on.
But to figure out the world or right way
to do the optimization took us another, let's say, half a century.
So what that means is that what is amazing is not the fact
that, OK, these are template-based as a generators that
have done amazing compression of the amazing amount of data
that looks to do something amazing.
But how we were able to actually build this.
So it's actually not that different from, let's say,
being amazed by, let's say, all those monsters.
It is doing something really, really amazing.
Inside that we don't know, understand how it's doing it.
Probably not in the way that human monsters are doing it.
But still, it is doing something that we didn't know
and that we still don't know exactly how it's doing.
So I think we're supposed to be amazed by this.
Now, of course, amazed.
And we have to try to figure out how not to be amazed as well.
Yes, and that's, I guess, our job.
Yes.
Well, connected to this, I mean, I think that the fact
that this is kind of a black box, that we don't know how
it generates this output from the prompt.
Also, leads us to computer science,
coding, programming, computer science, and AI,
used to be a discipline where you, the researcher,
the software engineer, the programmer,
were having some goal in mind for a machine to do that.
And then you were coding that.
So, and then there were verification, validation techniques,
and so on.
But you knew what the machine was going to do.
So, and then you would check that it would do it,
and you didn't put any bug, and so on.
Now, instead, so that was the computer science,
exact science kind of approach.
Now it's becoming, with data driven approaches,
more like a natural science.
So, you build this thing, but you don't know how it works,
and then you start experimenting and testing,
you develop some hypothesis, and then you test
whether the hypothesis is true or not.
So, it becomes closer to being a natural science,
rather than a computer science, right?
So, it's now merging these two approaches
that were typical of these two kinds of sciences,
disciplines, because of the nature of these machines
that we build, but we don't know how they work.
So, then we treat them as we treat the laws of physics,
that we try to find some properties of what we build,
because we don't know how they're going to behave.
I think that's it.
Let's go to another.
Okay, what we don't know?
Oh, I don't know.
Why?
Why don't we know?
So, I tried to answer the question myself every day.
So, yeah, but the reason why we don't know
how these things work is almost by construction.
And you've just pointed out,
so there are problems, small subset of problems
that we know how to solve by specifying what we want to do.
And then that's what the traditional computer science
has focused on over the past century, right?
And then there is a slightly bigger subset of the problems
for which we know the nature of the evolution
has figured out how to solve the problem.
So, yeah, for instance, us driving, right?
So, any person you take, give them 20 hours
of the driving lesson, we know that person
will be able to drive in the New York City without a nation.
So, there's a subset of the problems
where we know solution exists,
but we don't know how to implement the solution,
or implement the solution.
And then there's a subset of the bigger subset
of the problems where we don't yet know
whether nature or evolution or the universe
has figured out an answer to,
but then we want to solve those problems
because if you solve those problems,
it's going to be very, very practical and whatnot.
And then of course, there's a bigger set of problems
where maybe there's not even a solution to start with.
But then the machine learning, or this kind of,
this AEA, is there to solve the slightly larger subset
of the problems that we were not supposed to know
how to answer, but we know that the solution exists,
and we are going to build an algorithm
to come up with a solution to that one.
So, in a sense, if you knew how these solutions
that were given out by this machine learning
or the AEA algorithms were working,
then we probably would not have needed AEA
to be learning the solution.
So, let me give you some oversimplified view
of my answer to your question.
So, when you know how to solve a problem,
you can define an algorithm,
which just means a sequence of steps,
so you do this, and then you do this,
like a recipe, you take the ingredients,
then you mix the eggs and whatever,
then you do this, and then you get what you wanted as a result.
So, if you know how to solve a problem,
in that way, you could these steps into,
with whatever programming language you have,
you put this into a machine,
and the machine will follow the steps.
So, then you don't have a problem
of not knowing how the machine goes to the result,
because the machine followed exactly the steps
that you told them to get to the result.
But when you don't have these algorithms,
very clearly algorithm to find a result of a solution
to a problem, like in recognizing a face of a person,
or driving, or whatever,
you don't have this very nice recipe,
because there are so many variables to consider,
so many and so much uncertainty,
so what you do, you just give a lot of examples
of problems and solutions, problems and solutions.
And then let the machine learn from these examples,
by looking at the problem, finding something in output,
and if the output is different from the solution
that you gave, just modifying a little bit of parameters,
so that the distance between the two is smaller,
and then with the next problem, next problem is for it.
So, at the end, you get the machine that,
by testing it, it behaves rather well
in that problem of finding a solution,
but you don't know, you cannot see,
there is no sequence of steps,
that tells you what the machine is doing exactly
how to solve the problem,
because there is no sequence of that,
as he was saying, if there were a sequence of steps
to solve the problem, you would use it,
and you would not use these other algorithms,
but to recognize a face, for example,
there is no one sequence of steps that you can use,
so you have to use these other approach,
but then you're not sure exactly what comes out,
why it comes out.
Let me get an illustration of how little we understand.
So, all the big, famous successes have been driven
by a machine architecture called a transformer architecture.
However, so I think initially people thought,
well, look at this architecture,
that's really responsible for all this stuff,
but now it looks like people are moving
towards getting similar successes,
using other architectures,
so it's not even clear that it is the transformer architecture
that is really responsible for the big successes.
So, I think what you said about natural science,
it is much more like a natural science,
now you can build a successful model,
but understanding what they're doing
is like we're examining it like we examine
the nature of atoms or something.
So, it's a real puzzle.
Well, we don't know how we ride a bike,
I mean, in the sense in which we don't understand computers
and how they figure out problems,
we don't know how we ride a bike,
I mean, we train ourselves.
Careful with that, because there are things
that sound similar where there is a real simple algorithm,
like catching a fly ball.
Right, I was gonna say that.
There's a stateable algorithm,
it seemed mysterious like riding a bike,
it turns out there was a stateable algorithm.
But following along with Francesca's idea,
it required natural science investigations
to come up with that solution,
because the person on the street who rides a bike,
or the outfielder who catches a pop fly,
can't tell you, well, this is how my brain figures it out,
right, it's a scientific problem to look into, right?
So, another example like that is chicken sexting.
So, for many years, that was, you know, so, you.
But that was illegal.
But you used to have these, it was a school in Chicago,
actually called the Chicago School of Chicken Sexting.
And the problem was you had these baby chicks,
but you didn't know that they were hens,
or gonna grow up to be hens or roosters.
So, for commercial purposes, you need to separate them.
And for years, there were these trained people who did,
and nobody knew how they did.
And somebody did, you know,
quite investigated a little more,
and they figured out exactly what they were doing.
And now they have a very simple method to figure it out.
So, you can have a lot of things
that people can do through training,
and you don't understand them.
And then if you investigate, sometimes you understand them.
And that may well happen with these machines.
We, you know, if we get many different architectures
that can do the same thing, look, it could turn out
that the fundamental basis is just prediction.
That, you know, people didn't used to think
that the way the mind worked was so heavily reliant
on prediction, and, you know, for some years now,
there's people who've studied perception,
not cognition, but perception,
have been focusing on prediction.
Maybe that's the key here,
is that however you can do the prediction,
that's what makes the thing run.
If I can just add to that,
so what we've been doing in the lab for a number of years
is precisely this kind of experiment,
and it is extremely tricky,
because it is sampling from this distribution,
a probability, but you can actually tune it
to get more surprise or less surprise.
It doesn't work like a Google search,
so many times when our students start to work with it,
they think it's gonna work just like using Google search.
In fact, the interface is very different,
because it's gonna continue with whatever you give it.
If you give it very general language,
it won't give you anything back except generality.
So you have to give it the texture,
you have to think about language in terms of complexity
in order to get it to give you that complexity back.
And so, and then you have to think,
if you train it to write like a writer,
how long do I train it?
You can over train it, you can under train it.
So you're in this entire search space,
and then there's a question of how many times do you try it?
You try it a hundred times,
and how do you figure that out?
So it's actually extremely difficult even to experiment with it.
And just to give you an example,
we created this diva bot, an AI that the improv,
and we worked with an actress in LA.
And she had a very difficult time doing improv
in the typical way,
which is that you give a kind of general response
to your fellow improv,
improvisers so that they can do something cool with it.
Well, if you give a general prompt to the AI,
it gives a general prompt back.
She had a better time giving very specific things,
like she was comforting a crying baby,
and I hope I can say this on YouTube.
She says, how do I start my crying baby?
Well, the diva bot, the DPT2,
which is a fairly small model, said, where are condom?
Wow.
Is that created?
Oh.
You may have to delete that.
I apologize.
Oh, well.
No way.
It was too good.
Well, that's the computer to explain why that was so funny.
Well, they can do that together.
But you know, if we can go back to the question of not knowing,
and kind of, I feel like we were talking about
slightly different forms of not knowing.
So there is not knowing, there's a part inside of the process,
and there's a part which we can't quite explain.
And that doesn't seem to me that unusual.
So there's plenty of engineering solutions
where we sort of know that they work and we design them.
But like, if you, so for example, like slash memory.
Flash memory works by quantum tunneling.
If you ask somebody like, show me exactly how did the
electronic penetrate the blavo, you can't.
It's unpredictable.
But we know it's like a very predictable process.
Like we're comfortable with it.
It doesn't seem to be that.
But there is a part of it, part of the chain of explanation,
which is missing.
And it's, we're okay with that.
So that's one type of a knowing.
The other type of a knowing, I think very important
that you mentioned is we don't know the political,
social, cultural effects.
So we don't know what that technology will do to us.
And I think you're very, very, very,
it's very important to kind of like experiment and think
and reflect about what the effects will be.
You know, when I looked at, I was reading the papers
of Licklider.
So remember Licklider was a Palo Alto research lab.
They did the joystick and they did a lot of early,
also early, some of the first ward processors
were developed in this lab.
And so they developed, so for example, I remember,
they developed a copy and paste function.
And they're like, our mind is blown.
You can take a whole chunk of text
and like move it to another place.
But what I loved, what they did, and which we don't do is
they said, let's experiment with this
and have a diary of what that does
to our process of creativity.
Because we don't know, we don't know what this weird
copy and basting will do to our process of writing.
And then there's like this purposeful,
let's integrate this into our research.
Let's treat it like a natural.
It's a system that has complicated
and unpredictable social cognitive effects on writing.
And so they're like, let's have,
there are these cognitive diaries of like,
here is how my writing changed,
because I'm able to very fluently take a piece of text
and move it to a different place.
I can cut it apart, I can separate it.
So that's, and I'm like, I love it.
And we should do more of that.
We should kind of have a lab
which does the social, cultural,
and political experimentation with this new systems.
Well that raises the initiative
we really haven't talked about,
which is, are there gonna be bad effects of these systems?
And can we think about that or think about what to do?
But I mean, the thing that worries me the most is that,
you know, with a huge success of OpenAI,
every big tech company is making humongous machines,
you know, so GPD 3,
$175 billion in parameters and the new Google one
is $500 million GPT 3,
I'm sure GPT 4 will have sure be even bigger.
And the thing is, the bigger they get,
the more hidden are the failures.
So they'll be able to, you know,
so they won't, the failures won't be just,
so much they're out in the open,
but with all the money invested in them,
there's gonna be a lot of pressure to use them.
And people are experimenting with a pixel of inputs
and robotic outputs now.
And I think with all the so much money in this stuff now,
there's gonna be, you know, robots with eyes
and, you know, can do things in the world.
And they're gonna be powered by these machines
that in some fundamental way are unreliable.
I think that's the term that Francesca used.
They don't know, they don't have a world model,
they don't know what the facts are,
and they can't count very well,
they can't do, you know, I mean,
I'm sure that the GPT 4 and GPT 5
will be really good in small rhythmic problems,
but maybe there'll be some other rhythmic problems
where it'll completely fail.
And those things will be used commercially.
Yeah, and it's not just a matter of being correct
or failing, it's also that these models,
that they don't know exactly what it means
to fail and be correct.
But it's also that these models are trained in this way.
They don't have really a clue of the values
that we want to have in our technology
when we allow the technology to make decisions
or to recommend decisions to a human being.
Also, they don't know about fairness,
so it has been shown that in many examples,
these models are biased,
so they make, they say things,
they write things in a way that shows that they are biased.
And that's because there was really no curation
of the data that they were trained on.
And it's not possible to have a curation of that data.
Maybe it will be possible by using these two steps approach,
like first you'd build something like GPT 3,
and then you found a unit for a specific task,
and then you try to do the best you can
to do biased mitigation in that second step.
But in any way, you have to find a way
to make sure that they do things or write things
in a way that embeds our values.
Otherwise, they got, not only they tell you wrong things,
but they tell you things,
even when the problem is not right or wrong,
but they tell you things in a way that is,
I mean, the GPT 3 has been tested in a,
in a tested suicide hotline,
and somebody interacted saying that he wanted to,
he was thinking about committing suicide and GPT 3,
I don't know if it was GPT 3,
but the language model responded, said, you should.
So we have to be careful about deploying them.
First, we have to understand how to embed our values.
And I have this feeling that to embed values
in these models, you cannot just do it with more data,
more computing power, and only a data driven approach,
but you have to combine data driven approaches
with rule-based approaches.
Because bottom up and the top down approach,
you cannot, I mean, you cannot, it's a big word.
I'm saying, I don't see that emerging for now
from a data driven approach only.
I can push back on that, because especially on the idea
how we should, how we can now let you solve these problems
from the using the data driven approach.
In a sense that the, the set of approaches that we have used
so far in order to build this large-scale language model
is extremely, extremely narrow,
in particular, we've viewed the entire field of machine
learning more artificial intelligence.
So let me just call it, let's say, more of a passive,
let's say machine learning where the data was somewhat
collected with purpose that may not necessarily align well
with the purpose of how we use data.
And then we try our best with this statistical approach
in order to say, how better, compress better
so that the work is going to generalize better.
However, that's just a one very narrow sense
of machine learning.
In fact, you have another side that I would call
as, let's say, active machine learning
is where we introduce the assumption that the systems
are going to actually interact with the other systems
or even with the environment.
And then within, of course, we can either make them
actually interact with the environments
or we can also say that the, well, here's a set of data
that was collected based on the assumption
that there were some kind of interaction
that is going to happen in the future.
Then you can now start using all those offline reinforcement
learning algorithms, active learning algorithms,
and even some of the notions from causality
and things that are beyond the simple, let's say,
statistical approaches coming.
And then my sense is that it may not have to be role-based
but something that is beyond.
But already there are a lot of, let's say,
candid algorithms as well as these sub disciplines
of the machine learning that are being studied.
And then have been studied for many years.
OK, historical is saying, the Alan Turing already wrote
a lot about the machine intelligence.
And one of the things that Alan Turing did mention
and emphasized back then in the lesson in his writing
for this is the necessity of the reinforcement.
So the system interacting with the users or the aline farmers
and then get the signal that actually tells the system
about what it observes is a line well
with what is supposed to do.
So I almost feel like it's not really
about the overall data-driven approach
but more like the narrow subset of the algorithms
that we have used so far.
So why is the limitation of that?
And is it possible that we already have some solutions
to many of the issues that have been raised
with the current generation of language models?
Maybe GPT-4 is going to be trained with something else
or Fiverr before I heard that they already trained this
on everyone.
I like what you're saying.
And one of the reasons why I teach humanists
and social scientists and artists all about AI
is because I want all of us to have a seat at the table
and discuss these things.
But one of the most interesting things to me so far
is I ask students to come up with these rules.
And they're very uncomfortable and most do not.
And they find that their ethics and their value system
cannot be put into rules.
And so we're going to have to deal with this in an interesting
and then the question is whose rules and who decides.
And can we even decide amongst ourselves
if we agree on what these rules are?
So I agree with you that we want to have a human-centered AI
but I'm not sure it's as easy as just coming up with rules.
Isn't that the question to all technology?
And what you're saying and what you've mentioned
is embedded values are embedded.
OK, what are the embedded values in the automobile?
Look at the effects of roads and cars.
So it's interesting how we're in this moment.
I think because it's called artificial intelligence,
we expect more from you.
But other kind of fantastic technologies like driving,
they seem banal and mundane.
But yet they have, they like reformed the world
from the politics of energy to the way our cities are structured.
And also not, I can't say that I court to my values.
Maybe it's easier to commute but then at the same time
there's pollution.
And again, it's the problem where the value was not necessarily
embedded in engineering itself.
Because the engineering often has a specific problem.
Whereas the political and the social impact
has, it's immersed in the complexity of human existence,
which is not rule-based and it's not kind of resolved.
I'm glad you raised the point about just using
the phrase artificial intelligence makes us view it
one way.
One thought experiment I've found that kind of helps for me
at least is everything GPT-3 in these models know, of course,
is things that picked up from human data.
It's not like a chess program where an AI plays an AI.
This is a computer learning entirely from human data.
So one way to think about it, if everything it
knows is from humans, it's really a vehicle
for providing access to a human to our sort of amass
of human knowledge.
But we have other technologies that do that,
like Google Search, even Wikipedia.
Wikipedia is a distillation of collective human knowledge
that's accessible to an individual.
So somehow, I know that Google Search actually
does use neural networks.
But no one thinks of Search and Google
as talking to an artificial intelligence.
And certainly no one thinks of Wikipedia
as an artificial intelligence.
But somehow thinking of all of these things
as ways of taking the mass of human knowledge
and projecting it to an individual,
DPT-3 is a kind of more stochastic version of that.
But at the end of the day, somehow that helps me not think
of it as an AI and just think of it as a distillation tool.
But it also explains why working with it
can be so eerie, because you do feel
like you have access to this kind of collective mind
of humanity in a really interesting way.
But Google Search, I could say the same.
That's true.
You get all the toxicity, the misinformation, the mystery.
I mean, Google Search is amazing, because it's this bizarre
window into human creativity.
I do think there's also a kind of an existential question
that I see with a lot of my students who want to be artists.
They want to be writers.
And they move through these stages of grief
almost in working with this, right?
And we thought that robotics would be further along.
I would love a robot to clean my toilet
and make my scrambled eggs.
But in fact, that has proven very difficult.
And what we found is that disembodied AI
has done and now can do all these intellectual tasks,
creative tasks.
So I think what is most unnerving
that we have to somehow come to terms with
is that we thought we would have the Jetsons with the robots
first.
And instead, many of us whose entire way of being
is based on kind of intellectual work
are having to come to terms with the fact
that we've actually succeeded there first.
Well, I can think of a new rule.
All robots must wash their hands before leaving the path.
I wanted to say we all use rules when we speak to each other.
So language already has rules embedded in it.
But one of the things that hasn't come up so far
is that in the history of human discourse,
people make promises and vowels.
And they swear oaths and things that those are all basically
language-based behaviors that we don't see or hear so much
about.
And maybe it's happening.
I just don't know about it.
But I think that's such an important aspect
of human language generation that we make promises.
Well, one of the points that people often make
about the large language models is they have no goals.
So you can try giving them a goal.
You can say you can use text to tell them their purpose.
But then that's just more text that they will then
follow with more predictions.
So it's another way in which they're really fundamentally
unlike human, whatever kind of mind they have.
It's fundamentally unlike a human mind.
I have a question kind of about you.
Because I'm looking at the prompt that we were given.
And there's nothing in there that has the words intelligence
or mind.
And it's interesting to me that throughout those words,
immediately entered our conversation.
And in fact, in the AI research program,
from the very beginning, language and intelligence
and mind, the language is kind of one of the most marked
feature of intelligence.
But we talked a little bit about vision.
I mean, there's a whole bunch of other.
So I feel like there's a cognitive linguistic slide
that we are engaging in where we're
beginning to speak about compelling language generators.
But right away, we're saying, OK, is there a mind there?
Or is this intelligent?
Whereas, again, I don't have the same question
to a calculator or to a more complicated statistical
model that whatever predicts the weather.
I don't go like, oh, does it actually feel the weather?
Or is it intelligent in that way?
So there's something, I mean, I guess it's a question to everybody.
Why, what world is the, how is the connection between language
and mind and intelligence?
And why are we naturally sliding immediately
from language and text into intelligence and mind?
So I think there are two questions that
have been very much prevalent in the literature
on this stuff that have not come up up here.
And I think there's a good reason for it.
One of them is that.
Is it really intelligent?
I don't myself think that there are useful questions
because it's such a vague notion.
But here's the other one, which is kind of interesting,
which is a lot of the discussion has
stemmed from a paper by Dendrick Kohler, something
they call the octopus test.
How can they call these machines?
Decastic parrots.
So just trade on words.
And it out comes more words.
So you're never getting to the world.
And I think for good reason that hasn't come up here
because it's really completely irrelevant.
What we've, the issue of can something
trained on more words be in some reasonable sense
intelligent, creative, solve problems, do things.
It's a kind of non-issue, really.
So I think there's a good reason why that hasn't come up here.
But it has dominated a lot of literature.
Well, but does it have the failings
that you mentioned at the very outset,
the sort of glaring failings where it seems computers
don't have a worldview?
Isn't that an example of how intelligence would,
some definition of intelligence would perhaps
address that problem?
Well, I don't think you can solve it by definition.
There are certain, maybe we could use a neutral term,
intellectual capacities, that they do extremely well,
better than us, and others where they don't.
So I think we have really focused on that issue here,
and also try to understand what it is they are doing.
They do seem to know things a little, I mean,
you mentioned that as well.
And that's part of when we do experiments,
we're trying to figure out what does it know.
Now, do I mean no in a human way?
Absolutely not.
When you see it fail, those are clues.
So GPT-2, the much smaller model,
we had a student train it to write MTV Darya episodes.
It seemed to know the characters,
it seemed to know the kind of plots and the ways
that people interacted.
But it would make really bizarre goose,
a person would pick up the phone
and then pick up the phone later in the scene.
Somebody stirred lasagna.
You know, and you go, oh, I should start this on you.
It's ridiculous.
But then, right?
I do the fact that you notice,
means most of the time, it does actually know.
And so that's the fascinating thing.
And we had a creative writer give it a prompt
about an inky black sea.
It immediately knew that these people were probably
on a boat, that they might be fishing,
that when they pull up a net, usually what it has in it,
it's unusual to have a body, which it did.
It knows that the ocean is not native ink, right?
So it does seem to know stuff,
and that's what we're experimenting with.
When people get upset, because it's not knowing like a human,
but we're still trying to figure out what it knows
and what it doesn't know.
Coming back to the steering, the lasagna,
the dark never made the lasagna myself.
I'm really told that.
But you never know.
Maybe on the web, but there was somebody
still in La Jolla.
You never know what people can do to this.
But people actually make an argument.
And in fact, this is argument that quite a few people
have been making over the past few years,
and just two days ago, David Sharma said,
well, you actually made the same argument,
is that if the system had a word model,
we would expect it not to say lasagna,
or someone says that the idea was steering something, right?
But then another way to think about it
is that perhaps that actually tells us
that if the model had a perfect predictive model,
then you had the probability of side
to lasagna in that context.
It would be zero.
And then in that case, can we actually
say the other way around saying that, well, look
at this amazing predictive model it has.
It probably implies that it has the word knowledge,
or the word model in it already.
So then perhaps by simply making prediction better and better,
or the model's predictive capability better and better,
maybe it's going to automatically,
we'll have to come up with a word model that's
going to reflect how word works and how you think and so on.
So yeah, is it really?
No, I agree.
That's why when I say you cannot, I mean,
you cannot say this approach cannot build a word model.
Maybe by building a better prediction model,
then that would build not an explicit word model
that you can see.
But then as a result, the result would
be as if it had an explicit word model.
But yeah, but I don't know.
I don't know.
Is the big difference?
We can make explicit, we can formulate explicit ideas.
Right.
Keep a record of explicit facts.
And that's not what these machines do.
And I think the question is, can the kind of compression
training that they get give that effect?
I mean, I'm betting on no myself.
I'm betting that there is something
about being able to be explicit.
Yes, but of course, one could say, I mean, David's advocate,
that if one opens my skull and looks inside,
it doesn't find any explicit things there,
an explicit rule or logic rules or whatever.
But then I verbalize my explicit model in some way that
says, OK, I know about these rules and these and that.
But you don't find the rules by looking inside.
So one could say, this is similar to these huge machines
with a huge number of parameters.
What you see if you look inside is just
these billions parameters and the values of these parameters.
Then don't tell you anything about this machine having
a model or not.
But then by generating the output,
maybe you realize that this machine has a word more.
So in some sense, I see that this doesn't
rule out having a word model even without an explicit
characterization of the word model of the rules that are learned.
Well, we just had this example of lasagna stirring here.
And I think it's interesting the way in which humans sort
of curate their own information in a weird way,
because it may depend on who the expert is that's teaching them
about it.
So you had this wonderful reaction.
I hate to sound like I'm biased by the Italians
and what they think about things like lasagna.
But when you go, fuck them, we're not going to eat us
to lasagna.
No, I'm saying.
If I was much more weight to me, then
I'm sure there must be somebody in the world that still
is a lasagna.
I'm sure of that.
No, exactly.
Maybe in this country.
No, so if your parents says something to you
or someone who you know somehow as a human,
this person's advice here means a lot.
I'm going to pay attention to it.
There's other person's advice that you learn these rules
in a very different way, because of this sort
of emotional balance and understanding the world view
about who the expert may be.
What's interesting to me is, again,
we are the kind of problems that are being identified
in this conversation.
They're kind of a soon like a totalizing intellect.
And then like, oh, these are the things
that are missing to get to this notion of the perfect intelligence.
And again, there's something about language,
because the language pulls in all of the world.
And then so we expect it kind of to do better.
And we see this kind of, but they're universal failings.
We don't expect the same kind of sort of the same kind of,
what should I say, not hubris, but the same performance
from other robots.
So there are amazing robots that build machines.
And they're using complex statistical vision techniques.
We never go like, oh, why doesn't it have a world model
of lasagna?
Why doesn't the robot in whatever, Tesla, a factory?
And my question would be, why?
Why is it so?
But as soon as we start talking about language generators,
which are often built with the principle also,
it's a machine that's built with a particular purpose,
we right away want to say, does it have emotion?
Does it have intelligence?
Why doesn't it understand lasagna?
In a way that we don't demand about the machines.
But by the way, also with people,
like I was raised in Italy.
So in a very biased way about what you should do with lasagna,
for example.
So in some sense, I built during several years,
I built like a model of what you should do with that object
and what is appropriate and not appropriate to do.
I was not raised or trained with data coming
from all over the world.
Maybe if I were trained like that,
or if I grew up with data coming from all over the world,
for me it would be equally good to steer
or to not steer a lasagna.
Maybe, you know?
So in some sense, we are expecting from an object
that is framed with equally important data
from all over the world, all the different cultures,
all the different regions.
And maybe there is a lot of contradicting evidence
of what you should do with an object.
And then of course, I as a person,
I would say, well, you could steer or not steer a lasagna.
If I was raised with experiences from all over the world,
which I did not, you know, I was just raised
with experiences from one region of the world.
So in some sense, it's not surprising
that there are contradicting in pieces of information
that are collected by and are helping training these machines.
And then the machine speeds out pieces
that are maybe consistent with the sub path
of its information that it was trained on,
and not the other one.
Well, and to add to what you're saying,
we have creative writers who are trying to get it
to be creative.
So we are tuning the hyperparameters to get creativity,
and then we get stirred lasagna.
So it could be that that stirred lasagna is sampling
from the surprise, right, as well.
I'm going to try it.
So I'm going to push back on Dennis a little bit.
So you're sort of blaming the human saying,
why do we keep asking for more and expecting more
from GPT-3 than other things?
And I want to say it's own fault.
It's the computer, not the humans.
It's because it bullshits so much, right?
If you have a robot that's designed to assemble a car,
that's what it does.
But GPT-3 makes up things about everything,
and it acts like it knows so much, and it just BS's.
So yeah, there's some responsibility
of human that we look for more, but it's responsible.
It vastly outsteps.
It's not trained for a purpose.
It tries to do everything, and it embarrasses itself.
Isn't it?
Doesn't that make it very human in a way?
But the worst that human.
What about these failures that are so funny?
Now, we're getting a big laugh out
of some of the ways in which the generator comes up
with these gaffes.
Like, is that a clue to anything?
What do you think about creativity?
Yeah.
So there are actual genuine, let's say,
degenerate cases that arise from the current practice
of training these models.
And in fact, there are quite a few people,
including my own lab, where we actually
look into those failures and try to come up with the mathematical
or statistical, let's say, justification
why those failures happen and they have to fix them.
But there are so many of them at the moment.
The point that they were fixing one at a time.
But there is a chance that what we really need
is a new paradigm of how we train these models,
rather than fixing every single degenerate cases at a time.
Because we're just adding a new term to the loss function
every time.
I saw you write in lasagna on your schedule.
First thing I'm doing when I get back.
And just to add to why we are fascinated by language
or the language generator over at the Robos and Monadysopi,
we perceive or interact with the work
that they in many different ways.
We perceive the world by looking at them.
What you're hearing about is sometimes we touch,
we move things.
And the language is actually yet another medium
by which we can interact with the environments.
So by interacting with the other agents
or interacting with the other computers or not.
And then I think that one unique aspect of the language
is that it actually expresses very different spectrum
of the abstract mix.
So let's say what we see is extremely concrete.
We see what is often there.
I mean, there's a bit of hallucination.
Or not, but generally we see what is there.
We hear what is actually being, what is hitting our actual
ill-trauma.
And then we touch things that are linked here.
Again, you know, deep inside all those hallucinations.
The language is where we can actually express all those
as extremely abstract things.
As well as extremely concrete things
in a single-assist and single-phrase.
And I think that that actually makes this medium
a very, very unique and fascinating compared to other
things that we can do.
The other dichotomy we've not touched on
is the word the distinction between sort of syntax
and semantics that hasn't come up at all.
I mean, does anyone want to talk a little bit to that?
Well, that's the octopus test that I mentioned.
Yeah, so look, it depends on what your theory of semantics
is, what it is for the machine to know the meanings
of the terms.
I play, like a view called functional role semantics
or conceptual role semantics, which
says that if the roles of the representations in the machine
are the right roles, then it will understand the words.
And there's a recent paper by Steve Bientodosi
arguing for that view.
And it seems to me that they have substantial elements
of the right conceptual role.
So I think there is some amount of understanding
of the representation of these machines.
But if you forget a little bit about the importance,
of course, the main importance of the contents
of the semantics or what is being written.
But in terms of the syntax, this is really
where the first amazement is.
Is how can these machines without telling them
the rules of syntax, they can write in such a fluent
and eloquent way in a language or even more than one.
That, to me, was my first approach
to language generators, what this one?
Oh my god, it's writing in a very eloquent way.
Then, of course, if you go and look at the semantics,
then you have things to say about the quality of the semantics.
But the syntax is really much more amazing than the semantics,
I think.
You know, the way I think about it
is there is an underlying statistical representation
of language that assigns kind of words and their occurrences
to like a vector space model.
It looks like the stars.
And certain things are likely occur to next to other things.
So when you say, I want to eat, you know, blank,
some things are probable because they're
current in the training corpus.
And some things just rarely or improbable rarely occur.
So now, is that, so when you translate language
into a statistical model, this is where I begin to think,
OK, I don't, is that model sentient?
Does it have semantics?
It is what it is.
It's a particular mathematical model
that represents language in a way.
So I'm actually much more cautious to not go the next step
and to say, you know, to personify it and make a metaphor
and kind of animate it.
To say, like, is it going to do this kind of stuff?
I just want to add one more anecdote from history
is that the paper by Markov and Markov,
the original Markov change generator by Mr. Markov,
which was published in either German or Russian.
And then it made its way in translation, you know,
which had you earlier, you had a great explanation.
It's like, it's a chain, you know, it looks back,
it sees a letter, and then it says, what's the probability?
It was letter by letter.
That paper wasn't pushkin.
It was on generating pushkin's prose,
and he by hand created like a simple letter by letter generator
that produced, and it was also like, it's amazingly effective.
It's the simplest mathematical model.
It produces like very nonsensical pushkin,
but nevertheless, it was effective, right?
It right away got us to like the point where we are,
saying like, wait a second, this thing is producing sentences.
It also has the same kind of problems.
It has difficulty with context obviously,
because it only looks back one letter.
So these are, you know, so I think by trying to not fall
into the same metaphorical, the same kind of language
which we slide into, which is intelligence, mind,
you know, sentience, and just try to like restate what we mean
in other ways, like this is a statistical model.
Do we say, what do we, how do we talk about statistical models?
I think that helps us kind of move past some of the,
I think last night, you know, we saw some beautiful magic tricks.
Like there are some magical tricks here that are in our minds.
There are, it's, it's, I think there are our failings in the way
of incorporating these techniques into our lives.
There's a last comment and we're going to have questions from the audience.
One thing, and that just came to mind,
I haven't thought about this previously, is when you think
of like the historical context of Turing test,
it's kind of an artificial environment, right,
where you blind yourself to the human in the computer.
But one thing that's kind of amazing is, you know,
over the last 20 years, so much of our interaction is
in purely text form, right?
I text friends, I type on social media.
So we're in this world now where we interact with humans
in an entirely text-only way.
So when we see something like GPT-3 that I interact with text-only,
I think that's part of why it feels like it's more human,
or at least we think of those terms and ask about it,
because that's a standard form of interaction now.
We don't, you know, if you look at old school sci-fi,
it's about physical robots, because that's our world.
But now we live in a world of texting,
and GPT-3 is potentially as real as anything else.
It's not, but that's, it's just somehow our world has changed
separate from the AI as well.
Yeah.
Okay, we have some time for some questions from the audience.
Would you like to approach the microphone?
Thank you.
Okay, thanks for this conversation.
It's very interesting.
The prompt, as it's stated in the program here,
says that the program can create language that gives the
impression that it is thinking.
And thinking is the thing that I sort of want to press the
circle to talk a little bit more about.
And it strikes me that when computers first arrived,
we didn't tend to think of them as violating entropy.
You know, you get them to go, and then they break,
and they, you know, they have a lot of,
they require lots of energy and vacuum tubes and all of this.
But now with the rise of like language learning
and the sort of artificial intelligence,
there's this kind of impression that we have that the
computer is somehow like us violating the second law
of thermodynamics, that it's somehow creating
entropy and generating things outside the realm of like,
you know, the heat death of the universe.
I think what I'm basically saying is that,
is the problem with computer thinking basically the same
problem we have in understanding our own thinking?
Like if we don't understand what consciousness is in the
first place, how are we, I mean, it seems like there's a
really quick move to understand the computer is doing this,
because it seems to be doing what we do,
which is make connections, we're creative, we flourish,
we do all these things.
But in the end, are we even actually thinking,
according to that model?
One point, doesn't directly address that,
but kind of tangential is, the prompt also mentioned
something about does being aware of a code kind of effect?
It's realness.
And I hate to say it, but the fact that, okay,
we don't know exactly the black box of machine learning and
deep neural nets and all that, but we do understand neural
networks in the sense that we've designed the algorithms,
we know what a transformer is.
And I hate to say it, but the fact that we know exactly what
algorithm GPT-3 is running, not, you know, the parameters
after it's been trained, but the raw algorithm,
knowing that does take a lot of wind out of the sails,
it takes away a lot of magic.
We, I can't look in a human brain and understand it architecturally
to the same level that I understand a transformer.
So I think part of why it's easier to ascribe consciousness
and thinking and sentience and all these things to organic life
is we know much less.
We don't know everything about neural networks,
but we know so much that it's really hard to believe it's
thinking that it's conscious that it's any of these human type of
things or animal things.
The idea of amazement, which we brought up so much to beginning
also seems to be a fancy way to talk about that would be to say,
oh, we observe things with low entropy that surprise us, right?
And we've already learned how to do that.
And you're right, humans in our human intercourse among ourselves
discourse, I think is a better word, whoops.
That we're exposed to those sort of flashes of low entropy
that our consciousness can create, does that mean that when computers do it,
that's another instance of thinking like humans?
I don't think so, but people may disagree.
So again, we don't really have theories necessarily that make sense
of the data.
And so we are in this experimental phase where we're just, you know,
one of the frustrations of working on it is you just have to give
data point after data point after data point, but we can't necessarily
say what it all means.
You know, we had one student who was a Bernie Sanders supporter,
senior who decided to have GPT-3 write a lullaby by Marx,
and it did a beautiful job.
And then a conversation between Adam Smith and Karl Marx,
and it did a beautiful job.
And then, and one shot, not, you know, five times.
And then, you know, what would Karl Marx say about Bitcoin?
And it said, well, he might say this.
I mean, it was all very, you know, so is that thinking,
is it conscious, of course not?
But it is doing something that we recognize that is difficult
in terms of intellect, you know, and we don't really have a way of
making sense of that with our current theories.
We just have to look at the examples.
Yeah, I think the important thing to note here is that even humans,
so whatever we say and whatever the new knowledge that we seem to create
does not necessarily actually become important knowledge,
but it's always all about looking back, right?
High side is 2020.
So, you know, we do increase entropy.
You know, whenever we say something, almost everything,
it's going to be forgotten, and it's going to be concerned with
when we look better on the tops of your spec.
So, we do increase the entropy.
But general, and then, you know, the discussions are the same thing, right?
So, these models were trained to minimize the entropy based on the data.
And what we know is that the entropy of the trend of the learned distribution
has to be greater than equal to the original entropy.
So, it's always going to increase the entropy.
But then the thing is, it all comes down to distillation, like, process, right?
So, look at all those things.
And then we pick what are important, creative, amazing things,
and they were going to kind of, say, kill them.
And then maybe the more important process that is kind of language generation is.
Great, thank you.
Yes, thank you again for the great panel.
I have a question.
We talk about language, like in the literary sense, like putting words together.
But I would be curious, what if GPT-3 can formalize
and then try to predict mathematical language,
and what I'm trying to get at is, you know about Gedel's theorem, right?
Like the mathematical language of arithmetic is either incomplete
or inconsistent, I would be curious, if a system like GPT-3
can try to do all the combinations that mathematical language can generate
all the possible sentences and find out a contradiction in arithmetic,
which we say could exist, but I don't think anybody has fadmed
what inconsistent c lurks within arithmetic.
I would be curious if GPT-3 can be applied to mathematical language.
Thank you.
So, of course, as you see, some people are actually training these
or skilled language models on the formal language of the mathematical equations and so on.
But I think the one thing that is interesting is we don't even have to go into the incomplete theorem right?
But in mathematics and computer science theory, we have a very well established theory
of the hierarchy of the problems, problems that we can solve based on the complexity,
so the memory of the clearly just a practical complexity.
And the one thing we know is that if it is language models that we build
have a very fixed amount of compute that is assigned to each and every input.
So, for instance, let's say we are trying to solve traveling sales person problem
and we know that it doesn't be a complete problem, and then we know that each GPT-3
or one not has only the quadratic complexity with respect to the size of the implicit graph size.
So, what we know is that unless traveling sales person or the NP-tractal to be P,
unless that happens, we know that there will be instances of the traveling sales person problem
that cannot be solved by this GPT-3.
So, I don't think it's about the training model better and getting more data,
but there are some fundamental computational limitations that are actually being imposed
by our own construction, and how to go beyond that is the kind of, let's say,
research direction that people are looking into and they often call it
and you can very sort of transform or something.
But yes, to just connect to what I said that, yes, these large-head model have been
further trained to be used, for example, for generating code,
which is a special kind of text with some rules because of the coding,
or to generate plans, sequences of actions, or to generate other structural tests,
other forms of structural test.
And so the way that is done, as we mentioned at the beginning,
is that you take this large language model and you further train it for that specific domain,
whether it's code, or plans, or other forms of structural text.
So, not related to the computational complexity thing,
but to say that, yes, these language generators can be used to generate specific
forms of language such as code, plans, and other things.
But it's important to remember, even when you train it to create mathematical language,
it's still statistical, it doesn't know when it's right or wrong.
So no, it's not going to find some contradiction because it doesn't know when it's right or wrong to begin with.
Don't worry about souls, so not known.
What's that? No, we're still those two.
Yes, we do.
But we know when they're right.
We think we know, but again.
But actually, failures in logic is one of the main tells when you're trying to distinguish right now,
which is very surprising because we think of computers as highly logical,
and yet that's precisely what these models fail at.
Thank you all for this amazing talk.
We talked about world models a bit, and I think it might be interesting to take the view
that the successor failure of an algorithm is actually nothing to do with the algorithm,
but rather the human judgment that deploys in a given context, a world model to judge what a computer has done.
And we mentioned about the continuing progress of AI in the future or algorithms,
and I can see actually two vectors, one in which, based on how we deploy these in actual real-world systems,
we are so used to seeing all this quote-unquote PS that we actually lower our judgment function
to say that this is acceptable to us, or we don't have as sophisticated world models
to judge the outputs of AI because we're so used to growing up with them.
So if you have any comments on that point, and the other question I have for all of you is,
has coming to the earlier question on what does it say about mind that we started with,
has it changed for you how you understand yourselves as human beings?
That's a solid question.
On judging the output flow, GBT3 is about 20% or 27% correct on two digit multiplication problems,
you know, 25 times 72.
It's very poor, really.
You know, that's a fine view in centers that's not a particularly sophisticated kind of issue.
So, you know, the failures are severe.
And as we were discussing earlier, I think the key issue is,
is this a matter of different kind of training, bigger models, you know, more training?
I'm sure that some future model will be much better at these problems,
but will there still be, you know, some astonishing failure and some kind of mathematical logical thing?
But, I mean, to answer the question, the answer is yes.
I mean, because you've seen also in this discussion that we always often do these analogies.
And so whenever we try to test or even analyze and discuss this large language generator,
we always, or AI in general, we always think, at least I always think in terms of human beings,
you know, we learn from data, we learn from examples, we learn from rules, we learn,
we abstract from data to rules.
So, and that thinking about how humans do and reason, it's translated in our, for example,
in my work, in my research project, is translated by understanding of how humans do things,
is then translated and tries to be adapted into the AI space.
So, like, I don't know, for example, my recent project is about thinking fast and slow in AI.
So, to take that cognitive theory or how human make decisions by combining the thinking fast
and the thinking slow and see what it would mean inside the machine.
What is the thinking fast? It's just machine.
Is it a driven approach? Is it a thinking fast?
Or does it also generate then emerging thinking slow behavior?
Or you have to add the thinking slow behavior because it doesn't emerge from there?
So, in my job, I always do this analogy between humans and machines or differences
that certainly helped me in recent years to understand better how human minds work as well.
I can also say, when I confront questions like this, I'm reminded of the old distinction
between functional, like, is the proof going to be in the outward representations of intelligence?
Or is it going to be in the inward, some kind of inside structure that, you know,
there's a long philosophical tradition in thinking about it.
But I myself am skeptical about these algorithms telling us anything kind of internally,
kind of any answering existential questions about the mind or God or love or whatever, whatever it is.
But functionally, I think the answer to, you know, what effect, what will these algorithms teach us?
That's not a speculative question.
The question is how will these algorithms will be integrated into our daily practice.
And I think that's a matter of observation, that's a matter of engineering.
One example I'll give you is that I guarantee you, all of you who teach,
our students will be using these algorithms, they're already using these algorithms to write
fairly mediocre, like, C++ B- papers, because it's so easy.
You can right now go to a website, put in a bunch of, like, really good papers,
and produce, like, a somewhat nonsensical, but you'll be like, oh, that's an interesting idea.
I never thought of that, you know, B-. Now, that changes, that changes my, I mean, maybe, hopefully not at Columbia,
but that changes my practice of teaching.
That means when I sign papers, I can no longer view a paper as this, like, special insight into my students' ability to comprehend something.
Because I know now that the student is thinking with the computer in a hybrid way, in a way we've always been doing,
but now the computer is playing more of a part.
So now, and this is, I don't have an answer by the way, now I'm thinking, okay, to be in front of this trade,
can I give them, and I love your, the various experiments your lab is doing, can I give them papers and say,
actually, explicitly write them with GPT in some way, and then show me kind of what is the next student paper format,
what is it going to look like? And I think it's going to be something different post, but because these algorithms are unreasonably effective,
because they're magical and they seem to surprise us in a particular way, that means they will transform our practice of teaching in this example.
I think that's excellent, yes. Well, I hope my own internal language generator chooses the following word.
I think this was a staring conversation, and wonderful one at that.
Oh, we have another question? Yes. Oh, my goodness.
Now, I hope you will not call the wagon and have me sent to the loony bin for what I'm about to say.
I'm a Jungian psychoanalyst, and part of what we do is we try to learn all the mythologies of the world,
which is, of course, impossible, but get trained in mythopoaic approaches, mythopoaic analogous associative approaches.
And the way you're talking about what's in the computers is the same way we approach dreams.
I'm really so struck by this, because a person could have a dream that Egypt was sent over to the Golden Gate Bridge,
and we would look to see what the unconscious associations are.
Is this a person who's putting great value and renewal of life out of an Egyptian system,
but is drawn to commit suicide and thinks about doing it on the Golden Gate Bridge?
So then you look at all the underlying associations.
My fantasy is in some weird way, because you are all trained so well in rational thought that the unconscious
associative mythopoaic level is getting picked up in some way.
And so just think about it, but consider it, which means look at it from the point of view of the stars.
Okay.
Well, thank you again, everyone, and we will be reconvening at 2 p.m. for our talk on the metaphors.
Thanks again.
Thank you.
Thank you.
