I'm Rob Penzer, I'm the Associate Director of the Helix Center.
Welcome.
Before we start with today's program,
I have a few announcements.
Some upcoming round tables.
On Saturday, November 5, we have autism and the mind brain
with Andrew Gerber, psychoanalyst and medical director
of Austin Riggs, Nushin Hajikani, who's
Associate Professor in Radiology and Director
of the Neurolimbic Research Laboratory at the Martinez.
Should be Martinos.
Sorry, that's auto-correct.
I guess there are some interesting neurolimbic research
at Martini centers also.
But it's the Martino Center at Mass General.
Craig Neuschafer, who's the Professor of Epidemiology
and Biostatistics and Founding Director
of the Autism Institute at Drexel University,
School of Public Health.
Jeremy Vienstra van der Wiel, who's the Mortimer Sackler,
Sackler, Associate Professor of Psychology
at Columbia University, and Martha Welch,
Associate Professor of Psychiatry in Pediatrics and Pathology
and Cell Biology at Columbia University Medical Center.
On Saturday, January 28, we hope you'll
join Alberto Mangoel and other scholars
for the library as reality and metaphor.
Please follow us and like us on Facebook, as well as on Twitter.
And you can visit helix.org for further updates.
So today's program I'd like to introduce our speakers today,
Michael Bess.
And if you could raise your hand, so people recognize you,
who's Chancellor's Professor of History
at Vanderbilt University, Ned Block,
Silver Professor of Philosophy, Psychology
and Neuroscience at New York University, Jeffrey Kephart,
IBM Distinguished Research Staff Member,
Symbiotic Cognitive Systems, and IBM Academy
of Technology Member, Francesca Rossi,
Research Scientist at IBM Watson Research Center
and Professor of Computer Science
at the University of Padau.
Unfortunately, David Hansen, who was going to be joining us
from Beijing via Skype, took ill,
and he sends his regrets, so he won't be participating.
So I'll just, by way of a slight introduction,
we're going to be talking about embodied artificial intelligence
and we understand that intelligence now requires a body
and can't be understood by simple algorithms alone.
And one of the questions that I might also want the panel
to consider, are we thinking about sort
of an artificial constructed evolutionary developmental
biology when we talk about embodied AI?
Are we thinking of modeling things in terms of the way
that in organic systems, development and evolution play a role
in the attainment of intelligence?
So with that, we'll start.
Whoever wants to get going, you can get going.
OK, I'll start.
So I'm not sure how to answer that question, actually,
because when I think of embodied cognition
or embodied AI, we've discussed with Jeff many times already,
I think of an AI system that can help people
make better decisions, so that work in symbiosis with people
and help them do better whatever they have to do
in their private or professional life.
And so rather than maybe I didn't give much thought
that this, you know, a logical interpretation
of the artificial intelligence, but more I see the embodied
embodiment part of AI as a way to facilitate the interaction
with these humans or humans that are going to work together
with the system.
So I see this embodiment as a way to help in this interaction.
So for example, we are thinking about also with Jeff,
you know, about cognitive rooms like this one,
an example, for example, where people, suppose we are, you know,
people not just discussing here, but are, you know,
here a committee of people have to make a certain decision
and we need help in gathering the data and discussing
and, you know, resolving conflict and so on.
And the room itself can help us in doing all these tasks.
And the fact that the room can be aware of who we are,
where we look at, where we point, and what we do during this
discussion and decision process can help,
for example, interact with this AI system by facilitating
the conversation in the most natural way with the AI system
compared to what could be done with just a software
that is an our laptop and is not aware of the context
of who is interacting with it and so on.
So that's one way I see embodiment that is going
to be really helpful in, you know,
increasing the artificial intelligence,
but also in, you know, increasing the capability
of these artificial intelligence to interact
and help humans.
Yeah, so I agree with what Justin says,
but I want to introduce another issue that is sometimes comes
up under the heading of the body cognition
and that is whether results about cognition in the body
show that the difference between the body and the brain
isn't as the border between the body and the brain
isn't as important as people want thought.
You know, sometimes the word magical membrane is used,
that people have thought that the brain is really what's
important to cognition and not the body
and there's a magical membrane around the brain,
but it's really, it's a mistake as many people think.
Maybe I can go to the issue that you raised
about development and evolution.
So we have many systems that have co-evolved
and develop with respect to each other,
but we still think there's a really important divide.
Like take the difference between animals and plants,
for example.
Animals and plants evolve, co-evolved.
So, you know, color of plants and color vision of bees
have evolved together, but still we think
that animals and plants are very different kinds of things,
even though they interact.
In philosophy and in cognitive science,
I think a major issue which no doubt will come up
is the difference between a causal relation
and what philosophers call a constitutive relation.
So here's a sample experiment that is often
quoted by people to show that there's
no important difference between the body
as part of the mind or something like that.
So if you put a blindfold on people
and you ask them to point to things in the room
and people can do that pretty well,
now you give them a harder task.
Put a blindfold on and ask them to imagine
turning 90 degrees to the left and then point to everything
in the room.
People don't do so well on that, but here's a third thing.
Ask them until actually turn 90 degrees to the left.
They turn their body and then point to things.
People can do that perfectly well again.
So the difference between imagining turning
and actually turning is suggested to some people
that the body is actually part of our cognitive mind.
But I'm not so sure that's a good conclusion
because there's another way to think about it,
which is that we have mental maps
and that's an internal mental representation.
And we automatically orient or try to orient our map
to the room we're in.
And so the person who's asked to imagine turning
to the left is both maintaining a mental representation
of a rotated room and the mental representation of the room
that automatically is computed and that's two things to do.
And so of course you're going to be worse at it
than if you just turned your body and then only
have one mental map with one orientation.
So rather than showing that the body is part of the mind,
it just shows something about the effect of the body
on the mind.
So that's a causal relation rather than a constitutive
relation.
So the body then on the view that I would be more in favor
of is the body plays an important causal role.
So I think the things you mentioned
can help us a lot.
We wouldn't get very far without a body.
But with the room can help us too.
But that doesn't mean that these things are actually
part of the mind or part of the fundamental basis of the mind.
I think you were talking about your focus
was largely on embodiment and its nature in humans
and Francesca was focusing on how can we use embodiment?
Sort of at IBM we think of building things
that are creating things.
You're trying to understand things.
You understand things that we just build them
without understanding.
But I think you bring up some interesting things.
You were talking about some of the difficulties
that we humans face, some of the interesting corners
that you get into as you probe the limits of what people are
able to accomplish cognitively.
And as Francesca was saying, one thing we're trying to do
in our laboratory is to develop, to think of embodied AI
as a way to create partners for humans
in solving cognitive tasks.
And we believe that for us embodiment
is helpful because our belief is that people
have an easier time if they're interacting with something
that has some human-like qualities to it, something
with which we can engage in a sort of conversation
or maybe a more multimodal form of conversation.
And so that's why we're exploring both for one part
of what we're exploring is creating software agents
that are super competent cognitively in areas
where people are not so strong cognitively.
There's plenty of places where areas in which people
are very strong cognitively, but there
are plenty of places where they're not.
Decision-making being one of them, as you well know,
dozens of cognitive biases have been cataloged,
starting probably, at least with first-kin condiment,
as far as I know, but maybe further back.
So we tend to focus on those areas
where we can have an easier time creating
strong cognitive agents.
And then the second part is to endow those agents
with the ability to interact with us in more human terms,
not through a mouse and keyboard,
but through speech and gesture and combinations thereof.
I was somewhat surprised, Francesco,
when you gave that example, because I always
have assumed Embodied AI meant kind of the Rodney Brooks
point of view that the robots will become smarter more quickly
if we let them interact with their surroundings
and sort of learn by themselves and build their knowledge
practically through their own experience.
But what you're saying is Embodied AI is maybe
similar to what you're saying, an extension of our body.
I mean, the room that you're describing,
it's not just a better interface.
It would also be presumably offering us extensions
of our own thought process, suggesting new ideas to us
that were logical inferences or associative connections
that we might not have thought of.
Yes, it's going to, I mean, the idea
that this Embodied environment working with us
will be proactively working with us,
not just reacting or answering like you can ask Google,
find me something or you're going to see me find me something
like we just proactively tell us,
I think that at this stage of the discussion,
you want to have this data.
So let me give it to you in the form that is easy for you
to handle.
So yeah, so very proactive.
But those takes on embodiment are not incompatible.
We are focused on how do you create an Embodied Agent.
One means to arrive at that is through a sort of evolutionary
process where you build something, a robot, or maybe
some other thing that is situated in the world
and learns through experience.
And that's one, and some of, we aren't doing this
with some of our colleagues at IBM
are studying this approach to learning from the ground up.
And I'm aware of other efforts around the world.
There's Stephen Levinson, I believe,
at UIUC and the Engineering Department there who's also
studying, just letting robots be in the physical world
and let them learn through experience.
I think that's a very interesting approach.
Have we already built a room like this?
Or are we already sort of trying to experiment
by putting people into these kinds of interactive rooms
and letting the system?
For the last two or three years, we've been working on this.
Our first class of cognitive tasks
that we're working on, as Francesca was saying,
revolve around decision making.
And so we develop agents that are
able to help elicit from us what our preference is.
That's one thing that I think is important.
Sometimes it's hard for us to even know our own minds what
are our preferences, to tease that out.
There are agents that we're developing
his purposes to do that.
But then.
How does that work?
Well, I think there is already, I think,
here there is already in the realm of decision science,
a lot of techniques for doing this.
For example, for assessing risk tolerance,
there are various questions that can be put to people.
Do you prefer A or B?
Or how much of X would you trade for Y?
Things of that sort.
It's not perfect because if you ask the questions
in different ways, of course, people
are subject to cognitive bias.
And you get inconsistent answers.
But at least, if you can ask people
those questions in different ways,
you can look at the inconsistencies
and have a chance to correct them.
So that's one area, but also cognitive agents
can do things that much more readily and easily
than we can, simulation, optimization,
collaborating with humans and building models of risk
and uncertainty.
These are some of the things that we're starting to explore.
And this presumably is relevant also
to this notion of nudges, which is like when they pass laws that
put taxes on soft drinks.
So people are nudged not to drink too many soft drinks
or make the bottles smaller.
Presumably, you could program the AI
or encourage the AI, incentivize it,
to give you nudges in a certain direction
when it's giving you suggestions to try
to extend your thought process.
I think that programming and the incentives,
I mean, if I'm an individual decision maker,
yes, they're incentives, but the incentives affect
every individual differently because it's
a matter of our personal utility function.
So I wouldn't put in the hands of the developer
or the programmer that encoding.
I would rather have the agent interview me,
the user of it, so that it understands my preferences.
And I don't think this can all be done upfront either.
I don't think my 25-dimensional utility function
can be drawn out on my head very readily.
But in the context of making specific decisions,
over time, I think the system can get a reasonable feel
for what are my preferences and trade-offs.
And now Francesca is an expert in collective decision-making,
which is an area we haven't really probed yet.
But that seems like a very exciting space to start.
So instead of having just one person,
we want to help groups of people making better decisions,
which of course has to do not just with soliciting
individual preferences, but also how
do you put together these preferences of different people
and try to resolve conflict, try to check
possible negotiations and conflict resolution techniques
and preference aggregation to get to a collective decision,
like think of a hiring committee that
has to decide one among a list of candidates and each person.
Of course, this is not just based on preferences,
but on the actual value of the candidates.
But there may be actual individual subjective preferences
as well, given the same kind of skills
of several candidates.
And in this context, there can also be some,
many of these context, there can also be some guidelines
to follow in a decision process.
For example, when you hire somebody,
you have to make sure that you're not
biasing based on gender or race or whatever,
religion or whatever.
So there is also a role for these cognitive and body
systems to actually help follow these guidelines,
these professional codes, code of ethics,
and possibly being even more able than the humans
to follow them and alert when there
are deviations according to these guidelines.
So that's also another part of these projects
that we are studying right now, to embed
kind of ethical principles and professional codes
into these decision support systems.
And again, we think that the embodiment of the support
system is essential in making this the best way
to interact with humans also from this point of view.
So is this to eliminate individual prejudices?
Or in other words, or is it replacing it
by some other standard of selection or?
In other words, if you are going to choose a person,
in real life, you have certain criteria on which you base it.
And sometimes you are lucky and it works,
and sometimes you're not lucky and it doesn't work.
And some of the criteria as you choose,
you only realize after the fact that you had those criteria
and you shouldn't have let them influence you.
So how does all this come into the picture?
Does it clean all this up and just
becomes very neat and factual?
Well, I think it depends on the scenarios
that you are considering.
Sometimes you need decisions that may allow people to have time
to reason about the decision.
Sometimes you are, for example, in other scenarios
where you need a very fast decision,
like you're helping a doctor to make a very critical decision
in a surgery room.
So then you need to somebody that is not
going to have any bias, is going to be very factual
and very quick in deciding what's the best course of action
in that particular moment, because that's
a very critical life of that decision.
But in other cases, I think you could really
make humans more aware of their biases.
So help humans not just think afterwards.
Oh, if I would have done that way.
But the system can actually help you during the decision
process to discover that you have different courses of action
that maybe by remembering other decisions that were made
together with that system in the past, it can help you.
But look, I think that you also have these other criteria.
Because in the past, you showed that preference that
makes me think that maybe you want to follow that path
and not the other one.
So I think that there is a role for this embodied environment
to really help us be more aware of our criteria, our biases
as well.
But of course, one has to be careful.
I mean, these systems are not perfect.
We'll not be perfect.
We'll never be perfect.
So we also have to be aware of their limitations.
We have to trust the system, but with the right level of trust.
And so we have to, over the interaction over time,
we have to learn what their limitations and possible biases
are.
Because they could be biases, maybe even unwanted,
into such systems as well.
I'd like to distinguish clearly between preference and bias.
I think the systems we're trying to develop
are designed to, in the end, really understand human preference
better and reflect it better, subject maybe
to social norms and the like.
But we're trying to reduce bias, which I see more as those
annoying things in our reasoning process that
cause us to deviate interactions from what really
is optimal with respect to our preferences.
Sometimes you choose something for the wrong reason
and turns out to be very good.
But that's still a bad decision.
Because you just got lucky, but I'll still call it a bad decision.
You know, this emphasis on getting the machines
to help us figure out what we really want,
or maybe it fits with something that Francesca and I
heard this past weekend.
Francesca spoke at a conference that David Chalmers and I
were at our, we have a center for mind, brain, and consciousness.
We ran an ethics of AI conference.
And there were talks there by Stuart Russell and Ellezer
Yudovsky, which went into the issue of,
we were just talking about earlier today,
of how you get an AI to have a goal.
And they argued very strongly, and I thought very persuasively,
that you cannot just put a goal in a machine
and expect to get out of it what you want.
And the example they used was the Sorcerer's Apprentice,
or the Yudovsky used, which is slightly shifted
in the machine direction where the apprentice engages
a machine instead of a magical broomstick.
And the idea was this.
You tell the machine, here's your goal,
make sure that the cauldron is always filled with water.
OK, so the machine thinks, well, the way
to maximize the probability that the cauldron will be filled
with water is to always have it overflowing.
And then the machine reasons.
But these people will not like the water
to be all over the floor all the time.
And they will want to turn me off.
So I have to disable the off switch.
And furthermore, they may try to damage me,
so I can't keep doing it.
So I better make more copies of myself.
So the idea is just giving it a goal,
making sure the cauldron is always full,
isn't going to get you what you want,
because any goal can be understood in a machinist sort of way
that isn't the way you would expect a person to understand it.
And you can't really, there's really no way around it.
Other than, and this is a proposal
that a number of these people were making, which
is the machine really, what you should really
tell it to do is to figure out what people want.
And this fits with this emphasis on the collaboration
between the machine and the person
that you two were both emphasizing.
My hope is that we won't spend 10 years designing
a software agent that keeps cauldrons filled to the brim.
If we were to do that, I think it's very possible
that we would arrive at the state of affairs
that you described.
But we develop things incrementally for one thing
in our development cycle.
And also, I believe that for a good long time to come,
we are going to be not just delegating out to AI
and walking away going to the beach and coming back a week
later to see what happened.
But we're going to be actively engaged.
I think we're going to see what is happening.
And I think we have at least a chance, either as developers
or users of the system, to intervene.
At least that's my hope.
I do know that machines can be thousands of tons faster
than people.
So I may be wrong in certain aspects of this.
But that is my hope, that we would,
through our engagement with the system,
kind of see what's going on and say, wait,
that's not what I wanted.
Hold on.
If I understand you correctly, both of you
are now talking about an AI system that
is somewhere in the spectrum between today's AI's
and artificial general intelligence, or a human level,
AI.
It's somewhere below that, right?
In the sense that you can give it the set of instructions.
It's motivated to obey those instructions.
But it does so, actually, stupidly.
It does it in an alien way.
In an alien way.
So as we were also talking about earlier,
for humans, there's a cognitive background
that no one ever states.
And maybe it would be impossible to state everything.
You know that the floor won't bite you.
There are all these, the word background
is actually often used in philosophy for this.
It's just a shared set of what you might call assumptions.
But it's not clear that they could ever be codified.
And the machine would, depending on how it's made,
have a different background from us.
So the goal would be to get a machine that is raised
like a child through all the socialization,
if you watch a small child learning how to grasp
an object and learning, oh, if I don't
apply enough pressure, it falls to the floor.
Child isn't going through that conscious process.
But gradually, through the years, gets
all this background accumulation.
Would we then have an AI that is operating in the?
There was a problem raised with this in the symposia
that I'm talking about, which is that what the machine develops
depends not just on its environment and the skills it
learns, but the kind of processing that it starts off
with.
And if you had a machine, which at some point,
its development, realize these people
have a quite different idea of what to do than I do.
But if I reveal that difference, they will be upset.
And they'll try to operate on me or whatever.
So I better pretend to be doing to have the assumptions
that they have.
So that point was made in one of these symposia.
And it's a little troubling, because if you make a machine
that's maybe smarter than you know it is,
it could be fooling you.
That would be a human level AI.
It could be.
One of the points made in the symposia
is that you might move from human level or subhuman level
to better than human level rather quickly
without really realizing it.
And another point often made in this context
is that the points at which this is most likely to happen
when people are most likely not to be
taking care of that border, the dangerous border,
is in context where there's competition and there's
race, for example, in wartime.
And you're racing with the enemy to develop the most
sophisticated war machines.
The sophisticated war machines can get away from you.
Well, I think you're talking about a sort of possibly
a phase transition.
You're talking about emergence.
It is possible, and people do use that language
when they talk about the singularity.
That is a concept of phase transition and emergence.
I don't know if I believe in it or not,
but it's a possibility one has to consider.
What is your take on it?
Does it sound so sci-fi-ish that it's hard to?
Are we so far away from it that it's silly to worry about it?
Or even if we're far away from it,
it doesn't seem like it's silly.
You should say what it is for people who never
heard this term.
Yeah, the singularity is the idea that at a certain point
artificial intelligence will outshine human intelligence.
And it will get to the point where humans are basically not
good for anything, at least from the point of that alien machine
intelligence.
And so there are naturally concerns about this sort of thing.
Happening.
And this is not, I mean, we all know about this, at least
if we've watched any science fiction movie at all.
It's there for us, that sort of possible future is there.
I don't think I have any special insight into this.
If it happens, I think it's a way off,
but that doesn't mean we shouldn't be worried about it.
Now, something like the death of the sun,
four billion years out, that's a little hard for me
to get emotionally involved in.
But this is closer in.
So even if it's my great, great grandchildren,
I still feel some level of it's something
we ought to concern ourselves with.
What do we do about it?
I think we don't just blindly go ahead and create technology.
I think we do have to think about it and have an eye on things.
And I think some of the efforts like what Francesca is doing
with ethics and AI are very worthy things to be doing now
to have us thinking about the implications along
with our development of the technology.
I think the thing that makes a singularity somewhat
gives one a little shiver is the thought that maybe we
could one day make machines that can make machines that
are smarter than they are.
And maybe those machines will be able to make machines smarter
than them and so on.
So the idea is you could reach a point at which it takes off.
And it takes off exponentially.
I mean, that's what people think.
When the threshold of human level intelligence will be passed,
some people think that it will actually, maybe it
will take a long time to get there.
But then at that point, you will have to.
But I would be actually, I mean, of course, one
can have all sorts of speculation and vision,
all sorts of scenarios or where and when these will happen
and how will happen and what will happen at that point.
But I really think that what Ned was saying,
that we don't need to wait for that to happen if it will ever
happen to have concerns about the fact
that intelligence systems are even very narrow and very
specific for a task.
So no human level intelligence, because human level intelligence
means you are very broad and you can adapt your intelligence
to various tasks on every day.
Even very narrow, very specific AI can give undesired behavior,
like the one that was in the example that Ned made.
So we have to make sure that either we
can tell exactly what we want without leaving everything out,
which sounds very difficult because we don't tell each other,
yes, do this, but also take care of this, don't do that.
Or they discover themselves by observing us and then
inferring what the principles are,
what the common sense reasoning capabilities
that we use now in our everyday life, they should use as well.
But anyway, there should be a combination of these two things,
but there should be some way to provide them with these goals
that we want them to reach, but in a ethical, fair,
reasonable, common sense reasoning way.
So this is something that may sound strange,
but this is something that has not been considered a lot
in the history of AI, because all the machines,
something very recently, were very narrow, very smart
in doing that simple thing that simple, that small thing
that they needed to do.
But they were not capable of doing many other things.
So if you want a machine that can play,
go as good as possible, play chess as good as possible,
then you know what goals to give the machine.
And you don't need to specify many other collateral things
that you should be careful not to harm people.
I mean, that's not the point.
It's not relevant.
You should play chess as fast as good as possible.
But so in all the textbooks that you have,
you can see of AI, there is always the assumption
that you can easily give a machine the goal
that it should achieve.
And now we realize that the more the machine
get into the real world scenarios.
And they have to do with the uncertainty of the world,
and they have to take care of the things that can happen
in the world while achieving the main goal that we give them,
then really we need to be careful
that they also are aware of the fact
that they should not do this.
So another example that Stuart Russell always gives
is that if you leave at home your kids with the Butler
robot taking care of them, and you tell this Butler robot
to cook dinner for the kids.
And the robot opens the fridge, and there's nothing
in the fridge, but it sees a cat walking around with a house.
But you don't want the cat to be the dinner for your kids.
But if you just say cook the dinner for your kids,
you have to be careful that you also
say all these other things that you should not do.
Or I don't know if you have yourself driving car,
and you tell the car, bring me home as fast as possible,
period.
And then yes, OK, but you have to make sure
that you don't run over anybody.
You don't make me car sick because you go too fast and so on.
So all these other things are part of the goal.
But we still have to understand how
to communicate this to a machine.
Well, I think part of the answer that you were getting at
is in your very first comment was,
could these robots evolve?
And that brings up the question.
I mean, you're talking about AI and ethics.
Right now, we are thinking about ethics and AI,
but maybe part of the world experience of these robots
is we bring them up and we teach them.
Humans teaching robots and, oh, nuts.
That's not good behavior.
This is what we do.
This is the way we do things.
And it's a possible approach to having them
grow up understanding human social norms.
Yeah, but hopefully it doesn't take 18 years.
Right.
Or.
Like for you, I don't know what to hope for.
But given that there were also unanticipated results from that,
I mean, is there just going back to my initial,
is there a potential danger in trying
to model artificial intelligence development
on what we understand about humans
and mimicking the developmental steps
or creating a kind of evolutionary potential
and what if we're bad parents to our AI?
In a sense, or we're ignorant parents.
We don't know all the parameters.
Humans start off with a huge innate component
to all their cognitive abilities.
And you couldn't expect to get the same results
with a machine that doesn't have those innate components.
So for example, it has been, I think,
recognized that even very small kids, like three or four years,
so they have an innate nature to cooperate with others.
And nobody even nobody teaching them,
but they cooperate with each other.
They help each other.
They help adults.
They have other kids.
And machines don't come with this thing or others.
So there must be some other way to teach
them or to make them have these capabilities, which
is different from what we do for humans.
I suppose this is true, because if you
did the thought experiment of let's bring up an ape
in our household, you could treat it just as you would
your child, and it might end up somewhat different
in its behavior.
Yeah, that's in fact the issue that Francesca just raised
is actually directly relevant to this,
because apes don't have this cooperative impulse.
So Felix Varnequin showed with babies.
I know him, but he will.
With that a two-year-old who sees a person go to a cabinet
and put things in it and then goes to the cabinet
with a thing too heavy to occupy both hands,
the child will spontaneously go and open the door.
And many experimental approaches to monkeys
and chimps show that they do not tend to do this kind of thing.
In fact, if there's food involved,
they really seem to be especially competitive.
Well, there's learning and there's evolution.
And I wonder whether on an evolutionary time
scale we can get whatever it is in the hardware or firmware
better aligned with humans.
But another point I would make here is that on the one hand,
we would like the embodied AI to understand
something of our world so it can know something of our norms
and ethics and all that.
So that would indicate or dictate that we
want to bring it up as one of our own.
But on the other hand, it shouldn't
be the case that the only embodied AI that makes any sense
is a humanoid-like thing.
So I don't know how to reconcile that.
Certainly the cognitive room that we were describing
is not humanoid at all.
We talked to the room and it talks back to us
through the speakers, shows us stuff through the displays.
But there's nothing humanoid there.
And there are plenty of robots out there
that are not humanoid either.
So I don't know how to reconcile it.
So you're saying it's not possible
that we get to a point where they recognize emotion?
Oh, we can, to some limited degree, recognize emotion now.
It's trained.
I mean, through text, facial expression, tone of voice,
you can train a machine to classify
human-emotional state into a small number of discrete states.
Happy, sad, disgust, anger, things like that.
So why would there not be able also
to have them in terms of what you were saying,
in terms of singularity, recognize certain things,
or not to be down certain things, could hurt somebody,
and so they shouldn't do it.
In other words, have a controlled system built within it.
But just the source's apprentice example
is meant to show that there's no goal that you can give it,
that couldn't be understood in a way that
goes counter to what you wanted.
So it's really difficult to see how to put that into a machine.
The possibilities for screwing up are infinite.
Yeah, that's right.
In variety.
But that's true with humans.
No, but humans have this background of understanding.
I mean, not all humans.
There's a class of psychopaths that don't have to be.
We're talking about humans today, but humans in the past,
we're killing and all sorts of putting fire on people's homes
and so on was normal.
So it has evolved to the point we are here.
So when we worry about singularity,
why aren't we also thinking about that if we devolve
the same way humans have evolved?
Well, it may.
But that's a future.
That is, by creating a God of AIs, I don't know,
but somehow creating the same thing being part of the evolution
of it.
Well, here we get into what is our feeling
about the possible eventuality of a singularity.
We could take the view that, well, we humans in our present form
are maybe not going to be part of that future,
and that makes us sad.
Or it could be, well, some essence of us
is continuing forward.
Just it's part of the natural evolutionary process.
We could take that detached view.
I don't know if I can get myself there personally.
Maybe it will improve itself in some machine form
and become some perfect being.
But is that connected to me?
I don't know if I can connect that to myself.
So I don't know if I feel happy at that prospect.
But it may be that we sort of, some have suggested
that we're not the distinction between human and machine
is going to become fuzzy to the point
where it sort of doesn't matter anymore.
I mean, as we have artificial limbs today,
they're getting better and better.
Someday they'll be seamless, and you won't even
know the difference.
But then the same thing could happen to our brains.
Maybe parts of our brains start getting replaced
by something mechanical, or maybe it becomes
squishy and biological like.
Over time, maybe we just augment our bodies and our minds,
and we sort of meld together human and machine.
And then it's just part of our projection into the future.
So there are different views one can take at this.
One of the implicit distinction that I'm hearing
is do we want these AIs to be tools or instruments,
or do we want them to be agents?
And the paradox is that you want them to be agents,
because if they're mere tools, they're going to do the dumb thing.
You're going to give it commands,
and it's going to misunderstand and do harm.
So you want them to be more agent-like so that they
understand your intentions, or you try to get them to do that.
But the more agent-like they become,
the less you can control them.
Ultimately, a real agent distinguishes an agent
as they can take initiative and then start doing things
that they decide, having their own priorities.
The way out of that, at least in today's world,
is to they're both more, we're using them
because they're more intelligent than us in certain ways.
But I view them as idiot salons.
It's some of both.
They're still instruments at that level.
And yes, they're in narrow ways,
in cognitively stronger than we are, but in narrow ways.
And so like a pocket calculator.
Beyond pocket calculator, but they,
and I wouldn't ascribe any cognition to a pocket calculator,
particularly.
I would say that these are tools that really augment
our cognition in a more significant way.
But they aren't in their own right,
necessarily a fully autonomous, certainly not
an artificial general intelligence.
We're not going to be there for a while.
Kind of like a consultant, I like an expert in something
that you need for your job and you consult with.
And you are going to make the final decision
about what you need to do.
But you consult this expert who is going to help you
because he has more knowledge and he has more skills
for that particular thing that you need to do.
So machines will be much more capable of us
in certain things, as Jeff said, like in handling,
in reading, a lot of data, a lot of scientific articles,
a lot of information that you will never
be able to read in your whole life.
And then summarize it and give it to you,
the part that are relevant to what you have to do
at that particular moment.
But we'll see.
I mean, pocket calculator does not give the best idea,
I think, because it's very deterministic.
So you'll give the numbers and you'll
give the same numbers twice, it gives the same result.
While here we want somebody that can take care of the uncertainty
of the world, of the incredible number of scenarios
that can happen and so that it has its own ways of dealing
with these lines of conduct and probabilities,
probability reasoning, and so that maybe
different, slightly different, so now it
gives you a completely different suggestion of what to do.
So it's a context.
Yeah.
But of course, as soon as somebody gets a line on how
to give them a general intelligence,
since there will be no stopping it,
and then that's then we could enter into uncharted territory.
So we should really be thinking now about that transition.
I think earlier you gave a very good example
when you were talking about the singularity of the notion
that suppose a machine can create a machine that
is slightly more intelligent than itself.
What you have to think about is if a machine can create
a machine that is 99% as smart as it is,
then you can take it several generations ahead
and it'll just peter out.
But if you can make a machine that
makes a machine that is 1% smarter than it,
it's going to be a positive exponential instead
of a negative exponential.
And that's where you get a sharp phase transition between,
ah, don't worry about it, and oh my god.
And also, I mean, people I think
are kind of concerned about that scenario
because different from humans, machine
can replicate themselves instantaneously and almost
with no cost, at least from the software.
So then you have this exponential, not just for one machine,
but for a very huge number of machines.
But again, I think that I mean, I
don't know what we can do now while envisioning that far away
scenario, what we can do now thinking of that scenario.
But we can already do now and work now for narrow,
narrowly intelligent machines to actually help them
behave in the right way.
And I think this, of course, will also
help when and if that general intelligence will come out.
And maybe we can have AI researchers
to behave in the right way too.
I've heard that people have taught courses on AI and ethics
through the medium of science fiction.
Because science fiction has plenty of cautionary tales in it.
Well, there's a number of TV programs right now
that are exploring this border.
There's humans.
West world.
West world.
Yeah.
My problem as a historian is I've
tried to envision what would be plausible scenarios
if we decided that we needed at some point
to restrain the advance of AI as a science.
And I can't find any that would be, in my mind,
that would be successful.
Because as you said, we live in a competitive society
and a world that is not unified.
And if either one company competing against another company
or one nation competing against another nation,
anybody who gets the first pass the post in terms
of getting one of these general artificial general
intelligences is going to have an extremely powerful advantage.
And so there's an incentive built
into the very structure of our political systems
and our motivational systems to compete and kind of an arms
race sort of mentality.
Even if it's not at war, it could be some other.
It could just be just sort of the first pass
the post gets more powerful.
I wonder, I mean, reasoning from history,
are there any historical examples of being
able to suppress a technology?
The only thing I can think that at least goes slightly
in that direction is probably nuclear proliferation.
I mean, the US got to that point a little bit before Germany.
And we used it and then backed off.
The few examples tend to be cautionary in the other way.
The Chinese at one point banned large ocean-going vessels
for a while, a couple centuries.
And all that that did was postponed the inevitable
because eventually other people built big boats
and came to China.
And so in the competitive world, the game
is set up in such a way that if you
have to keep advancing, because otherwise your neighbor is
going to advance past you, and then you'll
be at a disadvantage.
So there have been attempts by people,
you think of the Amish.
They've been able to do it because they pose no threat
to anybody.
But in cases where you're actually getting competitive power
or advantage in a competitive situation,
sooner or later, the pressure makes,
if would there have been an atomic bomb if World War II hadn't
happened?
Probably within 20 years.
The war accelerated that process.
But probably by the 1960, somebody somewhere
would have figured out that the Germans were already
on the way to doing it anyway.
I read a wonderful book by a military historian, John Keegan,
that went through a number of phase transitions
and development of weapons in which there was a sudden advance
in either defense or offense.
And one that sticks in my mind is the development
of a much better cannon, which interacted
with the defense of the time, which were these high walls.
And so for example, the fall of Constantinople in the,
what was that, 1453, I think, was due to the fact that the Turks
perfected a cannon that allowed them to undermine the walls.
And then once, and then they would just fall.
So once this happened, defensive walls in cities
all over Europe, where everybody realized
that the cannon made their walls obsolete.
And what they needed was wide, very thick, and even,
and some, that can be low walls, but much thicker.
And they were building new walls for more than 100 years,
all the cities, the walled cities in Europe.
They even quote a wonderful letter from Michelangelo
saying, because he was selling his services as a wall
designer, saying, I don't know much about painting or sculpture,
but I really know about building walls.
LAUGHTER
But it's true that we live in a very competing world.
But I think that, I mean, a little bit for people
are starting to realize that especially in this,
understanding the issues in the advancement
of this very powerful technology is something that should not
be part of the competition, understanding
how to address these issues.
Like, for example, maybe Ned, remember that I mentioned that
last week, that two weeks ago, it
was launched a very interesting initiative,
where five of the main companies developing
AI, which is IBM, Google, Facebook, Amazon, and Microsoft,
they decided to get together and understand together
what it means to develop AI for the benefit of people
and society.
And we try to engage with everybody else.
It's not just a company kind of thing,
with non-corporate members as well,
non-profit organizations, scientific associations,
individual society in general.
But we really think that we should not
hide the maybe issues in this development
of this very powerful technology.
But the technology is so powerful,
it can be so beneficial for everybody
that we have to work together, even companies that are,
as you may know, competing a lot in the marketplace.
But they think on this, they should be really collaboration.
And I think this is, of course, doesn't mean that maybe
other companies or other countries are not going to compete.
But still, these are very companies
that can influence a lot all over the world,
because they are all used by billions of people everywhere.
And I think that this can start really
a very collaborative environment, where these issues are
discussed, addressed, solved, and understood
how to best trajectory for AI in the future.
But don't you think that if one of those five companies
makes a real breakthrough, it's likely
they're going to actually share it with the other four?
Yeah, but I mean, I did.
I know, but I don't know about the other four.
No, but I mean, the idea is not to share
new software or new advances, but to share the best practices
and how to deal with making AI, of course,
in a competing environment more and more smart,
but in a collaborative environment,
making it smart, but in the right way.
So it could be that one of these companies
is making tomorrow a very big advancement in AI,
and of course, it's going to be his own result,
and not the result of the other ones.
But we want together to understand
how to make whatever advancement is
going to be made by anybody in the best way
for the benefit of everybody.
And I think that that's really needed a lot,
a collaborative environment on these issues,
even among entities that are naturally competing
because of their business model, of course.
It's really encouraging.
Similar to what was done about a year and a half ago
with genetic engineering technology,
the new CRISPR-Cas9 pathway for modifying genomes,
and they convened a second.
The first such conference was with recombinant DNA technology,
the Asilomar Conference in the 1970s,
and voluntarily the leading figures in this field
met a year ago, and again, saying,
let's consult with each other, let's establish
basic ground rules and best practices.
I didn't know that this had happened with AI.
I mean, it needs to happen with synthetic biology.
It needs to happen with all these nanotechnology,
all these potentially disruptive,
but potentially also enormously beneficial technologies.
It is incumbent on the people doing it.
I remember in 2000, I think it was,
there was an article published in Wired by,
I'm blanking on his name right now,
he said, why the future doesn't need us,
what's his name again?
He basically was a computer programmer who basically said,
I'm leaving the field because I can no longer
ethically continue to something which I think
is going to lead toward the singularity.
I think, pardon?
It wasn't Jared Lanier, it's, no, it wasn't
Jared Lanier, it wasn't Jared Lanier, it's wild.
It'll come to me, of course, after.
But it made a big stink in the technology community
because he was saying, I'm taking an ethical stand,
and implicitly he was saying, if you don't also
quit doing this, you're doing something fundamentally
morally wrong.
It's like continuing to work on the atomic bomb
or something like that.
But quitting, it would also not allow us to get
the real benefits of this technology.
For example, just one in health care.
And cure cancer, so health care issues.
So I don't think we should quit.
We should continue in the best way.
Exactly.
Yeah, I think to state the obvious,
if the most ethical programmer
equates, the population of programmers becomes less ethical.
Somebody would have very quickly taken this question.
I feel the need to reiterate something
that you mentioned in a glancing way.
But, well, best practices are great,
and it's great to formulate them and share them.
But it's in a competitive environment
where they attention might not be paid to them.
And I think especially if there's a war-like situation.
Whoever is making in advance might not pay that much attention
to the best practices if they think they're going to make up,
they're going to defeat the enemy.
Doesn't even need to be competitive or malicious,
I guess.
It can be accidental.
Oops, sorry.
I didn't realize that would happen.
We're rushing.
No, it could even just be some emergent phenomenon
that it would have been difficult to anticipate.
And I guess this brings up the question of liability
for AI as well.
Who's responsible when something goes awry?
There's been a lot of talk about hacking.
So is it still possible, for example,
when you said these five companies have gotten together,
is it not, are you still able to keep things hidden
from the other companies if you don't want them to know
about your networks?
Isn't it possible with sophisticated technology
too for, let's say, Amazon to know exactly what IBM is doing
and vice versa?
Well, you've seen what happened yesterday.
That was this big cyber attack that
blocked many websites and services.
So I think that we have to be smarter than them,
and then they come smarter than us, and then so on.
So I mean, it's still not clear how
to avoid all these attacks and intruders
into systems.
So I don't know.
I'm not an expert in cyber security,
but it's not clear to me that these can be easily stopped.
Of course, I mean, companies can have their own security,
walls, and everything, but I'm not sure this can be done.
Even very sophisticated agencies here cannot keep secrets
that they have.
Then it means that we still have a long way to do it.
But I thought some of the sense of your question
was about among those five companies,
can they share while still remaining competitive?
And I think the answer is yes.
Companies can find ways to share and compete at the same time.
Yeah, but of course, they don't share.
I mean, the sharing is advising, sharing, best practices,
discussions, issues of concerns, and so on.
How to best develop AI?
How to be ethical while developing
AI from the idea to the product that you get?
And how to build these products in a way
that that product is going to behave ethically
when given to the world, and is going
to behave in the best way for benefit of society?
I can imagine standards developing as well.
I mean, going again back to science fiction,
you can think of Isaac Asimov's Three Laws of Robotics.
And these were cooked in at the very foundational level
into each robot.
And maybe we would develop something like this.
And through the combined power of these five companies,
they could all adopt this and then kind of exert pressure
on the rest of the world community to do the same.
Yeah, the standards are very important.
Of course, if not that five companies or even 10,
or 15 can build a standard, it has
to be reached by consensus building.
But over the course of the technology,
I think the information technology, especially,
the standards have played a very big role
in making the technology available to everybody.
Because the fact that all the things are compatible,
like, I don't know, the standard for the Wi-Fi.
At the beginning, there was no standard.
And then by consensus building, in our computers,
in our telephones, we all have the same system,
with the same protocol, being able to connect
to any Wi-Fi in the world.
Because it uses the same methodology.
So of course, it cannot be that technology
that precise of technological oriented,
a standard that tells these companies, or everybody else,
how to develop AI in the right way.
But there could be very precise guidelines
on how to do that.
And that's one of the things that we think this
would be very important to achieve.
It may not answer the limit case that you brought up
of a war, or something like that, or even just
a big military rivalry.
But it's a lot better than nothing.
Yeah, well, I think so.
I think it's very promising.
And I think that the fact that it comes from companies
has to be interpreted in the right way.
It's not that these companies want to decide how AI should
be done, and they're going to get together, and define it.
But because companies are those that are closer
to understand what it means to deploy AI system
in the real world.
Because they have clients, and these clients actually
use AI in the real world.
And those are the people that can tell us,
what are the issues that you see in health care,
in finance, in retail, in e-commerce, and so on.
And so tell us, and these are the issues
that we are going to address and resolve.
What are the problems that you see once you get your product,
and you just give it to customers?
So that's where you should start this discussion.
And then everybody else should be involved as well.
Speaking of everybody else, you mentioned the five companies.
Are you in this organization making efforts
to bring academia into discussion?
Yeah, yeah.
Academia, non-profit organizations,
and other professional associations.
Everybody, everybody, we just have
to understand how to put together the path
of to make everybody be part of the discussion
in the right way.
According to Jan Lecun, who's at NYU,
and also runs the deep learning part of Facebook,
at least at the moment, there are no secrets as far as how
to make an artificial general intelligence.
So at least his story is the academic research centers
are way better than the company was,
because none of the good people will
work for an organization where they can't publish their results
and collaborate with other academics who
are working on the same thing.
You should not assume that companies cannot work with academia.
Well, of course, he's working with a company, Facebook.
Yes.
And he says, well, but he says, take it for me.
The private efforts are just nowhere near as good
as the academic public efforts.
The best people are all in academic centers,
even if they're also in Facebook.
And at least at the moment, just the technology
in public academic centers is ahead of the technology,
the privately developed technology,
where they don't have the benefit of getting feedback
from a large public that books a few code.
But I don't know.
I mean, I have heard Ian saying this many times.
And by the way, Ian is also the Facebook representative
in this partnership that I discussed.
So he's also involved in that.
And I have them saying many times that what is developing
academia is much better.
But I don't know that at this point,
there is really a difference because there
is a lot of collaboration.
And big companies, of course, that do not have research centers.
They just want to develop better and better products.
Maybe they don't share.
They don't publish in academic venues.
But other companies that have research centers,
like IBM, Microsoft, and other.
And they do publish.
And they do collaborate with academia.
And they want to be exposed to the feedback
and to the common, positive, or negative of the rest
of the academic colleagues.
So I don't see so much different.
Also, I mean, there are a lot of data sets.
Many are openly available.
Data sets of which these AI systems can be trained
and can be structured.
But there is also a lot of data that companies can get access
just because they have the real world scenarios
they can work with.
So I think that I don't see much difference between my two.
Yeah, I'd like to comment on your quote from Jan also.
And I guess this is just a reflection
of my own cognitive bias, because no one
likes being implicated for being second rate.
OK.
So I don't know, actually, what is the utility
of making such comparisons?
I think we should be embracing what academia and industry
can bring to the table.
I think there are outstanding people in academia.
Obviously, Jan is one of them.
There are a lot of other people that one can cite.
I think something that I, and so the individual technology
is being developed, yes, they're outstanding.
I think we're developing some pretty good things too.
But one place where I think companies can really shine
is weaving it all together into something that actually works
and something that actually has an impact on the real world.
So rather than saying A is better than B,
I would rather focus on how can A and B work together
collaboratively in the best possible way
to create the best value for the world.
Yeah, of course he's focused on that too.
Right.
He has a foot in both worlds.
So he was just Francesca.
He was trying to combat the suspicion
that you see that there's some nefarious artificial intelligence
projects deeply hidden in some company
that is going to take over the world or something.
So he was speaking to that kind of issue.
He's saying, no, it's not going to happen.
No, it's only going to happen in academia.
Well, what he says is that it'll be public.
So the point he was making is that it's public.
What academics are doing is public.
They publish it.
They're at conferences.
All that sort of stuff.
But I thought academics chronically complain,
certainly in the medical biotech field and so on.
They're constantly complaining that they, in research,
that they don't have enough financial resources
and companies like IBM and so on have all that.
So it's a little bit puzzling to me
that that would be expressed like young music.
And I think this issue of resources
is important, not just because you
want to have more money to do your research,
but because in corporate environments,
especially big ones like IBM, you really, like Jeff just said,
you really have the opportunity to put together
experts in many different areas of AI, or even IT,
or even other disciplines.
And you put together software, hardware, and body
management, and so on.
So you really have the chance of putting together
the best of all these lines of work
to build something that is even more significant.
And this, I think, rarely happens in a university group
because of lack of resources, but also because within the department,
usually you have a critical mass.
Some people have it.
But not as much as you can get in a very big corporate environment.
And you are not exposed to this wide range of applications
that, for example, IBM can describe to us.
If I want to know, or Jeff wants to know,
what are the main things that are happening in AI
applied to health care, or AI applied to commerce,
or AI applied to anything else, or within IBM, you find it.
And so while in academia, you have to contact somebody else
and see whether you know, and then go to look at the conferences
and these and that.
In a big corporate environment, you are really
exposed to the real world.
And so that's very important for young.
For example, in Facebook, as a more narrow goal,
which is whatever is important to Facebook in terms of AI,
like personalizing the news feed, or recognizing people,
or other objects, or whatever, understanding what
is in a picture, things like that.
But I think that by being connected to Facebook,
he really can do much more than what he could do,
but just being just.
And why you're a professor is not just.
But he really gets exposed to the level of being
exposed to the real world issues in very wide reaching
enterprise like Facebook.
It's really very important for him as well.
I think he would not deny it.
Oh, he says, yeah.
I'd like to follow up on two points.
One, Ed, I think he said something about academia
complaining about financial resources
don't think that only holds in academia necessarily.
Researchers are insatiable in their desire
for financial support anywhere, academia or industry.
The other thing is one thing that I, on the point
of collaboration between industry and academia,
and bringing together the best of both worlds,
one thing I've been involved in is a collaboration
in the space of embodied cognition
that IBM started up with Rensselaire Polytechnic Institute
about a year ago.
We've been working with several professors
there and their students to set up environments there,
cognitive rooms, like the ones that we're developing at IBM.
And they're starting from the technology that we provided.
And now they're really building some very interesting things
on top of that.
But the thing I want to emphasize is that I think what
we brought to RPI in doing this was the idea of what
these professors and their students could do
if they worked together.
They're not so accustomed to doing that, as I find.
They have other professors at the same university.
They have their colleagues all around the world.
They have their own social networks and all that.
But they, by and large, tend not to work quite as much
with people at their own institution.
And so they haven't had the experience of building together
a much larger thing than they can do independently
of one another.
And I think bringing that model of what
we're able to do within a company to RPI
has been, I think, eye-opening for them.
And I'm hoping it is a good model for other universities
and for other university industry collaborations.
I'm curious about what you two are saying about IBM.
I mean, I have no real knowledge of industry,
but the impression one gets just reading,
and I do have one other source, is that many companies
are organized in informational silos
where the parts of the company really
operate in secret from one another.
Is that not true at IBM?
I don't say more because I joined one year ago, maybe.
I'm so well-hidden, you don't know the same.
No, but I see a lot of collaboration
and more than my, what is it?
You're talking, Phil.
No?
You're OK.
You're OK.
I don't know.
So more than I see, really, in academia.
In my department, for example, I work in AI,
and then somebody else works in other areas
of information technology, software engineering,
or formal methods.
But we all work with other colleagues
in that same area, as Jeff said, but not much
with other colleagues in other areas.
So that brings maybe more advancement in your own area.
But then when you want to build something,
you actually need other disciplines.
And within academia, you don't do that.
I'm so proud of this.
I mean, and I think that IBM is needed instead,
this collaboration between different disciplines,
because otherwise, you cannot build a product that
goes into the real world from the idea, and research,
and so on.
So you need different people with different capabilities
to be put together and to build, actually, that thing.
I've been it.
Yeah.
It's funny because there's a parallel between what
we're saying the AI needs to do to work better, which
is to be embodied in the real world
and grounded in the real world.
And we're saying the AI researchers also
need to be embodied in society and get that feedback.
I think that's been a recurring theme in this discussion.
But just to comment on industry, or at least
my own experience of it.
There are always silos that develop in any organization.
But I wouldn't say that, at least in my case,
they've been intentional.
It's more security through obscurity, I guess.
In my own case, I've felt that in my quarter century or so
at IBM, I've been able to collaborate very broadly.
And in particular, our work in embodied cognition
requires that, because our team, by itself,
weaves together technologies from a vast number
of other research teams working in our lab
and in the other labs around the world.
We need that.
We can't do it all ourselves.
And so we strive all the time to break down any barriers
that some of the barriers are time zones and things
that like this.
We strive continually to try to break down
those barriers, educate ourselves about what others are
doing, and avail ourselves of those technologies.
So I'm surprised what you said about Rensler Polytechnic
Institute, because at least in the parts of NYU
that I'm familiar with, it isn't like that at all.
So my colleague David Chalmers and I
who run the Center for Mind, Brain, and Consciousness,
we both have joint appointments in neuroscience.
I have a joint appointment in psychology.
I go to lab meetings of some of my psychology department
colleagues.
Of course, I talk to all my philosophy department
colleagues, too.
So we seem to have.
I think that maybe is different.
I may have missed sciences.
I think there may be the context of social sciences.
Maybe it's different.
Yeah.
Sure.
I have to go to a public question so people
can line up at the microphone and please
speak briefly and to the questions specifically.
Thank you.
Is it on?
No?
Is it on?
Yeah.
OK.
My question goes back a little bit toward when you were talking
about emotion with AI and then also
with a possible threat toward people.
And my question is, isn't there a fundamental difference
between humans and artificial intelligence regarding
concerns about mortality and then to extend that to worry
about survival of the species even?
I mean, is a machine ever going to have that?
Is a machine going to worry about if it ceases to exist?
And if that's the case that it wouldn't,
does that mitigate any possible threat that
could occur down the road?
You know, one point that's sometimes
made about this is that even if the machine doesn't care
about its own existence, it might care about its project
and its project may require its existence, in which case
it might act almost as if it was really extremely
concerned with its own existence.
The source's apprentice story is like that.
So we're not supposed to imagine that the machine cares
about its own existence, but it cares about getting
the cauldron full so it's going to protect itself
from being destroyed.
And it may not make that much difference.
The other example from a sci-fi would be in 2001
a space odyssey.
I was thinking of that too.
Hal had the greatest enthusiasm for this mission.
And that may have been the thing rather than its own self
preservation.
But is there any self preservation built in or not really?
Well, not today.
I don't know how it's going to go.
All right.
Thank you.
I was thinking of the metaphor you had of the castle,
and the white walls, and the high walls.
And I haven't mentioned the notion of social systems.
And I see the social systems, let's say,
Watson and the medical being the castle.
And then there's the smoke.
And I don't know how the, for example, let's say Watson
has cures for this efficient ways to do surgery, et cetera.
And the social system, i.e., let's say the medical system,
is with their gatekeepers, very protective,
wanting essentially money or protecting
their financial needs, et cetera.
So maybe this is something in the future where
AI can, people with any AI, have to kind of deal
with social systems, which is probably a whole different level
beyond the issue of ethics, et cetera.
Yeah.
Definitely.
I mean, to understand the dynamics within social systems
in order to support them, but also
know to be aware of what's the, I mean,
to understand what's the best way to interact
with the social system.
Of course, we don't want AI to be manipulating people
or make them believe something.
Well, let's say there's a type of surgery that Watson says,
this is best surgery.
Yeah.
The social system in the same medical says, well, that's good.
But there are other surgeries, which essentially
helps the finances of the surgeon.
And you're going to have this kind of play that goes on.
So I guess the deeper question I would say is,
what the role of the patient come in
in dealing with the health?
Hopefully, yes.
Hopefully the AI system could be not just
the support or like a consultant for the doctor,
but it could also be something that gives,
I mean, the possibility of engaging with all the stakeholders,
the patient as well.
And so in that.
So you can empower the patient to deal with the
power of the social system.
I would guess, look, right now there
is the robotic surgery for prostate cancer.
But in order to do the robotic surgery, you need a surgeon.
It seems to me, over time, if the robot could
do the surgery without the surgeon,
you would start not having urological surgeons
specializing in prostate cancer.
They would just, over time, drop out.
I'm just a very personal experience.
I had to try and find a surgeon to do with some,
none of these surgery.
I have gone to six surgeons, finally found the seventh,
who wanted to do a more traditional surgery.
This is over a two-year period.
So when you say, over time, I'm talking about a system.
And the system, to change the system,
is, I think, a variable that I'm soon somewhere along the line.
You're going to meet up with, the robotics
is fine by the system that's based on different values.
Then maybe you'll talk to your robot or what's
in to deal with.
There's going to be a gap or a conflict.
That's all I'm suggesting.
There's another aspect implicit in what you're saying,
which is the rise of automation in general.
And AI is going to probably accelerate that and feed
that process.
And when you talk about the rise of automation, higher,
higher level, either robotic or AI-assisted technologies,
interacting with a system that has certain established
patterns for humans to be carrying out certain functions
and livelihoods are built on that.
Mm-hmm.
One of the, it seems that one of the very hard
to avoid pathways with rising automation
is ever increasing unemployment.
How are people going to react when Google self-driving trucks
and cars, millions of people immediately out
of unemployment, taxi drivers, everybody else?
And now start imagining AI, this fundamentally
beneficial, well-intended technology spreading
throughout various aspects of our economy.
And you have half the population no longer able to be employed.
That's a different, it's a broader version of the case.
Well, the surgery was making half a million dollars a year.
Well, now we're making $100,000 a year.
And his professions, like the craft union,
the craft, not unions, during the 18th century,
they're going to put a barrier.
Is that all?
Well, you end up with one surgeon making the same amount
as that surgeon makes today.
But he wants to do the surgery.
All the others will be out there.
In my case, this surgery may be made $1,000.
The other surgery, they were going to get $5,000.
So they didn't want to hear about this surgery.
Well, here's how I would see your scenario playing out
potentially.
The surgeon may have some opinion and have some recommendations.
The surgeon might be assisted by some AI agent looking
at things from the surgeon's perspective.
The patient could listen to the surgeon,
maybe go to a couple of surgeons,
and have their own AI advisor saying, well,
here are the trade-offs.
You could do this surgery.
It's only $2,000.
And your new leg will last you five years.
Here's a surgery for $10,000.
And probably it will last a good 15 to 20 years.
It's up to you to make that choice.
And then the surgery itself, as an embodied AI,
a robotic surgeon might assist the surgeon
or do the surgery all by itself.
There would be decisions to make during the course of that.
So there are all sorts of different levels at which
embodied AI could play a role in the scenario that you don't want.
If you deal with the social system,
say the medical social system, to create change
is going to be very difficult.
I have had this experience.
So A surgeon goes to B surgery, and then goes to B surgeon.
B surgeon won't see me because A surgeon says,
this guy is asking something that we won't do.
And he goes to C surgeon.
So I'm trying to say that a social system built
into certain professions.
It's like the old guilt.
And they want to protect themselves.
So somewhere in the line, you're going to interface them
theoretically anyway.
And these are going to be the issues.
When you affect people's money, especially
really ingrained groups of people, like let's say.
Thank you.
We have to have other questions also.
Thank you.
I have a lot of questions, but I would like to ask too.
First, I heard a couple of years ago,
I heard about cognitive problems, but not from the development
developed not in IBM, but with genetic engineering.
And it was from a guy who was talking about that day,
for example, good old human eyes on mice.
And next stage of their experiments
is like making cognitive buildings or rooms.
And my question is, what do you think?
Should we stop to be afraid of the domination of robots
and start to be afraid of the domination of our buildings
and our like that day, control and demonet world
and devour us and kill us?
What should we do?
It's just a question.
I mean, I don't see there is really
a sharp boundary between a robot and the building.
I mean, with the internet of things,
everything will be connected.
Our fridge, our TV, our car, the traffic lights,
they will all be communicated with each other
to help us live better.
But not be afraid.
You shouldn't be afraid.
I mean, we have to make sure that this thing is built in a way
that is not harmless.
So they know that they're just building and it goes by itself
that there are no undesired effects.
But I think that we have to work hard in making it
in a way that is going to be helpful.
So not fear, but calm awareness.
Yes.
Thank you.
Maybe we can go ahead and ask the second question after.
Hi.
So I think I'm going to get a question,
but I'll try to keep it brief.
And talking about how the end goal of artificial intelligence
is to create a system that has agency.
But in the short term, we're talking
about building systems that have things that are used as tools
to an end.
So in the meantime, before we reach
some sort of human level consciousness,
you're probably going to be working, at least at IBM,
to design products that use artificial intelligence
and are bounded and embodied to some degree
by whatever it is that they are placed into.
So I guess what I'm kind of curious about
is if the end goal is a system that
has general principles for understanding
and applying to the world as a whole,
how the different kinds of embodiment
are going to affect an artificial intelligence system.
Like, for example, if you put one in a car,
how will that shape the way that the artificial intelligence
looks versus if you have one that's
like a maid in a house as you were talking about or a room?
Because surely that would have some impact
on the way that the system thinks.
And I don't know.
Yeah, cogitates.
Yeah, definitely the kind of embodiment
is going to be very, is going to impact on the way
the system will learn over time.
Because it will also be taped, how
it will interact with the environment, with the humans.
And also, even the kind of, to go back
to this ethical concerns that you
may have with the companion robot for elderly people,
with a self-driving car, with a cognitive room
to make a highly decision are very different.
So it's not clear to me, at least,
the relationship between the kind of embodiment that we choose.
And how to build in the best way the software that is
going to be, and the behavior of that machine.
Embodied AI that we build and apply in different environments,
they will be different.
I think a lot of the components will be similar,
but they'll be woven together.
I think we'll also have a common architecture,
but still the end product will be different in different cases.
And I think there is a lot of engineering and design
to be done that takes into account
how humans use these technologies.
And we do have a number of people who
are concerned with this aspect.
To study, we design these tools with an awareness
of the way humans are and the way humans like to use things.
We build the tools.
We probably don't get them right.
And we iterate until we find that they are indeed useful.
Toward trying to further humanize AI,
is making the interaction phase better.
Do you know of any projects, research,
and if serious kind on three things, first software,
trying to write really good jazz, given a theme?
Is that working on?
Secondly, trying to write a sonnet, given a theme,
and then on hardware, trying to juggle five balls.
Is there any serious progress work on that?
OK, so on the music I know of projects,
for example, Sony as a research center in Paris,
where they actually are building original songs
in the style of some bosa nova or jazz or whatever.
And they are trying to use AI techniques
to make sure that the song is original enough,
but is recognized to be according to a certain style.
Like very recently, they released the new song
in the style of the Beatles.
And I think I listened to it.
And yeah, I mean, if you didn't know the Beatles,
that it was not the Beatles song, you could say, yeah,
maybe it's not one of the best ones,
but I think it was a pretty reasonable new song.
And jazz as well.
So you may want to look at what they do there,
because it's really interesting stuff.
And the AI techniques are there to make sure
that the song is original while satisfying some constraints
that you need to satisfy, for example, in jazz
or in the different styles that you may want to use.
And so in that case, I think there
are these and other people that are working on that.
And in general, there are people working on making
AI apply to creative tasks.
They're not just songs or sonnets.
Like even doing portrait.
I've seen AI is doing portraits.
And that was very impressive.
Like it was a room full of robotic arms
that were looking at one human.
They were all looking at one human.
They were making a portrait with the pencil of that human.
And all these robotic arms, they were all,
each one was doing a different portrait.
That was impressive.
It was a very nice portrait.
But each one was different from the other one.
So there was some sort of creativity.
So there are people working on that.
But I see those technologies as components.
Being able to compose good jazz or Brandon
Berg and Sherrod in number seven.
I think, to me, that's a component that's not necessarily
an instance of cognition.
But very clever work on statistical characterizations
of music and then regeneration certainly
can be components of an embodied AI.
With regard to juggling, I don't know, particularly there.
I do know that there have been examples
of balancing the inverted pendulum, like that.
If the robot capable of juggling five balls
doesn't exist today, I don't see any real barrier
to being able to accomplish that.
Sure, you know Ron Graham.
Yes.
He did five.
He thinks it's just about impossible for a robot.
Who knows?
I think Ron Graham was capable of doing five.
Yes.
He can do it, but also balance on one hand.
But I have no concern about robots being
able to get there in the reasonable future.
Thank you.
Thank you.
Go ahead.
So before you were talking about how the best way to develop
AI might be to almost raise it as a child
or to teach it so that it can make
a lot of logical assumptions that it has a background.
I'm curious, at that point, wouldn't you
be concerned about the AI developing the same biases
that you were originally trying to avoid?
Presumably, you would be needing it
to develop a similar set of a sum of the data
you would then have to educate it that certain biases are not
socially acceptable.
Socialization is the accumulation of all this background
knowledge that's not only factual.
The floor doesn't come up and invite you,
but it's also normative in the sense
that you should do this under these circumstances,
but not in that context.
And so it would develop by what we could call what
it's developing biases.
But that's the necessary texture of that background knowledge
base that it needs to operate intelligently in the world.
Yeah, I think these biases, it's possible that a robot being
taught to live in this world might develop cognitive biases.
They'd be the same set of cognitive biases that
have been observed in humans.
No, it might be all the things that's
taken from ours.
I would hope that we would be able to correct those things
and get it to think in a less biased way.
But you're right that I think humans
that we develop short cuts.
There's a certain way our brains are wired.
These biases, I think, are a form of heuristic
that work in a lot of cases, but not in all cases,
and they sometimes get us into trouble.
So these embodied AIs may well develop something like this.
So some biases are just involved sensitivity
to background frequencies.
So here's an experiment that shows this.
So subjects were asked to judge the height of various people,
standing next to a standard object in a college campus.
And the people were men and women,
where the men and women were unknown to the subjects.
There were matched pairs.
That is, for every six foot man, there
was also a six foot woman.
For every five foot one, there was a five foot man.
And there was a bias towards judging the men to be taller.
And that is because of sensitivity to background frequencies.
Now, if you want your, of course,
the trouble with sensitive to background frequencies
is they can intrude in an unjust way.
But if you want your AI to be sensitive to background
frequencies, you will have to do something
about it in the way that we try to do something
about human judgment.
But wouldn't you say also that the AI, in order
to be properly functional, has to have those sensitivity
to background frequencies?
Yes.
It has to generalize.
Yeah.
It may categories.
I would hope we could build into it a much better
facility for dealing with probabilities
and conditional probabilities than we possess.
Yeah.
Great.
If we say that humans are still evolving,
and that human evolution is linked with machines and AI.
And kind of in the same way that when photography came along
and free painting to do other things,
then represent the world.
So it seems like AI has the capacity,
either to stifle or to enhance this human evolution.
And maybe some of the ways in which we could evolve
are toward a greater sense of personal fulfillment,
artistic creativity, lots of things.
So is anyone looking at that?
I mean, if you're telling me my preferences,
or you're notifying me every time I'm daydreaming,
that going to stop.
That is going to happen.
We are in a coevolutionary relationship
with the technologies we develop.
And it's going to happen in this case as well.
I think in small ways, we can start with small ways
to answer the question of is it happening now,
or is it soon to happen?
I think you can look at cases like assistance
for the elderly that could be developed,
where imagine you have an AI-enabled walker that
helps grandma understand that she needs to lift her feet
a little bit more.
And by doing that, she might learn a better gate,
such that maybe she won't need that walker anymore,
at least for a while.
So there are ways that AI can train us
and help us in ways that ironically make us less dependent
on this assistance.
That's just a small thing.
But I think imagine that we used to be a lot of people
used to be very good at riding horses.
And now I think that has atrophied,
but there's probably something changing in our brain
that distinguishes us from people
from the 1800s, where we're better drivers than they would
have been.
There's something that has changed in us.
And I think this is happening in small ways already
and might happen in very large ways,
maybe more explicitly as we start to more explicitly implant
devices in ourselves that are designed
to enhance recognition.
OK, my question is, have there been any experiments long term?
Have there been any successful or not experiments
with long term training of artificial embodied beings?
AIs, embodied AIs, like long term training,
like broader intelligence and just focusing
about a billion cases just to learn it as a very specific
task.
Well, there are some people that work on more general AI,
rather than narrow and specific to the task.
Like, maybe aware of these people
at this company called DeepMind, which is part of Google.
And they have this algorithm that can
learn how to play many different games on the screen.
Just by looking at how these games are played,
but just by looking at people playing these games.
And so you can learn not just one game only,
but many different ones, even with different rules,
different score functions, you know,
accumulate the score, the game, the win, and so on.
And so without communicating the rules of the games,
these machine learns just by observing many different ones.
And they think that that could be one, of course,
a game very specific.
Just games is not real life.
It's not a very realistic scenario.
But they claim that that could be a way
to build a machine that can learn to do many different things
that work according to different rules.
But there are also other people that,
I mean, whose goal is to work more generally?
I rather than a specific one.
But this is when they came to mind.
I mean, that they are, you know, working hard in expanding that.
Only only four minutes.
I just want to be more specific because it was not exactly
my question.
I mean, I know people have like been having
champs from babies and they train them like 15 years.
And my question is more like if there has been experiments
successful or not, like training AI is for 15 years
from nothing to something.
15 years ago, AI was very different.
The world was very different.
I mean, AI just started not long ago, just 50 years ago.
So I think that I'm not aware of this long training,
you know, experiments.
I don't think there have been some attempts
to learn some simple tasks with computer systems that
are modeled after the brain.
I remember some work that was done at IBM probably 10 years
ago now that wired together a bunch of pseudo neurons
into something akin to the many columns that
are in one part of the brain.
And I think they were trying to teach it to just like to train
a visual system.
Can you recognize something?
Just here's a bunch of circuitry.
And I'm going to put a bunch of images in front of you
and can you learn it?
I don't know how far it got.
But I know that there have been attempts in other research
institutions as well to do this.
But I don't know what was the outcome of these.
Thank you.
So one point I haven't heard raised
has to with the tendency of goals to be redefined
as a function of the available solutions
rather than the other way around.
So for example, Facebook has redefined the notion of friendship.
And I'm not convinced that it's an improvement
on the earlier version.
And for that reason, it's important to me,
it seems that the question of democracy be addressed.
And what I've been hearing about the likelihood of half
of all employment being eliminated.
And the question is, are the people who
are likely to be affected by that actually being consulted?
And is this decision to be made purely
on the basis of the availability of enormous resources
on the part of the corporations, Silicon Valley, these five
corporations that were mentioned and the ones that were not?
I'm my colleagues at Stanford tell me
they overhear conversations in the coffee shops.
And people are very, very seriously
discussing what they're going to be doing with all these unemployed
people.
And they feel concerned about it.
What I'm more concerned about, I'm more concerned about the absence
of restraint on the part of the people who exercise this power
and who have these resources rather than the likelihood
that machines will get out of control.
Yeah, I think there is a lot of discussion also
in terms of regulations of AI, possible regulation.
And I think that, for example, a few days ago, the White House
released a very interesting document on the future of AI
and strategic directions on how to, with the goal of facilitating
the good development of AI and possibly
mitigating the undesired consequences.
And I think that, so even last week, I
was speaking at the European Parliament.
And people there are very concerned about specific issues
about AI, like they apply the C ownership, they said that.
But they also are concerned about to understand
how to regulate this very powerful technology
in a way that it does not stop research and beneficial
advancement, but also it addresses these other concerns,
like, for example, the impact on the workforce.
Well, the European Parliament is not
a notably democratic body in the European Union as a whole.
And the question is, where is the public that
is going to be most profoundly affected
by taking part in the decisions?
I think there are some economists
who are thinking about the impact of AI
on the economy and displacement and the like.
I think Eric Brynjolfsson is one such person at MIT.
And there are others.
And I think it is important to explore these questions.
I don't know if we have good answers right now,
but it's one of those things that we
have to be conscious of and think about
and integrate into our thinking.
One historical example is in the 1930s, Gandhi's campaign
to resist the industrial encroachment.
