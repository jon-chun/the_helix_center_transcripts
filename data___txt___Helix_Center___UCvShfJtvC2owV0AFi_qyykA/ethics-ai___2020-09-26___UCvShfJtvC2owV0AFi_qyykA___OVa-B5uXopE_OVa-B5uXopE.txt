Good afternoon, everyone. Welcome to the Hill Center's very first Zoom roundtable. Of course,
you all appreciate we've been forced online by the ongoing coronapandemic, and we're looking forward
soon to having ourselves back in our normal venue. But until then, we're adapting as best we can.
And here we are with our first fall 2020 roundtable on ethics and AI. I'm joined here with the
Hill Executive Director Ed Nursesian. I'm Jerry Herrwith, the Associate Director. And today we have an
esteemed panel of experts. And I'll read your bios to you in just a moment, but we're looking
forward to having a really wonderful, robust conversation about this very important topic.
Let me just say a word about some of our participants. Our sort of acting moderate today is Brandon
Fittleson. And Brandon is a distinguished professor of philosophy at Northeastern University.
Before teaching at Northeastern Brandon had held teaching positions at Rutgers
University of California Berkeley, San Jose State and Stanford, and visiting positions at the Munich
Center for mathematical philosophy. Gabrielle Johnson is an assistant professor of philosophy
at Claremont McKenna College. Before joining Claremont McKenna, she was a Bursauf faculty fellow at NYU
affiliated with the Center for Mind, Brain and Consciousness. She works primarily in philosophy
of psychology, philosophy of cognitive science, philosophy of science, and philosophy of technology.
Her projects explore the nature and structure of social bias as it occurs in computational systems,
including the visual perceptual system, socio cognitive systems, scientific inference,
and predictive models and machine learning programs.
Rhaed Ghani is a distinguished career professor in the machine learning department and the
Heinz College of Public Policy at Carnegie Mellon University. Rhaed is a reformed computer scientist
and a one of these social scientists. But most mostly just wants to increase the use of large scale
AI machine learning data science in collaboratively solving large public policies and social challenges
in a fair and equitable manner. Rhaed works with the government and nonprofits across policy areas
including health, criminal justice, education, public safety, economic development, and urban
infrastructure. Rhaed is also passionate about teaching and practical data and science and started
the data science for social good and fellowship. Tracey Mears is the Walton Hale Hamilton professor
and a founding director of the Justice Collaborative at Yale Law School. Before joining the faculty
at Yale, she was a professor at the University of Chicago Law School from 1995 to 2007, serving
as Max Pam Professor and Director of the Center for Studies and Criminal Justice. She was the first
African American woman to be granted tenure at both law schools. Professor Mears is a nationally
recognized expert on policing in urban communities. Her research focuses on understanding how members
of the public think about their relationships with legal authorities such as police, prosecutors,
and judges. Tina Eliasi-Radd is an associate professor of computer science at Northeastern
University in Boston, Massachusetts. She is also a core faculty member at Northeastern University's
Network Science Institute. Prior to joining Northeastern, Tina was an associate professor of computer
science at Rutgers University. And before that, she was a member of the Technical Staff and
Principal Investigator at Lawrence Livermore National Laboratory. Tina earned her PhD in
computer science at the University of Wisconsin-Madison. Her research is rooted in data mining and machine
learning and spans theory algorithms and applications of big data from networked representations of
physical and social phenomena. She's had over 100 peer review publications, including a few
best papers and best paper runner-up awards, and has given over 200 invited talks and 14 tutorials.
So with all that, I'm handing it over now to Brandon Fittleson, who will get
a discussion underway. Thanks, Jerry. I just want to start by saying thanks to both Ed and Jerry
and to the Hillock Center. It's always a pleasure to do these round tables. I hope we can get back
to doing them in person in New York City next year. And I want to welcome everybody both on the
webinar here and also on YouTube in the live stream. I want to remind people that we, you can start
putting questions into the Q&A tab and zoom if you're on zoom, or you can write in some comments
and questions in the live stream on YouTube. And those are going to be compiled throughout the
course of the first part of the round table. And then after we get done with this first part,
which I'll say a little bit more in a second, we're going to have a Q&A at the end. So feel free to
start entering questions and comments as you have them. All right, so the plan for today, such as it
is, is roughly three parts, about 30 minutes each, where I'm going to ask the panel about first
problems and pitfalls pertaining to the current use of AI technology and some of the moral problems
that it currently faces and its applications now. And then we'll move on to potential solutions
to some of those problems. And finally, I want to end on an up note. I want to end on a positive
note here if we can, to say something about how we might harness these powerful technologies
for good, for morally good outcomes in the future. All right, without further ado,
starting with pitfalls and problems, I'm going to start, I'm going to ask Tracy to go ahead and tell
us some of her thoughts about pitfalls and problems that she's seen and in her work, her work, she's
addressing. Tracy? Good afternoon, everybody. As I was listening to the bios of my co-panelists,
I thought, wow, one of these things is not like the other. And that would be me. Most of my work
has focused on thinking about problems in the criminal legal system, although lately,
my center, the Justice Collaboratory has branched out, I think, about problems of social media governance,
which one might think is more directly related to what we're doing. But in the spirit of getting
as many things on the table as possible, and at the beginning, especially in terms of articulating
problems, I guess I would say, when I think about AI, big data, machine learning and automation,
and in the context of criminal legal processing, we're faced with some serious issues about the
difference between what we think about the fairness of how humans make decisions about outcomes,
especially keenal outcomes for individuals, and how we think about machines doing this. There are
many people who think that machines engaging in these processes make it fairer because they think
that machines themselves aren't impacted by bias. That may or may not be true. It depends on,
you know, how you think about how machines learn. But certainly, the crudest form of how this works
ultimately or initially depends on how the machines are programmed. I'm not saying anything that anyone
doesn't know already about thinking about risk assessments and who is denied bail and the extent
to which one can make those algorithms fair. Just one point on that, and then I'll say something
about social media. One point is to say, to the extent that people point to all of those problems
with machine learning, again, focused on the issues inherent in having a human program,
these machines, I think folks too often forget, of course, that the human beings making these
decisions, which would be the alternative, it's always compared to what, of course, have algorithms
in their head. And they always do. It's always when I'm at conferences like this, not this, but
conferences discussing bail reform and risk assessment, I will always say, well, what about the algorithm
and the judge's head? Because the critic always seems to posit the human alternative as better.
And it might be that they both just to use my kids' terms suck. And so the question is,
which sucks the least and how we think about which sucks the least, which gets to my second
point around social media governance. One way in which humans may suck less is the ways in which
we can be forced in a sense to explain what we do. There's ways in which machines and algorithms
in particular are especially inscrutable. Even if we can make them transparent, they're still
inscrutable in all the ways in which humans just have trouble at some level with the capacity
making the assessments, even if I can explain to you in words what the algorithm is doing.
And so in my social media work, and then I'll end with this because Brandon said I wasn't supposed
to talk much more than about five minutes. So it's table setting. One of the things we've tried to
do at the Collaboratory and the work on social media governance is to try to apply learnings from
the social psychology of procedural justice and legitimacy in the real world, mostly in the criminal
legal system context to how platforms for online interaction manage problems, disputes,
content moderation, and so on. And we've been relatively successful working with a number of
platforms to have them understand how they can use their data science to infuse that science
with these ideas that procedural justice points to. I'll just say one more thing and we'll wait
for the next session, but the key thing to understand about how procedural justice informs this is
that it focuses much more on process than it focuses on outcome. So to the extent that when
people are trying to make these outcomes fair through machine learning, the procedural justice
approach would say don't worry about the outcome as much. Focus on the process by which you reach
those outcomes and then the key would be is there a way in which we can have AI focus on these
processes and ways that are transparent and sit transparent and salient to humans.
Thanks so much Tracy. I think a natural this is a natural segue into asking Gabby to speak
because Gabby's research really is about the similarities and differences between
biases that occur in human judgment and and automatic biases people. So Gabby take it away.
Great thanks Brandon. Yeah and thanks for the great intro Tracy. So as Brandon said I'm interested
in cases of bias as they manifest both for human decision makers and for artificial decision makers.
So insofar as I'm highlighting various pitfalls and problems now, I guess my hobby
horse is the biases that manifest in these decision making procedures. And more so than with the
human decision making procedures I think the issue is really bias under the guise of neutrality or
scientific objectivity. And so I think every computer scientist has taught this motto of garbage
and garbage out. The computer decision maker is only ever as good as the data going into it.
And so I try to focus my area of research on two different focal points. One is on the data
going into the decision making procedure and how various systematic patterns of oppression
could be encoded in that data. But the other decision or focal point for me is on the decision
points of the algorithmic designer. And I think both allow opportunities for bias to creep into
what seem like objective or impartial decision making patterns. So I think that uncovering the
theory behind the data is something that philosophers of science have been worried about for a long
time and that various methodologies that are apparent in philosophy science could be useful in
the domain of machine learning algorithms. And so part of what I'm trying to do is to bring a better
theory to understanding both the patterns that are encoded in the data but also the theory of how
we take objective scientific inference to occur more generally. Which is of course what we're trying
to model in the case of algorithmic decision making. So just how human values get encoded in
basically every single step of that procedure or that model is something that I think is that the
four of those concerned with ethical AI. I also think as Tracy said, another issue is the lack
of transparency. So we have a lack of transparency on two fronts. One is that machine learning
programs are proprietary as we all know. And so because they're commercial and because they're
commercial programs, it seems like they are not going to be open to public scrutiny. And even if
they were the data that they're operating on would also cause issues of privacy concerns.
So that their proprietary is one issue. But the second one, and I think this is the one that
Tracy was intimating, is that they likely manifest what we call black box algorithms. That is even
if we could as it were appear under the hood and take a closer look, it's not obvious that even a
good computer scientist or philosopher would be able to understand exactly how the program is
operating. So the lack of transparency on those two fronts I think makes and compounds these issues
of bias. And then the third thing that I'll just say as a problem or pitfall is just the speed and
ubiquity with which artificial intelligence and machine learning technology has proliferated way
further than we anticipated at the rate that it has. And so it has been allowed to grow basically
unchecked and unfettered throughout many aspects of human lives. And so I think that the lack of
accountability or mechanisms for ethical responsibility is a big issue for our current technology industry.
And so one of the major pitfalls that those of us interested in ethical AI need to approach and discuss.
Thanks Gabby. That was great. So having heard from the legal and the philosophical
sides of this, I want to turn out to sort of the more practical, the more of the practitioner side.
So right, maybe you could share some of your thoughts from the more practical sides of one
sort of on the ground using the tools doing the stuff.
Yeah, so I think before we sort of, you know, before at least we start talking about all the
horrible things, you know, I think the reason we're having this conversation, I'm assuming,
is because we think there is some good that can be done. Right, that we have a lot of issues in
this world and humans have been doing trying to fix these issues for a very long time, but
they've created these issues. Right, it didn't just happen. And so I think if the premise is,
there is some potential in using computers and data and evidence to improve policies.
And there have been a lot of evidence about it. We've done a lot of good things that
these are the risks that we're talking about. And these risks are only we're talking about
because there is that potential for good. Otherwise, it would be very easy. Let's just stop, not do
any of these things. And I think that comparison to sort of trace these points is, you know, it has
to always be compared to what? Right, it's not compared to the perfection that we all desire,
but compared to what humans do today. And I think that's a point that I think we need to
keep repeating, which is why I'm repeating it, because it gets lost in practice very often,
where the critics, you know, if you look at any criticism of these types of tools, it's often,
you know, these machines, they are racist and they don't justify what they do. And you can
replace the machines with humans and they would be the same paragraph. So I think, but in my mind,
the bigger, so given that the risks are pretty much the same as they have been with humans all
this time, any human decisions that are made have had these issues, to me, the incremental risk that
comes up is partially what sort of, Gabrielle talked about, of how fast these tools are spreading,
but then also, unfortunately, the consistency in these tools, right? The humans, the hope is that,
you know, yeah, judges are, a lot of them are racist, but and they all make sort of horrible
decisions generally, but there's variants and each individual decision combined, you know,
it results in overall racism. We've seen that, but every single decision may not be racist,
but if these systems, let's say for criminal justice or allocating human services or health
services, if there are three such systems in the world that are being used, that variance goes away.
And if these three systems are bad, then there is no hope for us to recover from that. So I think
that's for me, it is an incremental risk, apart from what everything we've talked about, that makes
it worth really focusing on and thinking through, second thing is again, what, again, both like
Gabrielle and Tracy mentioned, was there is sort of this fake sense of trust, is, oh, because it's
based on data, it must be right, right? And yes, you know, the garbage in, garbage out, but not
garbage in doesn't mean not garbage out, right? You can put a lot of good data in, but the system
can still result in horrible outputs, but more importantly, I think at least the work I do is
less on autonomous decision making, if any sort, it's assisting humans in making better decisions.
So you could have a perfectly fair AI system resulting in horrible outcomes, because the
human that takes those recommendations does horrible things, or vice versa. A horribly biased AI system
given to a human who understands how it works and can use it to create more equitable outcomes.
And so I think that there is kind of this fake trust in data where data is never objective,
there is no such thing as good data, it's whatever sensors we use to collect that information and
how we use it. And the other risk I think is to connect to that is often people may use these
systems an excuse to do what they would have done anyway. And now it gives them another
evidence that see, this is what the computer told me to do. And there have been many cases again in
the criminal justice system of judges claiming, well, I didn't make this decision, the computer
told me to do that. Well, then why do we have you? Why not just have the computer take over?
So I think those are kind of some initial thoughts, and I can go into those later more.
Fantastic. That brings us to our final panelist, Tina, who I know has thought a lot about how
humans interact with systems, especially when they serve as assistants, and also on how to educate
people so they become more technically literate. Tina, you want to take it?
Yes, thank you very much. And one update to my bio, I was promoted to full professor in July,
even though it sounds like 100 years ago now. But thank you, thank you. So from my perspective,
I teach a class called Algorithms that effect lives for freshmen, for incoming freshmen,
and I don't require any math or programming. And my whole goal for them is should we do this? So
this goes back to where Rahid was saying, just because we have the technology and somebody won't
pay for it, should we do it? As citizens of this country, should we deploy these algorithms
for x, y, z, just because we can do it, and somebody's willing to pay for it? Maybe not, right? So we
should really think about that as one aspect of it. The other aspect of it is what Rahid touched in
is that at least from the computer science perspective, there's this notion that data comes from gods.
And when you talk to, for example, physicists or more natural scientists, no, there's a distribution,
right? And so which part of this distribution did you actually see? It would be good, for example,
to actually say, you know what, this algorithm only works on white men who are between 25 and 35.
But computer scientists typically are not that honest. We're all about it's a master algorithm,
right? My algorithm is the best algorithm on the planet, at least for five minutes, until the
next paper is published, right? And knocks it down. So these are some of the things that,
both in terms of basically putting doubt into the human in terms of that this algorithm is
fabulous, and also making them think that there are certain scenarios in which the algorithm is
really being treated as an expert witness, right? It's as if it's an expert witness, but it's not
really being treated like an expert witness, right? Where like you can actually like audit the algorithm
and ask you different questions. It seems like it's a one way street from the algorithm saying,
you know, I think Tina's going to default on this loan, right? And not being able to ask,
well, why do you think Tina's going to default on this loan, right? What if Tina was a white male,
right? And had the royal flush of hands, would Tina still default on the loan, right? What if Tina's
zip code was not so on and so forth, right? So there's some of these aspects of it, but I think in
general, just thinking about just because we can do it, and somebody is willing to pay for it,
should we do it? Is it good for our society, right? So one of the aspects is automation, for example,
and there is an element in terms of AI and ethics, where robots in factories are taking away jobs,
maybe you can say, oh, that's productivity growth, and that's a good thing. But then on the other
hand, what are you going to do with all these people who are unemployed? Do we not care about them?
Or for example, should we tax the companies who bring in robots into the factories and use that
money to re-educate our population, right? And so there's some of these kinds of things that
I've been teaching the freshmen, and they're amazing. They are amazing. And you know, so I'm
hopeful for the future because they ask the right questions, and on that I'll stop.
Well, that's great because that, see, I wanted this to be an increasing amount of optimism.
So I'd like the trend, and I'm just going to reverse the order. So Tina, keep going.
You're on a roll here. Solutions maybe. What are some ideas for how to solve or how to address
some of the extant problems? I think you're already addressing some of that, but maybe you could just
take the next segment, and then we'll just go and reverse order. All right? Go for it.
Yeah. I think some of it is a citizenry that actually knows what's happening. Like, for example,
a lot of my students ask, well, is there any way for me to figure out how much Google knows about
me, right? Then perhaps I can make an informed decision as to whether I want to use all the
Google products, whether I want to allow Google in my life. Or for example, many of them don't
know about the privacy law that was passed in California, where you can go to these tech companies
and ask them about your data. Now, of course, you can pass laws, but they may not have teeth,
right? So there's that as well. But there's also aspects of just having algorithms have labels on
them the same way prescription drugs have labels on them, right? Because these algorithms adversely
affect different populations. So I need to know the risks and benefits and who the algorithm was
for and what the algorithm audited and can I audit it? What are the privacy risks and rights?
So my colleague Patricia Williams always says, what the hell does consent mean here when I consent
to Apple to use their music, their iTunes? When am I consenting to exactly, right? So there's this
kind of stuff that we need to unpack for our population so that they know what they're getting
themselves into. I think that's one of the things that we don't know. And we can do that, right? We
do have the tools to do that. And lastly, I think this was something that was mentioned before,
which is you can't just lead the algorithm design to white men, because then they just think about
like other white men. So you really need representatives from other slices of the population to be in
the room to have this what's called human valued or human centric design to think about, well,
how would this operate on somebody like me, right? So those are some of the solutions and we can't
do them. It's just that, you know, why should I do it? If somebody's already paying me a lot of money
to just tell them, you know, Tina's bad and Brandon's good. That's easier for me.
Okay. But can I ask, but what about this issue where there's some value in a particular program
that you're invited to use? And it's so sexy and exciting and or and or useful. And it's
cheap to start up with. And then the implications and the, you know, the acceptance that you're
asked to grant, it's a lot of people don't take the time where they should have buy into it before
they really evaluate all the risk they're taking. What can we do about that? Is there something?
Yeah. So I think that actually gets to a very good question. And that gets into a bigger question
of what is the business model of these companies? You know, Google is an advertising company,
whether you want it or not, it is an advertising company. Should we change the business model of
them where you are willing to pay five dollars a month, right? Where you pay them five dollars a
month and they won't sell your data and they will stop being an advertising company. Now, when I
taught the same class in the spring, my students were like, I am not willing to pay five dollars
a month. Like I'm willing to pay twenty dollars for burrito, but I'm not willing to pay five dollars
a month. Now this semester, I have students who are like, no, I'm willing to pay five dollars a month
to Google so that like they don't sell my data and so on and so forth. Now, there's a bigger problem
in that it's not just Google that uses your data, right? Your credit card company is selling your data,
right? I'm sure you've heard these stories about like your friend may have moved from the
West Coast to the East Coast and Amazon knows where they have moved before you tell your parents.
It's because they're all these data brokerage, right, that collect and sell data. So it's not
just that you can like stock gap at one point, right? And this is also why you need basically a national
call, a set of rules and regulations with teeth again. Like for example, if the A still has teeth,
I think it's because our country is so litigious. Like people have to get sued. This goes back to,
I think it was Gabby that mentioned in terms of having somebody who you say it's Tina's fault,
right? I can put up a software out there and bring half the country down. I won't get sued.
Like I am not like, right? I am not liable, right? Microsoft is not liable for all the bugs that are
out there, right? In their Microsoft software. So there's some of that going on here as well,
but you ask a very good question and that touches on changing the business model, right?
I'm gonna reverse order here. Ray, you want to jump in on possible solutions or ways of
addressing some of the current pitfalls? Regulation. I guess that's the easy answer.
Connecting with Latina saying yes, we can change the business model. Yes, we can educate consumers.
And it takes time and it may not happen, but I think as you said, FDA, FTC, CFPB,
FEC, pick your favorite regulatory agency. And I think that's the solution,
except they're not ready yet. They don't know how to do it. They're not trained. They don't have
the expertise. And GAO, for example, a couple of weeks ago, shockingly, this country's
GAO had a two day event on how do we audit and govern AI systems. And it was actually pretty
reasonable. So I think that's one path. That's the only path. I think some of the things that I'm
working on and sort of anything or worth pushing is coming in from both angles from what Tracy
talked about, procedural, but also outcome focus, right? I mean, I think neither of them are enough.
Procedural is today, it's very hard to audit procedural things, right? It's just so complicated.
Outcomes are a little bit easier to audit. So that's a good starting point, but that's not a long
terminal. We need both. And the reason I say outcomes instead of procedural is at least in the AI world,
a lot of the focus in this area has been on fair algorithms or whatever that means, but that's
sort of this little tiny box in the middle. And then there's all this stuff that happens before
and all this stuff that happens after it. And if we just make those better, the world doesn't
change. For example, if you sort of have, again, let's say a system to recommend who should be
provided, prioritize for COVID testing or vaccines someday in the future. Well, you can have a fair
system, but then the outreach to do those vaccinations or testing is happening in English,
then it doesn't matter how fair your algorithm is, the system isn't fair. And in measuring
outcomes, there is really helpful, right, to kind of audit those things. The second thing is, I think
today, again, you can have perfect data, but there's still, you know, these systems optimize what
the people building the systems tell them to optimize for. It's not as this sort of magical
truth that they build. And we don't really teach those system developers to think about what to
optimize for. You know, it's like, oh, it's the best system is the one that best replicates the past.
Well, that's a horrible way of building these systems. And that's what accuracy often translates
to, which is efficiency. It's the more correct you are, the more efficient you are. But that's not
the framing we talk about in the policy world. There are trade offs and equity and effectiveness
and efficiency. So I think part of it is both training the people developing these systems
into these notions of there is no objective accuracy. It's not a trade off. It's your performance
measure has to include all of those things as opposed to just efficiency. I think the other thing
is that the point is, a lot of times, these systems are making, are forcing us to make some of these,
as Tina was saying, these human societal policy values explicit. You know, before, you know,
they were all implied in these systems, decisions were made inside people's heads or in political
speeches or, you know, and now if you have to build a system, I have to put these values as numbers
into an algorithm. And that's uncomfortable and painful. And again, we're not trained to have those
discussions, especially not with the people who are being affected by these types of systems.
Right. So it's not just, I'd say human centered is useful, but also culturally centered of the
culture that you're trying to build it for. And yeah, so I think, I think, thinking through all
these things, but I think in terms of moving forward, right, I think we have to expand the
existing regulatory environment in order to deal with these types of things. It's not. And rather
than having being kind of AI focused, they need to be kind of the focused on the area they're
regulating. So it's FTCM again, FDA and CFPB, and you know, all of those that have to kind of,
that are other than an AI regulatory body. I think we have to kind of create trainings for them in
tools and processes. And other thing is, I think we don't today have, so that's one, the second is
we need the same type of things for policymakers. Like right now, we don't have any tools and
trainings and guidelines for them to take all the these systems being produced, because for them,
the answer is either this is all magic and we're just going to use it, because it solves a problem.
Or shit, this is scary. I read this paper that says they're all biased. So I shouldn't use any
of this because again, humans are so wonderful in what they do today. And they also don't have
any way of sort of procuring these types of systems that are, you know, so we have to kind of have
procurement, procurement policies, trainings, policy, and then trainings for the people who are
building these systems to be able to have these conversations, have these discussions and
build these things that actually get to what we care about.
Thanks a lot, right. Gabby, what's your take? So I will,
yeah, redirect us away from the practical and applied back to the theoretical philosopher.
So I just completely second everything that Raeed and Tina said, I think
regulation and education are the way to go as far as applied safeguards. But another thing that
I think we need to do is to redirect theoretical attention toward what sorts of aims we should
be having. And one of the things that I focus on in my own work to just drill down for a second
is getting away from this idea of objectivity as an ideal. So not only is objectivity something
that we are incapable of achieving in a robust sense, but it's something that I think we shouldn't
even be aiming for. And so here, maybe I'll say some things that are controversial from a certain
sort of perspective, but I just want to be clear about what I'm saying. When I'm using the term bias,
I don't mean it in any normatively laid in sense. So I want to be normatively agnostic about whether
or not a bias is problematic either because it doesn't get us onto truth or it doesn't get us
on the justification, or because we think it's doing something morally problematic. So for me,
biases can be ethically good or bad, epistemologically good or bad. And so one of the things that we
recognize from coming at both human decision making and algorithmic decision making from a
naturalized perspective that is looking at how human beings actually make decisions is that we
need bias. And this is something that any computer scientist is going to say and then immediately
be confronted with like skeptical eyes. But it's something that we know just about how humans
operate in the world. So as philosophers of science, we often focus on what's called the problem of
underdetermination. That is any data set that you're given will vastly under determine the
possible hypotheses that you could come up with in light of that data. And I think if you're someone
who favors objectivity or impartiality overall, you'll notice that there's a problem here. If we
treated all of those many infinitely indefinitely many hypotheses equally, we would just be crippled
within decision. And so we wouldn't be able to make any decisions whatsoever. And so this is just
the proof of concept that objectivity in a strict sense is impossible. And so from one perspective,
if all implies can, we shouldn't even be aiming for it in that respect. But I think a more important
one, and again, this comes out of like a naturalized viewpoint that is how humans and algorithmic
decision making occurs, is that bias actually helps us. We know more not less when we have bias.
And so one of the things that I think is coming out of these more practical discussions about,
for example, so-called colorblind approaches or conceptions of fairness that involve not having
a marker for socially marginalized demographics and the actual features that you encode in your
algorithm. What all of those discussions are proving time and time again is that we're not able to
erase the markers of systemic injustice by stripping away features of our data. Those patterns are
so deeply ingrained in our environment that any program that we have that's intended to replicate
or find consistencies in that data will necessarily imbibe those same biases. It's just like the human
visual perceptual system encodes various biases like light comes from above. It's just how we,
as finite knowers, come to decisions on the basis of data. So I think that this redirection away from
impartiality in this robust sense will help us a lot. It's almost like if we thought of our
algorithms as coming in at the beginning of the game, then a level playing field would make sense.
But instead, we're coming in midway through a race for which some members of marginalized
demographics have been working uphill throughout most of the race. And then we're saying, okay,
from here on out, we're going to expect plateaus. And so we expect the level playing field.
It's like midway through the race is not the time to adopt impartiality. And so taking into
consideration the sorts of differences that have occurred up to this point will only help us better
create what I think of as a move away from equality in general to do equity. That is what sorts of
features or implementations do we have to adopt in order to give us a more playing field holistically.
Thanks Gabby. That really resonated with some of the stuff Ray was saying because
computer scientists will of course tell us that not only do we need bias if we want to learn,
we need variance too. And that speaks to Ray's point about how if you eliminate too much variance,
you're not going to be able to learn or do anything either or make decisions. So you need both bias
and variation. And those are definitely challenges given the current regime of a technology. That
brings us finally back to Tracy. The batting cleanup here on this topic Tracy. Yeah, batting cleanup.
So I actually wrote down a few notes of things I wanted to say after listening to Tina and Ray
Eid. And then Gabby went in kind of a different direction. And so when you're batting cleanup,
it's like, okay, how am I going to do this? So I think the way I'm going to do it is by making
four points, which are not necessarily like building on one another. So you should be thinking of
them as responding to different parts of the conversation that we heard so far. And you know,
hopefully the connections will be self-evident and it will be productive. So point number one,
I just want to note that when Tina was talking about Google and her students, she said something
about paying Google for your data. Think about that. Just that phrase. Or pay somebody for your
data. The point was is that you had to go to somebody else for your stuff. So to a lawyer,
that might sound strange that you would go to someone else for your stuff and actually even have to
pay them for your stuff. Because when you think about what property law is, by definition, it's
yours and you are entitled to it. And why would you pay somebody for something that was already yours
unless you had relinquished it in some other kind of transaction that just calls to mind
a relatively recent Supreme Court opinion, which some of you might be used to,
Carpenter. I might be aware of Carpenter in which Justice Gorsuch tries to solve some of the problems
around cell site location data. If you've read this opinion, you know that the justices were
all over the place in it. But one of the things that Justice Gorsuch tried to do was to
try to rely on these older concepts of property to solve these problems. I mentioned this because
I do want to say something about regulation in a second. So just that point for a little bit
of level setting. So then that does bring up the question if we think there are all these problems
that might be solved by regulation, what should regulation look like? Typically, when we're thinking
about regulation, for most people, ordinary folks, intuitively, regulation is about coming up with a
set of rules that you want other people to obey, which means that you have to have a theory about
compliance and the one that's typically available to most people is an idea that people will comply
with rules or the law because they fear the consequences of failing to do so. So then,
you know, your regulatory regime would be organized around punishment and would be
organized around identifying wrongdoers, going back to something Tina said, trying to figure out
blame worthiness in some sense, borrowing on concepts from criminal law. But I think,
or at least I hope that the conversation so far has revealed all the ways in which thinking about
that kind of regulatory structure doesn't work very well for this space. That doesn't mean that it
doesn't do something or even some things to address problems, problems that might just be about
identification of what's fair and unfair. But in the normal regulatory context, we're trying to
think about changing behavior wholesale in some way that is good, right? And, you know, usually
criminal law approaches and punishment regimes don't really help us get there. So, you know, you
might be thinking again about the alphabet soup of administrative type interventions,
which often have to do with getting entities to document a bunch of stuff. Like you can imagine,
for example, I think Andrew Selps, who's a law professor at UCLA, has made arguments about
requiring these companies to do a version of an environmental impact statement, you know,
just sort of becoming much more self-aware about the ways in which their products,
algorithms, and so on have particularly bad consequences for certain groups, right?
But that brings up the question then when we're talking about regulation about what authorities
are we even talking about in this context, right? So again, when we talk about regulation, at least
in this country, we typically think about what the state is doing vis-a-vis some other actor,
whether it's an individual actor or an entity. You can also imagine regulation being, that concept
being a little bit more capacious, like imagining if the, from a governmental perspective, it would
look like self-regulation, which a lot of people don't like, but it might be, depending on the
regulatory regime we adopt, something like requiring these entities to have certain kinds of rules
for themselves that could then be audited in certain ways, but outside of a punishment regime.
So a kind of a combination of rules towards moral serration because it won't necessarily
result in punishment. Hopefully that wasn't too confusing and I can say more as we talk about it.
But again, I'm motivated by the work that we've done in this social media context. So,
what have we done there? We have acknowledged the fact that in a lot of ways, that space is
unregulated in the typical sense, right? That they're not subject to the usual kinds of administrative
rules by the alphabet soup of administrative agencies. For good or ill, people have lots of
views about whether the FCC should be doing more so on. But take that as a given. You can
actually think about having these entities impose their own regulatory regimes in ways that are
transparent to users, consumers, whatever you want to call the people who are interacting
in that context. And you can also look to see what features you would want that regulation to have.
And so here's where I'm going to say a little bit more about, you know, my favorite regulatory
approach that depends on the social psychology of procedural justice. But the goal of that work is
to encourage people in a space to voluntarily comply with the rules that they set out, right?
Now, this view, this approach actually works in the real world too. In fact, it was developed
for the real world. It's a recognition of the fact that despite the fact that most people
seem to when they think about changing behavior, move immediately to crime-based deterrence
punishment regimes. The reality is most people do not obey the law in rules because they fear the
consequences of failing to do so. Most people obey the law at our rules because they agree with them,
period. But the regulatory problem is what you do in a context in which someone doesn't agree
with the rule or they think it's silly or inefficient or something. That's where the social
psychology of procedural justice comes in. And we know that people are more likely to conclude
that authorities slash rules are fair when four conditions obtain. First, when people have an
opportunity and a particular interaction to tell their side of the story or if we're talking about
articulation of rules, participate in the creation of those rules. And that's true even if that input
doesn't have any particular impact on an outcome that's subject to those rules. And there are limits
to that. But it's about having voice. That's what we call it. Second, people care a lot about being
treated with dignity and respect and being listened to. So the conversations that we were talking
about before, about transparency and the like have to do I think with these dignity concerns.
Third, people care a lot about being able to ascertain whether decisions are fair. This gets
to Gabby's point. I think about objectivity. I think what's interesting though about that is,
of course, people never know what is in fact objective. They're just looking for a dishebed
of objectivity. So they look for things they think are neutral. They look for factuality.
They look for explanations. That's where the explanation piece comes in. When you are explained,
when an outcome is explained to you, you are more likely to think that it is neutral and without bias,
even if that's not true. And then fourth, people care a lot about being able to trust the motives
of a decision maker. So this is obviously a huge problem for machines. How are you going to assess
the motives of the machine or artificial intelligence? You're going to go immediately to the programmer,
to the owner of the machine, to the sponsor of the project. That depends on these things,
to ascertain those things. We know finally, when all of those things happen and people are able to
to key in on those factors, we know that that folks are more likely to conclude that whatever
experience they've had or the law or the rule is they're more likely to follow that rule. They're
more likely to engage with the promulgator of the rule and they're more likely to cooperate
with the promulgator of that rule. So all this to say, if there's the possibility, if we're
talking about possible solutions, infusing these ideas into how we go about a regulatory structure,
I think is really incredibly important. So if I may follow up on something that Tracy said,
so right now, Google is providing a service and you are paying them with your data,
right? Hence, they're in the advertising business. They're in the attention economy, right? Zane
up to Fekki, who is a professor at UNC Chapel Hill, says it's like you're never hardcore enough for
YouTube. They want you on their site, right? And so part of a solution is to think about can we
change the business model of these companies? One is, okay, well, let's think of Google as
electricity, right? You pay for electricity, you pay them, they don't sell your data, right?
Because they need the infrastructure and all that to be for you to search the internet, right?
The web, the worldwide web. Now, there are actually three
approaches that people are talking about. One is the service model, right? Like electricity,
like cable, etc, that you pay them monthly. The other one is what Paul Roemer has been talking
about. Paul Roemer is a noted economist at NYU. He was one of the winners of the Nobel Prize in
2018, where he says, we will tax them. We will tax Google, right? If they make over some certain
number, we will tax them of profits. And then we will use that then for good for our society.
And then there's Jaren Lanier and Glenn Weil. Jaren Lanier is one of the father's virtual reality
and Glenn Weil is an economist where the, no, no, no, we'll keep the model as is, but Google will pay
you for however much money they make off of your data, right? Now, of course, you can have that
game, right? Any of these systems can be game the same way, like, for example, with a cable, right?
You could game the cable payments. But as part of it possible solution, and I saw in one of the Q
and A's, there was this thing about how people get hooked on the technology, is to think about,
well, how, what is a better business model as well, right? So not just regulation, but what is a
better business model. And they go hand in hand. So I just wanted to clarify that it's not so much
that you're paying Google to get your data. It's just that Google will agree to not sell your data
and for you paying them as a service. And in fact, there are certain other companies like Hulu,
for example, right? You pay them, you see less ads, right? That doesn't mean that they're not
selling your data, by the way. Everybody's selling your data. So rest assured, at this point,
everybody's selling your data. And you are just the data cloud as you walk by, right?
But Tina, I mean, yes, everybody's selling your data, but certain industries
have been better regulated, right? So the oldest industry selling data is all the consumer behavior
credit card. That industry is extremely regulated. They can only sell your data to market to you for
nothing else. I mean, it's the most, one of the worst things you can do with it. But still,
there are these rules that, oh, you want to do product development? Oh, we can't sell you the
data for that. And I think the data ownership world for the AI world is still so, well, it's not
new, but it's just hasn't been, it's like FPC not knowing how to regulate things on the internet,
when they know how to regulate things on the print and TV. So if Google and Uber self-driving cars
are training on pedestrians and taxpayer paid stop signs, and whose data are they training on?
So is the algorithm that they've built part owned by the people whose data they were using?
I think that's where I would put the legal profession right now on what about data ownership?
The laws, I think we need to catch up. So everybody's selling data, but I think
there are variations of that data, and there are better and worse models of consent. And every time
we eat, our data gets used, do we get notified? Are there presets that we can choose as opposed to,
and I think we just haven't caught up with that? Yeah, so I would completely agree with that.
And for example, in healthcare, right, there are a lot of rules and regulations in terms of,
you know, you getting access to let's say Tino's medical data. So yeah, it is, it is, you know,
wild, wild, last in terms of the data that you are trailing, or is trailing behind you on all the
apps that you use that you believe is good utility, right? So I am somebody that has like about 500
apps on my iPhone just to confuse them all. If I could, if I could just, this is great, if I could
just jump in and just, I'm seeing some connections here. So I think Tracy's remarks and also, Tino's,
there's, there's something that they have in common. If you change the incentive structure
in such a way that the users feel like they have more say in how everything works and is regulated,
and since they have more power in the process, then that plays into that social science,
social science, hit fake what we know about people in their group deliberations. Does that,
does that make sense, Tracy? Yes, sorry, I had my phone muted because somebody was calling and
I figured you didn't want to hear my phone ring. Oh, this is great. Gabby, you want to time in?
If you'd like to. Yeah, the two things I was going to add is just in so far as we're attempting to
theorize about various regulatory bodies that might transfer, not transfer well over to the
algorithmic or machine learning domain technology industry more generally. One of the issues that
came up in the discussions is just that the algorithms are proprietary. So one issue is the data on
which they're operating, but again, the actual algorithmic design is proprietary as well. And in
so far as we have regulatory bodies for proprietary recipes as it were, like the FDA is already
available. And so that we could have some, I think the main thing is just that we have something
external to the industry itself. And likewise, in scientific practice, we have institutional
review boards and it's a key function of those that they have individuals on the institutional
review boards who are divorced from the actual scientific practice itself. So that you just
have an external eye looking in on how things are operating. And so all of this just speaks back
to the importance of having a participatory voice in what's happening that the individuals who are
the subjects or victims of the order of the technology industry that they could have
a saying how things are being regulated. And so then this just shifts us back to the importance
of not just regulation, but also education. It's important that people understand exactly how
the algorithms are operating, what their data is being used for, what it could be used for,
how they're playing a role in everything in order for them to have an informed voice about how things
get regulated. Yeah, I think. Oh, sorry. Let me just ask a quick question. Didn't the EU pass
certain regulations about the use of data? And that every time you go on a site, they ask you what
data, what you want, what you don't want, which is, of course, an annoyance because you just want
to get the information and move on. But is that a model that is going to be useful or is a useless
model? Well, I guess from my perspective, not without education, because as you said, you go and
you say, okay, okay, follow me through, right? And it's horrible. I mean, if you look at the
cookies, I mean, there are software that you can see how many people are tracking you online.
And you're like, okay, perhaps I don't want to see how many people are tracking me online.
So, and then actually that dovetail suit, a comment that was going to say is that even if you decide,
you know what, I'm not going to be online. Try to live in America right now with our credit card.
Or as soon as you go to the ATM and get cash, they know where you are, right? So, you,
maybe they don't know as much about you, but they know what where you are. So, there's some of that
going on. On the flip side, I guess on a positive note here is that, for example, if there are images
of you up on the web, but nobody has ever tagged you, then they don't know that it's you, right?
It's not that it's basically like we're just giving them this data, right? And so, they take it and
they, you know, they use it to make a lot of money. And in fact, as part of that, I think it was
tracing that brought up explanation. So, supposedly on Facebook, actually, I know this for a fact,
on Facebook, when you get something, you can say, why are you showing this to me? Why are you showing
this ad to me? Or on Google and many other platforms? If you look at those explanations,
the explanations are too general, right? Oh, I'm showing this because you were a woman between 20
and 50 whose primary residence is in the US, you know? And I think that there's actually two
aspects of it. One is explanation is hard and two, they don't want to let you know how much they
know about you because that's going to creep you out, right? So, actually, this is one of the
experiments, it's one of the assignments I give to my students. I'm like, okay, go and see the kind
of ads you're getting and look at the explanations they're giving you and tell me, do you think that
those explanations are good enough? And it's always too general, right? It's not specific enough in
terms of why. And just one last thing about Google is even if you go to the Google private
incognito, like you clear everything, you restart your machine, you are in the
Chrome or Safari or Mozilla's Firefox's incognito or private mode, they're still tracking you. They
know your location. Just search and then you're going to get ads for the beast around the corner.
So, a lot of this has to do with us educating the public and by public, I mean everybody, right?
And this is why for me, it's a freshman course, like these kids come in and they're like, whoa.
So, I might just real quick add yet to some of these connections. So, I really like Tracy's
comment about how a critical aspect of autonomy and self governance is consent. And as Tina just
pointed out, we should be thinking about certain algorithmic decision making, our machine learning
products more generally as things like utilities. And so then when we're confronted with these
sorts of solutions where what we get is just bombarded with terms and conditions that everyone
scrolls to the bottom of and clicks except that's supposed to be a token of consent. But of course,
if you think about it on the model of utilities, that is we couldn't possibly opt out, we couldn't
possibly read through all the terms and conditions. Then from a conceptualized standpoint where
consent, you know, you can't give consent under duress or coercion, it seems like one of the
elements that we're missing is not just that you need to be educated, but the point of contact
where individuals consent, the use of these algorithms, be more informed and robust as well.
Amen to that. I had heard, so just speaking back to the FDA analogy, a couple people mentioned that,
I had heard maybe Tina, you were telling me about this, that there's some people are
proposing that like drugs, you should treat algorithms like drugs and have some similar kinds
of regular software regimes. Do you want to maybe say something about how would that work?
Yeah, so when you go get a prescription drug, you get this long pamphlet that nobody reads,
and then you get this very short label on the bottle itself. And it will be good if we could
do that for algorithms. And in terms of the long pamphlet, there have been recent movements on that,
so there was a group by Margaret Mitchell and another one by Tim Nitt,
Gibru from Microsoft and Google and lots of other universities where they came up with model
cards for models where for a particular model, like machine learning algorithm for the audience,
you would say, you know, who created it, what was its uses, how was it trained, how was it
evaluated, does it do well on the entire population, what are some of the ethical issues? So it's like
a long-form birth certificate for the machine learning algorithms. And then the other one was
data sheets for data sets, where, you know, again, it's like a long-form birth certificate for the
data set, how was it collected, who collected it, how was it being maintained, how was it cleaned,
right, lots of other kinds of stuff. Now, if you look at those papers and you look at these long-form
birth certificates or these pamphlets, they're a little bit too inside baseball. The same way for
me, I'm not going to read the big pamphlet for the prescription drug that I'm getting. So we also
need to have some kind of a label that the general public will understand that perhaps I don't want
to use this algorithm because they will have some adverse effects for me because then I will be
trailing data and somebody is going to use it and say, well, Tina is not a good person to hire for
this job, right, because of some data that they saw elsewhere that I did. So these kinds of labels
are extremely important. And I think the analogy to prescription drugs for algorithms is just spot
on here. I mean, I think I would want to go further, right, and I'm not disagreeing with
starting with, because FDA still has pretty macro outputs, right, they'll say this drug is safe
and this drug is not safe. Whereas I think that the question we're asking with these algorithms is
not that they're overall safe or not safe. It's they're bad for lots of subpopulations.
And today, FDA doesn't do a very good job of regulating that piece. It has a laundry list of,
well, if you have these things, you should be careful. Well, yeah, but I don't. What about me?
You know what things I have. Is this going to be useful for me? FDA doesn't do that. And I think
same for me as if I am, you know, if I'm allocating health resources or making criminal justice
decisions, I need sort of for my application, does this work, right? So one of the things we've
developed over the last couple of years is sort of this thing we call the fairness tree,
which sort of asks you, what are you using? What are you trying to do? What do you care about?
Again, it's not it's not at all for the sort of the consumer as the public, but consumers,
people, policymakers, decision makers are using these tools. It's sort of the input is, you know,
what are you trying to do? For example, right, if you're, again, if you're making punitive decisions
or interventions, then disparity and false positives is going to be much worse than this
disparity and false negatives. If you're trying to help people and give them additional services,
the false negative disparities are much worse rates. And those are a sort of concepts you can
explain, but it's much easier. So same system used for two different things can have very
different outcomes. So part of it is really kind of be much more deliberate about, you know, for
this type of problem, here are the issues for these types of problems and having audit tools
and all those things. So I agree that I think FDA is the closest we have, but I think we need to
push much further in terms of sort of auditing these types of tools and putting out these things.
The other thing is, I think, you know, we don't have the this sort of the practitioner set, you know,
sort of guidelines for people building these things. We're not a very mature field, right?
We only as a field discovered, there's a thing called ethics a few years ago, right? And we should
do something about it. You know, all this work on machine learning people trying, you know,
discovering the field of ethics. So I think we're just so new that we don't have reproducibility,
we don't have sort of documentation guidelines, and we just need all of those things. And if we
have those, this conversation would be much easier because we would start from that and say, well,
we need to make these tweaks as opposed to, you know, we call ourselves a science and we have no
reproducibility guidelines. And actually, we found ethics because we got back publicity.
Otherwise, we wouldn't have found ethics. I mean, found me, you know, we can spell it now.
Very good. So that's a start. But yeah, I think it's sort of embarrassing at some point, right?
No, no, no, no, we're not on like that. Yeah, I mean, if I could jump in because, you know, at Northeastern,
our ethics institute is one of the things we're really trying to do is be part of ethics education
of technological people, people who work in the field practitioners, also policymakers.
And I think that's so, well, just to throw this in, while we're talking about education, I do
agree that technological literacy is probably the most important thing of the general population.
But ethical is a little bit of thinking about ethics for the people doing the stuff and you
said that's also probably a good idea too. I'm just to throw that in there.
Brandon, can I throw in one little thing too, when you're talking about the general population?
You know, one of the things that we've faced at Yale Law School, which, you know,
it's a pretty good school. And we think that we get lots of really able students. We've been focused
not just on educating people about technology, but basic numeracy. I mean, you know, the level
of numeracy education in this country is really poor. And, you know, I think that precedes even
this question about how much you can understand technology, which of course relates to the ethics.
I also point out that all of us is, each one of us works in a different kind of school.
And there are very little relationship between those schools in terms of conversation where
they were talking about schools of information, schools of journalism, schools of communication,
you know, the computer science stuff, and then the regulatory people law, not to mention
business schools to get to Tina's point about business models. I mean, you really need to create
an entirely, entire new field to get this work done, honestly. Yeah, I mean, go ahead.
I just want to ask, there seems there is a difference between ethical and legal. So you were talking
about FDA. So you FDA approves a drug. And then the company starts putting all sorts of ads one
after the other and trying to have as many people on this drug as possible. Legally,
they protect themselves by reading you a whole list of their side effects and they, including
that you may die from it. But so you can control it legally, but ethically, it's a much harder
thing to define and control, isn't it? Yeah, I would agree with that. Absolutely. And so I would
say that speaks to the importance of ethical thinking, not just in the technical areas, but also in
government and in the regulatory bodies. They should learn more ethics. I don't have anyone in
particular mind, but you can imagine who I might be talking about. But like, for example, some of
the things that I, again, I just want to come back to education, maybe because now I'm a professor,
like it's all about education, you know, I haven't gone to the dark side. But this notion that,
like when I go and talk to people, I'm like, for example, we can tell who's your romantic partner
on Facebook, because not everybody says who their romantic partner is. And it's a very simple model,
right? You're like the center of this flower, there are petals around you, this petal is high
school, this petal is college, this petal is your book club, so on and so forth. People who are
outside of these petals, who are friends with the people inside these petals, they're either
your sibling or your romantic partner, because you're introducing them to different facets of your
life. Now, if you stop doing those introductions, it's a leading indicator that you will break up in
two months, and we can start pushing you single bar ads and other kinds of things, right? I think
most people will find that very intrusive, right? But people don't know, you know, we know a lot
about you guys, you know, we can find that really easily, be up to a lot more like bad stuff, which
we are not. So, tread softly. Well, okay, so it's about 342. I was thinking maybe between 15 and 20
more minutes before we get to Q&A possibly, so maybe, or we don't have to take that line, but maybe
we could sort of turn towards the future, since I'm an incorrigible optimist, sorry, maybe each of
you could try to throw in something about how you think, what are some ways we might be able to turn
this, turn this ship around to so to speak, and actually use it to leverage good moral outcomes,
and not be having to play catch up with all the pathology so much. Who wants to take that first?
Gabby? Sure, I'll take it first. So, I just want to maybe bring it back to something that Raheet said
that I really liked, which is, well, now putting it in my own words, every algorithm is an artifact.
It's like a tool that we use, and so we can decide whether we want to use that tool for good or for
bad. And so, as Raheet was saying, in a context where we're distributing resources, we might decide
that a certain decision procedure that increases false positives isn't as bad as in a case where
we're predicting recidivism risk, say. One of the things that's common about these cases, though,
and that I think really comes out of the influence and impact of computational procedures more
generally, is just a sort of computational prowess that we haven't seen before, and that will allow
for a lot of, I think, positive impacts on the world. So, going back to this issue of garbage
and garbage out, so I always ask computer scientists, you say that an algorithm is only as good as
the data going in, garbage in, garbage out, but at the same time, it's supposed to be more objective
than human decision makers, and so how do we reconcile these two claims that seem to be
intention? And they usually say something like, well, you know, we'll be able to get rid of the
hangry judges, so the judges who decide just before lunch and have harsher judgments, at least
computers don't get hungry, and so they won't make more angry decisions. So, there are some
personal level biases that I've studied in my own work, and that I think will be ameliorated
by the use of objective, more objective machine learning programs, but what that objectivity
means isn't necessarily robust strict objectivity. Rather, I think it comes out of just being able
to notice and pick up on certain features of our world, that when we march through it in this
individualized, over-intellectualized fashion, where we're focusing on human decision-making,
we think that we're better than we are. And so, when we redirect focus, so here's how I think about
the advantage that machine learning programs have on humans. It's like, if you think of just a simple,
well, I won't get into the details, but like a two-dimensional decision procedure,
where we have to just two features that we're making a decision on, and then we come to an
inference on the basis of that, and now you get into like 100-dimensional feature space where
machine learning programs are picking up on countless features, whether they're collecting
through collection practices that we're not even aware of. So, think of like three dimensions that
is easy enough, four dimensions that's harder. Now, think of like 100,000 dimensions folded in
on itself, and you get something like a spikey ball, just kind of existing there. And computers are
more objective than humans in the sense that they're able to deal with various spikey balls,
whereas we humans, I think, are inclined towards smooth surfaces. We like for things to be easy.
And because of their computational prowess, I think some of those spikes in that ball that
pick out things like injustice or patterns of oppression in the environment, that they're picking
up on them, I think, is good for us to redirect focus away from some of these questions about,
I think the question of who's the blame is still an important one, but it redirects us away from
what individual person has made a decision that is biased against a particular person to more
so focus on the environment in which these algorithms are being used. And insofar as what's
common in all these applications of machine learning programs is that they're picking up on those
patterns that are out there in the world where realists about those patterns, we're not denying
that those patterns exist, then the question just becomes, okay, how do we leverage their ability
to pick up on those patterns better than we can to ameliorate some of the problematic patterns that
we see in the environment? And so I think that's the main thing, and that should be the focus of
what we do with algorithms going forward. Right, great. Yeah, as opposed to using that process to
sell you more soap more effectively, which is basically what's happening now. Ray, do you want
to jump in on this? Sure, so I like, I like Abby's sort of description of those these spikey balls,
right? I think, I think that the part that's sort of extending that a little bit is, is what a lot
of these algorithms trying to do is they're given these spikey balls, and then they're given some
outcome, and they're saying, well, figure out the patterns of this spikey ball that lead to those
outcomes. Now, the problem is those outcomes are not objective, right? Those outcomes,
that there's sort of two types of problems we use AI for generally, right, for prediction,
machine learning, prediction things for one is classification, right, where some human knows
what a thing is. It's just too slow for us humans to figure that out fast and we're too slow for it.
So is this an image of a person and all the horrible things we've heard about there?
Their human biases and the outcome are the problem. The inputs the computer can adjust,
but if the outcome is wrong, then the computer is by definition going to be wrong, because it's
replicating that. So one example that is some work, we were talking about earlier, I've been doing
with police departments on identifying police officers who are going to do horrible things in
the future, shoot people and unjustified use of force and all those different things.
The key word that I just said was unjustified, like who determines it was unjustified?
Use of force happened, it got investigated, and some objective internal affairs team decided
justified, unjustified. And if you're a department that's a pretty horrible department,
like a lot of large police departments are today, it's going to be totally corrupt and you're going
to say everything is justified. And so the computer is just going to take this by keyball and as good
as identifying patterns, the outcome is not just perfectly justified. So that's I think one big thing
is that or even things like somebody is going to graduate high school on time, that's not a
objective. It's what support structures were in place, what their backgrounds were, how they grew
up. So you can't say here's an objective thing and let the computers figure out how to get to
that objective outcome because there is no such thing as objective outcome. We don't have counter
factuals. So I'll give you another example of where we're trying to sort of, again, to
think it was Gabby's point about equality and equity in the beginning of this was work we're
doing with Los Angeles City Attorney's Office on reducing misdemeanor recidivism through social
service interventions and diversion programs. And we sort of, they wanted to help in building a
system that would help them get ready for people who might be, the police might be arresting and
booking so that when they're called to come in front of the judge, they have a case file ready
with all the connections and social service programs in place. And they didn't, they would
have a couple of hours and that wasn't enough time. So we built the system and the first version
of that system we found was about 80% efficient. If all the 150 people they could, they could have
resources to prepare for, the list we would give them would be about 80% right. And the challenge
of what is that that system was more right for white people than Hispanic people. And
so playing that out of what that system does is helps both Hispanic and white people, but because
Hispanic recidivism rate is higher than white, over time it results in both of the recidivism rates
going down, but the disparity is increasing. That's the most efficient system. So we said,
okay, here's option number two, which is focusing on equality. So we built the system to unit so
that it's equally right for both. It's about 2% less efficient. What does that do? Well,
it reduces equally for both. So it preserves the status code disparity. And so that's what you
want. But here's option number two. Here's option number three, which is maybe another percent
more expensive, less efficient. And it's better for Hispanic people than white. So not focused on
equality, but what it results in is lowering the disparity in downstream, you know, a few years later,
it gets to equity in recidivism rates. And now you have these three policy options. It's the menu.
If you care about efficiency, you use option number one and you increase disparities. If you
care about equality, you use option number two, it's 2% less more expensive. And you get to equality,
but still preserving status code. And if you care about equity, definition of equity, you get to
that. And there's another 1% more expensive. And they chose number three. But I think that's
kind of an example where we can use these types of tools to help humans make decisions that lead
to equitable outcomes. But it requires policymakers to want that outcome and requires people like us
to provide them this menu that they can understand. And then all the math and everything else goes
in the background, right? We can talk about these algorithms and build these systems. But
reasoning at that level, I think there's a lot of hope of many other examples like that. And some
recent work that we did for these types of resource allocation problems, we found this sort of the
general assumption. You often go to these talks and AI fairness, most people will start with,
well, there's a tradeoff in inaccuracy and fairness. And actually, there is no empirical
evidence that there is such a tradeoff. It's just a thing we say. And so we actually found
a paper and a review, but we looked at five or six of these problems that we've worked on
with the last couple of years. And we found that for certain causes of problems, we could, you know,
pretty, with some explicitly focusing on equity and kind of dealing with that issue, we can
actually reduce disparities equal without losing any efficiency or accuracy, which I think, again,
gives us a path forward so that we can start talking about these things as, kind of, you know,
the equity is a first order goal in machine learning systems or any systems, any human decision-making
systems. So that's the positive that I'm going to leave everybody with.
Fantastic. We got a little bit of time left, Tina, and then I'll let Tracy take us on home.
Yeah, so, I mean, AI technology and machine learning in particular, obviously, have been used for
lots of good purposes, for example, disaster assistance or in medical informatics, imaging,
right? You have an MRI and, you know, you can train a machine to say, well, you should look at this
area, right? Or, for example, right now, during COVID-19, the next science institute that I'm
part of, we're doing a lot of work in terms of can we find better therapeutics for COVID? We have
COVID's fingerprint. We have the fingerprints of the drugs we know, the national compounds we know,
can we find one that would be better for COVID? Or, for example, in terms of network epidemiology,
you know, can we, for example, predict, you know, what is to come, right? And these are very
complicated models, but, you know, they're helping to figure out what to do in terms of what policies
should be enacted. The other aspect of it is basically like policing the police with these
algorithms, right? When you have a policy, as I understand it, that policy has to have some
intent. And then when you execute that policy, you're getting data from that execution, and you
could try to reconstruct intent of the policy. And if they don't match, then you can say, well,
something has to change, right? Stop and frisk in New York, for example, right? If you were to
collect that data, try to reconstruct the policy, it seems like the policy was to harass young
Black and Brown males, right? And clearly, that wasn't what they initially said, right? So there
are a lot of these kinds of things that one can do to benefit society, right? But these are more,
I would say, contained, right, in terms of disaster relief, or medical informatics,
and so on and so forth. Then it gets harder, for example, in terms of misinformation or democratic
backsliding, this is something I've worked at, where we know what to do to improve our democracy,
it's just that we don't want to do them, in terms of, for example, misinformation spreading
through the internet, et cetera, et cetera. It's just that we don't have the willingness to do that.
Thanks, Tita. I'll trace you. Maybe you could just take us on home here in the last five minutes.
Just a couple of points. I really like what Raeed said in terms of thinking about this,
you know, that the old trope fairness versus accuracy, it also connects up with something
he said earlier, which is that computer scientists, data scientists who are working on these issues
understand accuracy as predicting something that's happened in the past, right? Which, you know,
brings up the kind of path dependency point he illustrated with this three examples.
And so I guess the question I would have for the group and for people listening is, you know,
what is it that's going to motivate the people who are actually doing these things? I mean,
maybe some of them exist out there. We've got two great computer scientists on our panels
who are doing this. But to think forward, in a forward-looking way themselves, right? So, you
know, Raeed says we can do it. But, you know, who are you waiting to ask for someone to ask to do
rather than generating your own models of like, actually, let us be the leaders, let us show you
through our technological prowess, how to imagine a better world. I mean, so, you know, this is
supposed to be the part of the session where we imagine this future. And, you know, as a Black
woman who reads a lot of science fiction, I want to say that, you know, the Afro-futurist
vision is usually pretty pessimistic. You know, so if I'm going to be optimistic, you know,
what is going to be my model, you know, what's the world I'm imagining, you know, we have to think
about that. And I guess as we're imagining that world we want to live in, we don't have to think
necessarily about what our technological limitations are. You know, any one of us can do this to
imagine the world we want to live in. And then I guess, you know, it's the job of the tech folks
to do it. I guess I just want to put a little bit more impetus on them to participate in the
imagining of the future rather than being constrained by the world that has existed as,
you know, the load star for perfection in your work. Great. So great. What a terrific panel.
We're going to shift over to question to Q&A now for we got about a half an hour. And so our
moderator, Alex, has come on. So I'll hand it to him to give us a touch.
Actually, granted, before we go, Tracy asked the great question.
Oh, right on this spot. Oh, okay. Right. You want to take a step first to Tracy's question,
and then we'll go to Alex with questions from the audience.
Well, she asked a couple of different questions, so basically you want to end.
So I like the one about the, you know, who are you waiting for?
Yeah. And I think that's the question I ask a lot of the computer scientists who are kind of
on the deep end of theory of theorizing about fairness. Like, who are you waiting for to actually
do this? And I think that that is a problem with, you know, before we started the panel,
we were kind of chatting about conferences and there is this conference in that's kind of somewhat
of an intersection of different disciplines, computer science or science law. But it's still
a sort of computer science scene more than it needs to be that's sort of focused on the theory
of fairness. And I think unfortunately, a lot of this work is too theoretical without any actual
context. So I think as a field, there is there is a unfortunately a gap between practice and
the field. And I think it has to be the, so at least I'm trying to figure out how to, you know,
all of my work is with governments and nonprofits, because I feel like that's where the implementation
is happening, that's where the actions are happening. But that doesn't scale. You know, if you work
with one city, LA doesn't mean that every other city is going to do this. If you work with one
country or one state. So I think the question is how do we take, I think what we need is
ways to expose the computer science people to real problems and real people and real, you know,
I guess data because it's a combination of not objective data, just real data. But then I think
we need to kind of have more of these types of, I mean, this is a good example of different
fields talking and we're using different vocabulary and we're learning about what the words are. And
we do that, you know, again, all of us do that. And that's why we're here. But that doesn't mean
that that's the norm in any of our disciplines. So I don't think we're waiting for anyone. I think
it has to be kind of, right now it's both sides have to be proactive about going out and saying,
I just want to help. I'm not in it for 10 year or paper. Tina's heavily has 10 year, right? So
you don't care. And I think that's what we, that's part of it is our disciplines don't incentivize
this type of work today, at least in academia. And we need to change that. And again, that's,
you know, and then we can sort of say, well, it's somebody else's problem, but it is our problem.
Oh, absolutely. That was my non answer. No, that's terrific. And let me just say,
it's perhaps even more pronounced in philosophy, because very often ethicists
do not are really just working on very theoretical questions. They're not, they don't have the
lived experience. They're not even, they're just not aware of actual ethical problems in the
societies that in which they're living. I hate to say that. This is why we need more,
we need more, rubber meets the road, even in ethics in, I think, in every field. Yeah.
Yeah. And if I may follow up on that, the incentive structure is not good across the board. I mean,
if you look at the papers that are coming out in computer science, in these peer review conferences
and journals, it's really little tweaks to things, right? And then if you look at like a master
students, I teach some of the money maker courses, right? They're just like, teach me the algorithms
are going to make me a lot of money. I don't care, right? And when I try to talk to them about ethics,
it's just they don't care. But what's interesting is that they don't see that, for example, the
algorithm that you're developing may enable misinformation that may get somebody elected,
that will then change the line, you can no longer get H1 bv's up, right? They don't see
that link, right? And so the incentive structure is just not there. I just want to make money,
so teach me the money making algorithms. And it's either money or, you know, like I had this
comment, I'm teaching a class right now, it's a machine learning and public policy, it's half
to students or machine learning department half from the policy school, it's painful. And a lot
of the students, machine learning PhD student comes to me and says, I'm a machine learning PhD student,
I just want to do math. Why are you having us think about these things? And I think that's the
problem. We have, you know, another student came last year and said, well, I'm going to go to the
public private sector. So this ethics class that we did, I don't think it's relevant to me. Like,
what? It's exactly relevant to you, just because it's painful, you know.
I mean, and actually, like when I go to mind people and I give talks, I'm like, how many of you are
okay with your algorithm being used on you? Nobody raises their hand, not even the white guys
raise their own hand, right? So they know there's a problem, but it's just like, look, whatever,
like they don't see that one leads to another to another to another and, you know, game over,
which is very, very frustrating. But the incentive structure within the CS and the tech,
the STEM fields have to change. Because right now we're just like, oh, look, this is such an
interesting problem. Like, I remember when we were living in New York, I would go out with my
friends who work in finance, I would come back and I'm like, oh, this is amazing problems. And
Brandon would take get away from the cliff, get away from, because again, exactly, as Ray said,
like this is a really cool problem. I don't care about this, look at the math, right? And so you
have to get away from that and like, somebody could get hurt, right? And so you should know.
This panel is amazing. It could go on forever. I do want to give Gabby a chance to maybe get a
last word in here before we go to questions if you've got something. I'm in it for tenure.
Don't talk to me about incentives.
Point, that's a great way to segue into Q&A. Alex, you want to take this into Q&A?
Yes, I just want to know to, I think, S, Mason, Dan Brock, apologies to single you out in the
Zoom call, but you have your hand up. I can't really lay any question you may have. If your
hands up, you need to write it in the Q&A option within Zoom. So with that said, I'm going to
eradicate rhythmic decision-making. That forced me to this sort of deflationary view of how biases
can manifest just from data and innocuous processes in the human decision-making domain. So it actually
went the opposite direction for me. But insofar as both are cases, I think, where we're getting
away from this overly intellectual view, as I was saying, about how biases manifest, that there
is a person who is intentionally deciding to treat people differently on the basis of their belonging
to a separate society. These differences might be an emphasis more so than categorical differences.
And so one of the things I'm trying to bring out is that some of these decisions to use a
simpler model, like linear regression or like a non-parametric model, like these decisions have
ramifications that go up the line. And so even though it's true that we're using relatively innocuous
conceptualizations of bias, I take it that some of the more systematic biases or the social biases
that we're concerned about share some commonalities with these decision points earlier in the causal
net. And so there is a relationship there. Yeah, I think there's a deeper conversation which we
can leave for a later time, which means the design choices that a machine learning system developer
makes in the data sources to use how you process them, how you think about them, and the downstream
biases. And I think that's a very... We don't talk about them very much. There's no textbook that has
things to think about at each step to deal with bias. And so I'm teaching this, those are the
things that we're teaching about. But yeah, I think that's a different conversation because
that's a huge line spot for the developers right now. You know, before we were all online, we had
a little joke among ourselves about writing algorithms to check out our algorithms, right?
And that did seem a little silly or aggressive. But there are some people who have this hope,
talking about the far future. I don't subscribe to this, but I'm just going to say it that, you know,
there may be some way for the computers to learn to be more ethical. And how would you go about
doing that? And is it possible even, or are they just algorithms and ethics just really
also be ethical because that would be the only way for them to become super intelligent, blah, blah.
But it's interesting to think, well, could a super computer, we grant that maybe they'll
be more intelligent than we will be. But I don't think it would be possible, isn't this a parabox,
that they would be more ethical than we could be. Because how would we know that's the case?
That's weird. I think it's a little bit of a paradox. Anyway, anyone think that computers might
ultimately evolve towards making ethical decisions themselves without supervision?
Well, let me jump in on this. I think, I mean, I tend to think of things
more of a virtue ethics kind of way, like, who are the exemplars? Who do I look to?
Who do I really think that they're acting in a really virtuous ways?
And to the extent that we're looking for exemplars and we're modeling exemplars,
then I think sure machine learning could also model exemplars. I don't see why they couldn't.
Yeah, so that reminds me of something that Rima Bassuss said. Rima Bassuss is a professor
at Claremont McKenna philosophy. And she was like, imagine a time where you could order an Uber driver
who's a consequentialist or an Uber driver that is a virtue ethicist or something like that, right?
With autonomous vehicles, et cetera, where you could put in the requirement for the driver that you want.
I've stuck with me. It's an interesting idea.
And they get different tips as the next question.
Oh, I mean, think about chess. I mean, the deep learning chess machines are so much better than
us, but we still we know that they're better. How? If they are, how do we know?
Same. It's the same thing. I don't think it's fundamental. Well, we know they're better because
they beat us. Right. No, no, but I mean specifically, we still have the sense that they're improving,
even when they're way better than us. Yeah. Yeah.
Yeah. They could just search the space better, you know, it's a bigger space. They just, you know,
well, but maybe that's all ethics is to maybe maybe you said, yeah, the right rules and the right
exemplars, you have to learn what that space is. You know, this has been taped. I would love to
find out what Rahid and I don't know, Alex, where we are in the in the in the Q&A. So if this question
isn't appropriate, I just I'll throw it out. You can manage the questions and then go ahead. But
I just want to tie something that Gary said and Brandon said to something Rahid said earlier,
which is to add that machines could model exemplars. But of course, if we think of exemplars
of, you know, particularly virtuous people, no one, of course, is virtuous. Humans are not,
you know, unrelentingly virtuous, like all the time, except for Jesus. I'll use my own face.
Tradition. So, you know, and, and, you know, for a lot of people, and that wasn't real, right? So
like all of the exemplars that we have are of real people are never virtuous all the time,
which brings to mind Rahid's point about, he said, well, this is an incremental risk that the machine
is going to whatever we program is going to do it all the time in a way in which, you know,
humans never are that way. And I just wondering if if if you could reflect on just that idea a
little bit, Rahid, about using humans as the exemplar of the virtue that a machine could be
modeled after in a world in which we know there is no such thing as any virtuous human
in all the time. Does that even make sense? Yeah, I mean, it's an interesting and I think
Tina has talked about similar things before. It's sort of right now with these these systems
don't use people as exemplars. They use let's say people's decisions or actions as exemplars,
right? So we take historical judges, we take all the judges and we take their decisions and we say,
okay, let's build a computer to replicate and aggregate all these decisions. And the computer
is going to be wrong many of the time. So then we tell the computer which mistakes are worth more
than others. And right now we say every mistake is worth the same. So then it gets most white
decisions right. And then that's what happens in the world, right? And so now if we sort of think
about, and I totally honest haven't thought about that, right? But it's a really good point,
Tracy, you're making is what if the exemplars were humans and then some of this, some of this
variant sort of gets embedded where we're really using humans as examples and trying to figure out
what would this human do versus that human do. And then it's maybe also as Tina was saying, it becomes
kind of an expert witness or a medical test, which is just another input. Like, well, here's what
this human would say. And here's why. And here's what this human would say. And so you sort of have
this committee that's advising you that you can talk to and then make a decision. Some of it is
auditable, some of it is not. So I think it makes it makes a lot of sense to kind of think about it
that way and see. And again, I mean, the danger and all of these things is that eventually these
systems have some values embedded. And where are those values coming from? Does every time a new
owner or decision maker takes over, do they then change the values to suit their values and all
that kind of stuff? But I think it's, I'm sure other people have better thoughts around this.
Yeah, so this is something I have thought about. And usually I get to push back in that, for example,
there are better doctors or worse doctors, right? And in fact, if you think about law or medicine,
it is very much this idea of apprenticeship learning, right? And so can you build a machine
learning algorithm that's an apprentice to, let's say, a good judge, let's say, Ruth Bader Ginsburg,
who recently passed away, right? The problem is that is a very difficult machine learning problem,
right? To see why Ruth Bader Ginsburg made certain decisions, his reasoning processes,
that's very difficult. It's not as simple as thumbs up or thumbs down, right? Which is really what
it is now in terms of a lot of the algorithms you're seeing in terms of pre-trial disposition,
hiring, etc. They're like gladiator, right? Tina thumbs down, Brandon thumbs up. And so
it's a hard problem. And because it's a hard problem, we typically don't tackle it because
it takes longer to have a paper up. But it would an active mercy coming from a computer mean the
same to someone as an active mercy coming from a human, even if we're perfectly well modeled.
That's a good question. I think for the computer, it's just going to go with the objective function
it has, right? So in fact, usually for recommendation systems, we try to model items and not so much
humans because humans are more complicated. So, you know, we have this term mercy that means
something in particular that seems to be a very human quality. It's, anyway, it's interesting
dilemma when it comes to applying it through a computer interface. So there are people who are
working on this thing called affective computing, which is where the computer will develop empathy.
So my colleague Stacy Marcella is working on it and others are working on it too, where you,
the computer will learn empathy. And if like an element of being merciful is empathy, then
that's what they're working on. All right, Lex, is there more questions?
Yeah, we have three more. We can get through all three, I think. Is that
kosher? Is that good? Okay. So we have Sarah Chen, who was asking or who wrote.
I really like the idea of labeling algorithms based on user fit. For example, this algorithm is
only useful, such accurate with white males. If this becomes a norm, do you think this will
encourage more diverse teams slash community involved developments, or will it actually lead
to more exclusionary products or algorithms? I guess for me, I don't think of it that way.
I think of it more as like having forcing the algorithm designer to be more honest,
right? Because right now we tend to not be honest. We say my algorithm will work on everything under
the sun, right? So I work on complex networks for a long time in computer science, you would say
my algorithm will work on any complex network you give it. But that's clearly not true,
because biological networks are very different than social networks. Their structure is different.
So I think, so I look at it in terms of more holding the algorithm designer to be honest,
to say, okay, this algorithm works like this, like the auditing that Rahid mentioned, and it's
only designed for this subpopulation, which of course, as Rahid nicely put, right now,
drugs aren't so much like that, right? Though I guess there's some of it, right? Like if you're
pregnant, you should not take it. If you're under 12, you should not take it, right? There's some
of that in there. But I actually take it as just having the algorithm designer be honest.
It sounds, Martina, that you would think that people would be internally motivated though,
if they were honest with that kind of specificity to try to do better, precisely because of the
ground setting claims about what we think it works all the time. And so if they're constantly
faced with, it only works in this kind of context that, you know, in the back of your head, or left
unsaid, it's probably at the front of your head, is that they're going that you think they're going
to change what they do, but we don't know, right? This relates to the initial point that I brought
up about objectivity and the bias under the guise of neutrality or under the guise of objectivity.
And I think what Tina's mentioning is right that right now there's the skies that there's
universal applicability of one in fact, it's actually only useful for particular demographics.
And I think this question brings out that making those assumptions or biases explicit might help
in holding the programmers or designers speak to the fire. But I think it also has another
really important element, which is making good on some of the claims that people from marginalized
demographics are already aware of that is giving voice to the disparate effect of various programs
or drugs. Like that they're already trying to point out that this impartiality exists and that
it's currently being ignored. And so insulating the biases that are just a part of the normal
run of the mill programs, I think is really important that we could give voice to those sorts of
discrepancies. So I think there is a risk for it being leading to more exclusionary practices,
but I think also just making good on the fact that people are uncomfortable with the
bias or with the programs already is important.
Okay, so next question is from Ralph Alexuel off of YouTube. Before I get to his question,
I just want to shout him out because he's been commenting the whole time on YouTube. Very
interesting comments and summarization occurring. My favorite comment of his good summation was,
how many of you are happy to have your algorithms used on you? Nobody. I like that comment a lot.
But to his question, I was trying to, there may be some editorial work here because I'm trying to
like formulate the best way because he kind of wrote a broken comment into a question. So he's
sort of like balancing between accuracy, so using the passive protector and fairness,
what will motivate people to do the right thing?
I think Raheed, you want to take that because this whole notion that there's a trade-off as you just,
so Raheed had mentioned something about the fact that there's this false notion of accuracy versus
fairness. So it's not like you have to have one versus the other. I don't know if that answers
this question, but Raheed. Yeah, I mean, I wonder, I think again, I think of accuracy as sort of a
made-up construct. It's a very subjective, like we come up with some definition and we stick with it
in practice, I think in most of again, most of the work I do, accuracy is kind of another way of
saying efficiency, which is I have resources to help this many people and what I find is that the
list my model helps me select has as many people from that as possible. So it's efficiency. And so,
yes, there might be a trade-off in efficiency and equity and effectiveness. And I think
theoretically there would be, but I think what I was saying was that in practice,
for many problems of finding there is no empirical evidence of that, but also, I don't like the term
accuracy, because somehow it by definition seems like it's the right thing to do. And accuracy is
just saying, at least the definition we think of as accuracy in these types of systems is just saying
replicate as much of the past as possible and treat every individual case equally, each error
equally. And that's very narrow definition of what, and so I think I often sort of, you know,
at least when I'm working with students, it's sort of our policymakers as well,
let's come up with the performance goals that we have, what is the overall policy goal,
and then let's define the thing that we then call performance. And that might include a bunch of
different things. And we do that all the time for other types of systems. What's, why not do it for
this? So yeah, I mean, I think the, the, the, the tradeoff is kind of more how we think about these
things in a more of a knee jerk way, but, but it doesn't always appear to, to, to exist in practice.
Okay. Do we have time for two more questions? Just one more. What do you guys,
why don't you say both of them and then we can. Okay. Okay. So, so the first one's from Janu.
I hope I'm pronouncing that correctly. Janu writes, how widespread is the use of facial
recognition in the Western world and what regulation is coming? And then the, the, the second one is
from an anonymous viewer. How can the new Supreme Court justice make fair decisions? I assume they're
referring to the, I guess, what is the assumed nominee Amy Cohen Barrett or Coney Barrett, excuse me if
I, I watched her name is religion a bias when it comes to social justice.
Maybe someone should take the first one. Well, I'll say something about that since I am the law
professor, not that anyone else would have anything else to say. But the first thing to say is, is it
a bias? I really liked Gabby's discussion about biases before earlier. I don't know if the
questioner was, was on the webinar at that point. But yes, of course it is. You're right. But I, I
think, you know, implicit in the question, given the fact that that was the second part of the
question in the first half was like, how can this person make fair decisions? Is this idea that that
bias, you know, whatever it is, I already just mentioned, you know, my own faith tradition, which
of course is a bias, in a sense, is pointing to some sense of this person should she be
confirmed capability of making fair decisions. I think that goes back exactly to Raeed's question,
which is, what does it mean to say a fair decision, the right decision, it definitely ties into this
question of how we understand what accuracy is and tying that to what's right. And just to blow it
up a little and the work that I do in, you know, criminal legal systems, people often talk about
policy that works, you know, well, this works. Well, what the hell are you talking about when you say
that, you know, stop and frisk works, works to do what is always, you know, the question. So
that probably wasn't a satisfying answer for you, but like that is certainly an answer to the
and answer to the Supreme Court question. I have no idea about facial recognition.
Yeah, I guess in terms of facial recognition, as you have probably heard,
some companies like Amazon, Microsoft and IBM have announced that they would like stop or pause
their facial recognition offerings for law enforcement, but they're not the big companies. So facial
recognition is being used. I don't know how wide, you know, what is widely, but it seems to be
prevalent. So before the pandemic, when I was flying to Europe, they're like, we don't need your
boarding pass, right? And they would just scan my face and I would go through or coming back from
Germany, there was a faster way of going through immigration, where again, they would do facial
recognition and I would just go out and go through and I didn't have to talk to anybody, right?
And so and of course, you know, the facial recognition, the problems with facial recognition have been
very well documented by Joy Boliomini et al. in the algorithmic Justice League that she has.
In terms of regulations, I think somebody has to get sued and sued big with a lot of money.
So that's something would happen. And right now that has not happened. Perhaps, you know,
the unfortunate gentleman in Detroit that was arrested because the facial recognition
couldn't tell one person from another. I think he is suing the city. And again, like with the
facial recognition, in particular, just so you know, some of the ethical problems here,
Google knows that it has a problem with facial recognition, like they don't have enough
to put black people in their data set. So what do they do? They hire a contractor. The contractor
says, okay, which city has a lot of black people? They're like, Atlanta, they go to Atlanta,
then they start looking for easy, easy black people that they could take black pictures and
have them sign a consent in consent form. So they target homeless people and they target college
students. They have them sign a consent form. They give them $5, they take pictures of them.
And they're like, okay, now we got our black people. Well, clearly that's wrong, right?
And because now the system believes that black people are either homeless, predominantly male,
that's where they were going for our college students. I mean, so they just go from one ethical
quagmire to another. You know, and what I'm saying was, you know, it's publicly available.
There were news about it and Google is like it wasn't us. It was the contractor, right?
So yeah, and I think, I mean, so there are some regulations already there, right? So
over the last year, San Francisco, Seattle, Portland have been the larger places. Oakland,
I think, Somerville, Massachusetts, and have also banned. So these cities have banned the use of
face recognition tools by city agencies. And the big one has been the police departments,
but in general, by city agencies, there are probably other 10 states that have pending
legislation that's going on. Not, you know, New York has a bunch, actually, not a bit as from
what I remember has passed. So a lot of this is happening. On the other hand, I was at this
event a couple of weeks ago, where police departments were talking about their experiences
after these bans. And that was totally fascinating, where they're coming up with all sorts of loopholes.
Like, well, we can't use it directly. But if a consumer or another, a retail store that
has cameras and face recognition, if they give it to us, we can use it. So I think there's sort of
this adversarial thing going on right now where how do we, you know, the intention of the ban wasn't
who can own it and who can not own it, where it comes from, the intention was you cannot use it.
But the way it was implemented, very detailed, and we're doing the same thing right now in
Pittsburgh. And it's like, it's all these exclusions exist. So I think regulation is needs to happen,
but it's happening. But there also is going to be this adversarial thing going on.
Gabi, did you want to? Yeah. Yeah, add real quick. I think the example facial recognition software
brings out really nicely something that Ravi was saying about talking about accuracy, really like
narrowing us too much on a particular goal that might not be the sort of thing that we want to focus
on. So I too have heard Joy Bellowini talk about her audits of facial recognition software.
And when it initially started, it was like, and a lot of these discussions about the illegality
of them come back to the accuracy issue, that is that they're only something like 50% accurate with
non-pale male faces. And so this focus on accuracy, the response among corporations was of course,
okay, we'll fix it exactly as Tina said, get more data that makes it more accurate on women and
people of color and elderly individuals. And Joy talks about how the response anecdotally from
the black activist community was like, please don't do this. Like it wasn't we don't want accurate
machines for facial recognition programs. It's like we don't want facial recognition software at all.
And so this focus on accuracy, I think, is like a bit of a red herring because it makes it sound
like if we could only get more accurate facial recognition software. It's like we also need the
safeguards against those accurate programs being used for nefarious ends. And so the discussion
about what makes for an ethical or fair machine learning program, it really goes well beyond the
scope of just the pure mathematics behind the program to what context is it being used for
and what sort of ends is it trying to achieve. That might be a good place to stop. I think so.
What a fantastic panel. Tracy, Tina, Gabby, Raeed. Thank you. And thanks so much to Ed, Jerry,
and the center. And thank you, Brandon, for organizing it and doing such a great job moderating it.
Thanks, Ed. I hope to see you guys in New York City back in person soon. You know?
So do we. Yep, thank you all. And everyone, keep an eye out on our website and people in our mailing
list, future roundtables will likely be advertised shortly. Thank you. Thank you, and thank you.
Thank you, for thanks, everybody. Thank you. Thank you all. Be safe out there.
Take care. Thank you.
