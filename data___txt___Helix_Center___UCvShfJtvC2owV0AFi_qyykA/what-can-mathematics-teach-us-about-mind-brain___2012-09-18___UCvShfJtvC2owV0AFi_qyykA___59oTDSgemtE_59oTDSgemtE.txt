So I'm Ed Nurse-Essian, I'm Director of the Center and I'd like to welcome you to this
meeting on what mathematics can tell us about the mind and the brain. The idea for this round
table was generated during a dinner conversation with Sylvan Capel, who's head of the Mathematics
Institute at NYU and Charles Marmour, who's a psychiatrist and chairman of the department
at NYU. So they thought that this would be a very timely subject for us to have a round
table about at the Helix Center and I want to thank them for it. Just briefly, we have
a number of programs set up already for the fall and I'd like to mention them. I think
the first one after this is a program on why economists disagree, which will be on October
13th and we'll have economists from the two sides of the intellectual or scientific areas
discussing it. The following day on Sunday, and is it a two-third day? We will have a
program called Poetry and Jazz where we have a poet and a jazz musician who will play,
recite and discuss the relationship between poetry and jazz. Following that in October,
Friday and Saturday, we'll have two round tables. One of them is going to be on, I wrote the
exact title somewhere, Life and Movement and the other would be on male-male competition,
globalization, war and violence. These two round tables have been organized and proposed
by Maxine Schitz-Johnson, who is an emeritus professor of philosophy of science from Oregon
University and who was here during the Philip Deides days and did a two or three programs
for us. Our website is up and one of the ideas that has always propelled us is that these
round tables should not just end here, but there should be an ongoing conversation taking
place afterwards. We were not set up before for it and I think we are. Maybe Rob can say
what you do for that.
Yes, thank you, Rod. First of all, go to www.thehelixcenter.org or helixcenter.org and that will access
the website. There's a link to sign up after which you can participate in conversations
on the website. You can either join a topic, a question that has already been posed by
someone else related to one of the events here or you can propose a new subject for
discussion. The other thing is that if you want to comment on anything that's going on
here, you can Twitter at the helixcenter.
As I said, today's subject has a very easy title, but it is a rather complicated subject
and we have people who I think can discuss it who are leaders in this field. I will start
with Ned Bloch who's been to us a number of times. He's the silver professor of philosophy,
biology and neural science at NYU. He came to NYU from MIT where he was chair of the
philosophy program there. He works in philosophy of mind and foundations of neuroscience and
cognitive science. Bar, there are many trout who is unfortunately unable to be here today.
What is with us via Skype is distinguished university professor of computational biology
and professor of mathematics at the University of Pittsburgh. He's written more than 200
papers in math, philosophy, physics and math, biology, physics and neuroscience. He has a
software called XPPAUT for the simulation of an analysis of dynamical systems. As written
a number of books and one of them is available here for you if you would like to purchase
it. Ken Miller is professor, department of neuroscience, department of physiology and
center for theoretical neurobiology at Columbia University. He's co-director of the Schwartz
program in theoretical neurobiology and its center of theoretical neuroscience. He's also
co-director of its neurobiology and behavior graduate program. He serves as vice chair
of the department of neuroscience. He's the founding editor of the Journal of Computational
Neuroscience, recipient of the Alfred P. Sloan research fellowship, Sol Scholars Award, Del
Web Biology fellowship, National Science Foundation graduate fellowship and author of many articles.
All of the people have books and you can see them after the meeting. George Reakey Jr.
is associate professor with tenure and head of the laboratory of biological modeling at
the Rockefeller University and the senior fellow of the Neuroscience Institute. He developed
the first microprocessed processor controlled lab instruments, an x-ray camera and his Fourier
synthesis software was used to solve many protein structures worldwide. Beginning in the 70s
his interest has turned to problems of pattern recognition, perceptual categorization and
motor control which he studies primarily through computer simulations of relevant neuronal
systems. Xiao Zhing Wang, am I pronouncing it wrong? Is professor of neurobiology, a joint
professor of physics, applied mathematics and psychology, director of the Schwartz program
in theoretical neuroscience at the Yale University. He's a theoretical neuroscientist studying
executive and cognitive function whose group has pioneered neural circuit models of the
prefrontal cortex discovering a specific neural circuit mechanism for decision making.
He's also studying I believe schizophrenia. Dr. Wang was a recipient of the Alfred
Peace Loan Fellow, National Science Foundation Career Award and John Simon Guggenheim Memorial
Foundation Fellow. Is that it? Okay. So that's it and we can get going.
Thank you.
Probably a very naive scientist point of view which I'm sure the philosophers will correct me but
essentially the mind comes from the brain and the brain is part of the natural world and
mathematics is developed in the study of physics of the trajectories of moving objects and how
collections of objects and new properties emerge at a higher level. And in essence,
studying the mind by studying the brain is no different. We're studying a physical system that
has many, many levels, many interacting parts at each level leading to new phenomena at the
next level and at the next from the molecules to the cells to the circuits and the circuit
behavior to the ultimately to the animals behavior. And what we use math for is to
understand how these interacting pieces produce the phenomena that we see. And in particular,
I think one of the biggest things that we do is we're studying things at one level like we have a
lot of cells connected with some circuitry and they have some functional responses. Say I study
visual cortex and so as a visual image comes in the neurons have certain responses to the visual
world which have been well studied. But how do those responses with all of their details and
complexity emerge out of the circuit? One of the circuit motifs that the interactions between the
cells that lead to this behavior emerging. And then so these multilevel one behavior at one level
emerging from another level from interaction of many things at another level is sort of the essence
of what we try to understand as we try to understand the brain. And ultimately, all the elements of
our minds are things that we would like to identify as emergent properties of what neurons do in
the same sense. And so mathematics is a tool of taking objects that you might say are blindly
interacting. They're interacting according to some rules. The rules might have some intelligence,
but they're interacting according to some rules and understanding what emerges out of them.
And that's to the extent to which the mind is a emanation of the natural physical world.
Mathematics is a way to understand what happens there.
Could I go on from what you said because it's a perfect introduction? I'd like to make a distinction
that I would like the audience to go home with between math as a tool for studying the brain,
which involves both statistics, physics and chemistry, as you've pointed out,
how we use computers to simulate what goes on with neurons, with ions in the brain,
with blood flow and all of these things. And on the other hand, the question of whether
mathematics should have a role in our theories about cognition and things at a higher level like
the question of consciousness and free will and all these things where it's not so obvious that
there's a connection. I think the place where a connection does come in and Ed Block knows
something about this is the theory called functionalism, which says that the material that the brain
is made out of really doesn't matter. It's the functions that the various parts carry out and
how they interact with each other. And if we call that a kind of mathematics because to work out
in detail what it implies does involve some mathematics, then I think one has to be very careful not
to go too far. And what we see today is this metaphor, if you will, that the brain is a kind of computer.
You know, it started out with the first electronic computers after the war, which were called
electronic brains in the papers and magazines of that era. You still hear that phrase once in a while.
And so we have a whole field where every paper you read now says this piece of cortex computes
this and this does that. And I think exposed fact, you can't deny that. As signals come in,
there are electric signals on neurons. They go into some areas, some stuff happens and other
electrical signals come out and exposed fact, oh, that you can describe that as a computation.
And but what that hides is how did it get to work that way? What part did evolution play on,
especially on tajini, which I mean the development of the individual interacting with the world.
And that's why some of us think it's important to work not just with totally abstract mathematical
models, but things like robots actually moving around in the world and receiving visual input
from cameras and trying to do something like an animal or a human would do in the real world.
So I'll shut up now. I've taken enough time, but I just think it's very important to distinguish
between those two kinds of areas where we use math as a theory and as a tool. And the consequences
are different. So in the areas in which I work, the metaphor of the brain as a computer has faded.
I think it's heyday was the 80s and maybe it's still every day in neuroscience journals.
Okay, but here's the problem. One of the hallmarks of a computer is that there can be many ways of
realizing of making the same computational structure. So it's commonly said in introductory books
about computation that you can do a computation with using an electronic computer. You could do
the same computation using something with wheels and pulleys and gears. You could make a hydraulic
setup that uses the same computational principles, the same program at some level of abstraction,
but does it completely differently. But the upshot of a lot of neuroscience is that it's not so easy
to see how you could do the same computation differently. Multiple realizability seems to be
fading away when we see the complexity of the electrochemical processes. So neurons influence
other neurons by sending out chemicals. And those chemicals diffuse. And it's not clear that the
computational pictures are the right way of describing that. I would agree with that to some extent.
But the first, the guy right before that, I don't know who's speaking in anything,
but at any given time. But the robots and external world and internal world, I mean, I don't see how
that is the use of mathematics. Math is all those. And to address some recent point,
he has a good point. The brain is a computer. In fact, if you go back, the brain has been many
things. It was back in Aristotle's time. I think it was pumps and water and things like that.
And more recently, I heard a talk that it was all done by quantum mechanics and things like that.
So whatever the newest theory is, is what the brain is. And it always bugs me because I think
the brain is what it is. And math is just a way, as Ken said, of taking lots of abstract boxes
and arrows that experimentalists put together, at least to me, I'm a hardened reductionist.
And it's just the way of taking boxes and arrows and using it to, it's sort of an existence proof
that this is the right mechanism. So I'm too stupid to ask. But I'm a big fan of how.
Let me just add a few things about what has been said. So, you know, I guess one reason that math
is really important in neuroscience is that neuroscience is one of the fields in biology that
are the most quantitative experimentally. So the neurophysiology, the measurements,
the experiments are really quantitative, has had a very long tradition of very quantitative,
you know, measurements and analysis. And that actually is important to bear in mind.
And that's why we are capable of using mathematical models and theory in this field.
And it's clear already, I guess, from what we heard that we don't really know how to conceptualize
the brain. You know, we have all kinds of analogies. But suddenly, when you really compare brain
with computer, they are just so dramatically different, right? So what computer is really
great at, like making zillions of computations very quickly, we're not very good at it. What
we are really wonderful at, like recognizing objects, recognizing a face in a crowd, in a fuzzy,
you know, foggy kind of environment, computer is incapable of doing today's computer, right? So
this is dramatically different, you know, dramatic differences between brain and computers, as we
know today. But on the other hand, clearly, if not consciousness, we need to think about the
computation. What kind of computations that brain does? So that's, you know, another way of thinking
about the connection within the brain and the mathematics. I'm not sure the computation is going
to turn out to be so important. You know, as a number of people mentioned, there are different
levels of description. And computation would be important at one level, but not at another.
A standard example of this is the explanation of why a rigid square peg doesn't go through a hole
in a rigid board, which can be done on the basis of the rigidity of the materials and the geometry
of the hole in the peg. So at that level, there really isn't anything that would be called
computation. Of course, if you go to the elementary particle level, then you have a, you could get
an explanation, which is computational, but which obscures the simple level of description. So it's
not always clear. So I don't think you can, unless you trivialize the notion of computation by assuming
that anything is a computation. So commonplace that you could regard a river as computing the
rate of erosion of its banks by, as an analog model of itself, it's a computer that computes its own
rate of erosion. So you can trivialize the notion of computation, but I don't think we can just assume
our priori that the right way to think about the mind is going to be a computational level,
a computational way at the most abstract level. That's fair. I guess you could say, so what's the
alternative? I guess one alternative would be, what really matters is behavior and thought, maybe,
mental life and our behavior, right? Can you describe those things without talking about the
computation? Well, as the guy in the screen said, we're always using the latest technology
to describe the mind. And there's a famous paper by neuroscientist John Marshall that goes into the
history of this, and he mentioned a few cases, but at the time of the popularity of the catapult,
people's theory of vision was that there's little catapults on objects that catapult a tiny
simulacrum of the object into your eye. And then, of course, the famous
telephone exchange model of the brain, or in the early days of the telephones. So the computer is a
very notable artifact, and if you trivialize the notion of computation, we can describe everything
as a computer. But that doesn't mean that we know now that computation is going to be important
in describing, for example, consciousness. Well, I think, I mean, computer is a loaded word,
because it means the things that we have on our desk. Clearly, they don't know anything like the
brain does. But computation, I mean, I think the reason to use a word like that, I mean,
maybe it's a matter of semantics and defining what we mean by it. But the hard,
it's biological function, obviously, is to make sure that blood and nutrition gets to every cell
in the body. And so it has to be a very good pump, and has to pump more and less under different
circumstances. The brain is a piece of meat sitting inside my head, whose function is, in some way,
to taking information about the sensory world and guide behavior toward the animals,
you know, to hold the animals' goals and guide behavior toward those goals. And
to me, computation means that you have to process information, and that's the job of the brain.
And that's all that computation means to me. But it's very clearly, you know, it's not what the
heart does, that's not what the liver does, that's what the brain is for, for processing the
information coming in from the sensory world, and processing, you know, having built in sense of
your, of the animals' goals, and working out the behaviors that are going to achieve those goals.
That's a lot of information processing, and that's the reason for me for the word
computation. It doesn't process the information anything like a computer does.
It's a diffusion of a chemical, is that computation? It can be described
computationally, but is that a computation? No, it's a diffusion of a chemical on the brain.
So I don't see, you know, it just seems to me to be trivializing the issue to call that computation.
Well, that's all the question. I mean, people, Stephen Wolfram called his cellular automata
computation. And you know, I think we're getting hung up on semantics here. And I do want to say
that the metaphor of a computer or computation is going to be really careful because when we do
a computation on our computer, if we type the same thing, we get the same response every time.
And the really cool thing about the brain is the fact that you don't. And that turns out to be
really important, right? Because if you get the same thing every time, then you can't learn. You
can't change. You can't adapt. And that's one real difference, I think, of the metaphor of a computer
and a real brain is that it, you know, you don't get the same thing every time you do something.
Yeah. What I want to add to what Ken said, and it fits right with what you said, is I have a slogan,
the function of the brain is not to process information. It is to create information. And
there's a distinction really trying to draw it between a process in the sense of an algorithm or
a pre-planned design where you, you know, perform operations on numerical quantities and get some
result. But it's this renewal, this learning, this initialization, if you will, of the infant who
comes into the world not doing any of these things and somehow figures out not because a programmer
comes in and tweaks some neurons around to make it work right, but just by having experience in
the world. And that's what the industrial, what I say, the desktop computer doesn't have. And as
long as we understand that, then we're fine to say, as Ken does, you know, the visual cortex,
you know, computes edges and that sort of thing. Sure. I don't know what it computes. I think
we're very naive about what it computes, but it's clearly taking in the visual world and ultimately
leading us to open our eyes and see people and chairs and lights and to see objects and their
relationships and to know a lot about them. But exactly what it does, computes to do that,
I don't think we really know. I guess I would just add that, you know, it's true that when you ask
ourselves, so ask people, you know, in daily lives, you think, you know, I don't really always
think in terms of computation, I think, you know, what's my goal? What I need to do today? So in that
sense, you start with a goal, you make decisions in order to achieve your goal and you seek
actively information from the environment. And sometimes you have to process, you know,
information forced on you, but there is active process that the main thing is to try to achieve
your goals, right, the behavior goals. So in that sense, it's not, you know, it's different from
the suddenly, very different from information processing from the computer perspective.
And it's also always trying to predict, I mean, you get partial information and you anticipate,
nobody, you know, you watch somebody to ball and they're automatically, nobody is born with
Newton's in there, but you figure out how to do this because you're trying to predict trajectories
and you're predicting what somebody's going to say. There's all kinds of cool psychophysics
experiments that can exploit this. And that's another thing a computer really doesn't do very well
is sort of extrapolate to the future because we have to do that to interact with the real world.
So one, in terms of the question of the role of mathematics, I'd like to just, you know,
return to the idea of, yeah, but to the idea of emergence because I think, I mean, for me, you
know, where theory plays and theory is basically building models which are mathematical and computer
models to try to put together the pieces to figure out how they explain phenomena. And so it's,
when I say theory, you could substitute math if you like. And what the role of theory, the
biggest role in trying to understand the brain has to do with this idea of properties emerging
at one level out of the complexity at another level. I mean, physics has pioneered some thinking
about that kind of problem in terms of, you know, understanding how you take a lot of molecules
and they end up having properties of being liquid and having a certain volume and a pressure. And
so how the interactions at one level lead to these at a very different level, these qualities,
like being liquid and being viscous that, you know, in terms of molecule, I would have no meaning,
but it emerges out of the interaction of many, many molecules. But the apparatus that physics
developed is pretty specialized in those situations. And the brain is just unbelievably more complex,
of course. But always what we're, you know, where we really, where theory really gives you insight
is when you know a lot of things at one level and you know a lot of things at the next level.
You know, you know how these neurons respond to this situation and you know something about what
the neurons are made of and how they're connected to each other. But you have no idea how you get
from here to there. And you try to put them together in terms of what you know, you know,
you have some ideas and you find suddenly a principle, say, of how a circuit can be organized so that
these properties, these actual responses to the world up here will emerge from these neurons down
here. But again and again at every level, it's that process of new phenomena emerging out of the
interactions of complex interaction to many things at a completely different level. That's,
I think, the most striking point where math is absolutely critical. You can't believe that.
And the best example of that is one of the, I mean one of the best examples is the first example
of that, the Hodgkin-Hosley theory, which basically took a hypothesis about channels and gates and
used that equations down very simply just four differential equations and were able to completely
explain the action potential of the squid. That was the first real example of mathematics
doing something very specific. And you know, I think things have continued on like that.
I think that's exactly what, you know, it's what Ken said to you, how do you take pieces of stuff
and get emergency properties? The only way you can do that is with theory, and the only way you
can do that is, and the only way you can do that kind of theory is with mathematics. And that's a
big difference between neuroscience and, say, physics is every physics experiment is driven by theory.
And neuroscience experiments almost are ever driven by theory. And it's just kind of an immature
science so far, I think. I disagree that neuroscience isn't driven by theory. Certainly in the visual
system, most of our, at least a great deal of what we know was derived from the psychology of the
visual system that was discovered before we knew the neuroscience of it. You know, the three kinds
of cones, the opponent process system, that was all discovered by visual psychologists.
Well, before we understood the neural underpants. And Gestalt psychologists who contributed
at things about what salient in a visual scene and so forth. But it's, but,
well, I just push it experiment. I'm just making a kind of hard case there. I don't complete, I agree
with you guys. It is just not, you know, not completely. But maybe, I guess one way Ken and
Bard articulated is to understand how you explain behavior at some level in terms of the interactions
and dynamics at the level below underneath it. So, in a way, I guess my PhD, otherwise, I used to say
the world is like onion. There are many layers. And really, you can satisfaction the scientist
by be able to explain something in terms of what's more fundamental maybe underneath it.
There's another aspect of it that's really experienced why mathematics is really important
for neuroscience. That is, the nerve system has a lot of feedback loops. And there's positive
feedback loops. So this neuron excites the other neuron that excites back this neuron. Or there's
inhibitory, you know, negative feedback loops. Like any, you know, system with a lot of feedback
loops, it's very hard to predict what's going to have when you do something, right, somewhere.
So, I guess one example I like to give is, you know, this day people talk about connectivity,
right? And so if you know connectivity, the connectome among genes or neurons, it's very
important information. You really learn a lot by knowing the anatomy and connectivity. But the
example I'm going to give you illustrates why that's not enough to predict behavior.
Okay, imagine I give you a circuit with just two neurons that we know inhibit to each other. Okay,
neuron one, if it's neuron two, and neuron two, if it's neuron one, what are going to predict?
What's going to be the behavior to begin? Well, for a very long time, people think the behavior
would be half center oscillator. So neuron one is up, suppress the neuron two, and then for some reason
they go, this one goes up, the one goes down. Okay, that's the motif for generating movements.
So if you walk left, right, left, right. Okay, so that's the circuit to get this kind of pattern,
generation. But it turns out that when you really analyze the dynamics of this kind of circuit,
you can have some other way of behavior. You could have, for example, neuron one being up all the time,
and always surprising neuron two. So you have a switch. Okay, you give a little kick for neural
two, that switch is a neural one up, a neural two up, and suppresses neural one. So that,
you know, gives you a mechanism to build a switch. Does that make sense? And it turns out that
surprisingly, in fact, under some conditions, even if those two neurons inhibit each other,
sometimes they are completely in sync. So they go up together, they go down together,
right? And this is a very complicated, intuitive kind of behavior of perfect synchrony by mutual
inhibition. It turns out to be, you know, actually happening in real life in the brain. And this is
the mechanism now, one of the mechanisms at least, for explaining brain waves, that we all see,
you know, when you do EEG measurements, for example. This is, I think, a very, very interesting
example, we're only by looking at the dynamics with the help of math, we can really figure out
what's possible in this kind of system with feedback loops. And actually, let me just
build on that, because say in that case, you know, the idea of them working together is essentially,
they both fire, they both suppress each other, and then they both recover together, and then
they're active, they both fire, they both suppress each other. So it gives you a new intuition.
You study the math, maybe you hadn't thought of that before, but you've got down the equation,
you study them, and you discover this other regime where things are going together instead
of opposite, which is all you really thought of. And then once the math has shown you this new
regime, now I can explain it to you without any equations. So the math often acts like a scaffolding,
out of which we could, you know, that we use to gain an intuitive understanding that we can
then express without the math. And that's, at least when I really feel like I understand something.
But the math is incredibly rich. I mean, the full details are in there, but as you study it,
and you start to see it behave in ways you didn't expect, and then you start to try to work out
why exactly is it doing that, you ultimately come up with a story you can tell. And when you can
tell yourself a story that explains it, which is a new intuition that you didn't have before,
but you discovered it by working on the math and figuring out why it's doing what it's doing.
That's the scaffolding of math leading to the intuitive understanding.
So are you saying that it's not that you have an intuition, and then the math can explain it?
But you are saying that the math itself creates the intuition.
Elite, by banging on the math and the behaviors that you don't understand and figuring out why
they're doing what they're doing. You're getting new intuitions, yeah.
One thing to be important in this is that the correct math at one level may be quite different
from the correct math at another level. And the example of the computer is a good case.
So the computer operates by these binary elements, but of course at a deeper level,
they're not binary at all. They fluctuate fluctuations of voltage, but at the computational level,
we have to think of them as binary in order to understand the computation.
So if I understand correctly, I mean, math has always had a place in neurophysiology,
because when you talk about electric currents and voltages and so on, so you always had
a kind of high school math or college math when I went to medical school, that was there.
But what you are saying is that what we are doing is not just understanding what happens, let's say,
when a current goes along a nerve, how can we explain it physically and mathematically?
But that mathematical ideas allow us to understand things that without them, we couldn't even imagine
or dare. Absolutely. Yep, yep, yep. Yeah, like, you know,
if you press your hand and press your hand and you'll start to see little flashlight
for a while, you'll start to see geometric patterns like checkerboards flashing and things like that.
So why? What is going on there? And it's not obvious that this is all probably in the visual
cortex, but what's the mechanism? And math kind of gives you this umbrella. And for example, the
example that Aozhing gave of two mutually inhibitory neurons, one guy making one guy a switch,
okay? That's a switch and it's completely symmetric. One guy can be up, the other guy can be up.
Okay, that's an example, the simplest example of what we'll call spontaneous pattern formation.
And what math does is give you this sort of broad brush in which to explain lots and lots of patterns
that you see. For example, Ken has done lots of work on things called ocular dominance patterns
in the visual cortex. And it's all based and I've done stuff on the phosphor. I just told you
about the flicker and pressing your eyeballs. And they all work with the same basic principle of,
I guess we call it, he can hat that there's excitation or also we could call it Reaganomics,
where you help the guy that help your buddies and then you inhibit everybody else.
But it's called lapping ambition. I forgot to say that. You know, that's a,
it's been known from a horseshoe crab and all the way up and it pretty much can do,
you know, it's a very straightforward concept, but it allows you to explain
so many things. And it's all under sort of a, there's a very broad mathematical theory that
delivers all these cases. Well, are you Dunbar? Yeah, yeah, I'm sorry. I'm thinking back
something Ned said way back at the beginning about different multiple solutions to the same problem.
And just looking at the math alone and never mind the neuroscience, we find that
there are different kinds of math we can use to apply to a neural system or a neural network,
if I'll use that word. And so we find in our field that sometimes people get stuck in one or
another of these and maybe a conversation like this can help. What I'm thinking of is,
I edited a book where some authors, with a lot of different subjects, but there were authors in
there who felt that most of them neural modeling that people are doing today is all wrong,
because it divides neurons up into little pieces called compartmental models where you say
everything that's going on in this little bit is the same and then it's hooked to another little
bit by a resistor. And that simplifies the computation. But that's not really right because the
voltage along the membrane changes in a continuous manner. And so you really should use differential
equations. And so what you end up is pages and pages of so-called Green's function solutions and
things in the most complicated system, these guys have been able to work out is two neurons
talking to each other. And you get a lot of insight from that. And you get a lot of insight
from understanding how the diameter of the cable as it changes as you get farther away from the cell
body affects how the voltage is changing. And you can go on like that and spend a whole career
doing that. And meanwhile, you haven't gone to the next level up as Ken and many of us are interested
in. So it behooves all of us who are in this field, I think, to be aware of these different levels
and be willing to move up and down among them as befits the particular problem of interest.
Maybe what I said is super obvious. I'm glad to pick up on that.
One thing that I think maybe is maybe surprising to people, not in the field, is that
it's not the case that you put every detail into your model. If you do, you're all is lost. You don't
have any help at all. Because there's a couple of reasons. One is that there's many, many, many
details. Exactly what kinds of channels do you have in the membrane and what are the physical
properties of each of these channels? How do they respond in which way? What's their density?
And on and on and on? How exactly do your dendrites spread out in space?
Most of those details, we don't really know. We know that there are such things and that they
have some structure and we have maybe some range in which they all live. But so the more of these
details you put in, the more unconstrained by data, unconstrained by data freedom you put into your
model. The more unconstrained by data complexity you put into your model. And if your model then
goes and does something, how are you going to figure out why it does it when there's so many
unconstrained details? And so part of the real art of modeling is the art of simplification.
The art of knowing what are the key relationships that I want to model and then I want to understand
what emergency. And I'm going to throw all the rest away to simplify, to just understand what
do these dynamics of these simple entities that I'm going to model lead to and can I identify that?
Can I then understand what's going on in the brain with this simplification and then in some
way that's testable. This simple structure leads to all this stuff and it also should lead to
this other stuff people haven't looked at. So let's go and measure that. Somebody, I think what
Edan Segev is another theoretical neuroscientist and I think maybe he was quoting Picasso. I've
kind of lost track of the quotes but they talked about modeling as the lie that reveals the truth.
Because you start out with a lie. You start out with a simple question. You have to. And there's
actually a big to do in the field right now. There's something called the Blue Brain Project
of Henry Markham and Switzerland where his claim and belief is that he's going to put every detail
into the computer and now it is going to merge the brain and then we're going to understand the brain.
And I have to say every theoretical neuroscientist, I know thinks that is nuts, including myself.
Because something will happen. God knows why. God knows what it depends on. God knows which
detail was important and which wasn't. Maybe you can maneuver it to do things like the brain.
Maybe you can't. I mean at the moment they have some very basic behavior, things exciting and
inhibit other things. I mean nothing very specific unless they see we replicated the brain. But when
you throw in every detail, I mean the essence of understanding that when we build this scaffold
and then eventually I can tell you a story, that story can't have a billion moving parts in it.
Because our brains can't handle it. And so we don't understand it. Now if we knew what those
billion moving parts were down to the physical details, then like physicists where they do know
those billion moving parts in real detail, they can just put it in the computer and see what it does
because they really have control over all of that complexity from the data. But we don't.
And so we have to simplify and we have to come to
stories of how some interaction at one level leads to an interaction at another level. So that I
do models where very often, sometimes I use more complicated models but very often I use models
where I take a neuron which is a spatially extended object with dendrites that are extending over
hundreds of microns that are communicating with each other in complicated ways, receiving all
kinds of inputs at different places and integrating that input in complicated ways. And I describe it
as a point neuron that just takes a lot of input, sums them, does a certain unlinearity to them.
But then I study how these interactions, who's connected to who, the circuitry among these point
neurons, what behavior that leads to. And lo and behold, you discover things that tell you a lot,
give you new insight into how the brain works. Now we've thrown away a lot of detail of integration
and that worries me. Maybe those details of integration when we put them in are going to
radically change things. But the fact that you come up with an insight that unifies an awful lot
of behavior in a simple way and it's testable and the test bear out, that gives me a lot of confidence,
although not a certainty, that all those details are not going to overturn what I've learned by
studying things at this level. Well, so you learned both by what you can explain and what you can't.
So you can't explain something. It shows that the detail was important. Well, I know, because you
don't know, maybe you just missed it, right? You know, you don't know why you can't, you know,
negative results. When you can, then you've really got your hands on something. It's hard to get
non-existent proof in these things. Yeah. The other thing I think this is building what Ken said is,
you know, you start with something simple and you see what it does, a port neuron or whatever,
and then you use that to build up your model. I think I want to point the other side of the
coin. We've been talking a lot about what mathematics can do for neuroscience, but the other thing is
what can neuroscience do for mathematics? And, you know, there's one of these big questions,
or one question that many of us are very interested in is if you've got many, many detailed models
of neurons hooked together, is there a principled way you like a simplified model out of that? And
people call this trying to derive a mean feeling from some sort of spiking neurons.
Now, Jing has done a lot of this. I've done some of this. I don't know about the other guys Ken might
have, but I get that this requires new mathematics as well and difficult mathematics because there's
noise and stochasticity and things like that. So I think, you know, there's this great positive
feedback that neuroscience math and a lot of nice mathematics has come out of trying to answer some,
even some of the simplest neuroscience questions. We only heard about every other word of
Oh, yeah, it's cutting out a lot. Oh, well, you want to? Yeah, seems to be bad, right?
I don't know what's the matter. Is it my connection?
Who knows? We don't know. Okay.
You will go into power. I mean, I'm hearing enough that I can make sense of it, but maybe it's
because I know I know some of the words are being left out. Maybe it's just we'll recap a little
bit what Bartle is saying. Basically, he's saying that, you know, I guess, rephrasing it, you could
say, you know, in the past or even today, physics is really a major source of mathematics, right?
A lot of physics problems, you know, particle physics, especially, for example, has led to
many new branches in math. And maybe today, biological sciences, especially neuroscience,
in fact, is going to provide a new source for, you know, problems and maybe ideas that will
inspire new math. So one of the specific examples Bartle mentioned is the so-called
the mean field theory. How do you go from most biologically based action potential kind of
neural models and neural network models to population description, okay? Where you say, you know,
for each little group of neurons, you know, they all within this group, all the neurons are more
less doing the same thing. What I really care about is the dynamics of the activity, the overall
population activity of this neural group. So can I find a mathematical way, mathematical way,
to derive, you know, mobile, physically based, spiking neural model to a population description?
That's one example. And that's what I said. Yeah. Really?
Essentially, I guess I just add it's really true. People actually feel like
that today there is going to be really a lot of interesting questions in part from neuroscience
that's going to inspire new math. So for example, not just neuroscience, of course. I guess I
would just mention one example is, you know, the kind of dynamics that really deviate in very,
very high dimensional space. Okay. So if you think about, you know, really the nerve system
described it in certain ways, you basically need the zinnians of variables, zinnians of
activity variables for neurons on neural populations. So you have to describe the dynamics in a very
high dimensional space. And you know, the math of dynamics in very high dimensional space is
true in neuroscience, and maybe true in some other scientific branches, given the data we now have
today. So, and I think that's one of the examples where, you know, the science, including neuroscience,
drives mass. Yeah. I can give another example that I just ran into on a thesis committee of a
physics student at Rockefeller named Bo Tei Fumei, who was interested when he comes down to the
random walk problem, but expressed in terms of a neuron, you have a neuron, some noise sources,
the membrane potential is fluctuating around. And the question you'd like to ask is,
how long do you expect it will be before one of those fluctuations goes over threshold and the
neuron fires? And so if you look at the fluctuations as what was called a random walk in other areas
of biology or chemistry, you know, this is a problem where a lot of work has been done, but he was able
by looking at it from the point of view of a neuroscience problem to produce some new mathematics
that adds to that whole body of work. So there's another, just an example.
I think a very beautiful example of making some new math is some work done by Fred Wolfen,
his group in Germany that was recently published in Science, where without, they're trying to
understand certain patterns that form in the arrangement of the neurons in the visual cortex of what
neurons have been, the primary visual cortex are responsive to light dark edges of a certain
orientation, and they care very much about the orientation. Change the orientation of 20 or 30
degrees, they may stop responding. And the what orientation they prefer is laid out in a way that
sort of rotates periodically as you move across the cortex so that all the orientations are
being represented. But it's laid out in a particular kind of pattern, and Fred, so in physics,
there's quite a lot of literature on pattern formation, on how, you know, interactions among
chemicals may lead to some kind of stripy behavior and so forth, to some pattern emerging. And there's
a very established literature of how you can go about analyzing those processes. Those have always
dealt with real value variables, though, so I don't know if you all know about real numbers and
complex numbers, complex numbers, important part of mathematics, but in the pattern formation
literature, just because these are physical variables, these have all been real value variables that
are organizing into patterns. It turns out that the proper description of these patterns, you need
to use complex value variables, and that required Fred to really revise or extend the existing
methodology on pattern formation into that case, and then, and he also had to bring another extension
to it, which is in physics, all of the interactions are local. Things just talk to their neighbors.
They're not able to talk to somebody way over here directly, because there's no long-range
connections between molecules, say, but in among neurons, there are long-range connections,
and so he had to bring in the role of long-range connections as well into how this influences
pattern formation, and the, the upshot is, I mean, he spent 10 or 15 years developing this theory
layer by layer by layer by layer, and the great triumph was he was able to predict that the structure
of these maps, it should have a certain signature, which is the density of, of points where all
orientations meet, which are called pinwheels, or singularities, and that that density in the
right unit should be the number pi, and they went, and they measured it, and some people had measured,
and to do that, they had to develop new quantitative methods of measuring the density of pinwheels,
because there's always a lot of noise, and it depends on how you filter things, and they had to find
ways of filtering out the noise that didn't filter out the signal, which people, people hadn't had
good ways of measuring pinwheel density before, so they developed new mathematics for that,
and people had studied maybe a few maps at a time, but they studied 100 maps with 10,000
singularities in order to get good enough statistics, and they found that the density was pi to
within plus or minus 2%, meaning that, and what, what that meant for us, in terms of the physics of it,
or in terms of what happened, how do these patterns emerge, is basically this number pi, this organization
that's characterized by this, this number pi, emerges very naturally out of self-organization,
meaning it's not, every cell isn't told by genetics what orientation to develop,
rather they develop by interacting with one another, maybe they excite their neighbors, and they may
have some long range suppression, and so they're developing through interaction, and this self-organization
among all these moving parts leads to this pattern, and the fact that he was able to show that you
get this very robust prediction of pi under that scenario, and then he wouldn't measure it,
and that's what was there, and it was there across three different species that are separated by
hundreds of millions of years in evolution, and in fact, separated so far that the common ancestor
had such a tiny visual cortex that it probably didn't have these patterns at all, meaning that it had
to evolve, that this pi pattern had to evolve twice independently, well that would happen naturally
if it's just, if it happens by the self-organization process, if you think it's genetically specified,
that would be very hard to explain, so I think this was a huge triumph, and it was advanced in the
math and advanced in our understanding of the biology. An argument that the pinwheel structure isn't
genetically determined, at least not in any simple way, is McGonker serves ferrets where he was able
to set up something where they use their auditory cortex to perceive visually, able to rewire
the ferrets at a very early age, they also show that auditory cortex shows that pinwheel structure.
Although actually, it does show pinwheels, but I don't think it's that characteristic structure
that Fred has identified. So I think it's a different pinwheels.
Yeah, I think it's a lot, I think it's different, I'm not actually certain of that, but I think it's
different. Well, we seem to be at a dead moment, so I'll just change the subject slightly and say
there's still plenty of room for new progress in this whole field and the diffusion of molecules,
as I mentioned a couple of times. I just want to say, I suggested in a book chapter some time ago
that in studying the brain we needed to combine not just what neurons are doing, but what molecules
are doing it and propose using a kind of finite element modeling, which is what engineers do who
build skyscrapers and bridges, looking at the physics of how little blocks of matter interact
under pressure. And this was rejected by referees as being ridiculous, although I thought it might
have something to do, for example, with studying tumors in the brain and how a tumor might press
on another part of the brain and cause something not to function because pressure might be changing
the activity of channels or something. But I just saw in a recent document that I'm reading, which
well, it's a PhD thesis, that this is now being applied, this kind of idea in the
eye where people are working on retinal prostheses, you know, and the idea there is to
in a blind person who's lost the visual receptors, it's known that the so-called retinal ganglion
cells, the cells that send the output of the retina back to the brain, those are often still
functional even when the receptors are not. So the person is blind, but that part of the eye is good.
So the proposal and it's been tried now in real life by different groups, you know, is to stick
some sort of array of electrodes into the back of the eye and stimulate these retinal ganglion cells
to provide some sort of prosthetic vision. And so this thesis that I'm reading has done just,
he of course never saw my proposal, it must have come out of his own ideas, but
the idea of making a finite element model of the fluid in the eye and then using the laws of
you know, the laws of electricity and magnetism to figure out how when you
put a little current into an electrode, how does that spread through that fluid and which ganglion
cells does it affect? So you can figure out what's the best way to compute what, now in a computer,
you know, what's stimulus to put into that electrode array to get the most natural vision. So there's
a very new kind of extension of what we've all been doing, looking at just one neuron, talking to
another and taking into account this ionic environment. And I think, you know, this is a very positive
thing to be happening right now. You may know more about this than I do.
I think, you know, you bring up, okay, so it seems we've sort of switched a little bit to pathology,
I guess, because this is a bad retina. And can you guys hear me every other word or do we speak to
twice for or, okay? Yeah, right now. Okay, okay, all right. So, you know, let me give you an example
of where math can be very helpful in in pathologies is one of the classic mechanisms for Parkinson's
disease is something called deep brain stimulation. And the problem with deep brain stimulation is
it's really, really stupid. It just does the same thing over and over again. And it doesn't
depend on any feedback or anything else. But there's a number of groups. There's groups in Germany,
and there's groups in the US that have developed much smarter, much smarter versions of deep brain
stimulation, based on writing down some goals for what happens in the basal ganglia, whether you treat
them as oscillators, or as excitatory inhibitory networks. And by using that, they've been
in a motor come up with techniques or deep brain stimulation. And this is this could not be
have done could not have been done without the math predicting this method would work.
But much more efficient ways of doing deep brain stimulation that won't come on until
there's a sense that something's wrong. And when it comes on, it does it much more efficiently
instead of jolting it with, you know, hundreds of millivolts of stuff they can do things in
in much lower levels and therefore improve things like battery life and also reduce damage a great
deal to the brain. You know, some of that work is going on at Rockpower 2 in the lab of Donald
Faf, who I want to mention because he's on the board of this organization that's sponsoring us
today. He has a student doing deep brain stimulation in a mouse model of traumatic brain injury.
And they're not, perhaps, unfortunately not doing what you say in terms of the math of what the
network is doing. But what they have done is to try out different modes of stimulation, you know,
whether it's a random or a chaotic or a pure oscillator, a regular oscillation. And do these
different modes of stimulation produce better results in that mouse model. So again, this is
something kind of new that's going on in just a few places and very positive, I think.
Especially for this audience, maybe it's also worth mentioning something related. So actually,
I'm on my way to a kind of short summer school on so-called computational psychiatry. So there
are several places including Yale and Germany and UCL in London where people feel like if we have
some, we are starting to have some understanding about the second mechanisms of brain structures,
especially the prefrontal cortex that are implicated in mental disorders. So if we know something
about the circuitry in this part of the brain. So it's not, all the brains are really the same.
Okay, so we know that there are early sensory areas that are, you know, optimized for
information processing. And then there are some, you know, kind of more cognitive areas that are
more implicated in decision-making and control of our behavior. So prefrontal cortex is one,
you know, the best example perhaps of cognitive type circuitry that's implicating in many
mental illness disorder types. So there's a sense that, you know, and again, this system's
very complex to just try to understand by intuition along. And the second modeling has helped together
with experimentation to kind of tease out, you know, what really might go on in normal subjects
as well as seeing patients, like phoenix patients or autism, for example. I mean, we know a bit
about schizophrenia, maybe much less, very little still, but there's something about schizophrenia,
but even less on autism, I think. But still, the idea is that if mathematics really, you know,
biologically based model of prefrontal cortex, if such a thing can be done, would provide a very
useful platform to really explore what may go wrong if this goes down, if that goes up, you know,
if, you know, at the circuitry level. And if that's the case, it really provides a two, first,
to also try to understand what really underlines cognitive deficits in mental disorders.
And there's a, this is just hypothesis and speculation, but there are, so the cerebral cortex is,
you know, the main, the stuff that makes us smart. It's what, it's peculiar to mammals.
The part of the brain that you ball specifically in mammals, to all the folded up stuff you see
on the surface. And it's what you see with and hear with and think with. And pretty much,
I think it's fair to say that your whole conscious life is computed there. I don't want to say
where, what consciousness is, but let's say whatever does enter consciousness is computed there.
And it has what has always fascinated anyone who tries to study cortex is that
it looks so much the same no matter what it's doing. Whether it's doing, you know, here's a piece
that's, that's analyzing the visual scene. Here's a piece that's analyzing touch. Here's a piece
that's analyzing audition. Here's a piece that's involved in motor planning and, and thinking and,
you know, speaking. And there are differences. There definitely are differences. But the first
thing that strikes you is how much they look alike. How much they seem to be the same architecture,
the same processing unit. And so it makes all of us dream at least that there is a fundamental
processing unit that was invented in a million evolution that's very good at doing something,
which when we really understand it, we'll be able to fill in what that something is. But something
like being able to take a varying world, find invariant structure in it, represent that invariant
structure associatively, and then do that again and again and again. So there's also a hypothesis
that some diseases like schizophrenia or autism might be not specifically cognitive deficit to
the cortex, specifically deficits in, you know, particular cognitive regions. But they might be
deficits in that processing unit that therefore would manifest not only in higher order cognitive
processing, but you would also see it in low level visual processing. And indeed in schizophrenia,
for example, there's something in much of sensory cortex called surround suppression. It's a
lateral inhibition that Bard was talking about before, which is that if I have a neuron that
responds to some visual stimulus here, then what's around it, the context can suppress the
responses somewhat. And it turns out that the people with schizophrenia have much weaker
surround suppression in primary visual cortex. And you can imagine that that's some kind of,
you know, that that could be some kind of global deficit in integrating, you know, local
information here or local information there, putting it together, that perhaps in some way
that I couldn't explain to you might add up to the cognitive deficits that we see as schizophrenia
when that deficit is happening in the right brain region. So we don't know, but it's possible that
these are diseases of the cortical processing unit and not of very specific cognitive functions.
Would that kind of problem with the processing unit also be able to explain why schizophrenia
is sometimes a deteriorating condition? I always wondered, imagine if once that processing is that
way, then it should stay the same way. Yes, I know it's a good question. You know, I can't say that
I've thought out all the implications of this really deep into schizophrenia. I've been struck
by the fact that there are these very low level deficiencies in schizophrenia and certainly,
in autism, it's been shown that at low level sensory areas, there's much more variability
in the processing. The mean response to a given stimulus is the same, but there's much more
variability in the response. So that it gives me the hint that there are maybe more general
processing problems in the cortex, but I don't know enough at that level of detail.
There's lots of very specific deficits in, especially in the prefrontal cortex,
in the inhibitory circuitry, some of the receptors become slower, they become weaker,
maybe compensation because there's also deficits in the excitatory stuff. So I think it's basically
what Ken was saying is that it's this basic circuitry, this seven layer or six or seven layer
structure, and there's something that goes wrong with, and Ken pointed out, if you weaken some of
these things like Xiaojing, if you weaken these inhibitory things, for example, which inhibitory
guys, or what kind of keep everything controlled. I mean, the cortex is highly recurrent, highly
excitatory recurrent, and because of that, any kind of perturbation of this controlling inhibition
can lead to all kinds of dramatic pathologies, activity where you don't want it not being able to
hold activity, spread of activity where it shouldn't go. That's, for example, epilepsy,
and spontaneous activity when it's not there, for example, hallucinations that are some of the
negative things that happen, positive things that happen with, so yeah, I think, and math can
really help us tear apart how these deficits in vision or deficit in the circuitry can lead to
some sort of macroscopic measure. For example, one thing that they found associated with cognitive
deficits in schizophrenia, people have greatly reduced certain kinds of problems in the brain
Xiaojing alluded to, these ones that are related to mutual inhibition in generating these 40 to 60
Hertz rhythms in the brain, and those rhythms are shown to be diminished in schizophrenics also.
So, math can kind of connect these deficits in circuitry with the macroscopic rhythms and
hopefully some of the cognitive deficits. So, to relate to what Barton King just said,
I guess one way to view this is to say, yeah, there's a canonical general layout, there's a general
in an organization of, say, cerebral cortex, and that's universal, let's say, okay, it's true in
early sensory areas like primary visual cortex as well as prefrontal cortex. But what's interesting
is that there could be some quantitative differences, right, and so to use the same analogy as used
before, you have the same matter, same material, which can be either in the liquid state or solid
state depending on the temperature, right. So, you could say just by changing something in
temperature in a great fashion, sometimes by a small amount, if you are near the, you know,
solidification critical point, just really the small change of certain things will
give you a very different kind of behavior, right. And that's how I see it, like, you know, comparing
sensory area versus prefrontal cortex, okay. So, and that really is very important and very
interesting to realize. And then, you can ask even by the same amount of change of certain things
due to genetic defect or due to environmental, you know, insult, what's the impact?
You know, sensory system versus the more cognitive type of system. So, by understanding the
operational mode, the behavior of each system, we can, you know, indeed address, you know, look
at, exam, how, you know, abnormalities can occur in each of the areas.
Let me ask Ken a question, because you emphasize the unity of structure across the cortex, but
of course, as you well know, and I'll do, there's differences, you know, more than 100 years ago,
Broadman described all with these numbered areas, and they have slight differences in the
population of different cell types or the lengths of the dendrites or areas.
I'm sure you didn't mean to denigrate that, but just, do you feel that those anatomical
differences that are easily visible can be related, in fact, to the functional differences that
Shaoxing is talking about? Or, you know, should we be looking at whether we can understand why
those differences are there, or is it just sort of peripheral and unimportant accidental, I mean,
to say? No, I mean, I think ultimately we need to understand those variations, but just,
I very much take XJ's point, sorry, we call it XJ, so that's just a call again.
Very much like SJ's point, that, you know, there may be some commonality, but it may be
operating in a different regime, and in particular, one difference that's very well known is that,
in primary sensory cortex, the excitatory neurons make much fewer synapses onto each other,
so an individual neuron in primary visual cortex might receive 700 excitatory synapses,
but in prefrontal cortex it might receive 20,000 excitatory synapses, and that has an obvious,
theoretically what you expect from that is that the place with 20,000 excitatory synapses is much
more likely to be able to generate its own activity in the absence of a stimulus, which in fact is
one of the big differences between frontal cortex and primary sensory cortex, where without a stimulus,
there's some background activity going on, but it doesn't generate the kinds of activity.
It's stimulus, but frontal cortex has to generate its own activity, it's got to make motor plans
and make the animal go, so that's one example of, you know, a numerical change that leads to a
qualitative change in your operating regime, and you may have a lot of the same structure underneath,
I'm sure that that's probably, you know, if we got down to every one of broadment areas,
every one of them would have some specialization that gives us some little, some, I mean, every
specialization and structure has got to be there for some specialization and function, and we don't
know what it is, but I prefer, I prefer right now to try to focus on, you know, what is the basic
operation of this unit rather than before we tackle all of its variations, but I do think between
sensory and motor, those are really two fundamentally different things that we need to each understand.
Well, how about something? I want to add something, if I can, is, it's, you know, there's also
who you're talking to and who talks to you that really matters, so the prefrontal cortex gets
information from lots of other different things than the visual cortex, so even though the
the hardware is the same connectivity, and I guess we come back to the old connect story,
is different, and I think that can also be a big reason why there's a difference in
functionality. I mean, it's just simply that you're getting, you know, the, the, the CPUs are the
same, it's just the, the, the memory and who you're talking to is completely different.
Yeah, that's right. Right on the water finish.
Oh, yeah, I think, you know, the, the point you were making before about, we don't know what the
units are yet. Look, I, I, I take the unit story very seriously, for one thing, it seems evolution
is generally plausible. We know that there's a reason to think that the, you know, increase in
cortex that we have from earlier stages is just duplication. You know, we have all these areas
that seem to be basically descended from retinas that, you know, they needed more cortex to make
another retina. So there does seem to be some reason to think that from an evolutionary point
of view, it's a matter of just, you know, duplication, but it seems amazing that we don't know what
these units are. How many neurons in such a unit would you say?
What's the size of the unit? So what we think of as... What do you mean by it?
Yeah, I mean, the unit is a totally hypothetical speculative object right now, but,
what's, what's commonly thought going back to the work of Hubel and Wiesel 40, 50 years ago,
is that about a square millimeter of cortex is sort of processing a local, a local bit of
information, and that contains about 100,000 neurons. And then when you consider, though,
that these different units are talking to each other, because you don't only process local
information, you're modulated by your context, you know, so how big should there should we take
the processing unit to be? But that gives you a beginning. I've asked the same question to other
neuroscientists, and sometimes I get the answer 10,000 neurons. I mean, it just shows how hypothetical
it is if we don't even... If we don't have methods of estimating that... Well, I mean, we know how many
neurons there are in a given area, but the question is how big an area should we call a unit? Yeah.
Yeah. He has to... Well, there is a great... Well, there is a great...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
chats
I'm every perception that we make, every thought that we have.
And the whole world of metaphor depends
on a more sophisticated pattern recognition.
Now, my simple question is, is it conceivable
that there is some mathematical expression of whatever
matches one pattern to another?
Because we can't work without it.
That's my question.
I think that's the holy grail for understanding course.
I mean, I think we will get there,
but we're not there yet.
So just to say something about what we know about pattern
recognition, what we don't know, it was shown a long time
ago that pigeons can recognize patterns
that we have not been able to make a machine that can recognize.
So for example, it was shown, I think, 30 years ago that you,
so this was done by a famous, maybe infamous Harvard
psychologist named Heron Stein.
He got a bunch of pictures from the National Geographic
that some of which had people or parts of people in them.
Sometimes they were like a tree trunk with some fingers
on one side and others that had no people in them
or no parts of people.
And he got pigeons to, he trained pigeons
so that they could recognize the difference between a picture
that had a person or a part of a person.
And they did very well, better than some of the students
in his lab because that's why they're flying things that
could do better in aerial photograph.
They could do better in the aerial analysis.
We don't have any kind of artificial pattern recognized
so they can do that.
We don't know how it's done.
But I do believe that when we understand it,
we will understand it mathematically.
Until you can describe mathematically how you take the image
and what you do with it in order to recognize the pattern,
then we won't understand that.
And I think we will understand it.
My own feeling is that we're not going to figure it out
by trying to invent algorithms.
We're going to figure it out by studying how nature did it,
which is how our brains do it.
Because I don't think we're smart enough
to reinvent that ourselves.
I think we have to discover it.
It's an important point that I agree with what you just said.
But it reveals a real tectonic shift in this field
where there was a time when people thought they could discover
these just by thinking about it.
Just by thinking about it.
Yeah.
You know, David.
The analogy I'd like to make is that quantum mechanics
is the weirdest theory.
I mean, you get used to it and it makes sense to you.
But never in a billion years would anybody have figured it out
just by thinking about it.
Physicists had to knock their heads on atoms
and the weird way that adults behave
and knock their heads again and again and again
for decades until they finally somehow knocked their heads
enough that they managed to get to the point of quantum mechanics.
And then it started explaining things.
And I think understanding how the brain does instantly
recognizes things.
I think it's in the same category.
I know there's a line over here, but I really
want to throw in two things.
First, famously in that Heronstein experiment,
people complained just what you said
that the birds fly around so they would know about trees.
So he got the pigeons also to recognize scenes that
had fish or not fish, which the pigeons had never seen.
But no, my point was going to be, yeah,
we don't know how this works.
But I firmly believe that part of the story
is going to be what I've said earlier this afternoon,
the interaction of the young individual with the world.
The way we're going to recognize patterns
that connect with each other is because they occur together
in time or space on and off during development.
And parts of the brain are, I believe,
wired in such a way as to respond to those connections.
And that's what you cannot get by describing patterns
geometrically or with words.
It's all part of a scene.
And we know that we recall things in connection
with stimuli that were part of that scene
when we first saw that pattern that may be totally unrelated.
I recognize a certain Beethoven symphony
because when I played the record years and years ago,
there was a scratch at one point.
And that scratch is in my brain at that point in the music.
And that's how I know that with that.
So it's the whole surround, not just one thing that
makes patterns.
OK, I'll shut up.
I'm sorry, I'm sorry.
I'm actually a mathematically oriented psychiatrist,
which is relatively rare thing.
In 2003, I organized the first symposium
at the American Psychiatric on Game Theory in Psychiatry.
And it was roundly ridiculed.
I just have to, you know, it and I
were roundly ridiculed for this notion
that game theory could be applicable to psychiatry.
So I've been thinking about these ideas for a long time.
And I want to thank this panel for bringing this here.
I think it's absolutely fantastic.
And cutting edge kind of discussion.
I'd just like to say that if the hypothesis for today
is whether or not the mind can be described by mathematics,
I would consider the negation of that.
I mean, if the mind cannot be discovered
explained by mathematics, how could you possibly explain it?
What other language are you going to propose to do it in?
First of all, you're irreducibly a dualist.
If you propose another language, you're irreducibly a dualist.
Second of all, you're rejecting evolution
because evolution is about maximization under competition,
which is a mathematically solvable problem.
And third of all, you would have to propose
that there's some brain-dependent language that
do a better job than mathematics, which I think
is as close as we get to a brain-independent language.
I don't think we need any comments on that.
I don't think anybody will disagree with that here.
Well, I don't know.
Maybe that's a law.
But OK.
That's just so fun.
The notion of statistical mechanics
leads us to conclude that phenomena like pressure
merge from the interaction of molecules
is the prediction of the panel then
that a complete physiological and structural description
of some neural system is going to then
just lead to the deduction that we have in front of us
a cognitive system.
In other words, once the physical language of a system
of neurons is described completely,
do you think, or is it the opinion of the group,
that we will then come to the conclusion
that we've in essence described the mechanism of cognition
without being able to fully describe what cognition is?
Perhaps it touches on the notion of consciousness
versus not.
But just let's say even from an information
theoretic perspective, what are the thoughts of the panel?
Well, I said something that's relevant to this earlier.
The example of the square peg in the round hole.
OK.
So you could get an explanation of that, quote,
explanation in terms of the elementary particle
clouds, but it would just obscure the explanation
that you can see in terms of geometry and rigidity.
You could add to it by explaining
why those things that are rigid are rigid
or when they would fail to be rigid.
But some explanations have an appropriate level
that isn't illuminated from below.
And sometimes the what's below is so complicated
that you have no hope of deducing the molar behavior
from what's below.
So I think we have to find the right level
for any given phenomenon that we're interested in.
Is this in essence become a sort of short-time question?
But that doesn't answer his question, which is,
will this emerge?
And will consciousness or cognition
emerge from this low level?
And I think as reductionists, or at least most of us
as reductionists, the answer has to be yes.
I don't know how that levels will have to ignore.
At what course grading we have to do,
but I don't see how we can't answer that question is yes.
Look, if there's a distinction between being a physicalist,
which I am, I don't believe in any soul
that somehow interferes with the elementary particles,
there's a difference between being a physicalist
and being a reductionist.
So you're a physicalist, you think, OK, it's all just matter.
And somehow the consciousness and cognition
have to come out of matter.
But that's different from thinking.
You're going to be able to take any explanation you get
at the cognitive level and reduce it to an explanation
in terms of the smallest items.
That is a very adventurous, controversial claim,
even for physicalists.
So that's why I keep saying it's the right,
you have to find the right level.
I guess the old is in, I guess I would like to add is.
Well, I understand they all turn it.
Well, maybe it's pretty fair to say that for very good reasons
that neuroscience has been for a very long time,
focused on early sensory information processing
and the motor behavior.
But it's becoming more and more frequent, common now,
that people feel like you can study
the neuro-biological mechanism of cognitive processes,
such as decision-making in a rigorous way.
So that's a sea change in my mind.
So many things that people thought
were in the realm of psychologists
and now really been studied in a very cross disciplinary fashion
with cognitive psychologists and physiologists
and computational theorists.
They're going together to understand
really the mechanism of even cognition, I'd say.
And the answer to your comment, it seems
that one of the aspirations then is
to be able to come up with a descriptive mechanism
to talk about very high-dimensionality systems.
In a sense, it's very, for any of us,
it's very hard to do that in any discipline, it seems.
And so for such an perhaps the most exceptional high-dimensionality
system, it seems very hard to talk
with any micro and macro phenomenon other than the physicality.
From a CS person's point of view, it's hard to talk about.
We can come up with dumb little models.
Seems that logic itself has some problems with it.
But I'm just very curious as to what
practicing neuroscientists think of this.
I mean, I think every mental entity has ultimately
got to be understood as some in neural terms
and will be understood in neural terms,
except maybe conscious itself.
Given all of this is going on, why do you have a sensation?
I don't know that that ever can be answered.
But at least every mental structure
has a neural substrate ultimately, and that's
an article of faith, I think.
But I would distinguish between has a neural structure
is grounded in the neural structure?
That's on one side.
The other side is can be understood in terms
of that neural structure.
So there's a level of understanding.
You can't always get more understanding
by going to the smaller and smaller parts.
So I think that's a totally different point.
No, no, you work upwards.
You get the little, I'd explain the next guy,
and then you work.
I don't see why this is such a contradiction.
You work downwards too.
And sometimes the downwards is better than the upwards.
Oh, I agree.
I agree.
You have to meet the two ends at some point.
I mean, I think the point is that you're not
going to describe the brain in terms of elementary particles.
But it is ultimately made of them.
You've got to work at some intermediate level of structure
to understand the next level of structure.
But we're going to understand structure
we've made out of neurons that correspond
to the structures of our minds.
That's what I believe.
Maybe it's time to throw pen rows in here just briefly.
Because if we're physicalists, and believe that it's all
done with math, there's extensions
that we haven't made yet.
And there are people who try very hard to bring in stuff
that we can claim as physical.
But it's so complicated.
Quantum mechanical fluctuations in microtubules
in neural neurons.
In neural stools.
And so I think all of us believe that's probably wrong.
And I think we can understand how it might be wrong,
just because of all the complexity of these levels
that we've already talked about or enough to explain it.
But that's an intuition.
It's not a proof.
So that remains to be seen.
But it'll still be physical.
I think that there's no argument.
Go ahead.
Hi, everyone.
First, I want to feel that I love what you said today.
And sorry for the language.
But I'm the kind of people who like to formalize life
and whatever.
Even my speech, for example, this is why I was late tonight.
Because my husband speaks in such a linear way
where I speak of things in a three dimensional way.
I mean, I start and I open the parentheses and close.
And I remember exactly where I opened this.
And then I open another.
And so after the third parentheses, he says he's bored.
But I know that people is not bored.
It depends on which people are talking with.
I mean, for example, yesterday, now two days ago.
That's your parentheses?
Or is that your question?
Sorry.
This is one of the first questions.
Sorry.
And two days ago, I was, let's say, bored about some online
agendas, because I couldn't put inside what
I was thinking about.
I mean, also because my email address was hacked twice.
So I prefer to do something simple.
And I start putting some sticky notes on the mirror.
Look, OK.
Let's say the pink one for projects,
the blue one for contacts and meetings, like this.
The yellow one for all the other stuff
and the internet I have to do right now turns for this.
And the green one, something, all but the pieces, which means,
OK, you get up on this.
Share it, get out.
And then I start to divide into columns and rows,
because I work for databases since 15 years.
15 years.
15 years.
Yeah.
Yeah.
I don't know.
And so I just, in the sticky notes,
I put a kind of bullet points.
So I mean, sometimes I realize that my mind is like a database.
And that's why I love math, physics.
For example, I remember when I started physics, the first physics
hasn't.
After a while, I still have all the ground,
because everything happens.
It comes from a dynamic load.
So even a pen falling down.
Well, I stare at the pen because that pen was connected with this
and this and this.
So it was full of math, physics, and whatever.
And thank you for letting me understand better.
Thank you.
Thank you.
Sorry about that.
Thank you.
I want to talk to the guy who is a skype or whatever.
He told about, I don't know his name.
Sorry.
Anyway, for all of you, he told about computer.
You're still listening.
He's still listening.
He just checking his notes.
He's always the same.
He's starting from an input.
You can have all the same output.
Well, this is about, you can call them deterministic algorithms.
But in logic, there are also non-deterministic algorithms,
which are like this, something that can happen in different way.
Even the starting point, the same.
But it's hard to develop in a computer.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
So I was really interested by the idea of, sorry, how, you know, whatever type of unit we're
talking about, there may be similar ones across the cortex.
I just was doing research in like cross-modal stuff.
And so I'm really interested in that.
And I guess my question is just, you know, do you think that even though we're talking
about reproduction of units in a biological way, it may be just sort of the most efficient
way of, you know, growing the brain through time.
But there could also be this byproduct, which, you know, contributes to our sort of unified
experience of the world in the sense that these units are similar and therefore can maybe
communicate through, you know, communicate to each other more easily.
And I just wonder if any of you have anything to say about that?
I mean, yeah, that's confusing.
Well, just the idea that like whatever unit you're talking about, they are reproduced
across the cortex.
And does that, I mean, do you know if that has any, does that have any effect on how easily
they communicate to each other and how something happening in the visual cortex can be referred
to something in the auditory cortex?
Or is it just really, you know, basically, anyways, I'll stop.
But I thought it was interesting.
I mean, I should just say, I mean, there's unity at a lot of levels.
There's unity at the, you know, when you look at this one millimeter chunk that it, you know,
it looks to fruit, you know, there's a lot of similarity no matter where you are in cortex.
But there's also unity in, you know, a lot of sort of higher level themes of how different,
how the different chunks talk to each other across millimeters, how one area talks to another
area and talks back, how they get their input from the thalamus and how they send loops
through the basal ganglia.
There's a lot of themes that are conserved across, you know, many levels of the hierarchy.
So, but we're still, it's very early days for really figuring out what the hell that's all about.
It might be useful to mention that just the basic mechanisms of communication are pretty much the same.
I mean, there's a large number of neurotransmitters that have been identified, but there's a couple of major ones,
you know, glutamate and gala.
And those are used all over the brain.
So that allows them to communicate with another one.
The other minor transmitters modulate that activity and they may act differently in different places
to help produce the different types of, you know, functionality that exists in those different places.
But the overall scheme is very much, you know, calcium transmission through ion channels is the same everywhere.
I'm sorry, I mean, there's probably ten kinds of calcium channels, but you know what I mean?
Right, right, right.
It's the ion that sends signals.
So that facilitates what you're asking about.
Right, I guess just to crystallize, my question is, you know, if you have one unit over here and another unit over here,
and they're similarly structured, you know, the brain's whole function is to communicate things to two different areas.
And so, does that, I mean, since you guys know so much about this stuff, does that similarity in structure
in different areas does it often facilitate the communication, or is that an unrelated thing?
Is it just, I want to hear the frequency and...
I mean, you can, I could sort of, I could sort of say, I mean, there are some theories that if one area doing something,
for example, a famous theory is that one area is oscillating in a particular rhythm,
and it's much better, it has a better chance of communicating with another area that has a similar type of oscillation.
So I can say, you know, I don't know if that, I'm paraphrasing what you said there correctly or not,
but there, you know, that's a, that's so-called binding theory, and there's other things like that.
So yeah, I mean, you guys that are similarly firing are more likely to hook up together and communicate.
And there's one other thought I have, it just, I mean, somebody, the big commonality that you see is this six-layer structure of the cells.
And although we don't know exactly what it means, it's clear that sort of different kinds of input enter in different layers with different functions.
So it's so-called feed-forward input comes into layer four, top-down input tends to come into layer one.
So we don't, you know, we don't understand what that's about, but there is a commonality there,
which probably means that, you know, anybody who sends an input into layer one of somebody else,
that has a certain meaning as opposed to sending it into layer four of somebody else.
So that's maybe a lot of the lines I'm thinking about.
Yeah, that is, yeah, that helps. Thank you.
Okay.
I just wanted to comment on it since I'm here at the mic.
The metaphor is another way that different parts of the brain communicate using this sensory language, like sharp cheese, lavender, you know, so you're getting it into play.
My comment is about using mathematics to, for physiology and vice versa.
And the question is when the architecture of the brain starts to tweak a little bit, like in multiple sclerosis, you lose some island.
In autism, you have shorter connections as opposed to the long connections in the brain.
You have more white matter as opposed to gray matter.
In dyslexia, you have sort of a neurological junk drawer of cells that aren't aligned.
Can you use this in your model to infer what must be going on?
In other words, this is not working.
And that helps your model to decide what does work.
I think ultimately, yes.
You have to sort of know enough.
You have to have enough of a model of that particular phenomena that you then can begin to say, well, if I change this variable here, how's that going to change things?
So, you know, if you're just completely in the dark, you're not going to get there yet.
But I think in the long run, yes.
Webbing, you know, because the smaller changes give you small behavioral changes, especially with demyel.
So that's not necessarily true. Small changes don't always.
Small changes don't always give you small changes in behavior.
I mean, the cortex is very, parts of it are very tightly balanced and just small perturbations are enough to kick you into wildly different area.
And that's why it's so hard to control it.
So, but, yeah, I think I agree with Ken, and I think ultimately we're going to be able to use, you know, theory and things to say.
Like, if what we do, we say, you know, if this happens, then why does this lead to this sort of broader, you know, phenomena?
If we wreck this neuron, why do we get this kind of behavior? And that's where theory is very helpful.
Directed initially at NED, of course, anyone else subsequent to that.
My question is focused on the importance of the known differences between the predictive,
respectively, the predictive and conformational values of a mathematical description of an observed system.
Case in point would be Ptolemaic Cosmology.
It was predictive in terms of the observed motions of stars and planets, but had nothing to do with the physical reality of what was being observed.
You know, fixed shells with planetoids and so on embedded as points of light.
More to the context at hand would be the Blue Vraine Project, where the columnar stack has been modeled mathematically,
so that there is some allegiance to the input output signaling, but it says nothing about the physical structure,
inherently about the physical structure of the columnar stack.
So I wanted to ask that and others, is that important to a physicalist, such as myself it is,
but how important to the value of a robust mathematical theory of the neocortex?
Well, you know, there's a kind of platitude in this answer, which is, you know, this is a text,
the Ptolemaic astronomy system is a textbook case of how prediction by curve fitting isn't much use.
Prediction is impressive if you do it with a general theory.
So, you know, the Copernican theory had a much better qualitative explanation,
even though without adding in all that same extra stuff, it didn't do as well in prediction.
So, yeah, prediction is really important, it gives you reason to believe that the theory that did the prediction is true,
but it's no good if the way you got that whole system was just by sticking in all the data to begin with.
So then where are we at with regards to a mathematical description of neurophysiology in the same capacity?
Is it necessary for, and this has been touched on earlier and earlier questions and so on,
but how critical is it that at some point, for example, with string theory,
we don't know because we don't have the technology to investigate that level of scale just yet.
Maybe a 14 giga-litre of the spectrum.
I think everybody would agree that we're at a very early days in neuroscience,
and we don't have a lot of the theoretical structure that will allow us to do the kind of explanation we want.
We don't even really know what the units are.
We don't have explanations of things like pattern recognition.
We can do a lot, at certain levels we have a lot of predictive power,
but I think we're nowhere near having a satisfactory account.
I think there's always sort of a tension between the details of your predictions
and sort of the extent to which you're getting the structure of things right.
I mean, ultimately what you're looking for is you want to really generalize.
You want to predict something that wasn't what you put in to begin with.
You want to be capturing a structure that gives you new insight into stuff that wasn't what led you to start thinking about it.
You're never in neurobiology right now.
You're never going to get things quantitatively very precise because there's just too much stuff going on
that you're not incorporating while you're trying to capture some basic relationships.
And yet you can make a lot of qualitative, and to some extent quantitative predictions,
that you can test and verify, but what you really want is the insight that gives you the guess the structure right
so that you can generalize to new domains with it.
Go ahead.
From a different standpoint, I wonder if a more humanistic aspect of
people can be explained with the models that have been discussed and explained here.
For example, affect or emotion, why somebody will react to a particular event with joy, somebody else with sorrow,
another person with anger.
Can any of this be explained or predicted on the basis either a statistical mathematical model or a physicalistic model?
Not today, but Sunday.
Thank you.
Let's stop.
Let's take those.
Oh, man.
Certainly.
Okay.
Thank you.
Thank you.
