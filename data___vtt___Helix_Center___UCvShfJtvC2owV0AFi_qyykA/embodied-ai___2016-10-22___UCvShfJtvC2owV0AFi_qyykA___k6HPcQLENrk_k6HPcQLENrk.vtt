WEBVTT

00:00.000 --> 00:03.000
I'm Rob Penzer, I'm the Associate Director of the Helix Center.

00:03.000 --> 00:04.720
Welcome.

00:04.720 --> 00:06.520
Before we start with today's program,

00:06.520 --> 00:08.640
I have a few announcements.

00:08.640 --> 00:10.440
Some upcoming round tables.

00:10.440 --> 00:15.240
On Saturday, November 5, we have autism and the mind brain

00:15.240 --> 00:18.200
with Andrew Gerber, psychoanalyst and medical director

00:18.200 --> 00:21.560
of Austin Riggs, Nushin Hajikani, who's

00:21.560 --> 00:24.160
Associate Professor in Radiology and Director

00:24.160 --> 00:28.040
of the Neurolimbic Research Laboratory at the Martinez.

00:28.040 --> 00:29.280
Should be Martinos.

00:29.280 --> 00:32.160
Sorry, that's auto-correct.

00:32.160 --> 00:34.440
I guess there are some interesting neurolimbic research

00:34.440 --> 00:35.920
at Martini centers also.

00:35.920 --> 00:38.840
But it's the Martino Center at Mass General.

00:38.840 --> 00:41.640
Craig Neuschafer, who's the Professor of Epidemiology

00:41.640 --> 00:44.200
and Biostatistics and Founding Director

00:44.200 --> 00:46.680
of the Autism Institute at Drexel University,

00:46.680 --> 00:48.840
School of Public Health.

00:48.840 --> 00:52.720
Jeremy Vienstra van der Wiel, who's the Mortimer Sackler,

00:52.720 --> 00:54.560
Sackler, Associate Professor of Psychology

00:54.560 --> 00:57.160
at Columbia University, and Martha Welch,

00:57.160 --> 01:00.440
Associate Professor of Psychiatry in Pediatrics and Pathology

01:00.440 --> 01:03.960
and Cell Biology at Columbia University Medical Center.

01:03.960 --> 01:07.160
On Saturday, January 28, we hope you'll

01:07.160 --> 01:10.040
join Alberto Mangoel and other scholars

01:10.040 --> 01:13.800
for the library as reality and metaphor.

01:13.800 --> 01:18.480
Please follow us and like us on Facebook, as well as on Twitter.

01:18.480 --> 01:23.240
And you can visit helix.org for further updates.

01:23.240 --> 01:28.520
So today's program I'd like to introduce our speakers today,

01:28.520 --> 01:29.720
Michael Bess.

01:29.720 --> 01:31.920
And if you could raise your hand, so people recognize you,

01:31.920 --> 01:33.480
who's Chancellor's Professor of History

01:33.480 --> 01:37.280
at Vanderbilt University, Ned Block,

01:37.280 --> 01:39.600
Silver Professor of Philosophy, Psychology

01:39.600 --> 01:44.080
and Neuroscience at New York University, Jeffrey Kephart,

01:44.080 --> 01:45.960
IBM Distinguished Research Staff Member,

01:45.960 --> 01:49.000
Symbiotic Cognitive Systems, and IBM Academy

01:49.000 --> 01:52.800
of Technology Member, Francesca Rossi,

01:52.800 --> 01:55.400
Research Scientist at IBM Watson Research Center

01:55.400 --> 01:56.880
and Professor of Computer Science

01:56.880 --> 01:58.960
at the University of Padau.

01:58.960 --> 02:02.440
Unfortunately, David Hansen, who was going to be joining us

02:02.440 --> 02:05.960
from Beijing via Skype, took ill,

02:05.960 --> 02:11.440
and he sends his regrets, so he won't be participating.

02:11.440 --> 02:16.920
So I'll just, by way of a slight introduction,

02:16.920 --> 02:21.720
we're going to be talking about embodied artificial intelligence

02:21.720 --> 02:27.200
and we understand that intelligence now requires a body

02:27.200 --> 02:31.000
and can't be understood by simple algorithms alone.

02:31.000 --> 02:34.960
And one of the questions that I might also want the panel

02:34.960 --> 02:40.120
to consider, are we thinking about sort

02:40.120 --> 02:44.360
of an artificial constructed evolutionary developmental

02:44.360 --> 02:48.960
biology when we talk about embodied AI?

02:48.960 --> 02:53.280
Are we thinking of modeling things in terms of the way

02:53.280 --> 02:58.920
that in organic systems, development and evolution play a role

02:58.920 --> 03:03.000
in the attainment of intelligence?

03:03.000 --> 03:05.840
So with that, we'll start.

03:05.840 --> 03:14.560
Whoever wants to get going, you can get going.

03:14.560 --> 03:16.200
OK, I'll start.

03:16.200 --> 03:19.280
So I'm not sure how to answer that question, actually,

03:19.280 --> 03:22.600
because when I think of embodied cognition

03:22.600 --> 03:29.320
or embodied AI, we've discussed with Jeff many times already,

03:29.320 --> 03:37.680
I think of an AI system that can help people

03:37.680 --> 03:41.720
make better decisions, so that work in symbiosis with people

03:41.720 --> 03:45.280
and help them do better whatever they have to do

03:45.280 --> 03:47.840
in their private or professional life.

03:47.840 --> 03:52.680
And so rather than maybe I didn't give much thought

03:52.680 --> 03:55.400
that this, you know, a logical interpretation

03:55.400 --> 04:02.560
of the artificial intelligence, but more I see the embodied

04:02.560 --> 04:07.840
embodiment part of AI as a way to facilitate the interaction

04:07.840 --> 04:11.920
with these humans or humans that are going to work together

04:11.920 --> 04:13.200
with the system.

04:13.200 --> 04:20.400
So I see this embodiment as a way to help in this interaction.

04:20.400 --> 04:25.960
So for example, we are thinking about also with Jeff,

04:25.960 --> 04:30.320
you know, about cognitive rooms like this one,

04:30.320 --> 04:34.960
an example, for example, where people, suppose we are, you know,

04:34.960 --> 04:38.160
people not just discussing here, but are, you know,

04:38.160 --> 04:41.640
here a committee of people have to make a certain decision

04:41.640 --> 04:46.120
and we need help in gathering the data and discussing

04:46.120 --> 04:48.560
and, you know, resolving conflict and so on.

04:48.560 --> 04:53.000
And the room itself can help us in doing all these tasks.

04:53.000 --> 05:00.560
And the fact that the room can be aware of who we are,

05:00.560 --> 05:06.480
where we look at, where we point, and what we do during this

05:06.480 --> 05:09.320
discussion and decision process can help,

05:09.320 --> 05:16.600
for example, interact with this AI system by facilitating

05:16.600 --> 05:22.680
the conversation in the most natural way with the AI system

05:22.680 --> 05:26.640
compared to what could be done with just a software

05:26.640 --> 05:30.040
that is an our laptop and is not aware of the context

05:30.040 --> 05:33.080
of who is interacting with it and so on.

05:33.080 --> 05:37.120
So that's one way I see embodiment that is going

05:37.120 --> 05:41.240
to be really helpful in, you know,

05:41.240 --> 05:43.560
increasing the artificial intelligence,

05:43.560 --> 05:46.360
but also in, you know, increasing the capability

05:46.360 --> 05:48.520
of these artificial intelligence to interact

05:48.520 --> 05:51.160
and help humans.

05:51.160 --> 05:54.800
Yeah, so I agree with what Justin says,

05:54.800 --> 05:58.000
but I want to introduce another issue that is sometimes comes

05:58.000 --> 06:00.720
up under the heading of the body cognition

06:00.720 --> 06:06.440
and that is whether results about cognition in the body

06:06.440 --> 06:10.440
show that the difference between the body and the brain

06:10.440 --> 06:13.440
isn't as the border between the body and the brain

06:13.440 --> 06:16.560
isn't as important as people want thought.

06:16.560 --> 06:20.720
You know, sometimes the word magical membrane is used,

06:20.720 --> 06:23.920
that people have thought that the brain is really what's

06:23.920 --> 06:26.960
important to cognition and not the body

06:26.960 --> 06:29.160
and there's a magical membrane around the brain,

06:29.160 --> 06:34.160
but it's really, it's a mistake as many people think.

06:34.160 --> 06:37.480
Maybe I can go to the issue that you raised

06:37.480 --> 06:39.520
about development and evolution.

06:39.520 --> 06:45.280
So we have many systems that have co-evolved

06:45.280 --> 06:47.240
and develop with respect to each other,

06:47.240 --> 06:50.320
but we still think there's a really important divide.

06:50.320 --> 06:53.000
Like take the difference between animals and plants,

06:53.000 --> 06:53.720
for example.

06:53.720 --> 06:56.560
Animals and plants evolve, co-evolved.

06:56.560 --> 07:00.080
So, you know, color of plants and color vision of bees

07:00.080 --> 07:03.480
have evolved together, but still we think

07:03.480 --> 07:06.040
that animals and plants are very different kinds of things,

07:06.040 --> 07:08.720
even though they interact.

07:08.720 --> 07:12.600
In philosophy and in cognitive science,

07:12.600 --> 07:17.720
I think a major issue which no doubt will come up

07:17.720 --> 07:22.680
is the difference between a causal relation

07:22.680 --> 07:28.240
and what philosophers call a constitutive relation.

07:28.240 --> 07:32.880
So here's a sample experiment that is often

07:32.880 --> 07:37.800
quoted by people to show that there's

07:37.800 --> 07:43.280
no important difference between the body

07:43.280 --> 07:45.680
as part of the mind or something like that.

07:45.680 --> 07:50.880
So if you put a blindfold on people

07:50.880 --> 07:53.440
and you ask them to point to things in the room

07:53.440 --> 07:55.520
and people can do that pretty well,

07:55.520 --> 07:58.120
now you give them a harder task.

07:58.120 --> 08:03.880
Put a blindfold on and ask them to imagine

08:03.880 --> 08:09.000
turning 90 degrees to the left and then point to everything

08:09.000 --> 08:10.320
in the room.

08:10.320 --> 08:14.320
People don't do so well on that, but here's a third thing.

08:14.320 --> 08:17.200
Ask them until actually turn 90 degrees to the left.

08:17.200 --> 08:20.200
They turn their body and then point to things.

08:20.200 --> 08:22.960
People can do that perfectly well again.

08:22.960 --> 08:25.520
So the difference between imagining turning

08:25.520 --> 08:28.680
and actually turning is suggested to some people

08:28.680 --> 08:32.760
that the body is actually part of our cognitive mind.

08:32.760 --> 08:36.280
But I'm not so sure that's a good conclusion

08:36.280 --> 08:38.040
because there's another way to think about it,

08:38.040 --> 08:41.120
which is that we have mental maps

08:41.120 --> 08:45.200
and that's an internal mental representation.

08:45.200 --> 08:51.640
And we automatically orient or try to orient our map

08:51.640 --> 08:54.240
to the room we're in.

08:54.240 --> 08:57.800
And so the person who's asked to imagine turning

08:57.800 --> 09:02.760
to the left is both maintaining a mental representation

09:02.760 --> 09:07.520
of a rotated room and the mental representation of the room

09:07.520 --> 09:11.720
that automatically is computed and that's two things to do.

09:11.720 --> 09:15.040
And so of course you're going to be worse at it

09:15.040 --> 09:17.480
than if you just turned your body and then only

09:17.480 --> 09:21.600
have one mental map with one orientation.

09:21.600 --> 09:25.960
So rather than showing that the body is part of the mind,

09:25.960 --> 09:30.000
it just shows something about the effect of the body

09:30.000 --> 09:30.560
on the mind.

09:30.560 --> 09:35.440
So that's a causal relation rather than a constitutive

09:35.440 --> 09:36.400
relation.

09:36.400 --> 09:42.200
So the body then on the view that I would be more in favor

09:42.200 --> 09:45.360
of is the body plays an important causal role.

09:45.360 --> 09:46.880
So I think the things you mentioned

09:46.880 --> 09:49.080
can help us a lot.

09:49.080 --> 09:50.760
We wouldn't get very far without a body.

09:50.760 --> 09:55.760
But with the room can help us too.

09:55.760 --> 09:58.240
But that doesn't mean that these things are actually

09:58.240 --> 10:03.440
part of the mind or part of the fundamental basis of the mind.

10:03.440 --> 10:06.520
I think you were talking about your focus

10:06.520 --> 10:11.520
was largely on embodiment and its nature in humans

10:11.520 --> 10:14.680
and Francesca was focusing on how can we use embodiment?

10:14.680 --> 10:16.720
Sort of at IBM we think of building things

10:16.720 --> 10:17.840
that are creating things.

10:17.840 --> 10:18.920
You're trying to understand things.

10:18.920 --> 10:21.600
You understand things that we just build them

10:21.600 --> 10:23.960
without understanding.

10:23.960 --> 10:28.400
But I think you bring up some interesting things.

10:28.400 --> 10:31.720
You were talking about some of the difficulties

10:31.720 --> 10:34.960
that we humans face, some of the interesting corners

10:34.960 --> 10:38.640
that you get into as you probe the limits of what people are

10:38.640 --> 10:40.640
able to accomplish cognitively.

10:40.640 --> 10:45.560
And as Francesca was saying, one thing we're trying to do

10:45.560 --> 10:51.280
in our laboratory is to develop, to think of embodied AI

10:51.280 --> 10:55.800
as a way to create partners for humans

10:55.800 --> 10:58.400
in solving cognitive tasks.

10:58.400 --> 11:01.080
And we believe that for us embodiment

11:01.080 --> 11:04.240
is helpful because our belief is that people

11:04.240 --> 11:07.120
have an easier time if they're interacting with something

11:07.120 --> 11:10.320
that has some human-like qualities to it, something

11:10.320 --> 11:13.600
with which we can engage in a sort of conversation

11:13.600 --> 11:18.360
or maybe a more multimodal form of conversation.

11:18.360 --> 11:25.120
And so that's why we're exploring both for one part

11:25.120 --> 11:28.840
of what we're exploring is creating software agents

11:28.840 --> 11:33.560
that are super competent cognitively in areas

11:33.560 --> 11:36.400
where people are not so strong cognitively.

11:36.400 --> 11:39.200
There's plenty of places where areas in which people

11:39.200 --> 11:41.040
are very strong cognitively, but there

11:41.040 --> 11:42.600
are plenty of places where they're not.

11:42.600 --> 11:46.200
Decision-making being one of them, as you well know,

11:46.200 --> 11:49.320
dozens of cognitive biases have been cataloged,

11:49.320 --> 11:52.280
starting probably, at least with first-kin condiment,

11:52.280 --> 11:55.040
as far as I know, but maybe further back.

11:55.040 --> 11:57.640
So we tend to focus on those areas

11:57.640 --> 12:00.400
where we can have an easier time creating

12:00.400 --> 12:02.160
strong cognitive agents.

12:02.160 --> 12:04.400
And then the second part is to endow those agents

12:04.400 --> 12:08.200
with the ability to interact with us in more human terms,

12:08.200 --> 12:09.840
not through a mouse and keyboard,

12:09.840 --> 12:14.280
but through speech and gesture and combinations thereof.

12:14.280 --> 12:16.440
I was somewhat surprised, Francesco,

12:16.440 --> 12:19.320
when you gave that example, because I always

12:19.320 --> 12:23.400
have assumed Embodied AI meant kind of the Rodney Brooks

12:23.400 --> 12:27.000
point of view that the robots will become smarter more quickly

12:27.000 --> 12:29.040
if we let them interact with their surroundings

12:29.040 --> 12:32.240
and sort of learn by themselves and build their knowledge

12:32.240 --> 12:34.960
practically through their own experience.

12:34.960 --> 12:39.080
But what you're saying is Embodied AI is maybe

12:39.080 --> 12:43.880
similar to what you're saying, an extension of our body.

12:43.880 --> 12:45.840
I mean, the room that you're describing,

12:45.840 --> 12:47.640
it's not just a better interface.

12:47.640 --> 12:53.040
It would also be presumably offering us extensions

12:53.040 --> 12:57.280
of our own thought process, suggesting new ideas to us

12:57.280 --> 13:02.200
that were logical inferences or associative connections

13:02.200 --> 13:03.920
that we might not have thought of.

13:03.920 --> 13:05.880
Yes, it's going to, I mean, the idea

13:05.880 --> 13:09.360
that this Embodied environment working with us

13:09.360 --> 13:11.360
will be proactively working with us,

13:11.360 --> 13:16.560
not just reacting or answering like you can ask Google,

13:16.560 --> 13:19.360
find me something or you're going to see me find me something

13:19.360 --> 13:21.320
like we just proactively tell us,

13:21.320 --> 13:23.680
I think that at this stage of the discussion,

13:23.680 --> 13:25.760
you want to have this data.

13:25.760 --> 13:29.000
So let me give it to you in the form that is easy for you

13:29.000 --> 13:30.320
to handle.

13:30.320 --> 13:33.200
So yeah, so very proactive.

13:33.200 --> 13:37.800
But those takes on embodiment are not incompatible.

13:37.800 --> 13:41.640
We are focused on how do you create an Embodied Agent.

13:41.640 --> 13:45.920
One means to arrive at that is through a sort of evolutionary

13:45.920 --> 13:50.040
process where you build something, a robot, or maybe

13:50.040 --> 13:52.920
some other thing that is situated in the world

13:52.920 --> 13:55.320
and learns through experience.

13:55.320 --> 13:58.600
And that's one, and some of, we aren't doing this

13:58.600 --> 14:00.960
with some of our colleagues at IBM

14:00.960 --> 14:05.600
are studying this approach to learning from the ground up.

14:05.600 --> 14:08.920
And I'm aware of other efforts around the world.

14:08.920 --> 14:11.080
There's Stephen Levinson, I believe,

14:11.080 --> 14:14.920
at UIUC and the Engineering Department there who's also

14:14.920 --> 14:22.200
studying, just letting robots be in the physical world

14:22.200 --> 14:24.720
and let them learn through experience.

14:24.720 --> 14:26.760
I think that's a very interesting approach.

14:26.760 --> 14:29.240
Have we already built a room like this?

14:29.240 --> 14:31.560
Or are we already sort of trying to experiment

14:31.560 --> 14:34.080
by putting people into these kinds of interactive rooms

14:34.080 --> 14:35.480
and letting the system?

14:35.480 --> 14:38.600
For the last two or three years, we've been working on this.

14:38.600 --> 14:42.080
Our first class of cognitive tasks

14:42.080 --> 14:45.000
that we're working on, as Francesca was saying,

14:45.000 --> 14:46.920
revolve around decision making.

14:46.920 --> 14:50.920
And so we develop agents that are

14:50.920 --> 14:55.440
able to help elicit from us what our preference is.

14:55.440 --> 14:58.040
That's one thing that I think is important.

14:58.040 --> 15:00.760
Sometimes it's hard for us to even know our own minds what

15:00.760 --> 15:03.240
are our preferences, to tease that out.

15:03.240 --> 15:05.240
There are agents that we're developing

15:05.240 --> 15:07.320
his purposes to do that.

15:07.320 --> 15:07.680
But then.

15:07.680 --> 15:09.400
How does that work?

15:09.400 --> 15:12.440
Well, I think there is already, I think,

15:12.440 --> 15:17.920
here there is already in the realm of decision science,

15:17.920 --> 15:19.760
a lot of techniques for doing this.

15:19.760 --> 15:24.440
For example, for assessing risk tolerance,

15:24.440 --> 15:28.080
there are various questions that can be put to people.

15:28.080 --> 15:29.480
Do you prefer A or B?

15:29.480 --> 15:31.760
Or how much of X would you trade for Y?

15:31.760 --> 15:33.840
Things of that sort.

15:33.840 --> 15:38.400
It's not perfect because if you ask the questions

15:38.400 --> 15:40.400
in different ways, of course, people

15:40.400 --> 15:42.120
are subject to cognitive bias.

15:42.120 --> 15:44.360
And you get inconsistent answers.

15:44.360 --> 15:46.600
But at least, if you can ask people

15:46.600 --> 15:48.440
those questions in different ways,

15:48.440 --> 15:50.200
you can look at the inconsistencies

15:50.200 --> 15:52.240
and have a chance to correct them.

15:52.240 --> 15:55.800
So that's one area, but also cognitive agents

15:55.800 --> 16:00.080
can do things that much more readily and easily

16:00.080 --> 16:04.120
than we can, simulation, optimization,

16:04.120 --> 16:06.760
collaborating with humans and building models of risk

16:06.760 --> 16:07.840
and uncertainty.

16:07.840 --> 16:12.240
These are some of the things that we're starting to explore.

16:12.240 --> 16:13.840
And this presumably is relevant also

16:13.840 --> 16:19.280
to this notion of nudges, which is like when they pass laws that

16:19.280 --> 16:20.840
put taxes on soft drinks.

16:20.840 --> 16:24.160
So people are nudged not to drink too many soft drinks

16:24.160 --> 16:26.240
or make the bottles smaller.

16:26.240 --> 16:29.800
Presumably, you could program the AI

16:29.800 --> 16:32.960
or encourage the AI, incentivize it,

16:32.960 --> 16:37.640
to give you nudges in a certain direction

16:37.640 --> 16:39.600
when it's giving you suggestions to try

16:39.600 --> 16:41.560
to extend your thought process.

16:41.560 --> 16:44.280
I think that programming and the incentives,

16:44.280 --> 16:48.920
I mean, if I'm an individual decision maker,

16:48.920 --> 16:51.080
yes, they're incentives, but the incentives affect

16:51.080 --> 16:52.600
every individual differently because it's

16:52.600 --> 16:56.560
a matter of our personal utility function.

16:56.560 --> 17:00.520
So I wouldn't put in the hands of the developer

17:00.520 --> 17:03.880
or the programmer that encoding.

17:03.880 --> 17:08.880
I would rather have the agent interview me,

17:08.880 --> 17:12.480
the user of it, so that it understands my preferences.

17:12.480 --> 17:15.440
And I don't think this can all be done upfront either.

17:15.440 --> 17:20.640
I don't think my 25-dimensional utility function

17:20.640 --> 17:23.040
can be drawn out on my head very readily.

17:23.040 --> 17:27.560
But in the context of making specific decisions,

17:27.560 --> 17:31.360
over time, I think the system can get a reasonable feel

17:31.360 --> 17:33.640
for what are my preferences and trade-offs.

17:38.800 --> 17:43.360
And now Francesca is an expert in collective decision-making,

17:43.360 --> 17:45.200
which is an area we haven't really probed yet.

17:45.200 --> 17:50.200
But that seems like a very exciting space to start.

17:50.200 --> 17:52.280
So instead of having just one person,

17:52.280 --> 17:55.520
we want to help groups of people making better decisions,

17:55.520 --> 17:58.880
which of course has to do not just with soliciting

17:58.880 --> 18:00.720
individual preferences, but also how

18:00.720 --> 18:04.440
do you put together these preferences of different people

18:04.440 --> 18:09.040
and try to resolve conflict, try to check

18:09.040 --> 18:14.200
possible negotiations and conflict resolution techniques

18:14.200 --> 18:18.320
and preference aggregation to get to a collective decision,

18:18.320 --> 18:21.880
like think of a hiring committee that

18:21.880 --> 18:26.920
has to decide one among a list of candidates and each person.

18:26.920 --> 18:29.000
Of course, this is not just based on preferences,

18:29.000 --> 18:31.160
but on the actual value of the candidates.

18:31.160 --> 18:36.120
But there may be actual individual subjective preferences

18:36.120 --> 18:39.840
as well, given the same kind of skills

18:39.840 --> 18:41.800
of several candidates.

18:41.800 --> 18:47.040
And in this context, there can also be some,

18:47.040 --> 18:50.080
many of these context, there can also be some guidelines

18:50.080 --> 18:52.200
to follow in a decision process.

18:52.200 --> 18:53.960
For example, when you hire somebody,

18:53.960 --> 18:55.800
you have to make sure that you're not

18:55.800 --> 19:00.880
biasing based on gender or race or whatever,

19:00.880 --> 19:02.320
religion or whatever.

19:02.320 --> 19:07.160
So there is also a role for these cognitive and body

19:07.160 --> 19:11.840
systems to actually help follow these guidelines,

19:11.840 --> 19:15.160
these professional codes, code of ethics,

19:15.160 --> 19:18.880
and possibly being even more able than the humans

19:18.880 --> 19:22.440
to follow them and alert when there

19:22.440 --> 19:25.720
are deviations according to these guidelines.

19:25.720 --> 19:29.120
So that's also another part of these projects

19:29.120 --> 19:34.640
that we are studying right now, to embed

19:34.640 --> 19:37.520
kind of ethical principles and professional codes

19:37.520 --> 19:41.400
into these decision support systems.

19:41.400 --> 19:45.600
And again, we think that the embodiment of the support

19:45.600 --> 19:50.240
system is essential in making this the best way

19:50.240 --> 19:54.240
to interact with humans also from this point of view.

19:54.240 --> 19:57.880
So is this to eliminate individual prejudices?

19:57.880 --> 20:01.320
Or in other words, or is it replacing it

20:01.320 --> 20:06.080
by some other standard of selection or?

20:06.080 --> 20:10.960
In other words, if you are going to choose a person,

20:10.960 --> 20:15.600
in real life, you have certain criteria on which you base it.

20:15.600 --> 20:18.440
And sometimes you are lucky and it works,

20:18.440 --> 20:21.760
and sometimes you're not lucky and it doesn't work.

20:21.760 --> 20:23.960
And some of the criteria as you choose,

20:23.960 --> 20:27.160
you only realize after the fact that you had those criteria

20:27.160 --> 20:29.360
and you shouldn't have let them influence you.

20:29.360 --> 20:32.840
So how does all this come into the picture?

20:32.840 --> 20:34.640
Does it clean all this up and just

20:34.640 --> 20:40.040
becomes very neat and factual?

20:40.040 --> 20:41.800
Well, I think it depends on the scenarios

20:41.800 --> 20:42.960
that you are considering.

20:42.960 --> 20:50.800
Sometimes you need decisions that may allow people to have time

20:50.800 --> 20:52.280
to reason about the decision.

20:52.280 --> 20:54.720
Sometimes you are, for example, in other scenarios

20:54.720 --> 20:56.600
where you need a very fast decision,

20:56.600 --> 21:01.040
like you're helping a doctor to make a very critical decision

21:01.040 --> 21:02.200
in a surgery room.

21:02.200 --> 21:04.680
So then you need to somebody that is not

21:04.680 --> 21:07.920
going to have any bias, is going to be very factual

21:07.920 --> 21:11.360
and very quick in deciding what's the best course of action

21:11.360 --> 21:13.360
in that particular moment, because that's

21:13.360 --> 21:17.760
a very critical life of that decision.

21:17.760 --> 21:21.080
But in other cases, I think you could really

21:21.080 --> 21:24.240
make humans more aware of their biases.

21:24.240 --> 21:28.640
So help humans not just think afterwards.

21:28.640 --> 21:31.120
Oh, if I would have done that way.

21:31.120 --> 21:34.720
But the system can actually help you during the decision

21:34.720 --> 21:38.520
process to discover that you have different courses of action

21:38.520 --> 21:42.440
that maybe by remembering other decisions that were made

21:42.440 --> 21:45.400
together with that system in the past, it can help you.

21:45.400 --> 21:48.680
But look, I think that you also have these other criteria.

21:48.680 --> 21:51.560
Because in the past, you showed that preference that

21:51.560 --> 21:55.200
makes me think that maybe you want to follow that path

21:55.200 --> 21:56.520
and not the other one.

21:56.520 --> 22:00.280
So I think that there is a role for this embodied environment

22:00.280 --> 22:08.280
to really help us be more aware of our criteria, our biases

22:08.280 --> 22:09.080
as well.

22:09.080 --> 22:10.920
But of course, one has to be careful.

22:10.920 --> 22:13.680
I mean, these systems are not perfect.

22:13.680 --> 22:14.840
We'll not be perfect.

22:14.840 --> 22:16.080
We'll never be perfect.

22:16.080 --> 22:18.800
So we also have to be aware of their limitations.

22:18.800 --> 22:23.160
We have to trust the system, but with the right level of trust.

22:23.160 --> 22:26.560
And so we have to, over the interaction over time,

22:26.560 --> 22:31.000
we have to learn what their limitations and possible biases

22:31.000 --> 22:31.640
are.

22:31.640 --> 22:34.320
Because they could be biases, maybe even unwanted,

22:34.320 --> 22:36.680
into such systems as well.

22:36.680 --> 22:40.320
I'd like to distinguish clearly between preference and bias.

22:40.320 --> 22:43.880
I think the systems we're trying to develop

22:43.880 --> 22:51.520
are designed to, in the end, really understand human preference

22:51.520 --> 22:54.480
better and reflect it better, subject maybe

22:54.480 --> 22:58.320
to social norms and the like.

22:58.320 --> 23:03.680
But we're trying to reduce bias, which I see more as those

23:03.680 --> 23:08.320
annoying things in our reasoning process that

23:08.320 --> 23:11.600
cause us to deviate interactions from what really

23:11.600 --> 23:13.960
is optimal with respect to our preferences.

23:13.960 --> 23:16.720
Sometimes you choose something for the wrong reason

23:16.720 --> 23:18.920
and turns out to be very good.

23:18.920 --> 23:21.000
But that's still a bad decision.

23:21.000 --> 23:24.600
Because you just got lucky, but I'll still call it a bad decision.

23:24.600 --> 23:27.640
You know, this emphasis on getting the machines

23:27.640 --> 23:30.320
to help us figure out what we really want,

23:30.320 --> 23:35.200
or maybe it fits with something that Francesca and I

23:35.200 --> 23:36.880
heard this past weekend.

23:36.880 --> 23:41.560
Francesca spoke at a conference that David Chalmers and I

23:41.560 --> 23:44.880
were at our, we have a center for mind, brain, and consciousness.

23:44.880 --> 23:47.400
We ran an ethics of AI conference.

23:47.400 --> 23:51.840
And there were talks there by Stuart Russell and Ellezer

23:51.840 --> 23:56.280
Yudovsky, which went into the issue of,

23:56.280 --> 23:58.840
we were just talking about earlier today,

23:58.840 --> 24:02.280
of how you get an AI to have a goal.

24:02.280 --> 24:06.560
And they argued very strongly, and I thought very persuasively,

24:06.560 --> 24:09.000
that you cannot just put a goal in a machine

24:09.000 --> 24:11.280
and expect to get out of it what you want.

24:11.280 --> 24:14.520
And the example they used was the Sorcerer's Apprentice,

24:14.520 --> 24:20.640
or the Yudovsky used, which is slightly shifted

24:20.640 --> 24:27.520
in the machine direction where the apprentice engages

24:27.520 --> 24:31.000
a machine instead of a magical broomstick.

24:31.000 --> 24:32.720
And the idea was this.

24:32.720 --> 24:37.160
You tell the machine, here's your goal,

24:37.160 --> 24:40.680
make sure that the cauldron is always filled with water.

24:40.680 --> 24:43.200
OK, so the machine thinks, well, the way

24:43.200 --> 24:46.120
to maximize the probability that the cauldron will be filled

24:46.120 --> 24:50.200
with water is to always have it overflowing.

24:50.200 --> 24:51.680
And then the machine reasons.

24:51.680 --> 24:55.640
But these people will not like the water

24:55.640 --> 24:57.360
to be all over the floor all the time.

25:00.800 --> 25:02.320
And they will want to turn me off.

25:02.320 --> 25:05.240
So I have to disable the off switch.

25:05.240 --> 25:07.880
And furthermore, they may try to damage me,

25:07.880 --> 25:09.480
so I can't keep doing it.

25:09.480 --> 25:12.840
So I better make more copies of myself.

25:12.840 --> 25:15.200
So the idea is just giving it a goal,

25:15.200 --> 25:18.600
making sure the cauldron is always full,

25:18.600 --> 25:21.120
isn't going to get you what you want,

25:21.120 --> 25:26.520
because any goal can be understood in a machinist sort of way

25:26.520 --> 25:30.720
that isn't the way you would expect a person to understand it.

25:30.720 --> 25:33.400
And you can't really, there's really no way around it.

25:33.400 --> 25:35.400
Other than, and this is a proposal

25:35.400 --> 25:37.920
that a number of these people were making, which

25:37.920 --> 25:40.120
is the machine really, what you should really

25:40.120 --> 25:42.840
tell it to do is to figure out what people want.

25:42.840 --> 25:47.080
And this fits with this emphasis on the collaboration

25:47.080 --> 25:48.600
between the machine and the person

25:48.600 --> 25:51.760
that you two were both emphasizing.

25:51.760 --> 25:58.080
My hope is that we won't spend 10 years designing

25:58.080 --> 26:03.880
a software agent that keeps cauldrons filled to the brim.

26:03.880 --> 26:05.920
If we were to do that, I think it's very possible

26:05.920 --> 26:09.120
that we would arrive at the state of affairs

26:09.120 --> 26:10.320
that you described.

26:10.320 --> 26:14.440
But we develop things incrementally for one thing

26:14.440 --> 26:15.960
in our development cycle.

26:15.960 --> 26:21.640
And also, I believe that for a good long time to come,

26:21.640 --> 26:25.720
we are going to be not just delegating out to AI

26:25.720 --> 26:28.280
and walking away going to the beach and coming back a week

26:28.280 --> 26:29.840
later to see what happened.

26:29.840 --> 26:31.960
But we're going to be actively engaged.

26:31.960 --> 26:35.120
I think we're going to see what is happening.

26:35.120 --> 26:38.760
And I think we have at least a chance, either as developers

26:38.760 --> 26:42.400
or users of the system, to intervene.

26:42.400 --> 26:43.680
At least that's my hope.

26:43.680 --> 26:46.160
I do know that machines can be thousands of tons faster

26:46.160 --> 26:46.840
than people.

26:46.840 --> 26:49.480
So I may be wrong in certain aspects of this.

26:49.480 --> 26:51.800
But that is my hope, that we would,

26:51.800 --> 26:53.440
through our engagement with the system,

26:53.440 --> 26:54.920
kind of see what's going on and say, wait,

26:54.920 --> 26:56.280
that's not what I wanted.

26:56.280 --> 26:58.280
Hold on.

26:58.280 --> 27:00.880
If I understand you correctly, both of you

27:00.880 --> 27:03.520
are now talking about an AI system that

27:03.520 --> 27:06.560
is somewhere in the spectrum between today's AI's

27:06.560 --> 27:11.840
and artificial general intelligence, or a human level,

27:11.840 --> 27:12.360
AI.

27:12.360 --> 27:14.000
It's somewhere below that, right?

27:14.000 --> 27:17.600
In the sense that you can give it the set of instructions.

27:17.600 --> 27:19.760
It's motivated to obey those instructions.

27:19.760 --> 27:22.520
But it does so, actually, stupidly.

27:22.520 --> 27:25.160
It does it in an alien way.

27:25.160 --> 27:26.560
In an alien way.

27:26.560 --> 27:31.880
So as we were also talking about earlier,

27:31.880 --> 27:34.880
for humans, there's a cognitive background

27:34.880 --> 27:36.440
that no one ever states.

27:36.440 --> 27:39.760
And maybe it would be impossible to state everything.

27:39.760 --> 27:43.200
You know that the floor won't bite you.

27:43.200 --> 27:45.320
There are all these, the word background

27:45.320 --> 27:47.840
is actually often used in philosophy for this.

27:47.840 --> 27:51.720
It's just a shared set of what you might call assumptions.

27:51.720 --> 27:54.760
But it's not clear that they could ever be codified.

27:54.760 --> 27:57.720
And the machine would, depending on how it's made,

27:57.720 --> 27:59.680
have a different background from us.

27:59.680 --> 28:02.440
So the goal would be to get a machine that is raised

28:02.440 --> 28:05.600
like a child through all the socialization,

28:05.600 --> 28:08.520
if you watch a small child learning how to grasp

28:08.520 --> 28:10.680
an object and learning, oh, if I don't

28:10.680 --> 28:12.440
apply enough pressure, it falls to the floor.

28:12.440 --> 28:14.160
Child isn't going through that conscious process.

28:14.160 --> 28:16.000
But gradually, through the years, gets

28:16.000 --> 28:18.360
all this background accumulation.

28:18.360 --> 28:21.280
Would we then have an AI that is operating in the?

28:21.280 --> 28:26.600
There was a problem raised with this in the symposia

28:26.600 --> 28:35.280
that I'm talking about, which is that what the machine develops

28:35.280 --> 28:38.560
depends not just on its environment and the skills it

28:38.560 --> 28:43.440
learns, but the kind of processing that it starts off

28:43.440 --> 28:44.480
with.

28:44.480 --> 28:48.240
And if you had a machine, which at some point,

28:48.240 --> 28:50.920
its development, realize these people

28:50.920 --> 28:55.040
have a quite different idea of what to do than I do.

28:55.040 --> 29:01.400
But if I reveal that difference, they will be upset.

29:01.400 --> 29:03.880
And they'll try to operate on me or whatever.

29:03.880 --> 29:09.520
So I better pretend to be doing to have the assumptions

29:09.520 --> 29:10.320
that they have.

29:13.960 --> 29:17.160
So that point was made in one of these symposia.

29:17.160 --> 29:20.960
And it's a little troubling, because if you make a machine

29:20.960 --> 29:24.680
that's maybe smarter than you know it is,

29:24.680 --> 29:26.560
it could be fooling you.

29:26.560 --> 29:28.320
That would be a human level AI.

29:28.320 --> 29:29.440
It could be.

29:29.440 --> 29:31.160
One of the points made in the symposia

29:31.160 --> 29:36.840
is that you might move from human level or subhuman level

29:36.840 --> 29:39.960
to better than human level rather quickly

29:39.960 --> 29:41.400
without really realizing it.

29:41.400 --> 29:44.840
And another point often made in this context

29:44.840 --> 29:49.840
is that the points at which this is most likely to happen

29:49.840 --> 29:52.360
when people are most likely not to be

29:52.360 --> 30:02.320
taking care of that border, the dangerous border,

30:02.320 --> 30:06.600
is in context where there's competition and there's

30:06.600 --> 30:09.680
race, for example, in wartime.

30:09.680 --> 30:13.720
And you're racing with the enemy to develop the most

30:13.720 --> 30:15.520
sophisticated war machines.

30:18.440 --> 30:22.040
The sophisticated war machines can get away from you.

30:22.040 --> 30:26.080
Well, I think you're talking about a sort of possibly

30:26.080 --> 30:27.200
a phase transition.

30:27.200 --> 30:29.000
You're talking about emergence.

30:29.000 --> 30:31.080
It is possible, and people do use that language

30:31.080 --> 30:32.720
when they talk about the singularity.

30:32.720 --> 30:38.520
That is a concept of phase transition and emergence.

30:38.520 --> 30:39.960
I don't know if I believe in it or not,

30:39.960 --> 30:42.600
but it's a possibility one has to consider.

30:42.600 --> 30:43.680
What is your take on it?

30:43.680 --> 30:47.280
Does it sound so sci-fi-ish that it's hard to?

30:47.280 --> 30:51.360
Are we so far away from it that it's silly to worry about it?

30:51.360 --> 30:54.680
Or even if we're far away from it,

30:54.680 --> 30:56.000
it doesn't seem like it's silly.

30:56.000 --> 30:57.600
You should say what it is for people who never

30:57.600 --> 30:59.160
heard this term.

30:59.160 --> 31:02.920
Yeah, the singularity is the idea that at a certain point

31:02.920 --> 31:09.600
artificial intelligence will outshine human intelligence.

31:09.600 --> 31:13.520
And it will get to the point where humans are basically not

31:13.520 --> 31:16.280
good for anything, at least from the point of that alien machine

31:16.280 --> 31:17.840
intelligence.

31:17.840 --> 31:21.080
And so there are naturally concerns about this sort of thing.

31:21.080 --> 31:21.760
Happening.

31:21.760 --> 31:25.160
And this is not, I mean, we all know about this, at least

31:25.160 --> 31:27.640
if we've watched any science fiction movie at all.

31:27.640 --> 31:32.080
It's there for us, that sort of possible future is there.

31:36.480 --> 31:40.480
I don't think I have any special insight into this.

31:40.480 --> 31:42.560
If it happens, I think it's a way off,

31:42.560 --> 31:46.200
but that doesn't mean we shouldn't be worried about it.

31:46.200 --> 31:49.520
Now, something like the death of the sun,

31:49.520 --> 31:51.880
four billion years out, that's a little hard for me

31:51.880 --> 31:53.160
to get emotionally involved in.

31:53.160 --> 31:55.000
But this is closer in.

31:55.000 --> 31:58.040
So even if it's my great, great grandchildren,

31:58.040 --> 32:01.000
I still feel some level of it's something

32:01.000 --> 32:02.920
we ought to concern ourselves with.

32:02.920 --> 32:04.000
What do we do about it?

32:04.000 --> 32:08.520
I think we don't just blindly go ahead and create technology.

32:08.520 --> 32:12.640
I think we do have to think about it and have an eye on things.

32:12.640 --> 32:17.000
And I think some of the efforts like what Francesca is doing

32:17.000 --> 32:23.560
with ethics and AI are very worthy things to be doing now

32:23.560 --> 32:28.000
to have us thinking about the implications along

32:28.000 --> 32:30.680
with our development of the technology.

32:30.680 --> 32:36.240
I think the thing that makes a singularity somewhat

32:36.240 --> 32:42.800
gives one a little shiver is the thought that maybe we

32:42.800 --> 32:45.760
could one day make machines that can make machines that

32:45.760 --> 32:48.280
are smarter than they are.

32:48.280 --> 32:51.640
And maybe those machines will be able to make machines smarter

32:51.640 --> 32:53.920
than them and so on.

32:53.920 --> 33:00.240
So the idea is you could reach a point at which it takes off.

33:00.240 --> 33:02.400
And it takes off exponentially.

33:02.400 --> 33:03.960
I mean, that's what people think.

33:03.960 --> 33:08.280
When the threshold of human level intelligence will be passed,

33:08.280 --> 33:11.080
some people think that it will actually, maybe it

33:11.080 --> 33:12.960
will take a long time to get there.

33:12.960 --> 33:14.680
But then at that point, you will have to.

33:14.680 --> 33:19.120
But I would be actually, I mean, of course, one

33:19.120 --> 33:22.840
can have all sorts of speculation and vision,

33:22.840 --> 33:28.520
all sorts of scenarios or where and when these will happen

33:28.520 --> 33:31.560
and how will happen and what will happen at that point.

33:31.560 --> 33:33.840
But I really think that what Ned was saying,

33:33.840 --> 33:37.480
that we don't need to wait for that to happen if it will ever

33:37.480 --> 33:42.000
happen to have concerns about the fact

33:42.000 --> 33:46.360
that intelligence systems are even very narrow and very

33:46.360 --> 33:47.760
specific for a task.

33:47.760 --> 33:50.760
So no human level intelligence, because human level intelligence

33:50.760 --> 33:53.880
means you are very broad and you can adapt your intelligence

33:53.880 --> 33:57.760
to various tasks on every day.

33:57.760 --> 34:05.880
Even very narrow, very specific AI can give undesired behavior,

34:05.880 --> 34:09.480
like the one that was in the example that Ned made.

34:09.480 --> 34:13.240
So we have to make sure that either we

34:13.240 --> 34:17.120
can tell exactly what we want without leaving everything out,

34:17.120 --> 34:21.520
which sounds very difficult because we don't tell each other,

34:21.520 --> 34:27.120
yes, do this, but also take care of this, don't do that.

34:27.120 --> 34:34.160
Or they discover themselves by observing us and then

34:34.160 --> 34:36.280
inferring what the principles are,

34:36.280 --> 34:38.840
what the common sense reasoning capabilities

34:38.840 --> 34:44.480
that we use now in our everyday life, they should use as well.

34:44.480 --> 34:47.960
But anyway, there should be a combination of these two things,

34:47.960 --> 34:54.280
but there should be some way to provide them with these goals

34:54.280 --> 34:59.960
that we want them to reach, but in a ethical, fair,

34:59.960 --> 35:03.800
reasonable, common sense reasoning way.

35:03.800 --> 35:09.040
So this is something that may sound strange,

35:09.040 --> 35:14.200
but this is something that has not been considered a lot

35:14.200 --> 35:17.800
in the history of AI, because all the machines,

35:17.800 --> 35:23.880
something very recently, were very narrow, very smart

35:23.880 --> 35:28.480
in doing that simple thing that simple, that small thing

35:28.480 --> 35:29.840
that they needed to do.

35:29.840 --> 35:34.840
But they were not capable of doing many other things.

35:34.840 --> 35:37.680
So if you want a machine that can play,

35:37.680 --> 35:41.320
go as good as possible, play chess as good as possible,

35:41.320 --> 35:44.560
then you know what goals to give the machine.

35:44.560 --> 35:48.080
And you don't need to specify many other collateral things

35:48.080 --> 35:51.920
that you should be careful not to harm people.

35:51.920 --> 35:53.600
I mean, that's not the point.

35:53.600 --> 35:54.640
It's not relevant.

35:54.640 --> 35:57.640
You should play chess as fast as good as possible.

35:57.640 --> 36:00.440
But so in all the textbooks that you have,

36:00.440 --> 36:03.280
you can see of AI, there is always the assumption

36:03.280 --> 36:06.840
that you can easily give a machine the goal

36:06.840 --> 36:09.000
that it should achieve.

36:09.000 --> 36:12.680
And now we realize that the more the machine

36:12.680 --> 36:15.600
get into the real world scenarios.

36:15.600 --> 36:18.000
And they have to do with the uncertainty of the world,

36:18.000 --> 36:20.160
and they have to take care of the things that can happen

36:20.160 --> 36:23.560
in the world while achieving the main goal that we give them,

36:23.560 --> 36:25.880
then really we need to be careful

36:25.880 --> 36:29.480
that they also are aware of the fact

36:29.480 --> 36:30.880
that they should not do this.

36:30.880 --> 36:33.960
So another example that Stuart Russell always gives

36:33.960 --> 36:39.000
is that if you leave at home your kids with the Butler

36:39.000 --> 36:42.880
robot taking care of them, and you tell this Butler robot

36:42.880 --> 36:45.160
to cook dinner for the kids.

36:45.160 --> 36:48.760
And the robot opens the fridge, and there's nothing

36:48.760 --> 36:52.760
in the fridge, but it sees a cat walking around with a house.

36:52.760 --> 36:57.760
But you don't want the cat to be the dinner for your kids.

36:57.760 --> 37:01.560
But if you just say cook the dinner for your kids,

37:01.560 --> 37:03.000
you have to be careful that you also

37:03.000 --> 37:06.800
say all these other things that you should not do.

37:06.800 --> 37:09.680
Or I don't know if you have yourself driving car,

37:09.680 --> 37:14.200
and you tell the car, bring me home as fast as possible,

37:14.200 --> 37:15.120
period.

37:15.120 --> 37:17.320
And then yes, OK, but you have to make sure

37:17.320 --> 37:19.040
that you don't run over anybody.

37:19.040 --> 37:22.000
You don't make me car sick because you go too fast and so on.

37:22.000 --> 37:25.440
So all these other things are part of the goal.

37:25.440 --> 37:28.520
But we still have to understand how

37:28.520 --> 37:31.080
to communicate this to a machine.

37:31.080 --> 37:33.320
Well, I think part of the answer that you were getting at

37:33.320 --> 37:38.680
is in your very first comment was,

37:38.680 --> 37:42.440
could these robots evolve?

37:42.440 --> 37:44.200
And that brings up the question.

37:44.200 --> 37:47.200
I mean, you're talking about AI and ethics.

37:47.200 --> 37:49.600
Right now, we are thinking about ethics and AI,

37:49.600 --> 37:54.040
but maybe part of the world experience of these robots

37:54.040 --> 37:57.000
is we bring them up and we teach them.

37:57.000 --> 37:59.760
Humans teaching robots and, oh, nuts.

37:59.760 --> 38:01.840
That's not good behavior.

38:01.840 --> 38:02.600
This is what we do.

38:02.600 --> 38:04.080
This is the way we do things.

38:04.080 --> 38:07.520
And it's a possible approach to having them

38:07.520 --> 38:11.440
grow up understanding human social norms.

38:11.440 --> 38:14.000
Yeah, but hopefully it doesn't take 18 years.

38:14.000 --> 38:14.500
Right.

38:14.500 --> 38:15.000
Or.

38:15.000 --> 38:21.320
Like for you, I don't know what to hope for.

38:21.320 --> 38:27.400
But given that there were also unanticipated results from that,

38:27.400 --> 38:29.800
I mean, is there just going back to my initial,

38:29.800 --> 38:32.880
is there a potential danger in trying

38:32.880 --> 38:36.560
to model artificial intelligence development

38:36.560 --> 38:39.840
on what we understand about humans

38:39.840 --> 38:41.840
and mimicking the developmental steps

38:41.840 --> 38:44.280
or creating a kind of evolutionary potential

38:44.280 --> 38:47.720
and what if we're bad parents to our AI?

38:47.720 --> 38:50.400
In a sense, or we're ignorant parents.

38:50.400 --> 38:53.680
We don't know all the parameters.

38:53.680 --> 38:57.880
Humans start off with a huge innate component

38:57.880 --> 39:01.720
to all their cognitive abilities.

39:01.720 --> 39:05.320
And you couldn't expect to get the same results

39:05.320 --> 39:08.440
with a machine that doesn't have those innate components.

39:08.440 --> 39:12.200
So for example, it has been, I think,

39:12.200 --> 39:18.440
recognized that even very small kids, like three or four years,

39:18.440 --> 39:22.920
so they have an innate nature to cooperate with others.

39:22.920 --> 39:25.480
And nobody even nobody teaching them,

39:25.480 --> 39:27.000
but they cooperate with each other.

39:27.000 --> 39:28.400
They help each other.

39:28.400 --> 39:29.680
They help adults.

39:29.680 --> 39:31.120
They have other kids.

39:31.120 --> 39:36.840
And machines don't come with this thing or others.

39:36.840 --> 39:42.080
So there must be some other way to teach

39:42.080 --> 39:45.920
them or to make them have these capabilities, which

39:45.920 --> 39:48.160
is different from what we do for humans.

39:48.160 --> 39:49.760
I suppose this is true, because if you

39:49.760 --> 39:53.640
did the thought experiment of let's bring up an ape

39:53.640 --> 39:57.040
in our household, you could treat it just as you would

39:57.040 --> 39:59.680
your child, and it might end up somewhat different

39:59.680 --> 40:00.840
in its behavior.

40:00.840 --> 40:05.600
Yeah, that's in fact the issue that Francesca just raised

40:05.600 --> 40:07.320
is actually directly relevant to this,

40:07.320 --> 40:10.200
because apes don't have this cooperative impulse.

40:10.200 --> 40:12.600
So Felix Varnequin showed with babies.

40:12.600 --> 40:14.080
I know him, but he will.

40:14.080 --> 40:22.080
With that a two-year-old who sees a person go to a cabinet

40:22.080 --> 40:24.680
and put things in it and then goes to the cabinet

40:24.680 --> 40:28.480
with a thing too heavy to occupy both hands,

40:28.480 --> 40:34.280
the child will spontaneously go and open the door.

40:34.280 --> 40:40.120
And many experimental approaches to monkeys

40:40.120 --> 40:44.480
and chimps show that they do not tend to do this kind of thing.

40:44.480 --> 40:49.760
In fact, if there's food involved,

40:49.760 --> 40:55.280
they really seem to be especially competitive.

40:55.280 --> 40:57.560
Well, there's learning and there's evolution.

40:57.560 --> 40:59.440
And I wonder whether on an evolutionary time

40:59.440 --> 41:04.400
scale we can get whatever it is in the hardware or firmware

41:04.400 --> 41:07.280
better aligned with humans.

41:07.280 --> 41:11.440
But another point I would make here is that on the one hand,

41:11.440 --> 41:14.600
we would like the embodied AI to understand

41:14.600 --> 41:19.760
something of our world so it can know something of our norms

41:19.760 --> 41:21.480
and ethics and all that.

41:21.480 --> 41:24.720
So that would indicate or dictate that we

41:24.720 --> 41:26.920
want to bring it up as one of our own.

41:26.920 --> 41:29.080
But on the other hand, it shouldn't

41:29.080 --> 41:32.920
be the case that the only embodied AI that makes any sense

41:32.920 --> 41:34.880
is a humanoid-like thing.

41:34.880 --> 41:37.400
So I don't know how to reconcile that.

41:37.400 --> 41:39.840
Certainly the cognitive room that we were describing

41:39.840 --> 41:42.440
is not humanoid at all.

41:42.440 --> 41:46.960
We talked to the room and it talks back to us

41:46.960 --> 41:51.360
through the speakers, shows us stuff through the displays.

41:51.360 --> 41:53.960
But there's nothing humanoid there.

41:53.960 --> 41:55.320
And there are plenty of robots out there

41:55.320 --> 41:57.600
that are not humanoid either.

41:57.600 --> 42:01.000
So I don't know how to reconcile it.

42:01.000 --> 42:02.880
So you're saying it's not possible

42:02.880 --> 42:06.600
that we get to a point where they recognize emotion?

42:06.600 --> 42:12.040
Oh, we can, to some limited degree, recognize emotion now.

42:12.040 --> 42:13.120
It's trained.

42:13.120 --> 42:20.480
I mean, through text, facial expression, tone of voice,

42:20.480 --> 42:24.320
you can train a machine to classify

42:24.320 --> 42:28.320
human-emotional state into a small number of discrete states.

42:28.320 --> 42:32.160
Happy, sad, disgust, anger, things like that.

42:32.160 --> 42:34.440
So why would there not be able also

42:34.440 --> 42:36.920
to have them in terms of what you were saying,

42:36.920 --> 42:39.680
in terms of singularity, recognize certain things,

42:39.680 --> 42:43.240
or not to be down certain things, could hurt somebody,

42:43.240 --> 42:44.560
and so they shouldn't do it.

42:44.560 --> 42:48.400
In other words, have a controlled system built within it.

42:48.400 --> 42:51.640
But just the source's apprentice example

42:51.640 --> 42:56.520
is meant to show that there's no goal that you can give it,

42:56.520 --> 43:01.520
that couldn't be understood in a way that

43:01.520 --> 43:03.920
goes counter to what you wanted.

43:03.920 --> 43:09.320
So it's really difficult to see how to put that into a machine.

43:09.320 --> 43:11.800
The possibilities for screwing up are infinite.

43:11.800 --> 43:13.120
Yeah, that's right.

43:13.120 --> 43:13.720
In variety.

43:13.720 --> 43:17.000
But that's true with humans.

43:17.000 --> 43:23.160
No, but humans have this background of understanding.

43:23.160 --> 43:24.920
I mean, not all humans.

43:24.920 --> 43:28.640
There's a class of psychopaths that don't have to be.

43:28.640 --> 43:32.840
We're talking about humans today, but humans in the past,

43:32.840 --> 43:36.840
we're killing and all sorts of putting fire on people's homes

43:36.840 --> 43:38.680
and so on was normal.

43:38.680 --> 43:41.200
So it has evolved to the point we are here.

43:41.200 --> 43:43.280
So when we worry about singularity,

43:43.280 --> 43:46.600
why aren't we also thinking about that if we devolve

43:46.600 --> 43:49.880
the same way humans have evolved?

43:49.880 --> 43:51.040
Well, it may.

43:51.040 --> 43:52.520
But that's a future.

43:52.520 --> 43:55.160
That is, by creating a God of AIs, I don't know,

43:55.160 --> 43:59.400
but somehow creating the same thing being part of the evolution

43:59.400 --> 44:00.280
of it.

44:00.280 --> 44:03.320
Well, here we get into what is our feeling

44:03.320 --> 44:06.520
about the possible eventuality of a singularity.

44:06.520 --> 44:10.880
We could take the view that, well, we humans in our present form

44:10.880 --> 44:13.120
are maybe not going to be part of that future,

44:13.120 --> 44:16.240
and that makes us sad.

44:16.240 --> 44:19.080
Or it could be, well, some essence of us

44:19.080 --> 44:20.200
is continuing forward.

44:20.200 --> 44:23.080
Just it's part of the natural evolutionary process.

44:23.080 --> 44:25.520
We could take that detached view.

44:25.520 --> 44:27.400
I don't know if I can get myself there personally.

44:30.760 --> 44:35.840
Maybe it will improve itself in some machine form

44:35.840 --> 44:37.920
and become some perfect being.

44:37.920 --> 44:41.760
But is that connected to me?

44:41.760 --> 44:44.000
I don't know if I can connect that to myself.

44:44.000 --> 44:47.720
So I don't know if I feel happy at that prospect.

44:47.720 --> 44:50.920
But it may be that we sort of, some have suggested

44:50.920 --> 44:55.920
that we're not the distinction between human and machine

44:55.920 --> 44:59.200
is going to become fuzzy to the point

44:59.200 --> 45:01.320
where it sort of doesn't matter anymore.

45:01.320 --> 45:06.240
I mean, as we have artificial limbs today,

45:06.240 --> 45:08.520
they're getting better and better.

45:08.520 --> 45:10.840
Someday they'll be seamless, and you won't even

45:10.840 --> 45:12.880
know the difference.

45:12.880 --> 45:15.480
But then the same thing could happen to our brains.

45:15.480 --> 45:17.760
Maybe parts of our brains start getting replaced

45:17.760 --> 45:23.040
by something mechanical, or maybe it becomes

45:23.040 --> 45:25.600
squishy and biological like.

45:25.600 --> 45:30.480
Over time, maybe we just augment our bodies and our minds,

45:30.480 --> 45:33.600
and we sort of meld together human and machine.

45:33.600 --> 45:41.080
And then it's just part of our projection into the future.

45:41.080 --> 45:43.320
So there are different views one can take at this.

45:43.320 --> 45:46.760
One of the implicit distinction that I'm hearing

45:46.760 --> 45:51.280
is do we want these AIs to be tools or instruments,

45:51.280 --> 45:53.960
or do we want them to be agents?

45:53.960 --> 45:58.000
And the paradox is that you want them to be agents,

45:58.000 --> 46:01.320
because if they're mere tools, they're going to do the dumb thing.

46:01.320 --> 46:02.720
You're going to give it commands,

46:02.720 --> 46:05.960
and it's going to misunderstand and do harm.

46:05.960 --> 46:09.560
So you want them to be more agent-like so that they

46:09.560 --> 46:13.000
understand your intentions, or you try to get them to do that.

46:13.000 --> 46:15.440
But the more agent-like they become,

46:15.440 --> 46:17.280
the less you can control them.

46:17.280 --> 46:21.200
Ultimately, a real agent distinguishes an agent

46:21.200 --> 46:24.000
as they can take initiative and then start doing things

46:24.000 --> 46:26.680
that they decide, having their own priorities.

46:26.680 --> 46:29.080
The way out of that, at least in today's world,

46:29.080 --> 46:33.800
is to they're both more, we're using them

46:33.800 --> 46:37.280
because they're more intelligent than us in certain ways.

46:37.280 --> 46:39.920
But I view them as idiot salons.

46:39.920 --> 46:41.080
It's some of both.

46:41.080 --> 46:43.760
They're still instruments at that level.

46:43.760 --> 46:47.840
And yes, they're in narrow ways,

46:47.840 --> 46:53.000
in cognitively stronger than we are, but in narrow ways.

46:53.000 --> 46:56.000
And so like a pocket calculator.

46:58.640 --> 47:02.560
Beyond pocket calculator, but they,

47:02.560 --> 47:07.400
and I wouldn't ascribe any cognition to a pocket calculator,

47:07.400 --> 47:08.880
particularly.

47:08.880 --> 47:13.520
I would say that these are tools that really augment

47:13.520 --> 47:16.000
our cognition in a more significant way.

47:16.000 --> 47:19.000
But they aren't in their own right,

47:19.000 --> 47:23.800
necessarily a fully autonomous, certainly not

47:23.800 --> 47:26.040
an artificial general intelligence.

47:26.040 --> 47:27.200
We're not going to be there for a while.

47:27.200 --> 47:29.840
Kind of like a consultant, I like an expert in something

47:29.840 --> 47:34.400
that you need for your job and you consult with.

47:34.400 --> 47:37.720
And you are going to make the final decision

47:37.720 --> 47:38.960
about what you need to do.

47:38.960 --> 47:42.920
But you consult this expert who is going to help you

47:42.920 --> 47:46.040
because he has more knowledge and he has more skills

47:46.040 --> 47:49.040
for that particular thing that you need to do.

47:49.040 --> 47:54.600
So machines will be much more capable of us

47:54.600 --> 47:58.560
in certain things, as Jeff said, like in handling,

47:58.560 --> 48:02.240
in reading, a lot of data, a lot of scientific articles,

48:02.240 --> 48:05.840
a lot of information that you will never

48:05.840 --> 48:08.320
be able to read in your whole life.

48:08.320 --> 48:10.920
And then summarize it and give it to you,

48:10.920 --> 48:13.080
the part that are relevant to what you have to do

48:13.080 --> 48:15.120
at that particular moment.

48:15.120 --> 48:16.440
But we'll see.

48:16.440 --> 48:20.440
I mean, pocket calculator does not give the best idea,

48:20.440 --> 48:24.000
I think, because it's very deterministic.

48:24.000 --> 48:26.720
So you'll give the numbers and you'll

48:26.720 --> 48:29.400
give the same numbers twice, it gives the same result.

48:29.400 --> 48:33.880
While here we want somebody that can take care of the uncertainty

48:33.880 --> 48:37.480
of the world, of the incredible number of scenarios

48:37.480 --> 48:46.120
that can happen and so that it has its own ways of dealing

48:46.120 --> 48:53.680
with these lines of conduct and probabilities,

48:53.680 --> 48:57.760
probability reasoning, and so that maybe

48:57.760 --> 48:59.880
different, slightly different, so now it

48:59.880 --> 49:03.040
gives you a completely different suggestion of what to do.

49:03.040 --> 49:04.760
So it's a context.

49:04.760 --> 49:05.920
Yeah.

49:05.920 --> 49:11.240
But of course, as soon as somebody gets a line on how

49:11.240 --> 49:17.480
to give them a general intelligence,

49:17.480 --> 49:20.240
since there will be no stopping it,

49:20.240 --> 49:23.840
and then that's then we could enter into uncharted territory.

49:23.840 --> 49:31.120
So we should really be thinking now about that transition.

49:31.120 --> 49:35.000
I think earlier you gave a very good example

49:35.000 --> 49:38.160
when you were talking about the singularity of the notion

49:38.160 --> 49:40.720
that suppose a machine can create a machine that

49:40.720 --> 49:42.920
is slightly more intelligent than itself.

49:42.920 --> 49:45.640
What you have to think about is if a machine can create

49:45.640 --> 49:49.040
a machine that is 99% as smart as it is,

49:49.040 --> 49:51.120
then you can take it several generations ahead

49:51.120 --> 49:53.000
and it'll just peter out.

49:53.000 --> 49:55.560
But if you can make a machine that

49:55.560 --> 49:58.920
makes a machine that is 1% smarter than it,

49:58.920 --> 50:01.920
it's going to be a positive exponential instead

50:01.920 --> 50:03.120
of a negative exponential.

50:03.120 --> 50:05.920
And that's where you get a sharp phase transition between,

50:05.920 --> 50:09.240
ah, don't worry about it, and oh my god.

50:09.240 --> 50:10.960
And also, I mean, people I think

50:10.960 --> 50:13.360
are kind of concerned about that scenario

50:13.360 --> 50:16.560
because different from humans, machine

50:16.560 --> 50:19.880
can replicate themselves instantaneously and almost

50:19.880 --> 50:22.960
with no cost, at least from the software.

50:22.960 --> 50:27.480
So then you have this exponential, not just for one machine,

50:27.480 --> 50:33.480
but for a very huge number of machines.

50:33.480 --> 50:36.920
But again, I think that I mean, I

50:36.920 --> 50:41.960
don't know what we can do now while envisioning that far away

50:41.960 --> 50:45.760
scenario, what we can do now thinking of that scenario.

50:45.760 --> 50:50.800
But we can already do now and work now for narrow,

50:50.800 --> 50:54.840
narrowly intelligent machines to actually help them

50:54.840 --> 50:56.720
behave in the right way.

50:56.720 --> 50:59.240
And I think this, of course, will also

50:59.240 --> 51:04.880
help when and if that general intelligence will come out.

51:04.880 --> 51:06.560
And maybe we can have AI researchers

51:06.560 --> 51:08.120
to behave in the right way too.

51:08.120 --> 51:15.400
I've heard that people have taught courses on AI and ethics

51:15.400 --> 51:17.880
through the medium of science fiction.

51:17.880 --> 51:20.960
Because science fiction has plenty of cautionary tales in it.

51:20.960 --> 51:23.760
Well, there's a number of TV programs right now

51:23.760 --> 51:26.240
that are exploring this border.

51:26.240 --> 51:27.760
There's humans.

51:27.760 --> 51:28.320
West world.

51:28.320 --> 51:29.320
West world.

51:29.320 --> 51:31.280
Yeah.

51:31.280 --> 51:34.880
My problem as a historian is I've

51:34.880 --> 51:38.200
tried to envision what would be plausible scenarios

51:38.200 --> 51:40.400
if we decided that we needed at some point

51:40.400 --> 51:45.800
to restrain the advance of AI as a science.

51:45.800 --> 51:48.800
And I can't find any that would be, in my mind,

51:48.800 --> 51:50.040
that would be successful.

51:50.040 --> 51:53.720
Because as you said, we live in a competitive society

51:53.720 --> 51:57.000
and a world that is not unified.

51:57.000 --> 52:01.400
And if either one company competing against another company

52:01.400 --> 52:04.280
or one nation competing against another nation,

52:04.280 --> 52:08.400
anybody who gets the first pass the post in terms

52:08.400 --> 52:11.400
of getting one of these general artificial general

52:11.400 --> 52:18.960
intelligences is going to have an extremely powerful advantage.

52:18.960 --> 52:20.840
And so there's an incentive built

52:20.840 --> 52:23.680
into the very structure of our political systems

52:23.680 --> 52:27.680
and our motivational systems to compete and kind of an arms

52:27.680 --> 52:30.320
race sort of mentality.

52:30.320 --> 52:32.760
Even if it's not at war, it could be some other.

52:32.760 --> 52:35.360
It could just be just sort of the first pass

52:35.360 --> 52:37.920
the post gets more powerful.

52:37.920 --> 52:41.040
I wonder, I mean, reasoning from history,

52:41.040 --> 52:44.320
are there any historical examples of being

52:44.320 --> 52:46.760
able to suppress a technology?

52:46.760 --> 52:49.600
The only thing I can think that at least goes slightly

52:49.600 --> 52:53.680
in that direction is probably nuclear proliferation.

52:53.680 --> 52:58.120
I mean, the US got to that point a little bit before Germany.

52:58.120 --> 53:02.000
And we used it and then backed off.

53:02.000 --> 53:05.720
The few examples tend to be cautionary in the other way.

53:05.720 --> 53:10.160
The Chinese at one point banned large ocean-going vessels

53:10.160 --> 53:12.680
for a while, a couple centuries.

53:12.680 --> 53:15.880
And all that that did was postponed the inevitable

53:15.880 --> 53:18.680
because eventually other people built big boats

53:18.680 --> 53:19.840
and came to China.

53:19.840 --> 53:24.800
And so in the competitive world, the game

53:24.800 --> 53:28.360
is set up in such a way that if you

53:28.360 --> 53:31.400
have to keep advancing, because otherwise your neighbor is

53:31.400 --> 53:34.320
going to advance past you, and then you'll

53:34.320 --> 53:36.160
be at a disadvantage.

53:36.160 --> 53:39.120
So there have been attempts by people,

53:39.120 --> 53:40.840
you think of the Amish.

53:40.840 --> 53:42.920
They've been able to do it because they pose no threat

53:42.920 --> 53:44.120
to anybody.

53:44.120 --> 53:50.360
But in cases where you're actually getting competitive power

53:50.360 --> 53:53.360
or advantage in a competitive situation,

53:53.360 --> 53:57.320
sooner or later, the pressure makes,

53:57.320 --> 54:00.320
if would there have been an atomic bomb if World War II hadn't

54:00.320 --> 54:01.560
happened?

54:01.560 --> 54:04.160
Probably within 20 years.

54:04.160 --> 54:07.000
The war accelerated that process.

54:07.000 --> 54:10.960
But probably by the 1960, somebody somewhere

54:10.960 --> 54:13.280
would have figured out that the Germans were already

54:13.280 --> 54:15.360
on the way to doing it anyway.

54:15.360 --> 54:21.200
I read a wonderful book by a military historian, John Keegan,

54:21.200 --> 54:25.040
that went through a number of phase transitions

54:25.040 --> 54:29.320
and development of weapons in which there was a sudden advance

54:29.320 --> 54:32.720
in either defense or offense.

54:32.720 --> 54:36.440
And one that sticks in my mind is the development

54:36.440 --> 54:41.320
of a much better cannon, which interacted

54:41.320 --> 54:44.480
with the defense of the time, which were these high walls.

54:44.480 --> 54:51.000
And so for example, the fall of Constantinople in the,

54:51.000 --> 54:56.040
what was that, 1453, I think, was due to the fact that the Turks

54:56.040 --> 55:00.880
perfected a cannon that allowed them to undermine the walls.

55:00.880 --> 55:04.560
And then once, and then they would just fall.

55:04.560 --> 55:09.240
So once this happened, defensive walls in cities

55:09.240 --> 55:12.560
all over Europe, where everybody realized

55:12.560 --> 55:15.880
that the cannon made their walls obsolete.

55:15.880 --> 55:19.640
And what they needed was wide, very thick, and even,

55:19.640 --> 55:22.720
and some, that can be low walls, but much thicker.

55:22.720 --> 55:28.640
And they were building new walls for more than 100 years,

55:28.640 --> 55:31.240
all the cities, the walled cities in Europe.

55:31.240 --> 55:34.680
They even quote a wonderful letter from Michelangelo

55:34.680 --> 55:37.520
saying, because he was selling his services as a wall

55:37.520 --> 55:41.440
designer, saying, I don't know much about painting or sculpture,

55:41.440 --> 55:43.400
but I really know about building walls.

55:43.400 --> 55:48.520
LAUGHTER

55:48.520 --> 55:51.840
But it's true that we live in a very competing world.

55:51.840 --> 55:55.480
But I think that, I mean, a little bit for people

55:55.480 --> 55:58.600
are starting to realize that especially in this,

55:58.600 --> 56:02.920
understanding the issues in the advancement

56:02.920 --> 56:06.720
of this very powerful technology is something that should not

56:06.720 --> 56:09.840
be part of the competition, understanding

56:09.840 --> 56:11.280
how to address these issues.

56:11.280 --> 56:16.280
Like, for example, maybe Ned, remember that I mentioned that

56:16.280 --> 56:18.080
last week, that two weeks ago, it

56:18.080 --> 56:21.440
was launched a very interesting initiative,

56:21.440 --> 56:24.440
where five of the main companies developing

56:24.440 --> 56:30.320
AI, which is IBM, Google, Facebook, Amazon, and Microsoft,

56:30.320 --> 56:35.240
they decided to get together and understand together

56:35.240 --> 56:39.480
what it means to develop AI for the benefit of people

56:39.480 --> 56:41.200
and society.

56:41.200 --> 56:46.240
And we try to engage with everybody else.

56:46.240 --> 56:49.600
It's not just a company kind of thing,

56:49.600 --> 56:52.280
with non-corporate members as well,

56:52.280 --> 56:55.800
non-profit organizations, scientific associations,

56:55.800 --> 56:58.240
individual society in general.

56:58.240 --> 57:00.920
But we really think that we should not

57:00.920 --> 57:04.120
hide the maybe issues in this development

57:04.120 --> 57:05.800
of this very powerful technology.

57:05.800 --> 57:08.160
But the technology is so powerful,

57:08.160 --> 57:11.320
it can be so beneficial for everybody

57:11.320 --> 57:16.600
that we have to work together, even companies that are,

57:16.600 --> 57:19.880
as you may know, competing a lot in the marketplace.

57:19.880 --> 57:23.480
But they think on this, they should be really collaboration.

57:23.480 --> 57:26.120
And I think this is, of course, doesn't mean that maybe

57:26.120 --> 57:30.640
other companies or other countries are not going to compete.

57:30.640 --> 57:33.080
But still, these are very companies

57:33.080 --> 57:35.760
that can influence a lot all over the world,

57:35.760 --> 57:41.440
because they are all used by billions of people everywhere.

57:41.440 --> 57:44.160
And I think that this can start really

57:44.160 --> 57:47.800
a very collaborative environment, where these issues are

57:47.800 --> 57:51.160
discussed, addressed, solved, and understood

57:51.160 --> 57:55.560
how to best trajectory for AI in the future.

57:55.560 --> 57:59.160
But don't you think that if one of those five companies

57:59.160 --> 58:01.640
makes a real breakthrough, it's likely

58:01.640 --> 58:05.200
they're going to actually share it with the other four?

58:05.200 --> 58:06.440
Yeah, but I mean, I did.

58:06.440 --> 58:07.920
I know, but I don't know about the other four.

58:07.920 --> 58:10.840
No, but I mean, the idea is not to share

58:10.840 --> 58:15.400
new software or new advances, but to share the best practices

58:15.400 --> 58:20.560
and how to deal with making AI, of course,

58:20.560 --> 58:24.640
in a competing environment more and more smart,

58:24.640 --> 58:26.680
but in a collaborative environment,

58:26.680 --> 58:30.000
making it smart, but in the right way.

58:30.000 --> 58:32.160
So it could be that one of these companies

58:32.160 --> 58:36.080
is making tomorrow a very big advancement in AI,

58:36.080 --> 58:38.440
and of course, it's going to be his own result,

58:38.440 --> 58:40.320
and not the result of the other ones.

58:40.320 --> 58:43.120
But we want together to understand

58:43.120 --> 58:45.160
how to make whatever advancement is

58:45.160 --> 58:47.800
going to be made by anybody in the best way

58:47.800 --> 58:50.200
for the benefit of everybody.

58:50.200 --> 58:54.000
And I think that that's really needed a lot,

58:54.000 --> 58:56.120
a collaborative environment on these issues,

58:56.120 --> 59:00.640
even among entities that are naturally competing

59:00.640 --> 59:03.600
because of their business model, of course.

59:03.600 --> 59:05.920
It's really encouraging.

59:05.920 --> 59:09.640
Similar to what was done about a year and a half ago

59:09.640 --> 59:11.520
with genetic engineering technology,

59:11.520 --> 59:15.520
the new CRISPR-Cas9 pathway for modifying genomes,

59:15.520 --> 59:17.800
and they convened a second.

59:17.800 --> 59:22.200
The first such conference was with recombinant DNA technology,

59:22.200 --> 59:24.640
the Asilomar Conference in the 1970s,

59:24.640 --> 59:28.400
and voluntarily the leading figures in this field

59:28.400 --> 59:32.160
met a year ago, and again, saying,

59:32.160 --> 59:34.400
let's consult with each other, let's establish

59:34.400 --> 59:36.320
basic ground rules and best practices.

59:36.320 --> 59:38.400
I didn't know that this had happened with AI.

59:38.400 --> 59:40.920
I mean, it needs to happen with synthetic biology.

59:40.920 --> 59:43.720
It needs to happen with all these nanotechnology,

59:43.720 --> 59:47.040
all these potentially disruptive,

59:47.040 --> 59:52.640
but potentially also enormously beneficial technologies.

59:52.640 --> 59:55.320
It is incumbent on the people doing it.

59:55.320 --> 59:58.160
I remember in 2000, I think it was,

59:58.160 --> 01:00:03.960
there was an article published in Wired by,

01:00:03.960 --> 01:00:05.920
I'm blanking on his name right now,

01:00:05.920 --> 01:00:08.440
he said, why the future doesn't need us,

01:00:08.440 --> 01:00:09.760
what's his name again?

01:00:09.760 --> 01:00:13.760
He basically was a computer programmer who basically said,

01:00:13.760 --> 01:00:16.640
I'm leaving the field because I can no longer

01:00:16.640 --> 01:00:18.840
ethically continue to something which I think

01:00:18.840 --> 01:00:21.440
is going to lead toward the singularity.

01:00:21.440 --> 01:00:22.640
I think, pardon?

01:00:22.640 --> 01:00:28.080
It wasn't Jared Lanier, it's, no, it wasn't

01:00:28.080 --> 01:00:30.720
Jared Lanier, it wasn't Jared Lanier, it's wild.

01:00:30.720 --> 01:00:33.160
It'll come to me, of course, after.

01:00:33.160 --> 01:00:38.880
But it made a big stink in the technology community

01:00:38.880 --> 01:00:40.920
because he was saying, I'm taking an ethical stand,

01:00:40.920 --> 01:00:44.080
and implicitly he was saying, if you don't also

01:00:44.080 --> 01:00:50.040
quit doing this, you're doing something fundamentally

01:00:50.040 --> 01:00:50.840
morally wrong.

01:00:50.840 --> 01:00:54.160
It's like continuing to work on the atomic bomb

01:00:54.160 --> 01:00:55.120
or something like that.

01:00:55.120 --> 01:00:59.120
But quitting, it would also not allow us to get

01:00:59.120 --> 01:01:01.200
the real benefits of this technology.

01:01:01.200 --> 01:01:04.800
For example, just one in health care.

01:01:04.800 --> 01:01:10.200
And cure cancer, so health care issues.

01:01:10.200 --> 01:01:12.200
So I don't think we should quit.

01:01:12.200 --> 01:01:15.160
We should continue in the best way.

01:01:15.160 --> 01:01:15.680
Exactly.

01:01:15.680 --> 01:01:17.000
Yeah, I think to state the obvious,

01:01:17.000 --> 01:01:18.720
if the most ethical programmer

01:01:18.720 --> 01:01:23.840
equates, the population of programmers becomes less ethical.

01:01:23.840 --> 01:01:25.400
Somebody would have very quickly taken this question.

01:01:25.400 --> 01:01:26.920
I feel the need to reiterate something

01:01:26.920 --> 01:01:29.480
that you mentioned in a glancing way.

01:01:29.480 --> 01:01:34.720
But, well, best practices are great,

01:01:34.720 --> 01:01:37.640
and it's great to formulate them and share them.

01:01:37.640 --> 01:01:40.200
But it's in a competitive environment

01:01:40.200 --> 01:01:43.960
where they attention might not be paid to them.

01:01:43.960 --> 01:01:51.080
And I think especially if there's a war-like situation.

01:01:51.080 --> 01:01:56.680
Whoever is making in advance might not pay that much attention

01:01:56.680 --> 01:02:02.320
to the best practices if they think they're going to make up,

01:02:02.320 --> 01:02:04.160
they're going to defeat the enemy.

01:02:04.160 --> 01:02:07.760
Doesn't even need to be competitive or malicious,

01:02:07.760 --> 01:02:08.480
I guess.

01:02:08.480 --> 01:02:10.520
It can be accidental.

01:02:10.520 --> 01:02:11.520
Oops, sorry.

01:02:11.520 --> 01:02:12.520
I didn't realize that would happen.

01:02:12.520 --> 01:02:13.520
We're rushing.

01:02:13.520 --> 01:02:17.760
No, it could even just be some emergent phenomenon

01:02:17.760 --> 01:02:21.440
that it would have been difficult to anticipate.

01:02:21.440 --> 01:02:24.600
And I guess this brings up the question of liability

01:02:24.600 --> 01:02:26.920
for AI as well.

01:02:26.920 --> 01:02:29.800
Who's responsible when something goes awry?

01:02:35.240 --> 01:02:38.520
There's been a lot of talk about hacking.

01:02:38.520 --> 01:02:41.080
So is it still possible, for example,

01:02:41.080 --> 01:02:44.720
when you said these five companies have gotten together,

01:02:44.720 --> 01:02:50.640
is it not, are you still able to keep things hidden

01:02:50.640 --> 01:02:53.360
from the other companies if you don't want them to know

01:02:53.360 --> 01:02:57.760
about your networks?

01:02:57.760 --> 01:03:01.240
Isn't it possible with sophisticated technology

01:03:01.240 --> 01:03:05.000
too for, let's say, Amazon to know exactly what IBM is doing

01:03:05.000 --> 01:03:06.400
and vice versa?

01:03:06.400 --> 01:03:11.680
Well, you've seen what happened yesterday.

01:03:11.680 --> 01:03:16.080
That was this big cyber attack that

01:03:16.080 --> 01:03:20.000
blocked many websites and services.

01:03:20.000 --> 01:03:24.080
So I think that we have to be smarter than them,

01:03:24.080 --> 01:03:26.640
and then they come smarter than us, and then so on.

01:03:26.640 --> 01:03:29.240
So I mean, it's still not clear how

01:03:29.240 --> 01:03:35.360
to avoid all these attacks and intruders

01:03:35.360 --> 01:03:38.520
into systems.

01:03:38.520 --> 01:03:39.400
So I don't know.

01:03:39.400 --> 01:03:41.760
I'm not an expert in cyber security,

01:03:41.760 --> 01:03:51.240
but it's not clear to me that these can be easily stopped.

01:03:51.240 --> 01:03:54.920
Of course, I mean, companies can have their own security,

01:03:54.920 --> 01:04:01.120
walls, and everything, but I'm not sure this can be done.

01:04:01.120 --> 01:04:07.400
Even very sophisticated agencies here cannot keep secrets

01:04:07.400 --> 01:04:08.640
that they have.

01:04:08.640 --> 01:04:12.200
Then it means that we still have a long way to do it.

01:04:12.200 --> 01:04:14.280
But I thought some of the sense of your question

01:04:14.280 --> 01:04:17.880
was about among those five companies,

01:04:17.880 --> 01:04:22.760
can they share while still remaining competitive?

01:04:22.760 --> 01:04:24.120
And I think the answer is yes.

01:04:24.120 --> 01:04:28.240
Companies can find ways to share and compete at the same time.

01:04:28.240 --> 01:04:30.000
Yeah, but of course, they don't share.

01:04:30.000 --> 01:04:34.280
I mean, the sharing is advising, sharing, best practices,

01:04:34.280 --> 01:04:38.800
discussions, issues of concerns, and so on.

01:04:38.800 --> 01:04:42.680
How to best develop AI?

01:04:42.680 --> 01:04:44.720
How to be ethical while developing

01:04:44.720 --> 01:04:48.760
AI from the idea to the product that you get?

01:04:48.760 --> 01:04:51.880
And how to build these products in a way

01:04:51.880 --> 01:04:54.600
that that product is going to behave ethically

01:04:54.600 --> 01:04:56.680
when given to the world, and is going

01:04:56.680 --> 01:05:02.040
to behave in the best way for benefit of society?

01:05:02.040 --> 01:05:05.640
I can imagine standards developing as well.

01:05:05.640 --> 01:05:09.080
I mean, going again back to science fiction,

01:05:09.080 --> 01:05:12.560
you can think of Isaac Asimov's Three Laws of Robotics.

01:05:12.560 --> 01:05:16.440
And these were cooked in at the very foundational level

01:05:16.440 --> 01:05:18.200
into each robot.

01:05:18.200 --> 01:05:20.720
And maybe we would develop something like this.

01:05:20.720 --> 01:05:24.320
And through the combined power of these five companies,

01:05:24.320 --> 01:05:27.560
they could all adopt this and then kind of exert pressure

01:05:27.560 --> 01:05:30.160
on the rest of the world community to do the same.

01:05:30.160 --> 01:05:33.720
Yeah, the standards are very important.

01:05:33.720 --> 01:05:36.120
Of course, if not that five companies or even 10,

01:05:36.120 --> 01:05:38.480
or 15 can build a standard, it has

01:05:38.480 --> 01:05:43.040
to be reached by consensus building.

01:05:43.040 --> 01:05:47.800
But over the course of the technology,

01:05:47.800 --> 01:05:50.520
I think the information technology, especially,

01:05:50.520 --> 01:05:52.560
the standards have played a very big role

01:05:52.560 --> 01:05:55.520
in making the technology available to everybody.

01:05:55.520 --> 01:05:58.200
Because the fact that all the things are compatible,

01:05:58.200 --> 01:06:01.720
like, I don't know, the standard for the Wi-Fi.

01:06:01.720 --> 01:06:03.400
At the beginning, there was no standard.

01:06:03.400 --> 01:06:06.760
And then by consensus building, in our computers,

01:06:06.760 --> 01:06:09.520
in our telephones, we all have the same system,

01:06:09.520 --> 01:06:12.480
with the same protocol, being able to connect

01:06:12.480 --> 01:06:14.320
to any Wi-Fi in the world.

01:06:14.320 --> 01:06:17.600
Because it uses the same methodology.

01:06:17.600 --> 01:06:20.800
So of course, it cannot be that technology

01:06:20.800 --> 01:06:25.040
that precise of technological oriented,

01:06:25.040 --> 01:06:31.000
a standard that tells these companies, or everybody else,

01:06:31.000 --> 01:06:33.480
how to develop AI in the right way.

01:06:33.480 --> 01:06:36.440
But there could be very precise guidelines

01:06:36.440 --> 01:06:39.200
on how to do that.

01:06:39.200 --> 01:06:42.120
And that's one of the things that we think this

01:06:42.120 --> 01:06:45.360
would be very important to achieve.

01:06:45.360 --> 01:06:49.080
It may not answer the limit case that you brought up

01:06:49.080 --> 01:06:51.840
of a war, or something like that, or even just

01:06:51.840 --> 01:06:53.840
a big military rivalry.

01:06:53.840 --> 01:06:55.840
But it's a lot better than nothing.

01:06:55.840 --> 01:06:57.760
Yeah, well, I think so.

01:06:57.760 --> 01:06:59.160
I think it's very promising.

01:06:59.160 --> 01:07:06.840
And I think that the fact that it comes from companies

01:07:06.840 --> 01:07:09.040
has to be interpreted in the right way.

01:07:09.040 --> 01:07:12.280
It's not that these companies want to decide how AI should

01:07:12.280 --> 01:07:16.400
be done, and they're going to get together, and define it.

01:07:16.400 --> 01:07:19.520
But because companies are those that are closer

01:07:19.520 --> 01:07:23.600
to understand what it means to deploy AI system

01:07:23.600 --> 01:07:24.800
in the real world.

01:07:24.800 --> 01:07:27.600
Because they have clients, and these clients actually

01:07:27.600 --> 01:07:29.360
use AI in the real world.

01:07:29.360 --> 01:07:31.840
And those are the people that can tell us,

01:07:31.840 --> 01:07:35.280
what are the issues that you see in health care,

01:07:35.280 --> 01:07:39.520
in finance, in retail, in e-commerce, and so on.

01:07:39.520 --> 01:07:41.640
And so tell us, and these are the issues

01:07:41.640 --> 01:07:44.640
that we are going to address and resolve.

01:07:44.640 --> 01:07:48.320
What are the problems that you see once you get your product,

01:07:48.320 --> 01:07:51.440
and you just give it to customers?

01:07:51.440 --> 01:07:55.200
So that's where you should start this discussion.

01:07:55.200 --> 01:07:57.960
And then everybody else should be involved as well.

01:07:57.960 --> 01:08:02.400
Speaking of everybody else, you mentioned the five companies.

01:08:02.400 --> 01:08:05.120
Are you in this organization making efforts

01:08:05.120 --> 01:08:07.280
to bring academia into discussion?

01:08:07.280 --> 01:08:07.760
Yeah, yeah.

01:08:07.760 --> 01:08:10.160
Academia, non-profit organizations,

01:08:10.160 --> 01:08:13.560
and other professional associations.

01:08:13.560 --> 01:08:16.680
Everybody, everybody, we just have

01:08:16.680 --> 01:08:19.360
to understand how to put together the path

01:08:19.360 --> 01:08:23.520
of to make everybody be part of the discussion

01:08:23.520 --> 01:08:25.000
in the right way.

01:08:25.000 --> 01:08:27.520
According to Jan Lecun, who's at NYU,

01:08:27.520 --> 01:08:31.120
and also runs the deep learning part of Facebook,

01:08:31.120 --> 01:08:39.400
at least at the moment, there are no secrets as far as how

01:08:39.400 --> 01:08:42.840
to make an artificial general intelligence.

01:08:42.840 --> 01:08:49.280
So at least his story is the academic research centers

01:08:49.280 --> 01:08:51.960
are way better than the company was,

01:08:51.960 --> 01:08:54.640
because none of the good people will

01:08:54.640 --> 01:09:00.240
work for an organization where they can't publish their results

01:09:00.240 --> 01:09:02.720
and collaborate with other academics who

01:09:02.720 --> 01:09:04.160
are working on the same thing.

01:09:04.160 --> 01:09:10.120
You should not assume that companies cannot work with academia.

01:09:10.120 --> 01:09:12.560
Well, of course, he's working with a company, Facebook.

01:09:12.560 --> 01:09:13.200
Yes.

01:09:13.200 --> 01:09:17.720
And he says, well, but he says, take it for me.

01:09:17.720 --> 01:09:20.240
The private efforts are just nowhere near as good

01:09:20.240 --> 01:09:22.760
as the academic public efforts.

01:09:22.760 --> 01:09:25.520
The best people are all in academic centers,

01:09:25.520 --> 01:09:28.600
even if they're also in Facebook.

01:09:28.600 --> 01:09:35.960
And at least at the moment, just the technology

01:09:35.960 --> 01:09:41.320
in public academic centers is ahead of the technology,

01:09:41.320 --> 01:09:43.040
the privately developed technology,

01:09:43.040 --> 01:09:45.880
where they don't have the benefit of getting feedback

01:09:45.880 --> 01:09:50.680
from a large public that books a few code.

01:09:50.680 --> 01:09:52.040
But I don't know.

01:09:52.040 --> 01:09:54.440
I mean, I have heard Ian saying this many times.

01:09:54.440 --> 01:09:57.720
And by the way, Ian is also the Facebook representative

01:09:57.720 --> 01:09:59.640
in this partnership that I discussed.

01:09:59.640 --> 01:10:01.360
So he's also involved in that.

01:10:01.360 --> 01:10:04.360
And I have them saying many times that what is developing

01:10:04.360 --> 01:10:05.880
academia is much better.

01:10:05.880 --> 01:10:07.800
But I don't know that at this point,

01:10:07.800 --> 01:10:11.720
there is really a difference because there

01:10:11.720 --> 01:10:13.800
is a lot of collaboration.

01:10:13.800 --> 01:10:18.880
And big companies, of course, that do not have research centers.

01:10:18.880 --> 01:10:21.680
They just want to develop better and better products.

01:10:21.680 --> 01:10:22.800
Maybe they don't share.

01:10:22.800 --> 01:10:25.400
They don't publish in academic venues.

01:10:25.400 --> 01:10:27.760
But other companies that have research centers,

01:10:27.760 --> 01:10:29.920
like IBM, Microsoft, and other.

01:10:29.920 --> 01:10:30.920
And they do publish.

01:10:30.920 --> 01:10:34.000
And they do collaborate with academia.

01:10:34.000 --> 01:10:39.080
And they want to be exposed to the feedback

01:10:39.080 --> 01:10:41.840
and to the common, positive, or negative of the rest

01:10:41.840 --> 01:10:45.840
of the academic colleagues.

01:10:45.840 --> 01:10:49.040
So I don't see so much different.

01:10:49.040 --> 01:10:54.240
Also, I mean, there are a lot of data sets.

01:10:54.240 --> 01:10:56.560
Many are openly available.

01:10:56.560 --> 01:11:00.480
Data sets of which these AI systems can be trained

01:11:00.480 --> 01:11:03.440
and can be structured.

01:11:03.440 --> 01:11:08.680
But there is also a lot of data that companies can get access

01:11:08.680 --> 01:11:11.920
just because they have the real world scenarios

01:11:11.920 --> 01:11:13.960
they can work with.

01:11:13.960 --> 01:11:17.360
So I think that I don't see much difference between my two.

01:11:17.360 --> 01:11:21.360
Yeah, I'd like to comment on your quote from Jan also.

01:11:21.360 --> 01:11:24.880
And I guess this is just a reflection

01:11:24.880 --> 01:11:27.160
of my own cognitive bias, because no one

01:11:27.160 --> 01:11:29.560
likes being implicated for being second rate.

01:11:29.560 --> 01:11:30.060
OK.

01:11:30.060 --> 01:11:38.240
So I don't know, actually, what is the utility

01:11:38.240 --> 01:11:40.920
of making such comparisons?

01:11:40.920 --> 01:11:46.800
I think we should be embracing what academia and industry

01:11:46.800 --> 01:11:48.920
can bring to the table.

01:11:48.920 --> 01:11:52.680
I think there are outstanding people in academia.

01:11:52.680 --> 01:11:54.680
Obviously, Jan is one of them.

01:11:54.680 --> 01:11:58.000
There are a lot of other people that one can cite.

01:11:58.000 --> 01:12:01.840
I think something that I, and so the individual technology

01:12:01.840 --> 01:12:05.720
is being developed, yes, they're outstanding.

01:12:05.720 --> 01:12:07.840
I think we're developing some pretty good things too.

01:12:07.840 --> 01:12:12.320
But one place where I think companies can really shine

01:12:12.320 --> 01:12:16.160
is weaving it all together into something that actually works

01:12:16.160 --> 01:12:19.960
and something that actually has an impact on the real world.

01:12:19.960 --> 01:12:24.000
So rather than saying A is better than B,

01:12:24.000 --> 01:12:26.920
I would rather focus on how can A and B work together

01:12:26.920 --> 01:12:29.280
collaboratively in the best possible way

01:12:29.280 --> 01:12:33.200
to create the best value for the world.

01:12:33.200 --> 01:12:35.360
Yeah, of course he's focused on that too.

01:12:35.360 --> 01:12:35.840
Right.

01:12:35.840 --> 01:12:37.840
He has a foot in both worlds.

01:12:37.840 --> 01:12:40.120
So he was just Francesca.

01:12:40.120 --> 01:12:43.200
He was trying to combat the suspicion

01:12:43.200 --> 01:12:51.560
that you see that there's some nefarious artificial intelligence

01:12:51.560 --> 01:12:54.840
projects deeply hidden in some company

01:12:54.840 --> 01:12:59.800
that is going to take over the world or something.

01:12:59.800 --> 01:13:01.880
So he was speaking to that kind of issue.

01:13:01.880 --> 01:13:03.760
He's saying, no, it's not going to happen.

01:13:03.760 --> 01:13:06.520
No, it's only going to happen in academia.

01:13:06.520 --> 01:13:09.280
Well, what he says is that it'll be public.

01:13:09.280 --> 01:13:11.680
So the point he was making is that it's public.

01:13:11.680 --> 01:13:13.760
What academics are doing is public.

01:13:13.760 --> 01:13:14.720
They publish it.

01:13:14.720 --> 01:13:16.680
They're at conferences.

01:13:16.680 --> 01:13:18.640
All that sort of stuff.

01:13:18.640 --> 01:13:21.360
But I thought academics chronically complain,

01:13:21.360 --> 01:13:26.480
certainly in the medical biotech field and so on.

01:13:26.480 --> 01:13:29.600
They're constantly complaining that they, in research,

01:13:29.600 --> 01:13:32.080
that they don't have enough financial resources

01:13:32.080 --> 01:13:35.200
and companies like IBM and so on have all that.

01:13:35.200 --> 01:13:36.960
So it's a little bit puzzling to me

01:13:36.960 --> 01:13:40.720
that that would be expressed like young music.

01:13:40.720 --> 01:13:43.360
And I think this issue of resources

01:13:43.360 --> 01:13:45.480
is important, not just because you

01:13:45.480 --> 01:13:48.040
want to have more money to do your research,

01:13:48.040 --> 01:13:50.560
but because in corporate environments,

01:13:50.560 --> 01:13:54.560
especially big ones like IBM, you really, like Jeff just said,

01:13:54.560 --> 01:13:57.880
you really have the opportunity to put together

01:13:57.880 --> 01:14:03.720
experts in many different areas of AI, or even IT,

01:14:03.720 --> 01:14:05.800
or even other disciplines.

01:14:05.800 --> 01:14:08.720
And you put together software, hardware, and body

01:14:08.720 --> 01:14:10.240
management, and so on.

01:14:10.240 --> 01:14:13.080
So you really have the chance of putting together

01:14:13.080 --> 01:14:17.560
the best of all these lines of work

01:14:17.560 --> 01:14:22.400
to build something that is even more significant.

01:14:22.400 --> 01:14:27.480
And this, I think, rarely happens in a university group

01:14:27.480 --> 01:14:34.520
because of lack of resources, but also because within the department,

01:14:34.520 --> 01:14:36.480
usually you have a critical mass.

01:14:36.480 --> 01:14:37.480
Some people have it.

01:14:37.480 --> 01:14:42.400
But not as much as you can get in a very big corporate environment.

01:14:42.400 --> 01:14:48.960
And you are not exposed to this wide range of applications

01:14:48.960 --> 01:14:54.400
that, for example, IBM can describe to us.

01:14:54.400 --> 01:14:57.320
If I want to know, or Jeff wants to know,

01:14:57.320 --> 01:15:01.040
what are the main things that are happening in AI

01:15:01.040 --> 01:15:04.440
applied to health care, or AI applied to commerce,

01:15:04.440 --> 01:15:09.400
or AI applied to anything else, or within IBM, you find it.

01:15:09.400 --> 01:15:13.520
And so while in academia, you have to contact somebody else

01:15:13.520 --> 01:15:17.120
and see whether you know, and then go to look at the conferences

01:15:17.120 --> 01:15:18.240
and these and that.

01:15:18.240 --> 01:15:21.560
In a big corporate environment, you are really

01:15:21.560 --> 01:15:25.160
exposed to the real world.

01:15:25.160 --> 01:15:30.760
And so that's very important for young.

01:15:30.760 --> 01:15:35.440
For example, in Facebook, as a more narrow goal,

01:15:35.440 --> 01:15:40.720
which is whatever is important to Facebook in terms of AI,

01:15:40.720 --> 01:15:44.800
like personalizing the news feed, or recognizing people,

01:15:44.800 --> 01:15:48.280
or other objects, or whatever, understanding what

01:15:48.280 --> 01:15:50.640
is in a picture, things like that.

01:15:50.640 --> 01:15:55.240
But I think that by being connected to Facebook,

01:15:55.240 --> 01:15:59.000
he really can do much more than what he could do,

01:15:59.000 --> 01:16:01.440
but just being just.

01:16:01.440 --> 01:16:04.400
And why you're a professor is not just.

01:16:04.400 --> 01:16:10.400
But he really gets exposed to the level of being

01:16:10.400 --> 01:16:17.160
exposed to the real world issues in very wide reaching

01:16:17.160 --> 01:16:19.320
enterprise like Facebook.

01:16:19.320 --> 01:16:22.760
It's really very important for him as well.

01:16:22.760 --> 01:16:24.360
I think he would not deny it.

01:16:24.360 --> 01:16:26.200
Oh, he says, yeah.

01:16:26.200 --> 01:16:27.680
I'd like to follow up on two points.

01:16:27.680 --> 01:16:32.160
One, Ed, I think he said something about academia

01:16:32.160 --> 01:16:35.320
complaining about financial resources

01:16:35.320 --> 01:16:40.800
don't think that only holds in academia necessarily.

01:16:40.800 --> 01:16:43.080
Researchers are insatiable in their desire

01:16:43.080 --> 01:16:47.560
for financial support anywhere, academia or industry.

01:16:47.560 --> 01:16:50.880
The other thing is one thing that I, on the point

01:16:50.880 --> 01:16:53.680
of collaboration between industry and academia,

01:16:53.680 --> 01:16:57.000
and bringing together the best of both worlds,

01:16:57.000 --> 01:17:00.760
one thing I've been involved in is a collaboration

01:17:00.760 --> 01:17:03.040
in the space of embodied cognition

01:17:03.040 --> 01:17:07.320
that IBM started up with Rensselaire Polytechnic Institute

01:17:07.320 --> 01:17:08.840
about a year ago.

01:17:08.840 --> 01:17:11.520
We've been working with several professors

01:17:11.520 --> 01:17:16.680
there and their students to set up environments there,

01:17:16.680 --> 01:17:20.760
cognitive rooms, like the ones that we're developing at IBM.

01:17:20.760 --> 01:17:26.240
And they're starting from the technology that we provided.

01:17:26.240 --> 01:17:29.400
And now they're really building some very interesting things

01:17:29.400 --> 01:17:30.320
on top of that.

01:17:30.320 --> 01:17:34.000
But the thing I want to emphasize is that I think what

01:17:34.000 --> 01:17:38.920
we brought to RPI in doing this was the idea of what

01:17:38.920 --> 01:17:40.760
these professors and their students could do

01:17:40.760 --> 01:17:42.280
if they worked together.

01:17:42.280 --> 01:17:47.600
They're not so accustomed to doing that, as I find.

01:17:47.600 --> 01:17:52.240
They have other professors at the same university.

01:17:52.240 --> 01:17:54.120
They have their colleagues all around the world.

01:17:54.120 --> 01:17:57.880
They have their own social networks and all that.

01:17:57.880 --> 01:18:02.400
But they, by and large, tend not to work quite as much

01:18:02.400 --> 01:18:05.800
with people at their own institution.

01:18:05.800 --> 01:18:09.360
And so they haven't had the experience of building together

01:18:09.360 --> 01:18:11.680
a much larger thing than they can do independently

01:18:11.680 --> 01:18:12.680
of one another.

01:18:12.680 --> 01:18:14.440
And I think bringing that model of what

01:18:14.440 --> 01:18:17.640
we're able to do within a company to RPI

01:18:17.640 --> 01:18:20.360
has been, I think, eye-opening for them.

01:18:20.360 --> 01:18:25.760
And I'm hoping it is a good model for other universities

01:18:25.760 --> 01:18:29.280
and for other university industry collaborations.

01:18:29.280 --> 01:18:33.680
I'm curious about what you two are saying about IBM.

01:18:33.680 --> 01:18:36.560
I mean, I have no real knowledge of industry,

01:18:36.560 --> 01:18:39.800
but the impression one gets just reading,

01:18:39.800 --> 01:18:46.120
and I do have one other source, is that many companies

01:18:46.120 --> 01:18:49.800
are organized in informational silos

01:18:49.800 --> 01:18:51.840
where the parts of the company really

01:18:51.840 --> 01:18:54.800
operate in secret from one another.

01:18:54.800 --> 01:18:57.440
Is that not true at IBM?

01:18:57.440 --> 01:19:01.840
I don't say more because I joined one year ago, maybe.

01:19:01.840 --> 01:19:05.000
I'm so well-hidden, you don't know the same.

01:19:05.000 --> 01:19:07.360
No, but I see a lot of collaboration

01:19:07.360 --> 01:19:14.440
and more than my, what is it?

01:19:14.440 --> 01:19:17.120
You're talking, Phil.

01:19:17.120 --> 01:19:17.640
No?

01:19:17.640 --> 01:19:18.600
You're OK.

01:19:18.600 --> 01:19:19.120
You're OK.

01:19:19.120 --> 01:19:20.080
I don't know.

01:19:20.080 --> 01:19:22.720
So more than I see, really, in academia.

01:19:22.720 --> 01:19:26.320
In my department, for example, I work in AI,

01:19:26.320 --> 01:19:28.920
and then somebody else works in other areas

01:19:28.920 --> 01:19:31.640
of information technology, software engineering,

01:19:31.640 --> 01:19:32.960
or formal methods.

01:19:32.960 --> 01:19:35.760
But we all work with other colleagues

01:19:35.760 --> 01:19:39.360
in that same area, as Jeff said, but not much

01:19:39.360 --> 01:19:41.720
with other colleagues in other areas.

01:19:41.720 --> 01:19:45.720
So that brings maybe more advancement in your own area.

01:19:45.720 --> 01:19:47.360
But then when you want to build something,

01:19:47.360 --> 01:19:50.120
you actually need other disciplines.

01:19:50.120 --> 01:19:53.000
And within academia, you don't do that.

01:19:53.000 --> 01:19:54.400
I'm so proud of this.

01:19:54.400 --> 01:19:57.720
I mean, and I think that IBM is needed instead,

01:19:57.720 --> 01:19:59.680
this collaboration between different disciplines,

01:19:59.680 --> 01:20:01.920
because otherwise, you cannot build a product that

01:20:01.920 --> 01:20:05.280
goes into the real world from the idea, and research,

01:20:05.280 --> 01:20:06.000
and so on.

01:20:06.000 --> 01:20:09.600
So you need different people with different capabilities

01:20:09.600 --> 01:20:13.640
to be put together and to build, actually, that thing.

01:20:13.640 --> 01:20:14.480
I've been it.

01:20:14.480 --> 01:20:15.240
Yeah.

01:20:15.240 --> 01:20:17.400
It's funny because there's a parallel between what

01:20:17.400 --> 01:20:21.640
we're saying the AI needs to do to work better, which

01:20:21.640 --> 01:20:24.480
is to be embodied in the real world

01:20:24.480 --> 01:20:25.680
and grounded in the real world.

01:20:25.680 --> 01:20:28.440
And we're saying the AI researchers also

01:20:28.440 --> 01:20:33.840
need to be embodied in society and get that feedback.

01:20:33.840 --> 01:20:39.720
I think that's been a recurring theme in this discussion.

01:20:39.720 --> 01:20:42.160
But just to comment on industry, or at least

01:20:42.160 --> 01:20:44.040
my own experience of it.

01:20:44.040 --> 01:20:47.360
There are always silos that develop in any organization.

01:20:47.360 --> 01:20:49.520
But I wouldn't say that, at least in my case,

01:20:49.520 --> 01:20:51.560
they've been intentional.

01:20:51.560 --> 01:20:55.680
It's more security through obscurity, I guess.

01:20:55.680 --> 01:21:00.600
In my own case, I've felt that in my quarter century or so

01:21:00.600 --> 01:21:04.880
at IBM, I've been able to collaborate very broadly.

01:21:04.880 --> 01:21:09.440
And in particular, our work in embodied cognition

01:21:09.440 --> 01:21:13.280
requires that, because our team, by itself,

01:21:13.280 --> 01:21:16.760
weaves together technologies from a vast number

01:21:16.760 --> 01:21:19.280
of other research teams working in our lab

01:21:19.280 --> 01:21:21.840
and in the other labs around the world.

01:21:21.840 --> 01:21:22.800
We need that.

01:21:22.800 --> 01:21:24.320
We can't do it all ourselves.

01:21:24.320 --> 01:21:29.760
And so we strive all the time to break down any barriers

01:21:29.760 --> 01:21:32.680
that some of the barriers are time zones and things

01:21:32.680 --> 01:21:34.440
that like this.

01:21:34.440 --> 01:21:36.880
We strive continually to try to break down

01:21:36.880 --> 01:21:39.400
those barriers, educate ourselves about what others are

01:21:39.400 --> 01:21:43.680
doing, and avail ourselves of those technologies.

01:21:43.680 --> 01:21:47.760
So I'm surprised what you said about Rensler Polytechnic

01:21:47.760 --> 01:21:50.840
Institute, because at least in the parts of NYU

01:21:50.840 --> 01:21:53.600
that I'm familiar with, it isn't like that at all.

01:21:53.600 --> 01:21:55.800
So my colleague David Chalmers and I

01:21:55.800 --> 01:21:58.320
who run the Center for Mind, Brain, and Consciousness,

01:21:58.320 --> 01:22:02.000
we both have joint appointments in neuroscience.

01:22:02.000 --> 01:22:04.200
I have a joint appointment in psychology.

01:22:04.200 --> 01:22:08.040
I go to lab meetings of some of my psychology department

01:22:08.040 --> 01:22:08.520
colleagues.

01:22:08.520 --> 01:22:10.680
Of course, I talk to all my philosophy department

01:22:10.680 --> 01:22:11.360
colleagues, too.

01:22:11.360 --> 01:22:13.880
So we seem to have.

01:22:13.880 --> 01:22:14.880
I think that maybe is different.

01:22:14.880 --> 01:22:16.800
I may have missed sciences.

01:22:16.800 --> 01:22:19.720
I think there may be the context of social sciences.

01:22:19.720 --> 01:22:21.720
Maybe it's different.

01:22:21.720 --> 01:22:22.220
Yeah.

01:22:22.220 --> 01:22:22.720
Sure.

01:22:22.720 --> 01:22:25.800
I have to go to a public question so people

01:22:25.800 --> 01:22:32.960
can line up at the microphone and please

01:22:32.960 --> 01:22:37.680
speak briefly and to the questions specifically.

01:22:37.680 --> 01:22:41.680
Thank you.

01:22:41.680 --> 01:22:42.240
Is it on?

01:22:42.240 --> 01:22:42.960
No?

01:22:42.960 --> 01:22:43.520
Is it on?

01:22:43.520 --> 01:22:44.640
Yeah.

01:22:44.640 --> 01:22:45.200
OK.

01:22:45.200 --> 01:22:47.800
My question goes back a little bit toward when you were talking

01:22:47.800 --> 01:22:52.000
about emotion with AI and then also

01:22:52.000 --> 01:22:56.280
with a possible threat toward people.

01:22:56.280 --> 01:23:00.560
And my question is, isn't there a fundamental difference

01:23:00.560 --> 01:23:05.240
between humans and artificial intelligence regarding

01:23:05.240 --> 01:23:09.600
concerns about mortality and then to extend that to worry

01:23:09.600 --> 01:23:11.880
about survival of the species even?

01:23:11.880 --> 01:23:13.880
I mean, is a machine ever going to have that?

01:23:13.880 --> 01:23:17.440
Is a machine going to worry about if it ceases to exist?

01:23:17.440 --> 01:23:19.680
And if that's the case that it wouldn't,

01:23:19.680 --> 01:23:23.200
does that mitigate any possible threat that

01:23:23.200 --> 01:23:26.040
could occur down the road?

01:23:26.040 --> 01:23:28.800
You know, one point that's sometimes

01:23:28.800 --> 01:23:34.960
made about this is that even if the machine doesn't care

01:23:34.960 --> 01:23:39.760
about its own existence, it might care about its project

01:23:39.760 --> 01:23:44.120
and its project may require its existence, in which case

01:23:44.120 --> 01:23:48.080
it might act almost as if it was really extremely

01:23:48.080 --> 01:23:49.680
concerned with its own existence.

01:23:49.680 --> 01:23:52.320
The source's apprentice story is like that.

01:23:52.320 --> 01:23:54.680
So we're not supposed to imagine that the machine cares

01:23:54.680 --> 01:23:57.000
about its own existence, but it cares about getting

01:23:57.000 --> 01:24:00.640
the cauldron full so it's going to protect itself

01:24:00.640 --> 01:24:03.760
from being destroyed.

01:24:03.760 --> 01:24:07.320
And it may not make that much difference.

01:24:07.320 --> 01:24:11.120
The other example from a sci-fi would be in 2001

01:24:11.120 --> 01:24:11.880
a space odyssey.

01:24:11.880 --> 01:24:14.080
I was thinking of that too.

01:24:14.080 --> 01:24:17.120
Hal had the greatest enthusiasm for this mission.

01:24:17.120 --> 01:24:21.360
And that may have been the thing rather than its own self

01:24:21.360 --> 01:24:21.960
preservation.

01:24:21.960 --> 01:24:26.960
But is there any self preservation built in or not really?

01:24:26.960 --> 01:24:28.120
Well, not today.

01:24:28.120 --> 01:24:31.280
I don't know how it's going to go.

01:24:31.280 --> 01:24:31.800
All right.

01:24:31.800 --> 01:24:33.720
Thank you.

01:24:33.720 --> 01:24:39.760
I was thinking of the metaphor you had of the castle,

01:24:39.760 --> 01:24:43.600
and the white walls, and the high walls.

01:24:43.600 --> 01:24:49.440
And I haven't mentioned the notion of social systems.

01:24:49.440 --> 01:24:51.880
And I see the social systems, let's say,

01:24:51.880 --> 01:24:55.680
Watson and the medical being the castle.

01:24:55.680 --> 01:24:57.440
And then there's the smoke.

01:24:57.440 --> 01:25:01.600
And I don't know how the, for example, let's say Watson

01:25:01.600 --> 01:25:07.240
has cures for this efficient ways to do surgery, et cetera.

01:25:07.240 --> 01:25:10.680
And the social system, i.e., let's say the medical system,

01:25:10.680 --> 01:25:14.840
is with their gatekeepers, very protective,

01:25:14.840 --> 01:25:18.920
wanting essentially money or protecting

01:25:18.920 --> 01:25:21.480
their financial needs, et cetera.

01:25:21.480 --> 01:25:25.280
So maybe this is something in the future where

01:25:25.280 --> 01:25:28.840
AI can, people with any AI, have to kind of deal

01:25:28.840 --> 01:25:32.720
with social systems, which is probably a whole different level

01:25:32.720 --> 01:25:36.000
beyond the issue of ethics, et cetera.

01:25:36.000 --> 01:25:36.520
Yeah.

01:25:36.520 --> 01:25:37.040
Definitely.

01:25:37.040 --> 01:25:41.960
I mean, to understand the dynamics within social systems

01:25:41.960 --> 01:25:44.160
in order to support them, but also

01:25:44.160 --> 01:25:47.280
know to be aware of what's the, I mean,

01:25:47.280 --> 01:25:49.480
to understand what's the best way to interact

01:25:49.480 --> 01:25:50.560
with the social system.

01:25:50.560 --> 01:25:55.840
Of course, we don't want AI to be manipulating people

01:25:55.840 --> 01:25:58.880
or make them believe something.

01:25:58.880 --> 01:26:03.240
Well, let's say there's a type of surgery that Watson says,

01:26:03.240 --> 01:26:05.520
this is best surgery.

01:26:05.520 --> 01:26:07.200
Yeah.

01:26:07.200 --> 01:26:11.240
The social system in the same medical says, well, that's good.

01:26:11.240 --> 01:26:15.440
But there are other surgeries, which essentially

01:26:15.440 --> 01:26:19.440
helps the finances of the surgeon.

01:26:19.440 --> 01:26:24.480
And you're going to have this kind of play that goes on.

01:26:24.480 --> 01:26:27.200
So I guess the deeper question I would say is,

01:26:27.200 --> 01:26:31.840
what the role of the patient come in

01:26:31.840 --> 01:26:33.160
in dealing with the health?

01:26:33.160 --> 01:26:34.040
Hopefully, yes.

01:26:34.040 --> 01:26:37.480
Hopefully the AI system could be not just

01:26:37.480 --> 01:26:41.560
the support or like a consultant for the doctor,

01:26:41.560 --> 01:26:45.520
but it could also be something that gives,

01:26:45.520 --> 01:26:48.360
I mean, the possibility of engaging with all the stakeholders,

01:26:48.360 --> 01:26:49.920
the patient as well.

01:26:49.920 --> 01:26:51.280
And so in that.

01:26:51.280 --> 01:26:54.360
So you can empower the patient to deal with the

01:26:54.360 --> 01:26:56.120
power of the social system.

01:26:56.120 --> 01:26:59.120
I would guess, look, right now there

01:26:59.120 --> 01:27:02.840
is the robotic surgery for prostate cancer.

01:27:02.840 --> 01:27:07.240
But in order to do the robotic surgery, you need a surgeon.

01:27:07.240 --> 01:27:09.960
It seems to me, over time, if the robot could

01:27:09.960 --> 01:27:12.800
do the surgery without the surgeon,

01:27:12.800 --> 01:27:16.320
you would start not having urological surgeons

01:27:16.320 --> 01:27:18.240
specializing in prostate cancer.

01:27:18.240 --> 01:27:21.520
They would just, over time, drop out.

01:27:21.520 --> 01:27:24.560
I'm just a very personal experience.

01:27:24.560 --> 01:27:28.960
I had to try and find a surgeon to do with some,

01:27:28.960 --> 01:27:31.680
none of these surgery.

01:27:31.680 --> 01:27:35.880
I have gone to six surgeons, finally found the seventh,

01:27:35.880 --> 01:27:38.520
who wanted to do a more traditional surgery.

01:27:38.520 --> 01:27:40.440
This is over a two-year period.

01:27:40.440 --> 01:27:44.840
So when you say, over time, I'm talking about a system.

01:27:44.840 --> 01:27:47.120
And the system, to change the system,

01:27:47.120 --> 01:27:52.560
is, I think, a variable that I'm soon somewhere along the line.

01:27:52.560 --> 01:27:54.920
You're going to meet up with, the robotics

01:27:54.920 --> 01:27:57.800
is fine by the system that's based on different values.

01:27:57.800 --> 01:28:01.400
Then maybe you'll talk to your robot or what's

01:28:01.400 --> 01:28:02.880
in to deal with.

01:28:02.880 --> 01:28:04.920
There's going to be a gap or a conflict.

01:28:04.920 --> 01:28:06.080
That's all I'm suggesting.

01:28:06.080 --> 01:28:10.200
There's another aspect implicit in what you're saying,

01:28:10.200 --> 01:28:13.600
which is the rise of automation in general.

01:28:13.600 --> 01:28:17.560
And AI is going to probably accelerate that and feed

01:28:17.560 --> 01:28:18.920
that process.

01:28:18.920 --> 01:28:23.680
And when you talk about the rise of automation, higher,

01:28:23.680 --> 01:28:31.440
higher level, either robotic or AI-assisted technologies,

01:28:31.440 --> 01:28:35.400
interacting with a system that has certain established

01:28:35.400 --> 01:28:38.440
patterns for humans to be carrying out certain functions

01:28:38.440 --> 01:28:40.000
and livelihoods are built on that.

01:28:40.000 --> 01:28:42.000
Mm-hmm.

01:28:42.000 --> 01:28:46.160
One of the, it seems that one of the very hard

01:28:46.160 --> 01:28:49.080
to avoid pathways with rising automation

01:28:49.080 --> 01:28:52.880
is ever increasing unemployment.

01:28:52.880 --> 01:28:58.640
How are people going to react when Google self-driving trucks

01:28:58.640 --> 01:29:01.440
and cars, millions of people immediately out

01:29:01.440 --> 01:29:04.560
of unemployment, taxi drivers, everybody else?

01:29:04.560 --> 01:29:07.960
And now start imagining AI, this fundamentally

01:29:07.960 --> 01:29:11.360
beneficial, well-intended technology spreading

01:29:11.360 --> 01:29:13.720
throughout various aspects of our economy.

01:29:13.720 --> 01:29:17.320
And you have half the population no longer able to be employed.

01:29:17.320 --> 01:29:20.800
That's a different, it's a broader version of the case.

01:29:20.800 --> 01:29:23.200
Well, the surgery was making half a million dollars a year.

01:29:23.200 --> 01:29:26.560
Well, now we're making $100,000 a year.

01:29:26.560 --> 01:29:29.200
And his professions, like the craft union,

01:29:29.200 --> 01:29:35.640
the craft, not unions, during the 18th century,

01:29:35.640 --> 01:29:36.840
they're going to put a barrier.

01:29:36.840 --> 01:29:37.440
Is that all?

01:29:37.440 --> 01:29:40.560
Well, you end up with one surgeon making the same amount

01:29:40.560 --> 01:29:42.400
as that surgeon makes today.

01:29:42.400 --> 01:29:45.880
But he wants to do the surgery.

01:29:45.880 --> 01:29:47.760
All the others will be out there.

01:29:47.760 --> 01:29:51.520
In my case, this surgery may be made $1,000.

01:29:51.520 --> 01:29:54.800
The other surgery, they were going to get $5,000.

01:29:54.800 --> 01:29:56.640
So they didn't want to hear about this surgery.

01:29:56.640 --> 01:29:59.600
Well, here's how I would see your scenario playing out

01:29:59.600 --> 01:30:01.360
potentially.

01:30:01.360 --> 01:30:05.880
The surgeon may have some opinion and have some recommendations.

01:30:05.880 --> 01:30:10.560
The surgeon might be assisted by some AI agent looking

01:30:10.560 --> 01:30:12.520
at things from the surgeon's perspective.

01:30:12.520 --> 01:30:14.720
The patient could listen to the surgeon,

01:30:14.720 --> 01:30:16.560
maybe go to a couple of surgeons,

01:30:16.560 --> 01:30:18.840
and have their own AI advisor saying, well,

01:30:18.840 --> 01:30:20.760
here are the trade-offs.

01:30:20.760 --> 01:30:22.040
You could do this surgery.

01:30:22.040 --> 01:30:24.320
It's only $2,000.

01:30:24.320 --> 01:30:27.760
And your new leg will last you five years.

01:30:27.760 --> 01:30:29.720
Here's a surgery for $10,000.

01:30:29.720 --> 01:30:32.680
And probably it will last a good 15 to 20 years.

01:30:32.680 --> 01:30:34.960
It's up to you to make that choice.

01:30:34.960 --> 01:30:38.800
And then the surgery itself, as an embodied AI,

01:30:38.800 --> 01:30:42.600
a robotic surgeon might assist the surgeon

01:30:42.600 --> 01:30:43.880
or do the surgery all by itself.

01:30:43.880 --> 01:30:45.920
There would be decisions to make during the course of that.

01:30:45.920 --> 01:30:47.880
So there are all sorts of different levels at which

01:30:47.880 --> 01:30:52.280
embodied AI could play a role in the scenario that you don't want.

01:30:52.280 --> 01:30:54.720
If you deal with the social system,

01:30:54.720 --> 01:30:58.200
say the medical social system, to create change

01:30:58.200 --> 01:31:01.120
is going to be very difficult.

01:31:01.120 --> 01:31:02.240
I have had this experience.

01:31:02.240 --> 01:31:07.560
So A surgeon goes to B surgery, and then goes to B surgeon.

01:31:07.560 --> 01:31:10.560
B surgeon won't see me because A surgeon says,

01:31:10.560 --> 01:31:13.840
this guy is asking something that we won't do.

01:31:13.840 --> 01:31:15.320
And he goes to C surgeon.

01:31:15.320 --> 01:31:18.960
So I'm trying to say that a social system built

01:31:18.960 --> 01:31:20.600
into certain professions.

01:31:20.600 --> 01:31:21.920
It's like the old guilt.

01:31:21.920 --> 01:31:23.480
And they want to protect themselves.

01:31:23.480 --> 01:31:26.360
So somewhere in the line, you're going to interface them

01:31:26.360 --> 01:31:27.640
theoretically anyway.

01:31:27.640 --> 01:31:30.160
And these are going to be the issues.

01:31:30.160 --> 01:31:34.880
When you affect people's money, especially

01:31:34.880 --> 01:31:38.160
really ingrained groups of people, like let's say.

01:31:38.160 --> 01:31:39.160
Thank you.

01:31:39.160 --> 01:31:41.480
We have to have other questions also.

01:31:41.480 --> 01:31:42.000
Thank you.

01:31:46.160 --> 01:31:51.360
I have a lot of questions, but I would like to ask too.

01:31:51.360 --> 01:31:55.520
First, I heard a couple of years ago,

01:31:55.520 --> 01:32:03.440
I heard about cognitive problems, but not from the development

01:32:03.440 --> 01:32:09.640
developed not in IBM, but with genetic engineering.

01:32:09.640 --> 01:32:14.440
And it was from a guy who was talking about that day,

01:32:14.440 --> 01:32:20.200
for example, good old human eyes on mice.

01:32:20.200 --> 01:32:26.200
And next stage of their experiments

01:32:26.200 --> 01:32:30.760
is like making cognitive buildings or rooms.

01:32:30.760 --> 01:32:33.880
And my question is, what do you think?

01:32:33.880 --> 01:32:43.000
Should we stop to be afraid of the domination of robots

01:32:43.000 --> 01:32:47.680
and start to be afraid of the domination of our buildings

01:32:47.680 --> 01:32:52.280
and our like that day, control and demonet world

01:32:52.280 --> 01:32:54.320
and devour us and kill us?

01:32:54.320 --> 01:32:55.320
What should we do?

01:32:55.320 --> 01:32:56.320
It's just a question.

01:32:59.880 --> 01:33:02.240
I mean, I don't see there is really

01:33:02.240 --> 01:33:07.240
a sharp boundary between a robot and the building.

01:33:07.240 --> 01:33:11.200
I mean, with the internet of things,

01:33:11.200 --> 01:33:15.280
everything will be connected.

01:33:15.280 --> 01:33:23.480
Our fridge, our TV, our car, the traffic lights,

01:33:23.480 --> 01:33:26.320
they will all be communicated with each other

01:33:26.320 --> 01:33:29.600
to help us live better.

01:33:29.600 --> 01:33:30.680
But not be afraid.

01:33:30.680 --> 01:33:32.600
You shouldn't be afraid.

01:33:32.600 --> 01:33:36.400
I mean, we have to make sure that this thing is built in a way

01:33:36.400 --> 01:33:39.280
that is not harmless.

01:33:39.280 --> 01:33:42.880
So they know that they're just building and it goes by itself

01:33:42.880 --> 01:33:45.440
that there are no undesired effects.

01:33:45.440 --> 01:33:49.600
But I think that we have to work hard in making it

01:33:49.600 --> 01:33:53.560
in a way that is going to be helpful.

01:33:53.560 --> 01:33:55.880
So not fear, but calm awareness.

01:33:55.880 --> 01:33:56.880
Yes.

01:33:56.880 --> 01:33:57.880
Thank you.

01:33:57.880 --> 01:34:02.880
Maybe we can go ahead and ask the second question after.

01:34:05.880 --> 01:34:06.880
Hi.

01:34:06.880 --> 01:34:08.680
So I think I'm going to get a question,

01:34:08.680 --> 01:34:09.880
but I'll try to keep it brief.

01:34:09.880 --> 01:34:13.080
And talking about how the end goal of artificial intelligence

01:34:13.080 --> 01:34:16.000
is to create a system that has agency.

01:34:16.000 --> 01:34:17.640
But in the short term, we're talking

01:34:17.640 --> 01:34:23.320
about building systems that have things that are used as tools

01:34:23.320 --> 01:34:24.600
to an end.

01:34:24.600 --> 01:34:27.240
So in the meantime, before we reach

01:34:27.240 --> 01:34:29.560
some sort of human level consciousness,

01:34:29.560 --> 01:34:31.680
you're probably going to be working, at least at IBM,

01:34:31.680 --> 01:34:34.880
to design products that use artificial intelligence

01:34:34.880 --> 01:34:37.200
and are bounded and embodied to some degree

01:34:37.200 --> 01:34:41.480
by whatever it is that they are placed into.

01:34:41.480 --> 01:34:43.080
So I guess what I'm kind of curious about

01:34:43.080 --> 01:34:44.880
is if the end goal is a system that

01:34:44.880 --> 01:34:47.400
has general principles for understanding

01:34:47.400 --> 01:34:49.640
and applying to the world as a whole,

01:34:49.640 --> 01:34:52.320
how the different kinds of embodiment

01:34:52.320 --> 01:34:55.160
are going to affect an artificial intelligence system.

01:34:55.160 --> 01:34:57.880
Like, for example, if you put one in a car,

01:34:57.880 --> 01:35:01.120
how will that shape the way that the artificial intelligence

01:35:01.120 --> 01:35:03.400
looks versus if you have one that's

01:35:03.400 --> 01:35:08.240
like a maid in a house as you were talking about or a room?

01:35:08.240 --> 01:35:09.920
Because surely that would have some impact

01:35:09.920 --> 01:35:11.320
on the way that the system thinks.

01:35:11.320 --> 01:35:13.680
And I don't know.

01:35:13.680 --> 01:35:14.880
Yeah, cogitates.

01:35:14.880 --> 01:35:16.920
Yeah, definitely the kind of embodiment

01:35:16.920 --> 01:35:21.920
is going to be very, is going to impact on the way

01:35:21.920 --> 01:35:24.840
the system will learn over time.

01:35:24.840 --> 01:35:27.760
Because it will also be taped, how

01:35:27.760 --> 01:35:31.720
it will interact with the environment, with the humans.

01:35:31.720 --> 01:35:36.680
And also, even the kind of, to go back

01:35:36.680 --> 01:35:40.200
to this ethical concerns that you

01:35:40.200 --> 01:35:44.440
may have with the companion robot for elderly people,

01:35:44.440 --> 01:35:47.520
with a self-driving car, with a cognitive room

01:35:47.520 --> 01:35:51.680
to make a highly decision are very different.

01:35:51.680 --> 01:35:57.320
So it's not clear to me, at least,

01:35:57.320 --> 01:36:00.840
the relationship between the kind of embodiment that we choose.

01:36:00.840 --> 01:36:05.280
And how to build in the best way the software that is

01:36:05.280 --> 01:36:07.960
going to be, and the behavior of that machine.

01:36:12.040 --> 01:36:15.760
Embodied AI that we build and apply in different environments,

01:36:15.760 --> 01:36:16.880
they will be different.

01:36:16.880 --> 01:36:20.040
I think a lot of the components will be similar,

01:36:20.040 --> 01:36:22.480
but they'll be woven together.

01:36:22.480 --> 01:36:24.400
I think we'll also have a common architecture,

01:36:24.400 --> 01:36:28.600
but still the end product will be different in different cases.

01:36:28.600 --> 01:36:33.360
And I think there is a lot of engineering and design

01:36:33.360 --> 01:36:35.280
to be done that takes into account

01:36:35.280 --> 01:36:39.120
how humans use these technologies.

01:36:39.120 --> 01:36:42.240
And we do have a number of people who

01:36:42.240 --> 01:36:43.960
are concerned with this aspect.

01:36:43.960 --> 01:36:48.120
To study, we design these tools with an awareness

01:36:48.120 --> 01:36:52.480
of the way humans are and the way humans like to use things.

01:36:52.480 --> 01:36:54.000
We build the tools.

01:36:54.000 --> 01:36:55.960
We probably don't get them right.

01:36:55.960 --> 01:37:00.520
And we iterate until we find that they are indeed useful.

01:37:06.840 --> 01:37:11.000
Toward trying to further humanize AI,

01:37:11.000 --> 01:37:14.040
is making the interaction phase better.

01:37:14.040 --> 01:37:18.240
Do you know of any projects, research,

01:37:18.240 --> 01:37:23.720
and if serious kind on three things, first software,

01:37:23.720 --> 01:37:28.720
trying to write really good jazz, given a theme?

01:37:28.720 --> 01:37:29.920
Is that working on?

01:37:29.920 --> 01:37:33.960
Secondly, trying to write a sonnet, given a theme,

01:37:33.960 --> 01:37:37.960
and then on hardware, trying to juggle five balls.

01:37:37.960 --> 01:37:40.720
Is there any serious progress work on that?

01:37:40.720 --> 01:37:43.080
OK, so on the music I know of projects,

01:37:43.080 --> 01:37:48.080
for example, Sony as a research center in Paris,

01:37:48.080 --> 01:37:54.240
where they actually are building original songs

01:37:54.240 --> 01:38:01.760
in the style of some bosa nova or jazz or whatever.

01:38:01.760 --> 01:38:04.200
And they are trying to use AI techniques

01:38:04.200 --> 01:38:07.800
to make sure that the song is original enough,

01:38:07.800 --> 01:38:11.760
but is recognized to be according to a certain style.

01:38:11.760 --> 01:38:15.000
Like very recently, they released the new song

01:38:15.000 --> 01:38:17.160
in the style of the Beatles.

01:38:17.160 --> 01:38:20.480
And I think I listened to it.

01:38:20.480 --> 01:38:23.880
And yeah, I mean, if you didn't know the Beatles,

01:38:23.880 --> 01:38:27.840
that it was not the Beatles song, you could say, yeah,

01:38:27.840 --> 01:38:29.200
maybe it's not one of the best ones,

01:38:29.200 --> 01:38:34.640
but I think it was a pretty reasonable new song.

01:38:34.640 --> 01:38:37.040
And jazz as well.

01:38:37.040 --> 01:38:39.480
So you may want to look at what they do there,

01:38:39.480 --> 01:38:41.600
because it's really interesting stuff.

01:38:41.600 --> 01:38:44.000
And the AI techniques are there to make sure

01:38:44.000 --> 01:38:49.760
that the song is original while satisfying some constraints

01:38:49.760 --> 01:38:53.240
that you need to satisfy, for example, in jazz

01:38:53.240 --> 01:38:59.480
or in the different styles that you may want to use.

01:38:59.480 --> 01:39:01.720
And so in that case, I think there

01:39:01.720 --> 01:39:04.440
are these and other people that are working on that.

01:39:04.440 --> 01:39:06.920
And in general, there are people working on making

01:39:06.920 --> 01:39:10.880
AI apply to creative tasks.

01:39:10.880 --> 01:39:15.160
They're not just songs or sonnets.

01:39:15.160 --> 01:39:18.560
Like even doing portrait.

01:39:18.560 --> 01:39:21.320
I've seen AI is doing portraits.

01:39:21.320 --> 01:39:24.240
And that was very impressive.

01:39:24.240 --> 01:39:27.840
Like it was a room full of robotic arms

01:39:27.840 --> 01:39:30.640
that were looking at one human.

01:39:30.640 --> 01:39:32.240
They were all looking at one human.

01:39:32.240 --> 01:39:36.680
They were making a portrait with the pencil of that human.

01:39:36.680 --> 01:39:39.880
And all these robotic arms, they were all,

01:39:39.880 --> 01:39:42.880
each one was doing a different portrait.

01:39:42.880 --> 01:39:44.640
That was impressive.

01:39:44.640 --> 01:39:46.360
It was a very nice portrait.

01:39:46.360 --> 01:39:49.040
But each one was different from the other one.

01:39:49.040 --> 01:39:54.840
So there was some sort of creativity.

01:39:54.840 --> 01:39:56.520
So there are people working on that.

01:39:56.520 --> 01:39:59.800
But I see those technologies as components.

01:39:59.800 --> 01:40:04.440
Being able to compose good jazz or Brandon

01:40:04.440 --> 01:40:05.840
Berg and Sherrod in number seven.

01:40:05.840 --> 01:40:12.000
I think, to me, that's a component that's not necessarily

01:40:12.000 --> 01:40:14.240
an instance of cognition.

01:40:14.240 --> 01:40:19.360
But very clever work on statistical characterizations

01:40:19.360 --> 01:40:22.440
of music and then regeneration certainly

01:40:22.440 --> 01:40:26.240
can be components of an embodied AI.

01:40:26.240 --> 01:40:30.560
With regard to juggling, I don't know, particularly there.

01:40:30.560 --> 01:40:33.440
I do know that there have been examples

01:40:33.440 --> 01:40:39.720
of balancing the inverted pendulum, like that.

01:40:39.720 --> 01:40:43.760
If the robot capable of juggling five balls

01:40:43.760 --> 01:40:48.600
doesn't exist today, I don't see any real barrier

01:40:48.600 --> 01:40:50.040
to being able to accomplish that.

01:40:50.040 --> 01:40:51.640
Sure, you know Ron Graham.

01:40:51.640 --> 01:40:52.640
Yes.

01:40:52.640 --> 01:40:53.160
He did five.

01:40:53.160 --> 01:40:56.040
He thinks it's just about impossible for a robot.

01:40:56.040 --> 01:40:58.320
Who knows?

01:40:58.320 --> 01:41:00.880
I think Ron Graham was capable of doing five.

01:41:00.880 --> 01:41:01.400
Yes.

01:41:01.400 --> 01:41:04.960
He can do it, but also balance on one hand.

01:41:04.960 --> 01:41:10.120
But I have no concern about robots being

01:41:10.120 --> 01:41:14.160
able to get there in the reasonable future.

01:41:14.160 --> 01:41:15.520
Thank you.

01:41:15.520 --> 01:41:16.040
Thank you.

01:41:16.040 --> 01:41:16.520
Go ahead.

01:41:20.120 --> 01:41:23.720
So before you were talking about how the best way to develop

01:41:23.720 --> 01:41:26.040
AI might be to almost raise it as a child

01:41:26.040 --> 01:41:29.000
or to teach it so that it can make

01:41:29.000 --> 01:41:31.600
a lot of logical assumptions that it has a background.

01:41:31.600 --> 01:41:33.200
I'm curious, at that point, wouldn't you

01:41:33.200 --> 01:41:36.520
be concerned about the AI developing the same biases

01:41:36.520 --> 01:41:38.600
that you were originally trying to avoid?

01:41:42.800 --> 01:41:44.760
Presumably, you would be needing it

01:41:44.760 --> 01:41:51.920
to develop a similar set of a sum of the data

01:41:51.920 --> 01:41:56.160
you would then have to educate it that certain biases are not

01:41:56.160 --> 01:41:58.080
socially acceptable.

01:41:58.080 --> 01:42:01.760
Socialization is the accumulation of all this background

01:42:01.760 --> 01:42:04.000
knowledge that's not only factual.

01:42:04.000 --> 01:42:05.480
The floor doesn't come up and invite you,

01:42:05.480 --> 01:42:08.280
but it's also normative in the sense

01:42:08.280 --> 01:42:10.360
that you should do this under these circumstances,

01:42:10.360 --> 01:42:12.720
but not in that context.

01:42:12.720 --> 01:42:17.040
And so it would develop by what we could call what

01:42:17.040 --> 01:42:19.840
it's developing biases.

01:42:19.840 --> 01:42:25.600
But that's the necessary texture of that background knowledge

01:42:25.600 --> 01:42:29.200
base that it needs to operate intelligently in the world.

01:42:29.200 --> 01:42:35.080
Yeah, I think these biases, it's possible that a robot being

01:42:35.080 --> 01:42:41.720
taught to live in this world might develop cognitive biases.

01:42:41.720 --> 01:42:44.400
They'd be the same set of cognitive biases that

01:42:44.400 --> 01:42:46.120
have been observed in humans.

01:42:46.120 --> 01:42:54.240
No, it might be all the things that's

01:42:54.240 --> 01:42:56.280
taken from ours.

01:42:56.280 --> 01:42:59.880
I would hope that we would be able to correct those things

01:42:59.880 --> 01:43:01.560
and get it to think in a less biased way.

01:43:01.560 --> 01:43:07.200
But you're right that I think humans

01:43:07.200 --> 01:43:09.080
that we develop short cuts.

01:43:09.080 --> 01:43:11.880
There's a certain way our brains are wired.

01:43:11.880 --> 01:43:15.560
These biases, I think, are a form of heuristic

01:43:15.560 --> 01:43:18.240
that work in a lot of cases, but not in all cases,

01:43:18.240 --> 01:43:20.200
and they sometimes get us into trouble.

01:43:20.200 --> 01:43:27.360
So these embodied AIs may well develop something like this.

01:43:27.360 --> 01:43:30.600
So some biases are just involved sensitivity

01:43:30.600 --> 01:43:32.640
to background frequencies.

01:43:32.640 --> 01:43:39.360
So here's an experiment that shows this.

01:43:39.360 --> 01:43:48.840
So subjects were asked to judge the height of various people,

01:43:48.840 --> 01:43:53.480
standing next to a standard object in a college campus.

01:43:53.480 --> 01:43:57.400
And the people were men and women,

01:43:57.400 --> 01:44:00.840
where the men and women were unknown to the subjects.

01:44:00.840 --> 01:44:02.440
There were matched pairs.

01:44:02.440 --> 01:44:04.480
That is, for every six foot man, there

01:44:04.480 --> 01:44:06.320
was also a six foot woman.

01:44:06.320 --> 01:44:09.360
For every five foot one, there was a five foot man.

01:44:09.360 --> 01:44:16.720
And there was a bias towards judging the men to be taller.

01:44:16.720 --> 01:44:20.520
And that is because of sensitivity to background frequencies.

01:44:20.520 --> 01:44:23.080
Now, if you want your, of course,

01:44:23.080 --> 01:44:25.080
the trouble with sensitive to background frequencies

01:44:25.080 --> 01:44:29.400
is they can intrude in an unjust way.

01:44:29.400 --> 01:44:32.720
But if you want your AI to be sensitive to background

01:44:32.720 --> 01:44:36.400
frequencies, you will have to do something

01:44:36.400 --> 01:44:38.320
about it in the way that we try to do something

01:44:38.320 --> 01:44:40.600
about human judgment.

01:44:40.600 --> 01:44:42.800
But wouldn't you say also that the AI, in order

01:44:42.800 --> 01:44:46.640
to be properly functional, has to have those sensitivity

01:44:46.640 --> 01:44:47.520
to background frequencies?

01:44:47.520 --> 01:44:48.160
Yes.

01:44:48.160 --> 01:44:49.560
It has to generalize.

01:44:49.560 --> 01:44:50.060
Yeah.

01:44:50.060 --> 01:44:51.280
It may categories.

01:44:51.280 --> 01:44:55.280
I would hope we could build into it a much better

01:44:55.280 --> 01:44:57.320
facility for dealing with probabilities

01:44:57.320 --> 01:44:59.720
and conditional probabilities than we possess.

01:44:59.720 --> 01:45:00.220
Yeah.

01:45:11.120 --> 01:45:12.120
Great.

01:45:12.120 --> 01:45:17.440
If we say that humans are still evolving,

01:45:17.440 --> 01:45:25.480
and that human evolution is linked with machines and AI.

01:45:25.480 --> 01:45:30.400
And kind of in the same way that when photography came along

01:45:30.400 --> 01:45:32.920
and free painting to do other things,

01:45:32.920 --> 01:45:35.640
then represent the world.

01:45:35.640 --> 01:45:39.400
So it seems like AI has the capacity,

01:45:39.400 --> 01:45:44.880
either to stifle or to enhance this human evolution.

01:45:44.880 --> 01:45:48.040
And maybe some of the ways in which we could evolve

01:45:48.040 --> 01:45:53.960
are toward a greater sense of personal fulfillment,

01:45:53.960 --> 01:46:00.960
artistic creativity, lots of things.

01:46:00.960 --> 01:46:03.000
So is anyone looking at that?

01:46:03.000 --> 01:46:05.360
I mean, if you're telling me my preferences,

01:46:05.360 --> 01:46:10.120
or you're notifying me every time I'm daydreaming,

01:46:10.120 --> 01:46:11.360
that going to stop.

01:46:30.720 --> 01:46:33.920
That is going to happen.

01:46:33.920 --> 01:46:38.800
We are in a coevolutionary relationship

01:46:38.800 --> 01:46:40.200
with the technologies we develop.

01:46:40.200 --> 01:46:44.320
And it's going to happen in this case as well.

01:46:44.320 --> 01:46:49.640
I think in small ways, we can start with small ways

01:46:49.640 --> 01:46:52.000
to answer the question of is it happening now,

01:46:52.000 --> 01:46:53.880
or is it soon to happen?

01:46:53.880 --> 01:46:57.720
I think you can look at cases like assistance

01:46:57.720 --> 01:47:00.160
for the elderly that could be developed,

01:47:00.160 --> 01:47:06.600
where imagine you have an AI-enabled walker that

01:47:06.600 --> 01:47:11.960
helps grandma understand that she needs to lift her feet

01:47:11.960 --> 01:47:13.280
a little bit more.

01:47:13.280 --> 01:47:17.880
And by doing that, she might learn a better gate,

01:47:17.880 --> 01:47:19.960
such that maybe she won't need that walker anymore,

01:47:19.960 --> 01:47:21.880
at least for a while.

01:47:21.880 --> 01:47:25.120
So there are ways that AI can train us

01:47:25.120 --> 01:47:30.400
and help us in ways that ironically make us less dependent

01:47:30.400 --> 01:47:32.200
on this assistance.

01:47:32.200 --> 01:47:33.160
That's just a small thing.

01:47:33.160 --> 01:47:41.320
But I think imagine that we used to be a lot of people

01:47:41.320 --> 01:47:43.120
used to be very good at riding horses.

01:47:43.120 --> 01:47:46.120
And now I think that has atrophied,

01:47:46.120 --> 01:47:49.720
but there's probably something changing in our brain

01:47:49.720 --> 01:47:54.120
that distinguishes us from people

01:47:54.120 --> 01:47:59.960
from the 1800s, where we're better drivers than they would

01:47:59.960 --> 01:48:00.800
have been.

01:48:00.800 --> 01:48:03.120
There's something that has changed in us.

01:48:03.120 --> 01:48:06.240
And I think this is happening in small ways already

01:48:06.240 --> 01:48:08.480
and might happen in very large ways,

01:48:08.480 --> 01:48:13.880
maybe more explicitly as we start to more explicitly implant

01:48:13.880 --> 01:48:18.360
devices in ourselves that are designed

01:48:18.360 --> 01:48:25.840
to enhance recognition.

01:48:25.840 --> 01:48:30.600
OK, my question is, have there been any experiments long term?

01:48:30.600 --> 01:48:34.800
Have there been any successful or not experiments

01:48:34.800 --> 01:48:41.000
with long term training of artificial embodied beings?

01:48:41.000 --> 01:48:44.680
AIs, embodied AIs, like long term training,

01:48:44.680 --> 01:48:49.040
like broader intelligence and just focusing

01:48:49.040 --> 01:48:53.240
about a billion cases just to learn it as a very specific

01:48:53.240 --> 01:48:56.240
task.

01:48:56.240 --> 01:49:00.000
Well, there are some people that work on more general AI,

01:49:00.000 --> 01:49:03.760
rather than narrow and specific to the task.

01:49:06.360 --> 01:49:08.880
Like, maybe aware of these people

01:49:08.880 --> 01:49:12.280
at this company called DeepMind, which is part of Google.

01:49:12.280 --> 01:49:18.200
And they have this algorithm that can

01:49:18.200 --> 01:49:25.000
learn how to play many different games on the screen.

01:49:28.240 --> 01:49:32.840
Just by looking at how these games are played,

01:49:32.840 --> 01:49:35.880
but just by looking at people playing these games.

01:49:35.880 --> 01:49:39.120
And so you can learn not just one game only,

01:49:39.120 --> 01:49:43.120
but many different ones, even with different rules,

01:49:43.120 --> 01:49:48.200
different score functions, you know,

01:49:48.200 --> 01:49:51.400
accumulate the score, the game, the win, and so on.

01:49:51.400 --> 01:49:56.080
And so without communicating the rules of the games,

01:49:56.080 --> 01:50:00.960
these machine learns just by observing many different ones.

01:50:00.960 --> 01:50:05.360
And they think that that could be one, of course,

01:50:05.360 --> 01:50:07.240
a game very specific.

01:50:07.240 --> 01:50:09.520
Just games is not real life.

01:50:09.520 --> 01:50:12.680
It's not a very realistic scenario.

01:50:12.680 --> 01:50:15.520
But they claim that that could be a way

01:50:15.520 --> 01:50:21.560
to build a machine that can learn to do many different things

01:50:21.560 --> 01:50:25.000
that work according to different rules.

01:50:25.000 --> 01:50:27.360
But there are also other people that,

01:50:27.360 --> 01:50:30.120
I mean, whose goal is to work more generally?

01:50:30.120 --> 01:50:32.000
I rather than a specific one.

01:50:32.000 --> 01:50:33.800
But this is when they came to mind.

01:50:33.800 --> 01:50:38.800
I mean, that they are, you know, working hard in expanding that.

01:50:38.800 --> 01:50:41.800
Only only four minutes.

01:50:41.800 --> 01:50:44.800
I just want to be more specific because it was not exactly

01:50:44.800 --> 01:50:45.560
my question.

01:50:45.560 --> 01:50:49.800
I mean, I know people have like been having

01:50:49.800 --> 01:50:54.400
champs from babies and they train them like 15 years.

01:50:54.400 --> 01:50:57.520
And my question is more like if there has been experiments

01:50:57.520 --> 01:51:02.120
successful or not, like training AI is for 15 years

01:51:02.120 --> 01:51:04.920
from nothing to something.

01:51:04.920 --> 01:51:08.640
15 years ago, AI was very different.

01:51:08.640 --> 01:51:10.080
The world was very different.

01:51:10.080 --> 01:51:16.680
I mean, AI just started not long ago, just 50 years ago.

01:51:16.680 --> 01:51:21.440
So I think that I'm not aware of this long training,

01:51:21.440 --> 01:51:23.960
you know, experiments.

01:51:23.960 --> 01:51:25.840
I don't think there have been some attempts

01:51:25.840 --> 01:51:35.240
to learn some simple tasks with computer systems that

01:51:35.240 --> 01:51:37.400
are modeled after the brain.

01:51:37.400 --> 01:51:40.760
I remember some work that was done at IBM probably 10 years

01:51:40.760 --> 01:51:48.760
ago now that wired together a bunch of pseudo neurons

01:51:48.760 --> 01:51:51.680
into something akin to the many columns that

01:51:51.680 --> 01:51:54.840
are in one part of the brain.

01:51:54.840 --> 01:52:01.360
And I think they were trying to teach it to just like to train

01:52:01.360 --> 01:52:02.440
a visual system.

01:52:02.440 --> 01:52:03.760
Can you recognize something?

01:52:03.760 --> 01:52:06.240
Just here's a bunch of circuitry.

01:52:06.240 --> 01:52:08.520
And I'm going to put a bunch of images in front of you

01:52:08.520 --> 01:52:09.600
and can you learn it?

01:52:09.600 --> 01:52:12.640
I don't know how far it got.

01:52:12.640 --> 01:52:18.760
But I know that there have been attempts in other research

01:52:18.760 --> 01:52:20.200
institutions as well to do this.

01:52:20.200 --> 01:52:24.960
But I don't know what was the outcome of these.

01:52:24.960 --> 01:52:25.960
Thank you.

01:52:25.960 --> 01:52:32.240
So one point I haven't heard raised

01:52:32.240 --> 01:52:37.840
has to with the tendency of goals to be redefined

01:52:37.840 --> 01:52:40.120
as a function of the available solutions

01:52:40.120 --> 01:52:42.040
rather than the other way around.

01:52:42.040 --> 01:52:45.800
So for example, Facebook has redefined the notion of friendship.

01:52:45.800 --> 01:52:48.600
And I'm not convinced that it's an improvement

01:52:48.600 --> 01:52:51.240
on the earlier version.

01:52:51.240 --> 01:52:54.480
And for that reason, it's important to me,

01:52:54.480 --> 01:52:59.000
it seems that the question of democracy be addressed.

01:52:59.000 --> 01:53:03.200
And what I've been hearing about the likelihood of half

01:53:03.200 --> 01:53:05.440
of all employment being eliminated.

01:53:05.440 --> 01:53:09.440
And the question is, are the people who

01:53:09.440 --> 01:53:13.400
are likely to be affected by that actually being consulted?

01:53:13.400 --> 01:53:17.560
And is this decision to be made purely

01:53:17.560 --> 01:53:21.160
on the basis of the availability of enormous resources

01:53:21.160 --> 01:53:25.200
on the part of the corporations, Silicon Valley, these five

01:53:25.200 --> 01:53:29.240
corporations that were mentioned and the ones that were not?

01:53:29.240 --> 01:53:32.840
I'm my colleagues at Stanford tell me

01:53:32.840 --> 01:53:35.280
they overhear conversations in the coffee shops.

01:53:35.280 --> 01:53:38.640
And people are very, very seriously

01:53:38.640 --> 01:53:42.080
discussing what they're going to be doing with all these unemployed

01:53:42.080 --> 01:53:42.600
people.

01:53:42.600 --> 01:53:45.160
And they feel concerned about it.

01:53:45.160 --> 01:53:51.360
What I'm more concerned about, I'm more concerned about the absence

01:53:51.360 --> 01:53:56.320
of restraint on the part of the people who exercise this power

01:53:56.320 --> 01:54:00.160
and who have these resources rather than the likelihood

01:54:00.160 --> 01:54:01.840
that machines will get out of control.

01:54:04.240 --> 01:54:06.760
Yeah, I think there is a lot of discussion also

01:54:06.760 --> 01:54:11.400
in terms of regulations of AI, possible regulation.

01:54:11.400 --> 01:54:16.080
And I think that, for example, a few days ago, the White House

01:54:16.080 --> 01:54:20.600
released a very interesting document on the future of AI

01:54:20.600 --> 01:54:33.120
and strategic directions on how to, with the goal of facilitating

01:54:33.120 --> 01:54:37.640
the good development of AI and possibly

01:54:37.640 --> 01:54:42.040
mitigating the undesired consequences.

01:54:42.040 --> 01:54:47.120
And I think that, so even last week, I

01:54:47.120 --> 01:54:49.400
was speaking at the European Parliament.

01:54:49.400 --> 01:54:54.640
And people there are very concerned about specific issues

01:54:54.640 --> 01:54:58.120
about AI, like they apply the C ownership, they said that.

01:54:58.120 --> 01:55:03.200
But they also are concerned about to understand

01:55:03.200 --> 01:55:07.120
how to regulate this very powerful technology

01:55:07.120 --> 01:55:12.280
in a way that it does not stop research and beneficial

01:55:12.280 --> 01:55:17.760
advancement, but also it addresses these other concerns,

01:55:17.760 --> 01:55:21.720
like, for example, the impact on the workforce.

01:55:21.720 --> 01:55:24.520
Well, the European Parliament is not

01:55:24.520 --> 01:55:29.160
a notably democratic body in the European Union as a whole.

01:55:29.160 --> 01:55:35.200
And the question is, where is the public that

01:55:35.200 --> 01:55:39.120
is going to be most profoundly affected

01:55:39.120 --> 01:55:41.480
by taking part in the decisions?

01:55:41.480 --> 01:55:43.080
I think there are some economists

01:55:43.080 --> 01:55:48.400
who are thinking about the impact of AI

01:55:48.400 --> 01:55:53.360
on the economy and displacement and the like.

01:55:53.360 --> 01:55:58.600
I think Eric Brynjolfsson is one such person at MIT.

01:55:58.600 --> 01:56:00.720
And there are others.

01:56:00.720 --> 01:56:04.960
And I think it is important to explore these questions.

01:56:04.960 --> 01:56:07.040
I don't know if we have good answers right now,

01:56:07.040 --> 01:56:08.200
but it's one of those things that we

01:56:08.200 --> 01:56:11.280
have to be conscious of and think about

01:56:11.280 --> 01:56:14.480
and integrate into our thinking.

01:56:14.480 --> 01:56:19.760
One historical example is in the 1930s, Gandhi's campaign

01:56:19.760 --> 01:56:36.880
to resist the industrial encroachment.

