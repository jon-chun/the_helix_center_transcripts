WEBVTT

00:00.000 --> 00:07.680
I'm Ed Nurekseion, the Director of the Center for Today's Roundtable on Transhumanism was

00:07.680 --> 00:16.000
proposed by Bill Grasse, whom I will introduce and then he will introduce the rest of the

00:16.000 --> 00:17.600
participants.

00:17.600 --> 00:19.880
Just two words about future programs.

00:19.880 --> 00:28.320
We have a program on boredom on April 22, I believe, and you'll get the notice from

00:28.320 --> 00:37.160
our mailing list and then on May 12, we have a program on the completeness of physics.

00:37.160 --> 00:38.160
Thank you.

00:38.160 --> 00:41.840
We're interested in this topic of transhumanism.

00:41.840 --> 00:48.640
For me, it started in 2000 when I was involved in a conference at the University of Pennsylvania

00:48.640 --> 00:55.480
on life extension technology and then I started meeting all these people.

00:55.480 --> 01:01.560
The science itself is really interesting, but the proponents of it are also very interesting.

01:01.560 --> 01:07.880
The whole question of who we are as humans is something that engages me philosophically

01:07.880 --> 01:10.640
and otherwise.

01:10.640 --> 01:17.480
This question of how we're changing raises all kinds of really important questions.

01:17.480 --> 01:19.120
That's how I got into it.

01:19.120 --> 01:25.440
My formal training is in comparative religion, religious studies, and of course death is

01:25.440 --> 01:27.640
a big issue in that as well.

01:27.640 --> 01:33.280
These days I call myself more a big historian.

01:33.280 --> 01:38.400
So I'm really interested in the big story that science is telling and how we understand

01:38.400 --> 01:40.720
and interpret it today.

01:40.720 --> 01:42.960
How about you, Lee Silver?

01:42.960 --> 01:43.960
Hi.

01:43.960 --> 01:49.400
I'm Lee Silver, professor Princeton.

01:49.400 --> 01:53.680
Training is in biophysics, molecular biology.

01:53.680 --> 02:02.440
In the 1990s I wrote a book called Remaking Eden which talks about the technological advances

02:02.440 --> 02:08.000
that allow genetic engineering and the potential that genetic engineering could be used on

02:08.000 --> 02:10.600
humans in the future.

02:10.600 --> 02:16.040
The biggest negative response I got was from other scientists who said what you're talking

02:16.040 --> 02:19.040
about is never going to happen.

02:19.040 --> 02:27.560
20 years later, in fact, the technology is going faster than even I had predicted.

02:27.560 --> 02:29.000
So it's going to happen.

02:29.000 --> 02:31.000
We'll talk more.

02:31.000 --> 02:32.200
So I'm Father Sandelson.

02:32.200 --> 02:35.160
I'm from Arizona State University.

02:35.160 --> 02:37.160
I work on three areas.

02:37.160 --> 02:42.040
One is the relationship between philosophy and mysticism within the Jewish tradition.

02:42.040 --> 02:45.200
Another one is religion and ecology.

02:45.200 --> 02:48.400
And the third is religion science and technology.

02:48.400 --> 02:53.280
So my interest in transhumanism is through that angle.

02:53.280 --> 03:00.040
I actually got to know Billy Grassy through my work with the Templeton Foundation.

03:00.040 --> 03:01.520
So I've been in that conversation.

03:01.520 --> 03:07.760
I would say since 2003 but seriously since 2006.

03:07.760 --> 03:10.520
And I've been writing as a critic of transhumanism.

03:10.520 --> 03:14.640
So you're going to hear from me some of the criticism to the views that you're going to

03:14.640 --> 03:15.640
espouse.

03:15.640 --> 03:16.640
Hi everybody.

03:16.640 --> 03:19.640
I'm Francesca Rossi.

03:19.640 --> 03:26.080
I work at the IBM Research Center.

03:26.080 --> 03:31.800
I also have a professor of computer science at the University of Padawan, Italy.

03:31.800 --> 03:36.680
And I've been working in AI, artificial intelligence for many years.

03:36.680 --> 03:40.280
I think that my first conference was 30 years ago.

03:40.280 --> 03:43.760
So I've seen a lot of ups and downs in this technology.

03:43.760 --> 03:51.800
Now is a very high point for various reasons.

03:51.800 --> 03:58.120
And this also led me to being interested into the impact of this technology.

03:58.120 --> 04:04.560
Not on society or individuals or making us think of what we really want this technology

04:04.560 --> 04:09.880
to achieve by itself in autonomy or also together with us.

04:09.880 --> 04:16.800
So I think that that's what led me to being interested in the topic of this panel.

04:16.800 --> 04:23.880
Because I think that I'm optimistic that by understanding better how we want to shape

04:23.880 --> 04:30.480
the future of AI will also make us understand better what we have ourselves and how we want

04:30.480 --> 04:32.560
to relate to that technology.

04:32.560 --> 04:39.360
But having said that, I'm here I think much less expert than the other ones on the topic

04:39.360 --> 04:40.360
of the panel.

04:40.360 --> 04:46.080
So I'm here mostly to learn like everybody else.

04:46.080 --> 04:47.600
My name is Stephen Post.

04:47.600 --> 04:54.200
I direct the Center for Medical Humanities, Compassionate Care and Bioethics at Stony Brook

04:54.200 --> 04:56.640
University, Seattle Medicine.

04:56.640 --> 05:03.440
I've been there 10 years having migrated across Route 80 from Cleveland, Ohio in case

05:03.440 --> 05:05.560
med.

05:05.560 --> 05:13.040
I've been interested in pretty much all the great themes related to transhumanism.

05:13.040 --> 05:20.120
In the 1990s I was publishing a lot with Peter Whitehouse who had the patent on the colon

05:20.120 --> 05:23.640
S. race inhibitors in the treatment of Alzheimer's disease.

05:23.640 --> 05:30.000
And we were writing about cognitive enhancement using medications, not therapeutically, but

05:30.000 --> 05:35.600
actually to elevate certain kinds of fundamental capacities.

05:35.600 --> 05:41.920
I also went to that conference that Lee and Billy were at in 2000 at the University of

05:41.920 --> 05:47.880
Pennsylvania and we published a book called The Fountain of Youth which had every kind

05:47.880 --> 05:54.880
of scientific and philosophical and world religious and psychological perspective on

05:54.880 --> 06:00.360
the benefits and difficulties of anti-aging technologies.

06:00.360 --> 06:03.520
And that's certainly just rocketed now.

06:03.520 --> 06:08.040
I think way beyond whatever we thought might be possible.

06:08.040 --> 06:11.920
I've written a book, Why Good Things Happen to Good People, which is essentially a treat

06:11.920 --> 06:14.200
as on happiness.

06:14.200 --> 06:23.080
And thinking about whether certain forms of happiness can be technologically sustained,

06:23.080 --> 06:28.720
hedonic flow happiness, eudeministic happiness and so forth.

06:28.720 --> 06:35.240
Very interested in genetics and the perfect child over the many years in bioethics of

06:35.240 --> 06:37.240
course.

06:37.240 --> 06:45.440
So all of these aspects of transhumanism come together and they challenge us to think about

06:45.440 --> 06:53.440
how we want to recreate human nature going forward.

06:53.440 --> 06:56.120
So what do you think?

06:56.120 --> 06:57.120
Can we live forever?

06:57.120 --> 06:58.120
I don't know.

06:58.120 --> 06:59.120
It seems preposterous.

06:59.120 --> 07:01.120
But some of the...

07:01.120 --> 07:05.280
Lee, you're the biologist here.

07:05.280 --> 07:12.000
What are the technologies that are pushing us to the possibility of radical life extension?

07:12.000 --> 07:15.640
So when we say we, I assume you're talking about...

07:15.640 --> 07:18.560
The human species.

07:18.560 --> 07:19.680
And into the future.

07:19.680 --> 07:28.640
So I wrote a book that's 20 years ago talking about how technologies could be used to do

07:28.640 --> 07:31.360
everything basically that we could.

07:31.360 --> 07:35.720
Another DNA, it's not called genome editing because that sounds better than genetic engineering.

07:35.720 --> 07:44.400
And we could put in characteristics that would allow humans to avoid disease, live longer,

07:44.400 --> 07:46.600
be healthier.

07:46.600 --> 07:53.240
And what I've seen over the last 20 years in the field is that these technologies are

07:53.240 --> 07:57.640
happening faster than even anybody expected.

07:57.640 --> 07:59.440
It's going to happen.

07:59.440 --> 08:04.440
So I would say, I don't know, I mean, I don't believe in infinity, but will people be able

08:04.440 --> 08:07.760
to... will you be able to have children who live longer?

08:07.760 --> 08:08.760
Absolutely.

08:08.760 --> 08:12.880
I mean, will we already know how you could do that?

08:12.880 --> 08:16.200
Can you do it to yourself?

08:16.200 --> 08:18.640
That's not quite as easy.

08:18.640 --> 08:20.960
I don't have a response on that.

08:20.960 --> 08:22.680
So we're all familiar.

08:22.680 --> 08:28.480
Ray Kurzweil's book, The Singularity, it's what now, five, six, seven years old.

08:28.480 --> 08:34.560
He argues that there's all these exponential trends and we're moving towards this place

08:34.560 --> 08:44.320
where by 2045, we will pass a threshold in which we're going to be able to replenish

08:44.320 --> 08:51.280
the telomeres and our chromosomes, replace our organs with genetically cultivated organs

08:51.280 --> 08:55.840
out of pigs or other animals.

08:55.840 --> 09:06.200
And you know, it's, I guess, the natural human, maximum human lifespan is 125, somewhere

09:06.200 --> 09:07.200
around there.

09:07.200 --> 09:08.200
It's a current genome.

09:08.200 --> 09:11.960
Pardon, for the current genome, right.

09:11.960 --> 09:17.000
There are some species that live really long time.

09:17.000 --> 09:18.000
Turtles?

09:18.000 --> 09:19.000
Turtles.

09:19.000 --> 09:27.040
Anyway, that's only one aspect of the Transhumanist project.

09:27.040 --> 09:30.720
There's also the enhancement issues that are going on.

09:30.720 --> 09:32.520
You're jumping a little bit too much here.

09:32.520 --> 09:33.520
Okay, go ahead.

09:33.520 --> 09:37.160
You started with living, radical life extension is just one aspect.

09:37.160 --> 09:42.280
I would like to suggest that there's a difference between living longer and living forever.

09:42.280 --> 09:47.560
But over the gray and other proponents of radical life extension want is basically the postponement

09:47.560 --> 09:48.560
of death.

09:48.560 --> 09:51.000
There's a perpetual postponement of death.

09:51.000 --> 09:56.120
So the first task on the burden of proof is on you in this case to show us that this

09:56.120 --> 10:01.000
is a, doable, feasible, and b, is desirable.

10:01.000 --> 10:04.760
I would argue that it's definitely not desirable.

10:04.760 --> 10:05.760
I'm not a scientist.

10:05.760 --> 10:09.760
I cannot say if it's doable, but that's where the science should come in.

10:09.760 --> 10:14.400
So we can talk about radical life extension, but it's really one element in transhumanism

10:14.400 --> 10:17.640
that requires a lot more conversation and unpacking.

10:17.640 --> 10:19.560
Why is it not desirable?

10:19.560 --> 10:24.280
Okay, so, and what's the, you're playing into my field of that?

10:24.280 --> 10:25.280
Yeah, what's the option?

10:25.280 --> 10:31.400
So the option is, options, object, not desirable to who.

10:31.400 --> 10:32.560
Okay, fair enough.

10:32.560 --> 10:35.840
So let's talk about radical life extension.

10:35.840 --> 10:38.600
Why is it a good idea?

10:38.600 --> 10:40.800
And why is it not a good idea?

10:40.800 --> 10:45.240
So the argument is a title of the book suggests we all want to be young.

10:45.240 --> 10:50.680
I would first begin with a question, do we really, do I really want to be 20 years old?

10:50.680 --> 10:51.680
I definitely don't.

10:51.680 --> 10:55.560
Do I want to be a healthy person in my current stage?

10:55.560 --> 10:56.880
I mean my late 60s.

10:56.880 --> 10:58.440
Yes, I do.

10:58.440 --> 11:02.960
But between that and radical life extension, there's a big difference.

11:02.960 --> 11:08.160
So let's figure out what over the gray and other people who push for that agenda really

11:08.160 --> 11:09.160
want.

11:09.160 --> 11:11.920
They want, or reason.

11:11.920 --> 11:13.800
Everybody knows the name over the gray.

11:13.800 --> 11:15.680
No, you should know his work.

11:15.680 --> 11:19.440
He's a major proponent of radical, operate the gray.

11:19.440 --> 11:20.440
Radical life extension.

11:20.440 --> 11:21.760
He's a major prophet of that.

11:21.760 --> 11:22.760
What does he want?

11:22.760 --> 11:30.920
He wants us to be really living forever through perpetual remaking of the human body.

11:30.920 --> 11:33.240
The main metaphor is the car.

11:33.240 --> 11:35.880
The, the, yeah, well-built car.

11:35.880 --> 11:39.200
Are we really, is that a very good metaphor for a human?

11:39.200 --> 11:40.200
Not sure.

11:40.200 --> 11:41.520
I would argue that it's not.

11:41.520 --> 11:46.200
So, but you didn't say, I think, or I didn't hear why it's not there's arable.

11:46.200 --> 11:52.920
I mean, nobody wants to go back to being unexperienced, you know, like when we were 18 or so.

11:52.920 --> 11:58.240
But moving forward with more experience, more.

11:58.240 --> 11:59.680
And so does that.

11:59.680 --> 12:05.800
So can we avoid the degrade of our body so that not, not, I mean, so that, you know, we

12:05.800 --> 12:06.800
can.

12:06.800 --> 12:11.800
But with aging with some, some kind of you lose some, some capacity, yes, indeed.

12:11.800 --> 12:13.200
No, but they're not.

12:13.200 --> 12:14.200
What's wrong with that?

12:14.200 --> 12:16.120
What's wrong with being able to do that?

12:16.120 --> 12:17.120
Even though I don't know.

12:17.120 --> 12:18.760
There's nothing going on with my life.

12:18.760 --> 12:19.760
We will be healthier.

12:19.760 --> 12:21.800
The healthier we are, the better it is.

12:21.800 --> 12:23.520
But that's not what over is all about.

12:23.520 --> 12:27.440
And all the radical life extension project, as well as cryonics, don't forget that.

12:27.440 --> 12:29.080
That's also part of the story.

12:29.080 --> 12:31.800
What this is about is really immortality now.

12:31.800 --> 12:38.640
I would question the logic and the wisdom of this idea of immortality as they understand

12:38.640 --> 12:44.560
it in a very materialistic and especially machine based kind of approach to immortality.

12:44.560 --> 12:45.560
That's my art.

12:45.560 --> 12:47.560
But what would be the problem of it?

12:47.560 --> 12:50.560
So let's talk about the, okay.

12:50.560 --> 12:51.560
What would be the problem?

12:51.560 --> 12:56.080
Number one, what would people do if they live to be 500 years old?

12:56.080 --> 12:57.520
Learn, continuously learn.

12:57.520 --> 12:58.520
Learn the development.

12:58.520 --> 12:59.520
New science.

12:59.520 --> 13:02.080
I would make most people's worries.

13:02.080 --> 13:04.040
And what exactly would they do?

13:04.040 --> 13:10.000
And would the world as we know this planet, a very vulnerable planet can support all those

13:10.000 --> 13:13.360
billions of people who are going to live here forever?

13:13.360 --> 13:15.160
We can be doing planetary.

13:15.160 --> 13:17.880
So then we're going to space the colonization.

13:17.880 --> 13:22.160
So space colonization is the extension of radical life extension.

13:22.160 --> 13:24.160
That's exactly what's going on.

13:24.160 --> 13:26.160
So what's wrong with this debate?

13:26.160 --> 13:32.800
Is ancient, of course, and when Francis Bacon wrote, the New Atlantis, there were the waters

13:32.800 --> 13:35.560
of paradise, basically a fountain of youth.

13:35.560 --> 13:41.920
A hundred years later, when Jonathan Swift wrote Gulliver's Travels, Gulliver comes among

13:41.920 --> 13:49.520
a people called the Leagnagians and unusually, occasionally Leagnagians are born who are

13:49.520 --> 13:51.240
called Strollbrugs.

13:51.240 --> 13:53.280
That is to say, Immortals.

13:53.280 --> 13:56.560
And at first, Gulliver is excited about this.

13:56.560 --> 14:05.480
But then the chief of the Leagnagians tells him that they become forgetful, deeply forgetful,

14:05.480 --> 14:10.200
in their eighth decade of life and can no longer remember the words that were said immediately

14:10.200 --> 14:11.800
prior to the moment.

14:11.800 --> 14:16.440
And eventually, everyone hates them because they're incapable of communication.

14:16.440 --> 14:20.120
So he says to Gulliver, take a few of these Immortals back to your own people and tell

14:20.120 --> 14:21.920
them not to fear death.

14:21.920 --> 14:25.440
So there's always been the utopian and the dystopian aspect to this.

14:25.440 --> 14:30.040
And you bring it right up to the 20th century and you have JBS Haldane and then the reaction

14:30.040 --> 14:37.640
of Tolkien whose elves were immortal unless they were killed by a physical blow or in

14:37.640 --> 14:39.160
war.

14:39.160 --> 14:44.400
But there was a kind of malaise, a kind of listlessness about them, a kind of lack of

14:44.400 --> 14:46.520
intensity and purpose.

14:46.520 --> 14:51.280
Again, mortality being the mother of creativity, if you will.

14:51.280 --> 14:53.280
And so this debate-

14:53.280 --> 14:56.040
The mortality is good according to that, the way just back it's-

14:56.040 --> 15:02.680
Well, so that's well for Tolkien, for C.S. Lewis, for many others, mortality is a virtue.

15:02.680 --> 15:09.680
And Erwin, who is the great figure, gives up her immortality to marry Aragorn because

15:09.680 --> 15:12.520
she has a vision of a son.

15:12.520 --> 15:20.360
And she realizes that she can live a more fulfilled, eudaministic life by embracing

15:20.360 --> 15:24.640
a son as a mother than being an eternal elf.

15:24.640 --> 15:28.360
So it seems to me that this notion of mortality is good.

15:28.360 --> 15:33.920
When we were at this conference in 2000, Leon Katz, and there was another philosopher,

15:33.920 --> 15:37.560
the pro-death camp, is what I call them.

15:37.560 --> 15:40.080
And they were arguing against- No, it's a pro-life.

15:40.080 --> 15:41.240
I would say it's a pro-life.

15:41.240 --> 15:42.240
It's not pro-death.

15:42.240 --> 15:44.440
It's life and death at two sides of the same coin.

15:44.440 --> 15:47.800
So I mean, I think there's a religious component to this.

15:47.800 --> 15:49.880
It was, you know, until the 20th century.

15:49.880 --> 15:55.160
There was nothing we could do to expand, you know, make life longer.

15:55.160 --> 15:58.000
And it goes along the same lines with disease.

15:58.000 --> 16:02.040
There was nothing we could do to prevent most diseases.

16:02.040 --> 16:06.040
And so the religious response was, well, it's good to suffer.

16:06.040 --> 16:08.880
It sort of, you know, improves your soul in some way.

16:08.880 --> 16:12.640
And I think that's, I mean, I don't believe that nonsense.

16:12.640 --> 16:22.040
But I also think there's a difference between immortality versus life expansion, you know,

16:22.040 --> 16:24.400
in units, you know?

16:24.400 --> 16:25.400
Okay.

16:25.400 --> 16:28.120
So the human life, some people lived to 120.

16:28.120 --> 16:30.240
I think that's probably the maximum right now.

16:30.240 --> 16:34.600
And it goes up to 150, 200.

16:34.600 --> 16:36.720
I don't see the problem.

16:36.720 --> 16:38.640
I mean, the population is not a problem.

16:38.640 --> 16:44.720
The Japan, which has the longest lifespan is going down in population.

16:44.720 --> 16:46.480
I mean, they're very, very worried.

16:46.480 --> 16:50.360
They're not community people left because the women aren't having babies.

16:50.360 --> 16:58.880
But of their 120 million people, 14% of them have probable Alzheimer's disease because

16:58.880 --> 17:00.360
age is the main predictor.

17:00.360 --> 17:06.160
So the whole thing of life extension to me, I don't have an issue with it.

17:06.160 --> 17:11.600
And life extension, I mean, life span is the longest at any single representative of a

17:11.600 --> 17:13.520
species is known to have lived.

17:13.520 --> 17:20.080
Life expectancy is the average life, length of life in a given population in time.

17:20.080 --> 17:25.600
Japanese have an 88 year life expectancy.

17:25.600 --> 17:30.040
Huge problems with now, if you were to get to a point where we had people living longer,

17:30.040 --> 17:34.000
and I actually don't object to it terribly because I think aging is a disease, at least

17:34.000 --> 17:35.160
it's a syndrome.

17:35.160 --> 17:36.160
Okay?

17:36.160 --> 17:43.040
Our cluster of symptoms, it doesn't feel good to me.

17:43.040 --> 17:46.720
You have to include in that the compression of morbidity.

17:46.720 --> 17:52.280
So if you didn't have the compression of morbidity, for example, the eradication of

17:52.280 --> 17:59.160
these chronic diseases of old age, then you would have Swift's worst case scenario,

17:59.160 --> 18:04.160
which is extended life, but incredible dysfunction.

18:04.160 --> 18:07.160
Yeah, I don't think anybody wants that.

18:07.160 --> 18:08.320
For the purpose of what?

18:08.320 --> 18:10.880
That's the main issue that we need to put on the table.

18:10.880 --> 18:12.840
What's the purpose of being a human being?

18:12.840 --> 18:13.840
What do you want to accomplish?

18:13.840 --> 18:20.680
So tell us more, transhumanism gives us a purpose, a telos, the telos is posthumanism.

18:20.680 --> 18:24.240
So how do they envision the purpose of the whole operation?

18:24.240 --> 18:27.800
Ultimately, it's really the mechanization of the human.

18:27.800 --> 18:33.680
It's the total immersion and infusion and it's not an interface between the human and

18:33.680 --> 18:34.680
the machine.

18:34.680 --> 18:36.560
So if we ask, what is it all about?

18:36.560 --> 18:38.840
Why are we to live longer?

18:38.840 --> 18:42.280
It's ultimately in order to become a super intelligent machine.

18:42.280 --> 18:46.120
So that's the difference between transhumanism and posthumanism.

18:46.120 --> 18:52.280
Transhumanism is the process of enhancement of all the things that we are doing right

18:52.280 --> 18:53.280
now already.

18:53.280 --> 18:58.280
We are in a sense in a transhumanist position already or status already, but that's not

18:58.280 --> 18:59.440
where we're going to end.

18:59.440 --> 19:00.720
What it's all about?

19:00.720 --> 19:02.120
It's posthumanism.

19:02.120 --> 19:04.480
The total fusion between humans and computers.

19:04.480 --> 19:06.480
Are you for that?

19:06.480 --> 19:07.480
Are you for that?

19:07.480 --> 19:09.000
I'm definitely not for that.

19:09.000 --> 19:20.440
So I mean, we are already with technology and science and we are already, our capabilities

19:20.440 --> 19:25.080
are already extended by many technological things.

19:25.080 --> 19:28.520
And we also have the responsibility to critique what's happening.

19:28.520 --> 19:34.400
There's no doubt that transhumanism has impacted each and every aspect of human life today.

19:34.400 --> 19:35.400
No doubt about that.

19:35.400 --> 19:40.240
That's why I think it needs to be taken very seriously and not dismissed just a full-hardy

19:40.240 --> 19:41.240
thing.

19:41.240 --> 19:43.280
Some people have done in the beginning of the conversation.

19:43.280 --> 19:46.840
When I started working on these 15 years ago, people said, why are you spending time

19:46.840 --> 19:47.840
of that?

19:47.840 --> 19:48.840
It's all nonsense.

19:48.840 --> 19:49.840
I said, no, it's not nonsense.

19:49.840 --> 19:50.840
This is where we're going.

19:50.840 --> 19:51.840
We've got to critique it.

19:51.840 --> 19:53.320
We've got to look at it carefully.

19:53.320 --> 19:55.320
I think we're already there there.

19:55.320 --> 19:56.320
I think consulting is actually...

19:56.320 --> 19:59.640
It's actually better to...

19:59.640 --> 20:04.880
I'm back in the late 80s, a really unique thinker.

20:04.880 --> 20:09.520
We're Donna Haraway wrote a famous essay called Cyborg Manifesto.

20:09.520 --> 20:12.160
And I thought, wow, that's kind of cool.

20:12.160 --> 20:13.160
And it's true.

20:13.160 --> 20:18.640
And in some sense, we need to think about our technology as part of who we are and it changes

20:18.640 --> 20:20.400
who we are.

20:20.400 --> 20:23.280
And it's not just the glasses or the computers.

20:23.280 --> 20:26.280
Cars are part of nature.

20:26.280 --> 20:33.080
It's just a very complicated sex life that uses us as symbionts in their replication.

20:33.080 --> 20:34.080
And so...

20:34.080 --> 20:39.640
I'm saying this from an environmental perspective or what?

20:39.640 --> 20:45.920
I'm saying it in the sense that rather than think of our identity ending with our epidermis,

20:45.920 --> 20:51.600
sometimes it's inside our epidermis too, when we have transplants and so on and so forth.

20:51.600 --> 20:56.480
But rather than think of it that way, I think we should think of all these prosthetic devices

20:56.480 --> 20:59.800
as things that change who we are.

20:59.800 --> 21:04.680
And so whether you call us cyborgs or whether you call us transhumanists or when you have

21:04.680 --> 21:11.200
a different understanding of posthumanists, I think we're already radically changed species.

21:11.200 --> 21:19.040
And we don't experience this so much partly as individuals, but it's primarily a collective

21:19.040 --> 21:24.840
expression of who we are as a species and how we're evolving.

21:24.840 --> 21:28.520
And if you say, all right, so we're already post-homing, we're already transhumanists.

21:28.520 --> 21:33.840
That sort of takes away some of the utopic and dystopic dimensions of it.

21:33.840 --> 21:36.400
It's just a big muddle.

21:36.400 --> 21:41.800
And it's good things and bad things are both happening at the same time.

21:41.800 --> 21:44.160
I think it's great if we can improve health.

21:44.160 --> 21:47.200
I think it's great if we can improve longevity.

21:47.200 --> 21:50.360
I think it's very unlikely we're going to cure death.

21:50.360 --> 21:51.640
And I'll come back to that.

21:51.640 --> 21:59.360
I think it's unlikely that we're going to have the runaway artificial intelligence, which

21:59.360 --> 22:01.880
is going to take over the human species.

22:01.880 --> 22:08.640
But it is very likely and it's already happening that AI will augment all our capabilities.

22:08.640 --> 22:10.760
That's more likely than immortality.

22:10.760 --> 22:14.440
But the question is whether it's you welcoming it.

22:14.440 --> 22:18.480
You're not going to try to create friendly AI.

22:18.480 --> 22:28.200
Just before we turn it over to you, I just want to point out that there are people, Elon Musk,

22:28.200 --> 22:34.960
I mean Elon Musk, for instance, who thinks that the greatest danger that humanity faces

22:34.960 --> 22:42.400
is runaway artificial intelligence that's going to take over the planet and find us dispensable.

22:42.400 --> 22:45.800
And so I just want to point it out.

22:45.800 --> 22:50.800
And then do you think that's a realistic scenario or something to worry about?

22:50.800 --> 22:53.600
Some people worry about yes.

22:53.600 --> 22:56.120
Whether I think it's a realistic scenario.

22:56.120 --> 23:05.280
Usually it's a question that I don't work on a lot because I think that whether that's

23:05.280 --> 23:11.520
going to be realistic or not, I can be better prepared for whatever and whenever that moment

23:11.520 --> 23:12.520
will be.

23:12.520 --> 23:21.400
If I think about more current issues in augmenting our humanity with AI right now, there are

23:21.400 --> 23:23.960
issues to be considered right now.

23:23.960 --> 23:30.440
And there are very powerful algorithms and techniques even right now, even if we don't

23:30.440 --> 23:35.240
have general artificial intelligence, a very narrow one for the specific task.

23:35.240 --> 23:38.520
But still it is super intelligent on that task.

23:38.520 --> 23:45.480
So there are issues to be considered right now to work on, to find solutions, to guide

23:45.480 --> 23:53.960
these human intelligent augmentation via technology in a way that we can welcome this.

23:53.960 --> 23:58.360
Yes, I would like to welcome that.

23:58.360 --> 24:02.800
But not that you're asking if I welcome this fusion.

24:02.800 --> 24:07.840
If it's a fusion without even thinking about the implications and without thinking about

24:07.840 --> 24:11.120
the issues without working on that, maybe not.

24:11.120 --> 24:17.480
But if it's a fusion that is done day by day, improving and thinking about the issues and

24:17.480 --> 24:23.080
finding collective solutions and guiding it, then yes, I do welcome it.

24:23.080 --> 24:26.400
And you didn't tell me why I should not welcome it.

24:26.400 --> 24:27.400
Okay.

24:27.400 --> 24:32.880
So what I find problematic about the whole AI operation, we are obviously not talking

24:32.880 --> 24:38.840
the same language because you're talking about something very specific in the practice of

24:38.840 --> 24:40.240
artificial intelligence.

24:40.240 --> 24:44.600
I'm talking about transhumanism as basically as the way it should be understood, which

24:44.600 --> 24:46.000
is a social imaginary.

24:46.000 --> 24:47.000
It's a story.

24:47.000 --> 24:53.440
It's a story about humanity, about the destiny of humanity and what we need to do.

24:53.440 --> 24:55.520
And I critique it on that level.

24:55.520 --> 25:00.400
I critique it from a point of view of a political thinker and intellectually story and so forth.

25:00.400 --> 25:01.400
Okay.

25:01.400 --> 25:03.760
So we are not really talking the same language.

25:03.760 --> 25:04.760
Related.

25:04.760 --> 25:05.760
Yeah, definitely related.

25:05.760 --> 25:11.760
So my problem is, yes, we are going to live, we are living with machine next to machines,

25:11.760 --> 25:13.160
but I don't want to become a machine.

25:13.160 --> 25:14.480
I want them to understand.

25:14.480 --> 25:18.080
I want people like Nick Bostrom, the idea logs of transhumanism.

25:18.080 --> 25:24.280
I want them to understand why becoming a super intelligent machine is a problematic idea.

25:24.280 --> 25:26.960
I'll give you these three arguments.

25:26.960 --> 25:30.440
Number one, I'm first and foremost a body.

25:30.440 --> 25:35.240
I'm not just a pattern that is going to be instantiated in Silicon.

25:35.240 --> 25:43.800
I'm an embodied person and this embodied entity has mental cognitive as well as physical,

25:43.800 --> 25:45.640
emotional and all the rest.

25:45.640 --> 25:48.240
And I want to keep that integrity intact.

25:48.240 --> 25:54.760
But if you are going to upload me onto a computer, I'm not going to be the embodied thing that

25:54.760 --> 25:55.840
I am right now.

25:55.840 --> 25:57.200
I would call it an organism.

25:57.200 --> 26:02.880
I will use the organism element once I become instantiated in a computer.

26:02.880 --> 26:04.520
Okay, that's point number one.

26:04.520 --> 26:09.920
Point number two, the embodied aspect is very crucial because it has something to do with

26:09.920 --> 26:11.320
sex and with gender.

26:11.320 --> 26:15.600
The one thing that gets lost in this entire conversation is the gender dimension as if

26:15.600 --> 26:20.280
it doesn't exist, as if there is no sexuality, there is no reproduction, there is none of

26:20.280 --> 26:21.280
that exists.

26:21.280 --> 26:22.280
Why?

26:22.280 --> 26:25.280
Because all this business was created by men, excuse me.

26:25.280 --> 26:28.120
They are all AI operations.

26:28.120 --> 26:30.920
But anyway, I would like to protect embodiment.

26:30.920 --> 26:32.800
Okay, so that's a very important thing.

26:32.800 --> 26:35.240
Also the environmental dimension is very important to me.

26:35.240 --> 26:41.080
I don't think that what's going to happen with this AI taking over is good for the planet

26:41.080 --> 26:43.280
from environmental perspective.

26:43.280 --> 26:45.960
Unless you prove otherwise, let them prove otherwise.

26:45.960 --> 26:47.200
Right now, I think that they...

26:47.200 --> 26:49.440
What ways are not good for the planet?

26:49.440 --> 26:54.840
For example, extracting all those metals that you need in order to do what you're doing.

26:54.840 --> 26:57.960
Has anybody talked about what happens?

26:57.960 --> 27:00.000
Where do you get all these material from?

27:00.000 --> 27:01.720
It comes from the earth and some form...

27:01.720 --> 27:04.000
How are they more efficient than human beings?

27:04.000 --> 27:05.000
Yeah.

27:05.000 --> 27:06.000
So we co-sold from...

27:06.000 --> 27:07.000
So we co-sold from...

27:07.000 --> 27:08.000
How do you measure efficiency?

27:08.000 --> 27:13.480
As energy, human body consumes about 100 watts.

27:13.480 --> 27:14.480
Yeah, yeah, of course.

27:14.480 --> 27:18.480
I mean, out of being in our brain, it's about 20 watts.

27:18.480 --> 27:24.640
When the Europeans proposed some years ago to build a computer simulation of a brain,

27:24.640 --> 27:29.640
it was going to take nuclear power plants to do what...

27:29.640 --> 27:31.680
Yes, to just to run the computers.

27:31.680 --> 27:39.360
So, but then if you take out this uploading of whatever, so the disappearance of the embodiment,

27:39.360 --> 27:43.760
which is not something that I'm considering at all, but I mean, if you take those...

27:43.760 --> 27:45.320
That's true, that's fine.

27:45.320 --> 27:46.320
It's fine.

27:46.320 --> 27:49.760
So, I don't have to agree with everybody.

27:49.760 --> 27:51.320
But, yeah, there's one more.

27:51.320 --> 27:52.560
That's the third element.

27:52.560 --> 27:57.040
The real thing that bugs me is the reduction of the human to a pattern.

27:57.040 --> 27:59.040
You're dealing with patterns, right?

27:59.040 --> 28:00.040
When you're...

28:00.040 --> 28:02.040
You turn us into an algorithm.

28:02.040 --> 28:06.800
No, I'm trying to use algorithm to augment our capabilities.

28:06.800 --> 28:09.280
That's not a reduction to a pattern.

28:09.280 --> 28:14.000
Why don't you tell us a little bit about what your research is and what's going on in IBM

28:14.000 --> 28:15.000
once?

28:15.000 --> 28:17.000
Yeah, but I mean, not just in IBM.

28:17.000 --> 28:18.000
I mean, everywhere.

28:18.000 --> 28:25.400
I mean, so there are many people in AI that think that the purpose of AI is to augment

28:25.400 --> 28:26.400
our intelligence.

28:26.400 --> 28:27.400
When are called?

28:27.400 --> 28:30.720
So, that's why at IBM at some point, they didn't want to use artificial intelligence

28:30.720 --> 28:36.080
because it was kind of misleading as if you were creating another thing by itself and

28:36.080 --> 28:37.080
so on.

28:37.080 --> 28:41.400
So, at some point, they called it AI, but it was augmented intelligence.

28:41.400 --> 28:46.240
And now they said, okay, let's call it AI artificial intelligence, but what we mean is

28:46.240 --> 28:48.240
augmenting human intelligence.

28:48.240 --> 28:57.040
So, we have our own form of intelligence which brings intuition, asking the right questions,

28:57.040 --> 29:03.760
judgment, you know, and the AI has other things that you can do much better than us.

29:03.760 --> 29:09.440
So, this complementarity can bring the things together, but with the goal of not making

29:09.440 --> 29:15.960
us a machine, but using the machines to help us being better humans, whatever we want.

29:15.960 --> 29:17.680
We can define that.

29:17.680 --> 29:19.520
We're not going to disappear.

29:19.520 --> 29:21.040
We are going to be enhanced.

29:21.040 --> 29:23.600
So, I have this optimistic view.

29:23.600 --> 29:28.400
Of course, you have to be careful in doing this teaming, AI and humans.

29:28.400 --> 29:31.200
You have to be careful in the issues that can come out.

29:31.200 --> 29:36.200
You have to be careful in making the machines understand what humans want and not vice versa.

29:36.200 --> 29:41.360
You have to be careful in making the machine speak the language that we speak and not the

29:41.360 --> 29:43.160
opposite.

29:43.160 --> 29:50.160
And so, it really has to be an enhancement of ourselves and not us having to come to terms

29:50.160 --> 29:52.160
with the machines desire things.

29:52.160 --> 29:55.800
Do they want things to happen?

29:55.800 --> 29:58.600
If you tell, I mean, the machine has a goal.

29:58.600 --> 30:05.160
The question is, we know that natural selection is a natural process.

30:05.160 --> 30:10.560
So, this sounds very science-fiction-y and I wouldn't have thought, I mean, ten years

30:10.560 --> 30:13.160
ago, I would have thought this is a total nonsense.

30:13.160 --> 30:18.880
But machines that can learn how to replicate themselves or not the machines as much as

30:18.880 --> 30:24.720
the algorithm, the programs, if they can learn how to replicate themselves in the world,

30:24.720 --> 30:27.280
then natural selection takes over.

30:27.280 --> 30:35.040
Kevin Kelly, one of the founders of Wired Magazine wrote a book, What Technology Wants.

30:35.040 --> 30:40.400
And it's the old adage, you know, if all you have is a hammer, then every problem looks

30:40.400 --> 30:42.760
like a nail.

30:42.760 --> 30:49.560
And so, there is a way that, you know, the machines in our lives, whether it's a car,

30:49.560 --> 30:56.000
whether it's a computer or, you know, somewhat direct our behavior.

30:56.000 --> 31:05.400
Well, that's the autonomous technology thesis and it's certainly the case in medicine.

31:05.400 --> 31:09.000
The machinery becomes more elaborate.

31:09.000 --> 31:14.720
It takes a lot of effort, but it can be done to get a control over the utilization of those

31:14.720 --> 31:15.720
machines.

31:15.720 --> 31:22.880
I think, I mean, we keep talking about we as if that means something.

31:22.880 --> 31:23.880
Because I don't...

31:23.880 --> 31:24.880
You'll mind to...

31:24.880 --> 31:26.800
Well, but we're all individuals, right?

31:26.800 --> 31:27.800
There's a lot of individuals.

31:27.800 --> 31:32.240
I'm not going to, you know, leave my body and go into a machine.

31:32.240 --> 31:35.240
I'm not going to let somebody else make me into a machine.

31:35.240 --> 31:39.440
But people think that way, then, who is the we that's going to do it?

31:39.440 --> 31:40.440
So that's...

31:40.440 --> 31:42.080
So now we're on the same page here.

31:42.080 --> 31:48.680
I'm saying, and you seem to agree with that, that we need to look at those proposals and

31:48.680 --> 31:50.120
be critical of them.

31:50.120 --> 31:55.360
We shouldn't just add on this, kind of buy into it without any critical analysis.

31:55.360 --> 32:01.680
Let me remind all of us that when Nick Bostum started the whole conversation at least 15

32:01.680 --> 32:05.520
years ago, nobody spoke about the dangers of AI.

32:05.520 --> 32:13.120
In his most recent book, 2015, Super Intelligent Machines, only there he even he begins to

32:13.120 --> 32:18.400
doubt or begins to admit that there may be some things that we don't want to happen

32:18.400 --> 32:19.400
with artificial intelligence.

32:19.400 --> 32:26.280
But 15 years ago, he did not even entertain the possibility that there may be some malicious

32:26.280 --> 32:27.280
AI.

32:27.280 --> 32:29.800
So now they're busy doing friendly AI.

32:29.800 --> 32:34.000
If you read The New York Times just a few days ago, there is an essay precisely on that

32:34.000 --> 32:35.000
topic.

32:35.000 --> 32:36.920
I have it here.

32:36.920 --> 32:39.560
And just where is it?

32:39.560 --> 32:44.440
Yes, how to make a human friendly?

32:44.440 --> 32:51.920
So my problem is, why didn't they listen to the critics 15 years ago?

32:51.920 --> 32:54.440
Because 15 years ago, AI was not so pervasive.

32:54.440 --> 33:01.040
It was not so impactful on our life and society 15 years ago, we didn't have enough computing

33:01.040 --> 33:07.520
power and enough data to make AI learn and be aware of the environment and be used in

33:07.520 --> 33:09.400
the uncertainty of the world.

33:09.400 --> 33:17.080
So there was no issue because AI was not really giving was only used in very controlled environments.

33:17.080 --> 33:18.880
Now, instead, it's different.

33:18.880 --> 33:21.400
So that's why people think about these issues.

33:21.400 --> 33:22.400
Are we too late?

33:22.400 --> 33:28.600
So 15 years ago, I wrote an essay in which I said, our official intelligence that could

33:28.600 --> 33:34.920
drive a car across town and park it in the parking lot would pass the Turing test.

33:34.920 --> 33:41.480
That would be for me proof of its intelligence and the way that even more than the traditional

33:41.480 --> 33:42.480
Turing test.

33:42.480 --> 33:49.120
Of course, now that's doable.

33:49.120 --> 33:52.840
So one of the things that interests me is the limits, and I've written about this as

33:52.840 --> 34:01.200
well, the limits of computation and the possible limits of the genetic engineering as well,

34:01.200 --> 34:03.840
that these are complex systems.

34:03.840 --> 34:11.280
And the more complex the system gets, the more you start pushing against nonlinear dynamics

34:11.280 --> 34:13.000
and unpredictability.

34:13.000 --> 34:20.600
And so the genome, I understand to be a bureaucracy, not a bunch of all-off switches.

34:20.600 --> 34:26.160
And within computer science, I'm not an expert in this, there are problems that can't be

34:26.160 --> 34:27.160
solved.

34:27.160 --> 34:31.080
There are problems that, because you can't write an algorithm for it, and there are other

34:31.080 --> 34:34.520
problems that are just too hard to solve.

34:34.520 --> 34:41.240
And I just wonder whether in these two domains, we might be running up against a complexity

34:41.240 --> 34:42.240
horizon.

34:42.240 --> 34:45.040
And these two domains are joining.

34:45.040 --> 34:47.520
And the brain as well, I would say, is also another potentially.

34:47.520 --> 34:51.480
Yes, and those three domains are joining forces.

34:51.480 --> 34:55.280
I mean, deep learning now, nobody knows how these algorithms work.

34:55.280 --> 34:57.120
Well, that's not exactly true.

34:57.120 --> 34:58.680
I mean, nobody knows.

34:58.680 --> 34:59.680
No.

34:59.680 --> 35:03.760
There's a bit of a black box, but somebody had to set up the black box to work and they

35:03.760 --> 35:06.120
have data into it, and they have to...

35:06.120 --> 35:12.200
Yeah, but if you ask why you did something, why you made the second decision,

35:12.200 --> 35:14.880
which is still a bit opaque.

35:14.880 --> 35:20.360
I mean, in some cases, you can go back to the kind of training data that allowed to make

35:20.360 --> 35:27.160
that decision, but it's still a bit opaque, much more opaque than traditional logic-based

35:27.160 --> 35:33.160
AI systems, which however, which were much less accurate than deep learning.

35:33.160 --> 35:36.640
And now there's a reproducibility problem in some of the AI.

35:36.640 --> 35:37.640
Yeah.

35:37.640 --> 35:39.440
And there should be.

35:39.440 --> 35:40.440
Okay, why?

35:40.440 --> 35:46.840
I mean, well, first of all, if you're talking about deep learning, and there's many, many

35:46.840 --> 35:52.120
hidden layers in the process of the machine learning how to recognize things.

35:52.120 --> 35:55.680
So you've given a lot of data and you give it some algorithms and...

35:55.680 --> 35:57.120
Yeah, but you know, but it's...

35:57.120 --> 35:59.520
Do people know how deep learning works?

35:59.520 --> 36:00.520
There is a...

36:00.520 --> 36:01.520
Go ahead, play it.

36:01.520 --> 36:03.200
No, I mean, it's very simple.

36:03.200 --> 36:07.520
I mean, it's not in the details of the algorithm, but it's very, very simple.

36:07.520 --> 36:16.160
So if you want an algorithm that can recognize whether in a picture that is a cat or a dog,

36:16.160 --> 36:17.160
okay?

36:17.160 --> 36:21.800
So you give it a picture and you want this algorithm to say whether it's a cat or a dog,

36:21.800 --> 36:22.800
okay?

36:22.800 --> 36:28.320
So what you do, you start with an algorithm that is behaving very randomly, okay?

36:28.320 --> 36:30.200
It doesn't know anything.

36:30.200 --> 36:33.600
And you start giving a lot of examples.

36:33.600 --> 36:34.600
What is one example?

36:34.600 --> 36:41.080
An example is one picture and the information that I give to the algorithm that I say whether

36:41.080 --> 36:43.360
in that picture there is a cat or a dog, okay?

36:43.360 --> 36:47.080
So I say, there's a picture, there's a cat, there's a picture, there's a dog, there's

36:47.080 --> 36:48.080
a picture, there's a cat.

36:48.080 --> 36:50.320
And I give a lot of these examples.

36:50.320 --> 36:55.800
And then the algorithm, by looking at one example after the other one in sequence, the

36:55.800 --> 37:02.040
beginning is kind of random and then it starts tuning its parameters in a way that it learns

37:02.040 --> 37:06.200
this relationship between pictures and whether there is a cat or a dog there.

37:06.200 --> 37:11.680
At the end of this, that we call the training phase, you have an algorithm that has some

37:11.680 --> 37:16.800
parameters tuned in a certain way that hopefully has learned the relationship between pictures

37:16.800 --> 37:22.160
and these two animals and the can able, is able to generalize it to picture that he has

37:22.160 --> 37:23.680
never seen before.

37:23.680 --> 37:28.600
So now you give it another picture, you don't tell if it's a cat or a dog and possibly the

37:28.600 --> 37:33.040
algorithm will be able to recognize whether there is a cat or a dog.

37:33.040 --> 37:37.600
And usually with this algorithm, you give it enough data, but not just enough data.

37:37.600 --> 37:42.920
If you give it data which is diverse enough, inclusive enough of all the possible situation,

37:42.920 --> 37:45.480
then the algorithm is able to generalize.

37:45.480 --> 37:51.600
And many deep learning approaches to, for example, automatic vision, like recognizing

37:51.600 --> 37:53.320
what is in a picture.

37:53.320 --> 37:56.040
Right now a very small percentage of error.

37:56.040 --> 37:59.600
An error is always there, but the percentage value is very, very small.

37:59.600 --> 38:01.760
So they are very, very good at generalizing.

38:01.760 --> 38:07.280
But you need a lot of data for training these examples and you need a lot of computing power

38:07.280 --> 38:10.320
because of this amount of data that you have to deal with.

38:10.320 --> 38:11.680
So the idea is very simple.

38:11.680 --> 38:16.960
You learn by example, which by the way, is something that AI people are trying to move

38:16.960 --> 38:18.920
forward from that.

38:18.920 --> 38:25.000
Because if you learn only from examples, the relationship between input and output, then

38:25.000 --> 38:29.600
if you give it another task, which is very similar to that, but not exactly the same,

38:29.600 --> 38:32.720
you have to start this whole process from scratch.

38:32.720 --> 38:38.520
Because the algorithm is actually not lamp general idea what is a cat or what is a dog.

38:38.520 --> 38:40.760
But it's just lamp this input-output relation.

38:40.760 --> 38:42.560
We're just still stupid.

38:42.560 --> 38:43.560
Yeah.

38:43.560 --> 38:47.920
Of course a two-year-old could do that.

38:47.920 --> 38:48.920
Yeah.

38:48.920 --> 38:49.920
So what's the utility?

38:49.920 --> 38:52.920
And why should we be worried that that's going to take over the world?

38:52.920 --> 38:55.760
That's not going to take over the world.

38:55.760 --> 38:59.800
That's just got to stop.

38:59.800 --> 39:03.480
So basically, this is analogous to the genome.

39:03.480 --> 39:08.880
The genome, each of us came from a single cell.

39:08.880 --> 39:16.240
And that single cell had information equivalent to 6 billion bits of digital data.

39:16.240 --> 39:17.240
That's it.

39:17.240 --> 39:24.840
Well, that cell with 6 billion bits of information grew into each human being.

39:24.840 --> 39:31.480
Can you change some of those bits and become a mouse, not a human being?

39:31.480 --> 39:33.200
And that's an algorithm.

39:33.200 --> 39:37.040
We don't know how that works.

39:37.040 --> 39:39.320
Evolution created this thing.

39:39.320 --> 39:47.200
In a sense, machine learning, the process, there are lots of hidden layers that are

39:47.200 --> 39:49.800
not telling the machine how to learn.

39:49.800 --> 39:52.680
It's just learning from what you're giving it.

39:52.680 --> 39:58.280
And what's happening with the genome and what's going to happen is using the computational

39:58.280 --> 40:06.800
tools to be able to see perturbations, how perturbations in the genome affect the output,

40:06.800 --> 40:07.800
the final output.

40:07.800 --> 40:13.280
How many human genetic diseases have been identified?

40:13.280 --> 40:14.280
Simple genetic diseases?

40:14.280 --> 40:15.280
Well, you could do 6,000.

40:15.280 --> 40:20.160
Well, diseases are not a good categorization.

40:20.160 --> 40:30.400
I mean, there are hundreds of thousands of mutations that can cause various diseases.

40:30.400 --> 40:33.960
Least started a company called Gene Peake.

40:33.960 --> 40:34.960
Gene Peake.

40:34.960 --> 40:35.960
Gene Peake.

40:35.960 --> 40:44.600
That does pre-pregnancy screening to identify what would be a good match.

40:44.600 --> 40:53.520
How reliable, how many traits are you screening for and how reliably.

40:53.520 --> 41:04.680
So, you know, I know this is used extensively in acidic Orthodox Jewish communities for certain

41:04.680 --> 41:06.200
recessive diseases.

41:06.200 --> 41:10.240
But tell us a little bit about where this is leading.

41:10.240 --> 41:13.960
The technology has exploded.

41:13.960 --> 41:19.880
So just a couple of years ago, you could look at, you know, 20 diseases first, 100.

41:19.880 --> 41:22.200
We now look at 7,000.

41:22.200 --> 41:29.680
And we look at, we create, we take a presumptive mother and people want to be a mother and

41:29.680 --> 41:35.160
a father, we take their DNA, it's digital information, then we create virtual babies,

41:35.160 --> 41:36.160
virtual genomes.

41:36.160 --> 41:40.920
You don't really know who's contributing the genes.

41:40.920 --> 41:41.920
It's 50, 50.

41:41.920 --> 41:46.200
Well, we create lots of virtual babies from a couple.

41:46.200 --> 41:49.880
And what do the virtual babies look like?

41:49.880 --> 41:54.040
Well, so we see whether they have disease right now.

41:54.040 --> 41:57.160
But the question is, okay, are there any limits?

41:57.160 --> 42:02.000
I'm talking primarily about ethical limits to this kind of screening.

42:02.000 --> 42:03.000
Wow.

42:03.000 --> 42:09.280
So in my view, people should be able to do what they want with their own data.

42:09.280 --> 42:12.720
But what's the goal of this analysis?

42:12.720 --> 42:18.800
So you take two people, the DNA of two people, you understand what are most of the many possibilities

42:18.800 --> 42:19.720
that are offspring.

42:19.720 --> 42:25.240
And then they can, if they want, they can use that information to avoid disease by picking

42:25.240 --> 42:26.240
an embryo.

42:26.240 --> 42:33.080
If you can embryo that doesn't have the genes causing disease, those in vitro fertilization,

42:33.080 --> 42:34.800
they start with a bunch of embryos.

42:34.800 --> 42:41.600
So you generate these offsprings, not just virtually, not just on a computer saying,

42:41.600 --> 42:48.240
this is the DNA that can come out, but as embryos, that's for them to decide.

42:48.240 --> 42:52.480
We give them the information so they can do that.

42:52.480 --> 43:00.320
How do you encounter this in hospitals today, Steven?

43:00.320 --> 43:12.920
Actually, there is a counter-cultural critique of the perfect baby notion, which is subjectively

43:12.920 --> 43:15.280
defined.

43:15.280 --> 43:17.600
And that's really the disability model.

43:17.600 --> 43:25.760
So people like Adrian Ash, for example, but many others have claimed that with our constant

43:25.760 --> 43:39.800
urge to enhance the lives and the lifespans that we bring into the world, that we eliminate

43:39.800 --> 43:49.600
the virtue of tolerance, of inclusion of a common or a shared humanity.

43:49.600 --> 43:58.280
And we begin as a culture to select certain traits which become definitive of a human being,

43:58.280 --> 44:02.720
having any kind of moral considerably.

44:02.720 --> 44:05.960
It becomes, if you will, sort of neo-ugenic.

44:05.960 --> 44:12.720
Now, that may not be the case, but that's the disability critique is that we benefit when

44:12.720 --> 44:19.760
we have, quote unquote, imperfection in the world because it teaches us to be more inclusive

44:19.760 --> 44:25.640
and to be understanding that in fact there are deeper things in community than hypercognitive

44:25.640 --> 44:29.080
values, hypercognitive achievements, and so forth.

44:29.080 --> 44:37.000
So the first thing I think it's very important to point out is that the perfect baby is fiction.

44:37.000 --> 44:39.320
There's no such thing as a perfect baby.

44:39.320 --> 44:41.320
My mom thought I was.

44:41.320 --> 44:47.720
I mean, that's not what people are doing right now.

44:47.720 --> 44:53.560
What they're trying to do is, you know, prevent a disease or, you know, provide resistance

44:53.560 --> 44:54.560
to a disease.

44:54.560 --> 44:56.560
I mean, that's not perfection.

44:56.560 --> 45:00.600
And I think it's really, I think people who are against the technology, they use this

45:00.600 --> 45:01.880
and it's a red herring.

45:01.880 --> 45:06.440
I mean, it's not what we're doing and there never will be a perfect baby.

45:06.440 --> 45:10.760
So can I comment on this though that so I actually agree with what you just said because

45:10.760 --> 45:21.480
in my view, I don't really engage in the transhumanist conversation at high philosophical levels.

45:21.480 --> 45:23.960
To me, it's all incremental.

45:23.960 --> 45:26.960
So we're going to have successful anti-aging.

45:26.960 --> 45:33.840
I was looking at a remarkable piece of work going on now as of, well, 2016 at the Mayo

45:33.840 --> 45:40.200
Clinic and its scripts where they're using an agent which combines chemotherapy and a

45:40.200 --> 45:42.440
plant dye, believe it or not.

45:42.440 --> 45:49.720
And they're actually eliminating senile cells from the kidneys of individuals with failing

45:49.720 --> 45:50.720
kidneys.

45:50.720 --> 45:53.480
They've been incredibly successful in mice.

45:53.480 --> 45:58.680
Now, this is therapeutic.

45:58.680 --> 46:04.600
We're doing things therapeutically but then we'll look pretty good as generalized enhancements.

46:04.600 --> 46:10.600
We'll all want to keep our organs from becoming senile, if you will.

46:10.600 --> 46:17.440
And similarly, the National Institute on Aging is devoting 55% right now but it's budget

46:17.440 --> 46:21.200
strictly to the science of anti-aging.

46:21.200 --> 46:27.920
Now that's why because they've given up, frankly, on finding solutions to many of the chronic

46:27.920 --> 46:34.640
illnesses, the main precipitating or risk factor for which is age itself.

46:34.640 --> 46:40.440
I mean, a hundred years ago, people got dementia because of syphilis, right?

46:40.440 --> 46:48.240
Now that they're living into their 70s and 80s, it's age itself and of course Alzheimer

46:48.240 --> 46:51.080
himself wasn't sure if he discovered a disease.

46:51.080 --> 46:57.560
He actually thought that he discovered a natural part of human senile aging.

46:57.560 --> 47:03.920
So because we've got to this kind of midway point where we're all living longer on average,

47:03.920 --> 47:10.280
we're so much more subject to these many diseases of old age, including cancer and so forth,

47:10.280 --> 47:18.080
that maybe the solution so the NIA thinks is to actually figure out, you know, telemirrically

47:18.080 --> 47:21.080
and in other ways what the process of senescence is.

47:21.080 --> 47:26.800
And if we can turn that around, okay, maybe we'll be living to be on average 112 years

47:26.800 --> 47:33.680
old or maybe 110 depending on who you talk with, you know, people do discuss these things.

47:33.680 --> 47:38.640
But if we can compress morbidity and we can get rid of these chronic illnesses, that would

47:38.640 --> 47:41.640
become an enhancement but a valuable enhancement.

47:41.640 --> 47:49.840
So in many of these areas, genetics, anti-aging, even psychiatry and so forth, I think that

47:49.840 --> 47:55.960
we start trying with the attempt to address a therapeutic need and then it just spills

47:55.960 --> 47:58.040
over because it's a good thing.

47:58.040 --> 47:59.400
I don't know if that makes any sense.

47:59.400 --> 48:00.400
No, it makes sense.

48:00.400 --> 48:01.400
That's a lot of sense.

48:01.400 --> 48:07.600
But usually the boundary between therapy and enhancement is very blurred and it's very

48:07.600 --> 48:08.600
fluid.

48:08.600 --> 48:09.600
It is.

48:09.600 --> 48:13.400
You're saying that it's kind of a natural growth that once we figure out the therapeutic

48:13.400 --> 48:15.320
stuff, it becomes enhancement.

48:15.320 --> 48:18.280
I don't think that that's where it comes to attention.

48:18.280 --> 48:19.280
It tends to.

48:19.280 --> 48:20.280
Well, maybe.

48:20.280 --> 48:25.000
But a lot of time it's this enhancement that I would call it an ideology.

48:25.000 --> 48:26.760
It's ideological thrust.

48:26.760 --> 48:29.240
It's kind of, we've got to be enhanced.

48:29.240 --> 48:31.600
If you're not enhanced, there's something wrong with you.

48:31.600 --> 48:33.680
And don't you dare stand in the process.

48:33.680 --> 48:40.040
I'm quoting actually Hugo DeGarris, the Australian transhumanist who said, don't you stand in

48:40.040 --> 48:43.160
the middle of this process of becoming perfect?

48:43.160 --> 48:46.120
So they use the concept of perfection.

48:46.120 --> 48:47.120
That's what they want.

48:47.120 --> 48:48.120
But they're just idiots.

48:48.120 --> 48:49.120
They're just idiots.

48:49.120 --> 48:50.120
I mean, Aubrey's degrade.

48:50.120 --> 48:52.680
You know, by the way, it doesn't have a position.

48:52.680 --> 48:55.200
He's not even a scientist.

48:55.200 --> 48:57.200
He's just a guy who hangs out in coffee shops in Cambridge.

48:57.200 --> 48:58.200
He's never met him.

48:58.200 --> 49:03.600
No, he's in the United States and he's now funded by the Metuzela.

49:03.600 --> 49:05.240
Well, that's true.

49:05.240 --> 49:08.120
But I mean, I won't say anything more about Aubrey.

49:08.120 --> 49:09.600
I mean, I don't know him too.

49:09.600 --> 49:12.000
I've encountered him a number of times.

49:12.000 --> 49:15.520
I think these people are sort of, what should I say?

49:15.520 --> 49:23.440
They're like adolescents in a science class at age 14 and some crazy idea comes along.

49:23.440 --> 49:25.680
And they think, oh my God, that's it.

49:25.680 --> 49:32.720
And they have no connection with the narrative of the human experience to be able to think

49:32.720 --> 49:34.480
critically about it, which is your point.

49:34.480 --> 49:40.760
So just to develop this conversation one more bit here, I wrote the other day to a friend

49:40.760 --> 49:46.440
of mine, Francisco Cardeso Comes de Matos, who's the world's leading south.

49:46.440 --> 49:47.440
He's a linguist.

49:47.440 --> 49:50.240
He's incredibly well known.

49:50.240 --> 49:52.520
And I asked him to reflect on France's humanism.

49:52.520 --> 49:55.200
Here's what he wrote back in an email.

49:55.200 --> 49:59.240
This is from Ressif Brazil.

49:59.240 --> 50:02.080
Human health enhancing question mark.

50:02.080 --> 50:05.840
Human dignity elevating question mark.

50:05.840 --> 50:09.960
Human mind expanding question mark.

50:09.960 --> 50:14.080
You know, jobs slept on my floor at Reed College, and he never let his kids play with computers

50:14.080 --> 50:16.400
growing up.

50:16.400 --> 50:19.960
Human spirituality probing question mark.

50:19.960 --> 50:22.800
Human creativity amplifying question mark.

50:22.800 --> 50:29.360
You should take a look at Delaney Rustin's great video screenagers, the movie, about

50:29.360 --> 50:35.360
how we're struggling with creativity and somatic learning and memory and knowledge and

50:35.360 --> 50:36.360
so forth.

50:36.360 --> 50:37.660
It's very interesting.

50:37.660 --> 50:44.480
With peaceful nature strengthening, human interaction facilitating, human science is

50:44.480 --> 50:46.480
integrating.

50:46.480 --> 50:49.840
So those are just reflections from someone down in the south.

50:49.840 --> 50:52.000
Yeah, but what does this question mark?

50:52.000 --> 50:53.000
Yes, it's what does it mean?

50:53.000 --> 50:54.480
Well, he's asking me to think about that.

50:54.480 --> 50:59.400
Well, the questions that you're asking, Hana, that we need to think carefully about these

50:59.400 --> 51:00.400
possibilities.

51:00.400 --> 51:04.120
But are those things that are, you know, that would be welcome or not?

51:04.120 --> 51:06.160
Well, he's wanting us to question them.

51:06.160 --> 51:10.280
Well, I think the implication is that those would all be positive.

51:10.280 --> 51:11.280
Yeah.

51:11.280 --> 51:14.120
And the question is, does transhumanism work?

51:14.120 --> 51:15.120
Right.

51:15.120 --> 51:17.880
Well, he's raising an ideology or his actual technology.

51:17.880 --> 51:22.800
He's raising many doubts about the transhumanist ideology because he doesn't think that it's

51:22.800 --> 51:24.880
attending necessarily.

51:24.880 --> 51:29.840
Now I'm not ruling it out, but he's saying his sense is that it's not attending to the

51:29.840 --> 51:35.920
most important elements of, shall we say, human enhancement in human, if you will, perfect

51:35.920 --> 51:46.120
ability, which has to do with character traits, dignity, empathy.

51:46.120 --> 51:48.160
You know, I have a close friend.

51:48.160 --> 51:49.160
He's in intelligence.

51:49.160 --> 51:50.160
Right.

51:50.160 --> 51:53.680
Well, let me just say that I have a close friend who's 90 years old and she was my

51:53.680 --> 51:56.720
dorm mother at a high school in New Hampshire.

51:56.720 --> 51:58.440
She lives in Martha's Vineyard.

51:58.440 --> 52:00.840
She's got one grandson.

52:00.840 --> 52:06.920
His son's 12 years old, he was literally raised on screens, okay, unfortunately.

52:06.920 --> 52:13.200
And she took her daughter and her grandson to the Swiss Alps for a vacation.

52:13.200 --> 52:17.640
And she came back and she called me just this December and she was in tears because she

52:17.640 --> 52:21.480
said her grandson has absolutely no empathic qualities.

52:21.480 --> 52:23.200
Cannot interact.

52:23.200 --> 52:26.080
Cannot even make eye contact.

52:26.080 --> 52:28.000
And she was so frustrated.

52:28.000 --> 52:31.680
She said it was the worst nightmare of her entire life.

52:31.680 --> 52:33.240
But that's why.

52:33.240 --> 52:37.400
So that's what we're asking is whatever it is, is it dignity and enhancing?

52:37.400 --> 52:41.840
Now some of these things, including, you know, by the way, some of the genetic ideas, are

52:41.840 --> 52:44.320
not contrary to human dignity by any means.

52:44.320 --> 52:45.760
I don't want to say they are.

52:45.760 --> 52:49.040
But these are the deeper questions that you need to be asking.

52:49.040 --> 52:50.040
Yeah.

52:50.040 --> 52:55.840
The point is, I think the discussion is at many levels and I'm not clear because we talk

52:55.840 --> 53:03.560
about increasing lifespan, which is one thing, creating immortality, it's another thing.

53:03.560 --> 53:08.480
Becoming all of us just computers and algorithms is yet another thing.

53:08.480 --> 53:14.080
But the thing that you are talking about, Francesca, has to do more with AI and the dangers of

53:14.080 --> 53:15.080
AI.

53:15.080 --> 53:16.080
It's specifically...

53:16.080 --> 53:17.080
I'm not the dangers.

53:17.080 --> 53:18.080
I will not put it that way.

53:18.080 --> 53:27.640
But I will put AI helping us to enhance our own traits and all those things with the question

53:27.640 --> 53:28.640
marks.

53:28.640 --> 53:30.480
I will say, yes, we can do that.

53:30.480 --> 53:33.280
If you are careful enough, we can do all of those.

53:33.280 --> 53:34.280
But what helps really?

53:34.280 --> 53:35.280
And they should be welcome.

53:35.280 --> 53:36.880
But I thought what works.

53:36.880 --> 53:44.480
Because with our creativity, enhance our traits and answer, we can do much more scientific

53:44.480 --> 53:45.480
discoveries.

53:45.480 --> 53:48.080
And discover fewer for money more diseases.

53:48.080 --> 53:49.520
No, we know that.

53:49.520 --> 53:56.200
But what you were saying also, I thought, was that we need to be aware of the potential

53:56.200 --> 54:02.440
problems and dangers in order to not end up in a place we don't want to end up.

54:02.440 --> 54:08.640
The fact is that the grandson of your friend has already ended up there.

54:08.640 --> 54:12.040
So there was no way to prevent it.

54:12.040 --> 54:17.160
And part of the issue seems to me in trying to say we can, in fact, figure out what the

54:17.160 --> 54:22.640
problems would be, we may not be able to figure out, just as we didn't know what the

54:22.640 --> 54:31.640
effect of social media on us would be, what the effect of the iPhone or the telephone

54:31.640 --> 54:32.960
would be on us.

54:32.960 --> 54:35.400
So only after the fact we are saying.

54:35.400 --> 54:36.920
So it's retrospective.

54:36.920 --> 54:42.000
And now people are saying, well, maybe we shouldn't buy into the MacArthur ideology of

54:42.000 --> 54:46.520
doing away with teachers in the grade schools and just having people on iPads.

54:46.520 --> 54:48.160
Maybe we should be more thoughtful about it.

54:48.160 --> 54:54.400
The American Journal of Pediatrics had an entire volume devoted in December to the problem

54:54.400 --> 54:59.880
of raising children and getting them into basic social skills.

54:59.880 --> 55:04.720
And so now the suggestion is, well, maybe we should hold off until they're 10 or 11

55:04.720 --> 55:05.720
before they're.

55:05.720 --> 55:06.720
Look at this.

55:06.720 --> 55:09.880
Now we are going to have driverless cars.

55:09.880 --> 55:14.080
It seems to be from everything I hear around the corner.

55:14.080 --> 55:15.080
So to be.

55:15.080 --> 55:28.000
What are going to be the consequences of having driverless cars?

55:28.000 --> 55:32.680
What is going to be the consequence on our ability to drive or on our ability to do other

55:32.680 --> 55:34.040
physical things?

55:34.040 --> 55:37.840
What is going to be the consequence in terms of traffic situations?

55:37.840 --> 55:40.320
What is going to be the consequence economically?

55:40.320 --> 55:44.520
What's going to be the consequence in terms of insurance companies?

55:44.520 --> 55:46.680
And some of these things you can think ahead.

55:46.680 --> 55:52.000
But it seems to me that really parts of it, you cannot think until you're dealing with

55:52.000 --> 55:53.000
it already.

55:53.000 --> 55:54.000
That's too late.

55:54.000 --> 55:59.320
Well, it's not too late, but you can't think about it until you have some experience.

55:59.320 --> 56:06.720
But the whole conversation has various themes or strengths in it.

56:06.720 --> 56:10.280
And it's very hard to really differentiate between them.

56:10.280 --> 56:15.760
And it's very hard also to debate with transhumanists because when I asked, you know, I attacked

56:15.760 --> 56:20.280
them on one issue and said, oh, no, I didn't say, hey, I actually believe in B. So, okay,

56:20.280 --> 56:24.400
you go to B, oh, I don't believe in B, I believe in C. So it's kind of a moving target.

56:24.400 --> 56:27.040
It's very, very hard to pin down what's going on.

56:27.040 --> 56:28.040
In other words.

56:28.040 --> 56:35.360
They don't really take ownership of the consequences that will come about if they're dream, if

56:35.360 --> 56:37.760
that social imaginary will come to be a reality.

56:37.760 --> 56:38.760
I don't know.

56:38.760 --> 56:42.520
I mean, there is this book I was talking to, and I mentioned to you and I was talking

56:42.520 --> 56:47.360
to Francesca, but by Max Tech Markets focused on how we can prevent.

56:47.360 --> 56:48.360
Life 3.0.

56:48.360 --> 56:49.360
Yeah.

56:49.360 --> 56:51.520
How we can prevent some of the complications.

56:51.520 --> 56:55.160
So, I mean, there is an effort to think about it.

56:55.160 --> 56:56.920
And I think there is more of a focus.

56:56.920 --> 57:02.720
But you're right that people now think a lot about these issues and how to resolve them

57:02.720 --> 57:07.080
because there have been issues that have been shown in some examples.

57:07.080 --> 57:12.280
So it was like a trial and, you know, and you were right.

57:12.280 --> 57:17.040
Maybe one should have thought that at the beginning, but it was impossible to predict

57:17.040 --> 57:18.600
or very, very difficult.

57:18.600 --> 57:23.280
So, we saw some issues and I was like, oh, okay.

57:23.280 --> 57:24.960
So, let's rethink from scratch.

57:24.960 --> 57:27.160
You know, how are we going to develop AI?

57:27.160 --> 57:33.880
How are we going to design the AI and embed in the design itself, not just at the end

57:33.880 --> 57:35.120
and check how it behaves.

57:35.120 --> 57:41.760
And yeah, but embed, since the design phase, these issues that it should not be by as they

57:41.760 --> 57:46.000
should be fair, they should be explainable that they should all these things that you

57:46.000 --> 57:52.320
want the AI to have in order to not have these collateral negative aspects.

57:52.320 --> 57:58.000
But remember, the people who raised those critiques where dismissed as bio-conservatives

57:58.000 --> 58:01.120
way back at least 15 years ago, you were bio-conficertives.

58:01.120 --> 58:03.640
How can you raise those problems?

58:03.640 --> 58:07.960
All the people who spoke about the perils of technology were dismissed off-end.

58:07.960 --> 58:11.920
Now it turns out that they were closer to the truth probably than they were.

58:11.920 --> 58:18.000
And then they all, it's a tough balance because you don't want to say, okay, no more technology,

58:18.000 --> 58:19.160
no more advancement.

58:19.160 --> 58:21.040
We are happy the way it is.

58:21.040 --> 58:22.040
Let's stay here.

58:22.040 --> 58:28.120
On the other hand, there are things that are constant improvements, what you were talking

58:28.120 --> 58:33.560
about and what they're doing now, for example, in psychiatry where somebody gives blood and

58:33.560 --> 58:38.920
you can tell from the blood what medications will be working properly or better than others,

58:38.920 --> 58:45.080
what they would metabolize better than others, so that these are useful.

58:45.080 --> 58:52.240
But there is, however, the issue that I think you were alluding to is are there other things

58:52.240 --> 58:56.600
that could be dangerous and is it possible to prevent those?

58:56.600 --> 59:00.960
And I think some may be so and some may be hard to tell.

59:00.960 --> 59:07.360
But I think that now we are in a better position than 15 years ago because 15 years ago, the

59:07.360 --> 59:12.280
silos of different disciplines were much more well-defined.

59:12.280 --> 59:17.040
You know, technologies to one side, sociologists on the other side, economists on the other

59:17.040 --> 59:20.840
side, and they were rarely talking to each other.

59:20.840 --> 59:26.120
Now I regularly work and organize events with all these people.

59:26.120 --> 59:32.960
And the high people together with psychologists, sociologists, you know, and see all of these.

59:32.960 --> 59:35.960
How many humanists do you include in the interdisciplinary conversation?

59:35.960 --> 59:37.360
Maybe we should have more.

59:37.360 --> 59:40.800
So what I'm saying is that I really see a change in that.

59:40.800 --> 59:46.120
And that's what makes me optimistic that we can solve because the high people can find

59:46.120 --> 59:51.760
maybe technical solutions to issues, but the definition and the understanding and identification

59:51.760 --> 59:54.240
of the issues should be done together with everybody else.

59:54.240 --> 59:58.640
Okay, so if you bring the humanist perspective into, and I'm very happy to hear what you

59:58.640 --> 01:00:03.840
just said, but if you bring the humanist perspective, the humanist scholar into this conversation,

01:00:03.840 --> 01:00:06.360
into the mix, and the humanist would say, you know what?

01:00:06.360 --> 01:00:15.560
There is a dimension which is nonmeasurable, non-allegrizable, non-reductionable or nonreducable.

01:00:15.560 --> 01:00:17.160
What do you do with that?

01:00:17.160 --> 01:00:18.840
Are you saying to this guy, you know what?

01:00:18.840 --> 01:00:19.840
This is nonsense.

01:00:19.840 --> 01:00:24.480
Or are you saying, no, I really have to take you seriously.

01:00:24.480 --> 01:00:31.440
Let me figure out how the non-observable, the non-reducable, how do I fit it into my analysis?

01:00:31.440 --> 01:00:32.440
That's a challenge.

01:00:32.440 --> 01:00:35.240
Go ahead and tell us what you think that is.

01:00:35.240 --> 01:00:36.240
Yeah, one example.

01:00:36.240 --> 01:00:39.640
Well, you wrote a book called that by whatever names, right?

01:00:39.640 --> 01:00:45.160
So, I'm happy to talk about a soul as an emergent phenomenon.

01:00:45.160 --> 01:00:47.240
Okay, so talk about the soul.

01:00:47.240 --> 01:00:48.240
That's fine.

01:00:48.240 --> 01:00:49.240
I don't know how you call it.

01:00:49.240 --> 01:00:50.240
You can call it soul.

01:00:50.240 --> 01:00:51.240
You can call it psyche.

01:00:51.240 --> 01:00:55.920
You can call it all sorts of things, which will be problematic for the people who are doing

01:00:55.920 --> 01:00:57.360
this kind of work.

01:00:57.360 --> 01:01:01.480
And all I'm saying that I would like to see these people at least admit the possibility

01:01:01.480 --> 01:01:04.520
of the nonreducable dimension.

01:01:04.520 --> 01:01:08.120
The non-materialistic, the non-allegory ziable, the non-mathematical.

01:01:08.120 --> 01:01:09.600
You cannot mathesize that.

01:01:09.600 --> 01:01:29.200
Why are you saying non-mat

01:01:29.200 --> 01:01:35.440
of the non-mathematical.

01:01:35.440 --> 01:01:36.440
You see that there's no contribution.

01:01:36.440 --> 01:01:38.840
One book on AI today, almost.

01:01:38.840 --> 01:01:40.280
Yeah, I believe you.

01:01:40.280 --> 01:01:45.720
So this particular book, he's arguing that the AI project has been mistaken because it

01:01:45.720 --> 01:01:48.880
was basically based on mind-body dualism.

01:01:48.880 --> 01:01:52.900
And what he's telling us that we need to go back from a platonic position which under

01:01:52.900 --> 01:01:58.640
Giro did the entire project to a Aristotelian position that takes the body into account

01:01:58.640 --> 01:01:59.640
from the very beginning.

01:01:59.640 --> 01:02:04.120
In other words, the AI, the artificial intelligence has to think through the body or like the

01:02:04.120 --> 01:02:10.240
body thinks, not like the mind thinks as we think the mind thinks, but as the body thinks.

01:02:10.240 --> 01:02:12.040
It's a very interesting argument.

01:02:12.040 --> 01:02:15.880
But if that's the case, it goes against what transhumanism is trying to accomplish.

01:02:15.880 --> 01:02:18.760
Two things I want to say.

01:02:18.760 --> 01:02:28.000
Next is that what AI was conceived of 30 or 40 years ago was the AI project, as you call

01:02:28.000 --> 01:02:29.000
it.

01:02:29.000 --> 01:02:31.240
I think it's different than what signed.

01:02:31.240 --> 01:02:35.840
I mean, scientists are just basically, it doesn't matter if the artificial intelligence

01:02:35.840 --> 01:02:39.440
algorithms match what actually happens in the brain.

01:02:39.440 --> 01:02:42.920
It's whatever works to be able to solve problems.

01:02:42.920 --> 01:02:43.920
Problem solving, that's all it is.

01:02:43.920 --> 01:02:51.720
The typical analogy that is at the beginning of every AI textbook is that planes fly,

01:02:51.720 --> 01:02:54.680
but they don't fly like bears fly.

01:02:54.680 --> 01:02:56.960
They don't fall fair.

01:02:56.960 --> 01:02:58.960
But still, they do fly.

01:02:58.960 --> 01:03:07.080
So, it's not necessary to have some form of intelligence or to be able to help ourselves

01:03:07.080 --> 01:03:11.600
to be monitored that they have to replicate our way of doing it.

01:03:11.600 --> 01:03:16.240
I think the point is, we use AI now.

01:03:16.240 --> 01:03:22.440
We're not trying to replicate and challenge, to solve problems.

01:03:22.440 --> 01:03:26.120
The second thing I wanted to come back to is the transhumanism.

01:03:26.120 --> 01:03:34.560
And just the notion of what Billy said that we're always transitioning human-wise.

01:03:34.560 --> 01:03:40.200
I mean, 100,000 years ago, people didn't have writing yet.

01:03:40.200 --> 01:03:47.720
They didn't have the brains that were able to produce novels or anything like that.

01:03:47.720 --> 01:03:52.520
I mean, they didn't wear clothes until 70 or 80,000 years ago.

01:03:52.520 --> 01:03:55.080
And this evolution took place.

01:03:55.080 --> 01:03:58.080
And so, humans have been transitioning.

01:03:58.080 --> 01:04:06.320
The difference is that transhumanism is for what they call a directed evolution.

01:04:06.320 --> 01:04:07.320
It's not evolution.

01:04:07.320 --> 01:04:08.480
It's directed evolution.

01:04:08.480 --> 01:04:09.800
It's controlled evolution.

01:04:09.800 --> 01:04:12.560
It's actually accelerated evolution.

01:04:12.560 --> 01:04:14.760
That's the whole point of evolution.

01:04:14.760 --> 01:04:16.400
The question is, who's directing it?

01:04:16.400 --> 01:04:17.400
So, I have a question.

01:04:17.400 --> 01:04:21.680
Tenshu read the books by Tenshu, if you know Tenshu's work.

01:04:21.680 --> 01:04:25.360
And he's going to create what he calls Kobe's.

01:04:25.360 --> 01:04:26.360
Kobe's art.

01:04:26.360 --> 01:04:32.040
But I think it's really critical to ask the question when you say it's directed evolution.

01:04:32.040 --> 01:04:35.320
I think directed by a society, like in the...

01:04:35.320 --> 01:04:36.320
Actually, that's...

01:04:36.320 --> 01:04:42.080
Well, I would say it's directed by those who take control of the process of engineering.

01:04:42.080 --> 01:04:43.280
It's the engineer.

01:04:43.280 --> 01:04:46.960
Those who will engineer the process, they will be in the position of directing it.

01:04:46.960 --> 01:04:51.880
So, I want to take a big history perspective on this.

01:04:51.880 --> 01:04:55.320
The sun is going to be a reliable partner on...

01:04:55.320 --> 01:04:59.480
For complexity on Earth for about 2 billion years.

01:04:59.480 --> 01:05:02.040
And that's a really big future.

01:05:02.040 --> 01:05:08.040
There's no reason, based on what we know about the past, to think that our species unchanged

01:05:08.040 --> 01:05:09.800
will be here forever.

01:05:09.800 --> 01:05:12.200
I mean, we might be here for...

01:05:12.200 --> 01:05:15.240
Our descendants might be here for 2 billion years.

01:05:15.240 --> 01:05:21.440
They have a possibility of living on this planet without science fiction traveling late

01:05:21.440 --> 01:05:24.040
years with technology that we don't have.

01:05:24.040 --> 01:05:25.680
And they never have.

01:05:25.680 --> 01:05:28.480
So the planet is a pretty amazing place.

01:05:28.480 --> 01:05:33.120
And it's already had a really amazing journey in our species, but it's problems.

01:05:33.120 --> 01:05:34.120
But what...

01:05:34.120 --> 01:05:40.440
I mean, isn't it just take it for granted that we're going to evolve into something different?

01:05:40.440 --> 01:05:45.840
And they may or may not call themselves humans.

01:05:45.840 --> 01:05:52.880
We're already a domesticated species compared to 100,000 years ago.

01:05:52.880 --> 01:05:55.200
And how it happens, even if it's...

01:05:55.200 --> 01:05:59.160
It's still has to work.

01:05:59.160 --> 01:06:01.600
It has to be selected.

01:06:01.600 --> 01:06:04.640
It has to have function.

01:06:04.640 --> 01:06:08.520
A certain amount of dysfunction will be in there too, for sure.

01:06:08.520 --> 01:06:10.560
And there'll be unintended consequences.

01:06:10.560 --> 01:06:19.040
But I don't want to see it through the lens of utopia or dystopia, because I think those

01:06:19.040 --> 01:06:23.280
are dangerous tropes to get into.

01:06:23.280 --> 01:06:29.680
But the point is that, of course, we're going to evolve.

01:06:29.680 --> 01:06:31.200
But there's a difference.

01:06:31.200 --> 01:06:32.760
Evolution is a process.

01:06:32.760 --> 01:06:34.080
It's a very slow process.

01:06:34.080 --> 01:06:35.080
It's a process that...

01:06:35.080 --> 01:06:40.080
Nobody knows really what's going on, but they claim that they can control the process.

01:06:40.080 --> 01:06:41.480
I don't want them to control.

01:06:41.480 --> 01:06:43.480
No, but that's not a really professional.

01:06:43.480 --> 01:06:46.480
It's totally a crucial, more generally.

01:06:46.480 --> 01:06:49.400
The process is based on reproduction.

01:06:49.400 --> 01:06:51.680
Who controls reproduction?

01:06:51.680 --> 01:06:53.960
The women will reproduce, right?

01:06:53.960 --> 01:06:55.960
As long as the women have the freedom to reproduce.

01:06:55.960 --> 01:06:56.960
That could be true.

01:06:56.960 --> 01:06:57.960
It's not the engineers.

01:06:57.960 --> 01:06:59.920
I mean, I'm in this field, right?

01:06:59.920 --> 01:07:02.560
I know it's women controlling.

01:07:02.560 --> 01:07:07.120
If they want to give their child a gene which prevents disease, or they want to avoid...

01:07:07.120 --> 01:07:08.600
It means the women are doing it.

01:07:08.600 --> 01:07:10.480
Now, that's one way, right?

01:07:10.480 --> 01:07:16.200
So, you know, directed evolution could be each individual woman or couple.

01:07:16.200 --> 01:07:19.480
Culture has been directing human mating for a long time.

01:07:19.480 --> 01:07:21.480
Yeah, but I think you're talking about...

01:07:21.480 --> 01:07:23.480
You're talking about a societal...

01:07:23.480 --> 01:07:27.760
But on this point, I mean, just because this is a great immediate segue and then please

01:07:27.760 --> 01:07:34.840
go on, but that's where Leon Cass and Hans Jonas come in.

01:07:34.840 --> 01:07:37.680
Because they would tell you that...

01:07:37.680 --> 01:07:39.760
I mean, Leon Cass wrote an interesting book.

01:07:39.760 --> 01:07:40.760
I don't agree.

01:07:40.760 --> 01:07:44.880
I was his TA for a while, but it's called Toward a More Natural Science.

01:07:44.880 --> 01:07:51.160
And his argument is that all of the things we really think matter in the quality of our

01:07:51.160 --> 01:07:57.360
human experience at the family and at the level of community, all of it somehow has to

01:07:57.360 --> 01:08:00.840
do with the evolution of empathy.

01:08:00.840 --> 01:08:06.120
As Aldous Huxley said at the end of his life, when someone asked him for some advice for

01:08:06.120 --> 01:08:09.720
the younger generation, he said, well, it's a little embarrassing because I've written

01:08:09.720 --> 01:08:15.240
so many books, but I would say try to be a little kinder.

01:08:15.240 --> 01:08:23.440
His compassion, empathy, attentive listening, all these kinds of virtues are really critical.

01:08:23.440 --> 01:08:25.120
And where do they emerge from?

01:08:25.120 --> 01:08:31.040
Well, evolutionary biologists and evolutionary psychologists will spin this a little differently,

01:08:31.040 --> 01:08:40.440
but fundamentally from the fact that we are a reproductive species, that humans invest

01:08:40.440 --> 01:08:47.000
greatly in their offspring, who are dependent for very protracted periods of time compared

01:08:47.000 --> 01:08:48.800
to any other species.

01:08:48.800 --> 01:08:54.440
And so a lot of these things that we view, you know, altruism and so forth, all the noble

01:08:54.440 --> 01:09:02.640
aspects of the human endeavor really emerge from this kind of turning over of the generations.

01:09:02.640 --> 01:09:09.920
So if you move, he would say, toward something that is strongly anti-aging.

01:09:09.920 --> 01:09:15.160
For example, in Japan, by the way, they are living really long, but they're not reproducing

01:09:15.160 --> 01:09:16.160
much.

01:09:16.160 --> 01:09:20.320
Well, that's why the population is going to be opposite of what people thought.

01:09:20.320 --> 01:09:22.160
Yeah, and so the investment, correct?

01:09:22.160 --> 01:09:23.160
But because of that?

01:09:23.160 --> 01:09:26.400
Well, parental investment, well, I won't go.

01:09:26.400 --> 01:09:27.400
That's not quite the question.

01:09:27.400 --> 01:09:33.640
I mean, the issue is that so much of this is what Tolkien pointed out.

01:09:33.640 --> 01:09:42.840
This is why, you know, Erwin separates herself from the immortality of the elves so that

01:09:42.840 --> 01:09:46.080
she can experience the love of a child and so forth.

01:09:46.080 --> 01:09:47.320
So that's really important.

01:09:47.320 --> 01:09:49.920
That's a natural evolutionary dimension.

01:09:49.920 --> 01:09:57.200
It's not, and for Cass and for Hans Jonas, that dimension is something that is imperiled

01:09:57.200 --> 01:10:03.360
by anti-aging technologies, which would be humanly directed.

01:10:03.360 --> 01:10:09.320
Now, I'm not a, but understand, I'm so concerned about these chronic illnesses associated with

01:10:09.320 --> 01:10:13.000
old age, that I can actually tolerate a little transhumanism.

01:10:13.000 --> 01:10:17.160
I see that you're going with it, but you forget what they really want.

01:10:17.160 --> 01:10:18.840
Let's go back to the telos.

01:10:18.840 --> 01:10:27.080
The telos is, I'll put it very bluntly, it's to make the human biological species obsolete.

01:10:27.080 --> 01:10:28.080
That's what it's about.

01:10:28.080 --> 01:10:32.720
It's about the planned obsolescence of the human species.

01:10:32.720 --> 01:10:37.600
What it is for Ted Shoe, for Giulio Prisco, for Nick Bostrom, for all the people who write

01:10:37.600 --> 01:10:39.400
in that genre.

01:10:39.400 --> 01:10:42.720
You can say, okay, they don't really matter because they don't really do the science.

01:10:42.720 --> 01:10:46.000
They just popularize a certain kind of myth.

01:10:46.000 --> 01:10:47.000
What difference does it make?

01:10:47.000 --> 01:10:51.040
It makes a lot of difference because it affects the culture.

01:10:51.040 --> 01:10:52.040
Even if ideologies are true.

01:10:52.040 --> 01:10:53.040
It makes a lot of difference.

01:10:53.040 --> 01:10:54.040
I don't read those people.

01:10:54.040 --> 01:10:55.040
I have to say, I know that these forces were made out of.

01:10:55.040 --> 01:10:56.040
Why?

01:10:56.040 --> 01:10:57.040
Why do you think?

01:10:57.040 --> 01:11:02.320
I don't read those people.

01:11:02.320 --> 01:11:05.600
Why do you think humans as currently defined?

01:11:05.600 --> 01:11:09.720
Why do you think it's so essential that we stay the way that we live?

01:11:09.720 --> 01:11:11.200
That's a bad idea.

01:11:11.200 --> 01:11:15.360
But why does the species have to stay the way that we open up for questions?

01:11:15.360 --> 01:11:16.880
Then we open up for questions.

01:11:16.880 --> 01:11:21.000
Here is, I guess, it depends kind of where you come from.

01:11:21.000 --> 01:11:22.680
I come from the Jewish tradition.

01:11:22.680 --> 01:11:26.520
In the Jewish tradition, as well as in the Islamic and the Christian traditions, we do

01:11:26.520 --> 01:11:28.760
talk about creation in the image of God.

01:11:28.760 --> 01:11:32.320
One argument you can dismiss it, you can say, who cares about what religions say.

01:11:32.320 --> 01:11:35.840
But if you do care about what religions say, you can say, well, there's something really

01:11:35.840 --> 01:11:37.640
precious about being human.

01:11:37.640 --> 01:11:43.640
Being human comes with all those vulnerabilities and limitations and kind of words and all.

01:11:43.640 --> 01:11:45.000
We are not perfect.

01:11:45.000 --> 01:11:46.000
It's not about perfection.

01:11:46.000 --> 01:11:47.000
We never will be perfect.

01:11:47.000 --> 01:11:49.000
Thank God we are not going to be perfect.

01:11:49.000 --> 01:11:51.680
We should aspire for that kind of perfection.

01:11:51.680 --> 01:11:55.520
We should aspire more to perfection along the lines that Steve proposed, which is the

01:11:55.520 --> 01:11:58.480
tradition of Aristotelian and religious tradition.

01:11:58.480 --> 01:12:02.240
Talk about virtue, talk about character, talk about morality.

01:12:02.240 --> 01:12:04.160
In that sense, there's an offering.

01:12:04.160 --> 01:12:07.760
There's a way religion has always been against the kind of progress that we have.

01:12:07.760 --> 01:12:09.440
No, that's absolutely not true.

01:12:09.440 --> 01:12:11.400
Not in the middle ages, for sure.

01:12:11.400 --> 01:12:14.640
I will have to take issue with you here.

01:12:14.640 --> 01:12:18.920
Actually, many of the people who were doing the forefront of science in the 13th and 14th

01:12:18.920 --> 01:12:20.520
century were all religious people.

01:12:20.520 --> 01:12:22.520
How were they handled?

01:12:22.520 --> 01:12:26.520
I know they were religious, but how were they accepted?

01:12:26.520 --> 01:12:29.520
The most fantastic thing.

01:12:29.520 --> 01:12:31.320
Yes, how was Gaggilio?

01:12:31.320 --> 01:12:34.680
And he was doing the same thing.

01:12:34.680 --> 01:12:36.000
My manna is the same thing.

01:12:36.000 --> 01:12:39.520
It's not the case that the religious person was always dismissed by...

01:12:39.520 --> 01:12:40.520
No, no, no.

01:12:40.520 --> 01:12:42.520
But that was before Darwin.

01:12:42.520 --> 01:12:44.520
That was before Darwin.

01:12:44.520 --> 01:12:49.520
Fair enough, Darwinism is an important turning point.

01:12:49.520 --> 01:12:55.520
The realization that 100,000 years ago, the human species was different than it is today.

01:12:55.520 --> 01:12:57.520
That didn't come in...

01:12:57.520 --> 01:12:59.520
Notion did not come to existence.

01:12:59.520 --> 01:13:04.400
What I was saying, by the way, doesn't have much to do with religion.

01:13:04.400 --> 01:13:09.960
I remember looking back now, 30 years, when my wife and I had our first child, we went

01:13:09.960 --> 01:13:10.960
home.

01:13:10.960 --> 01:13:15.280
My older sister observed me for a while.

01:13:15.280 --> 01:13:19.480
A couple of days into this, she said, you know, Stevie, I never really liked you.

01:13:19.480 --> 01:13:21.480
Until you became a father.

01:13:21.480 --> 01:13:23.680
Which is a great compliment.

01:13:23.680 --> 01:13:31.240
But what she saw was that somehow my being had expanded in the care of a child.

01:13:31.240 --> 01:13:32.240
And there was a growth.

01:13:32.240 --> 01:13:34.760
Now I don't think that's everybody's journey by any means.

01:13:34.760 --> 01:13:37.000
Don't get me wrong.

01:13:37.000 --> 01:13:45.280
But simply stated, many of the key assets that we feel make a human life meaningful and

01:13:45.280 --> 01:13:51.720
not narcissistic, not I-it, but I-vow, are related to the natural evolutionary trajectory.

01:13:51.720 --> 01:13:55.520
Now when you get an Aldous Huxley talking about a brave new world, where you sort of

01:13:55.520 --> 01:14:01.880
engineered that it is dystopian out of the picture, then what do you have left?

01:14:01.880 --> 01:14:04.880
And that was the question.

01:14:04.880 --> 01:14:10.440
So again, but I don't want to go any further with it, but I do think it doesn't hang on

01:14:10.440 --> 01:14:11.440
a religious argument.

01:14:11.440 --> 01:14:13.280
Let's take some questions from the audience.

01:14:13.280 --> 01:14:14.280
Wonderful.

01:14:14.280 --> 01:14:15.280
I'll go around.

01:14:15.280 --> 01:14:16.280
I'm going to start on this side.

01:14:16.280 --> 01:14:17.280
I ask you to be brief.

01:14:17.280 --> 01:14:19.280
If you're going to make a comment, make it short.

01:14:19.280 --> 01:14:21.880
If you're going to make a question.

01:14:21.880 --> 01:14:24.280
And then I'll go-yes, I'm sorry.

01:14:24.280 --> 01:14:25.280
Go ahead, please.

01:14:25.280 --> 01:14:26.280
That's right.

01:14:26.280 --> 01:14:27.280
Go ahead, Steve.

01:14:27.280 --> 01:14:28.280
Go ahead, Steve.

01:14:28.280 --> 01:14:29.280
I'll post your name.

01:14:29.280 --> 01:14:30.280
Stuart Danbrought.

01:14:30.280 --> 01:14:32.280
Just brief background.

01:14:32.280 --> 01:14:33.280
Brief.

01:14:33.280 --> 01:14:38.960
Research Fellow at the Artificial General Intelligence Society Research Fellow at Brain Machine Interface

01:14:38.960 --> 01:14:39.960
Consortium.

01:14:39.960 --> 01:14:45.720
I do work with autonomous systems at IEEE.

01:14:45.720 --> 01:14:51.840
I'm ridiculously interdisciplinary and I also focus a lot on ethics.

01:14:51.840 --> 01:14:57.800
So it's kind of a basic question, really, because we don't have time to go into the

01:14:57.800 --> 01:14:59.640
technical details.

01:14:59.640 --> 01:15:08.000
But if, for example, Haba, you had a young person in your family or someone who was still

01:15:08.000 --> 01:15:15.160
in utero and it was known that they were going to express with, let's say, a fatal heart

01:15:15.160 --> 01:15:16.160
condition.

01:15:16.160 --> 01:15:21.320
But CRISPR-Cas9 could be used to prevent that.

01:15:21.320 --> 01:15:22.960
Would that be something you'd be comfortable with?

01:15:22.960 --> 01:15:26.000
Yeah, actually I'm going to speak precisely on that issue.

01:15:26.000 --> 01:15:33.120
Okay, so now how is that different when writ large of the transhumanism discussion we're

01:15:33.120 --> 01:15:34.120
having?

01:15:34.120 --> 01:15:35.120
Yeah.

01:15:35.120 --> 01:15:42.280
It's already been done, a paper about two months ago in utero child had a genetic anomaly

01:15:42.280 --> 01:15:45.120
that was going to express with heart failure.

01:15:45.120 --> 01:15:51.440
At some point in his or her, I forget the sex of the fetus, young life.

01:15:51.440 --> 01:15:59.560
So why is that not extensible to the major objections that you've been raising in this

01:15:59.560 --> 01:16:00.560
discussion?

01:16:00.560 --> 01:16:07.480
That goes back to the issue of the separation or at least raising the awareness.

01:16:07.480 --> 01:16:09.560
There's a difference between therapy and enhancement.

01:16:09.560 --> 01:16:14.520
So by the way, from the Jewish perspective, Jewish perspective is very pro biotechnology,

01:16:14.520 --> 01:16:19.440
very pro-crispuh, including the very orthodox and even ultra orthodox.

01:16:19.440 --> 01:16:25.200
They're very much for using gene editing or genome editing in order to solve that particular

01:16:25.200 --> 01:16:27.200
kind of problem.

01:16:27.200 --> 01:16:32.200
I think there's still a difference between that and enhancement, the way that Nick Bostrom

01:16:32.200 --> 01:16:33.560
and the rest of them speak about.

01:16:33.560 --> 01:16:35.560
So let me ask, so I...

01:16:35.560 --> 01:16:40.960
So but there must be a threshold, I mean, because otherwise if you say, oh, we are human

01:16:40.960 --> 01:16:46.680
also because our vulnerabilities are limits and so on, then you will not even cure any

01:16:46.680 --> 01:16:48.840
disease because it's part of you.

01:16:48.840 --> 01:16:52.480
So there must be a threshold that where do you put that threshold?

01:16:52.480 --> 01:16:54.160
Well, so here's the problem.

01:16:54.160 --> 01:17:01.920
The problem is that, you know, taking off of this crisper, it may be possible to change

01:17:01.920 --> 01:17:07.920
the genes of a human being such that a child would be born with a heart that was stronger

01:17:07.920 --> 01:17:11.600
and longer lasting than a normal human heart.

01:17:11.600 --> 01:17:13.720
Same thing with every other organ.

01:17:13.720 --> 01:17:17.000
Now that's enhancement, right?

01:17:17.000 --> 01:17:19.720
So that, I agree with you that it's tricky here.

01:17:19.720 --> 01:17:27.360
But I don't have a clear thing, but I don't like is the narrative, the enhancement narrative,

01:17:27.360 --> 01:17:36.000
the myth that we've been fed by those shall we say, prophets of transhumanism as if that

01:17:36.000 --> 01:17:38.200
would be the solution to all the problems.

01:17:38.200 --> 01:17:44.680
That's all, I'll say, we have to go much more, I'll try an error and much more one at a time.

01:17:44.680 --> 01:17:48.320
Here's an example of something that, yes, the position would be positive.

01:17:48.320 --> 01:17:50.320
The unbelievable.

01:17:50.320 --> 01:17:51.320
Right.

01:17:51.320 --> 01:17:53.320
And we could prevent it.

01:17:53.320 --> 01:17:55.320
We could change that.

01:17:55.320 --> 01:17:56.320
Yeah.

01:17:56.320 --> 01:17:57.320
Yeah.

01:17:57.320 --> 01:18:06.320
But, but, you know, Nick Prostam used to say, let's try to self-stoip it in the end.

01:18:06.320 --> 01:18:09.320
We have about six or seven hands up to working, moving.

01:18:09.320 --> 01:18:10.320
Yes.

01:18:10.320 --> 01:18:11.320
Stand up.

01:18:11.320 --> 01:18:12.920
My name is Todd Esig.

01:18:12.920 --> 01:18:15.800
I'm a psychoanalyst here in the city.

01:18:15.800 --> 01:18:21.880
And I really appreciated the comments about incrementalism and making things work.

01:18:21.880 --> 01:18:28.440
And I want to bring up an issue of the fact that every enhancement always has a downside.

01:18:28.440 --> 01:18:33.480
It's impossible to have a technological advance that doesn't have a loss as well as a gain.

01:18:33.480 --> 01:18:38.200
And that one of the front lines for seeing the losses of technologies are our practices.

01:18:38.200 --> 01:18:45.640
And for example, I see many people who are using Adderall for enhancement purposes and

01:18:45.640 --> 01:18:49.040
their careers and relationships are being destroyed.

01:18:49.040 --> 01:18:54.040
Like your grandson, we see many people who are having their communications kind of enhanced

01:18:54.040 --> 01:18:59.520
by communications technology and their capacity for intimacy is being destroyed.

01:18:59.520 --> 01:19:03.960
So I'd like the people who are kind of involved in the technological front lines to comment

01:19:03.960 --> 01:19:17.760
on how we can better communicate with you.

01:19:17.760 --> 01:19:23.240
So much of what medical ethics has been about, you know, there was a time when it was a horrible

01:19:23.240 --> 01:19:27.840
idea to think that someone didn't have to die with a tube in every orifice, natural

01:19:27.840 --> 01:19:29.400
and unnatural.

01:19:29.400 --> 01:19:35.400
And now 70% of people in hospitals die after treatment has been tried and withdrawn.

01:19:35.400 --> 01:19:40.880
And so we make progress and we have committees and we do case consultations.

01:19:40.880 --> 01:19:48.840
And so you begin to get a handle through experience and no, nobody can anticipate all of these

01:19:48.840 --> 01:19:52.280
possibilities coming on down the truck, the track.

01:19:52.280 --> 01:19:57.400
But as you deal with them as an active agent, you can make successes.

01:19:57.400 --> 01:20:03.440
But certainly, I mean, I really recommend Delaney Rustin's movie Screenagers, which

01:20:03.440 --> 01:20:10.120
has been all over the country, just helping families begin to deal process-wise and psychologically

01:20:10.120 --> 01:20:15.400
with the struggle of getting some control over the kid's screen time, you know, which

01:20:15.400 --> 01:20:20.960
I guess was a problem when I was a boy because we wanted to watch TV all the time.

01:20:20.960 --> 01:20:21.960
Right.

01:20:21.960 --> 01:20:29.680
So throw in a comment, a lot of this stuff, the profit that drives it, is designed to

01:20:29.680 --> 01:20:33.800
appeal to like our brain stem, our hunter-gatherer brain.

01:20:33.800 --> 01:20:37.400
It's meant to be addictive intentionally.

01:20:37.400 --> 01:20:46.920
And so it's partly in the design and the profit motive of Google or Facebook or whatever,

01:20:46.920 --> 01:20:50.200
to make these devices and their utilities.

01:20:50.200 --> 01:20:51.760
That's Ken's son of Brookhaven.

01:20:51.760 --> 01:20:53.800
He wanted to finish up one comment.

01:20:53.800 --> 01:20:59.600
Yeah, in the medical area, there are M&M conferences and processes are built in.

01:20:59.600 --> 01:21:04.480
I'm not so sure those processes are currently built into AI development, into genetics

01:21:04.480 --> 01:21:05.480
development.

01:21:05.480 --> 01:21:09.840
And so the question is how can those processes be built in with those of us on the front

01:21:09.840 --> 01:21:14.640
lines, being the downside of technology, be part of your creative process?

01:21:14.640 --> 01:21:15.640
Yeah.

01:21:15.640 --> 01:21:21.480
I agree that in all this multidisciplinary initiative that I'm involved, I didn't see

01:21:21.480 --> 01:21:29.520
much presence of those like you that see the fact, may see some effects of the technology.

01:21:29.520 --> 01:21:34.960
So definitely I think that that should be more present.

01:21:34.960 --> 01:21:40.520
And like we, for example, we have put together something which is called the partnership on

01:21:40.520 --> 01:21:46.080
AI that started from six companies, but now as more than 60 partners of which only 30

01:21:46.080 --> 01:21:47.080
percent are companies.

01:21:47.080 --> 01:21:50.160
And then everybody else, you know, from various disciplines.

01:21:50.160 --> 01:21:56.560
But I don't think there is any initiative or entity or organization that has to do with

01:21:56.560 --> 01:22:00.360
SAC one analysis and the effect of the technology.

01:22:00.360 --> 01:22:02.880
So that definitely is something that should be there.

01:22:02.880 --> 01:22:08.680
And the goal is to build together best practices in designing and developing AI.

01:22:08.680 --> 01:22:12.200
So that, you know, these negative effects are not there.

01:22:12.200 --> 01:22:13.840
And drawing the right boundaries.

01:22:13.840 --> 01:22:15.560
Boundary creation is key.

01:22:15.560 --> 01:22:21.360
You know, I mean, the reason why positions have the highest suicide rate of any profession

01:22:21.360 --> 01:22:27.480
per capita in the country right now and why 50 percent of them respond from coast to coast

01:22:27.480 --> 01:22:32.520
to surveys about satisfaction saying they quit if they could afford to.

01:22:32.520 --> 01:22:35.480
I mean, this is serious stuff.

01:22:35.480 --> 01:22:40.560
You know, a lot of it is because, you know, let's take email, just take email, let alone

01:22:40.560 --> 01:22:42.360
electronic medical records.

01:22:42.360 --> 01:22:46.120
They're not connecting empathically with their patients so they lose meaning.

01:22:46.120 --> 01:22:48.040
It becomes depersonalized.

01:22:48.040 --> 01:22:50.640
And also they can't get away from it.

01:22:50.640 --> 01:22:53.480
The email is a machine, that's a machine.

01:22:53.480 --> 01:22:54.720
They're a human being.

01:22:54.720 --> 01:22:55.960
They need time away.

01:22:55.960 --> 01:22:57.640
They need to balance, but they can't get it.

01:22:57.640 --> 01:23:03.640
And so in that sense, boundary drawing in all these different areas is what's really

01:23:03.640 --> 01:23:07.080
important and we're not very good at it sometimes.

01:23:07.080 --> 01:23:10.880
First, I'm going to remove the other side.

01:23:10.880 --> 01:23:18.600
One thing I'm surprised that I didn't get into is the coming deep integration of DNA

01:23:18.600 --> 01:23:23.720
from among different organisms, which is already happening.

01:23:23.720 --> 01:23:26.320
You know, chimeric organisms are already here.

01:23:26.320 --> 01:23:30.440
But we're now getting to the point where we're going to have deeper integration of human

01:23:30.440 --> 01:23:35.960
DNA with that of other, you know, not just pigs in order to grow kidneys, which Bob

01:23:35.960 --> 01:23:40.600
Langer is doing up at MIT, but also, you know, we're going to get to the creation of new

01:23:40.600 --> 01:23:46.480
beings, the integration of human DNA with that of other non-humans.

01:23:46.480 --> 01:23:51.200
And we will begin to see things that haven't existed before that have various characteristics

01:23:51.200 --> 01:23:52.440
good and bad.

01:23:52.440 --> 01:23:54.560
So sort of curious.

01:23:54.560 --> 01:23:57.800
That goes back to their environmental argument that I made.

01:23:57.800 --> 01:23:58.800
Right?

01:23:58.800 --> 01:24:06.840
So environmentalist are really against that kind of, that's a whole debate about GMOs,

01:24:06.840 --> 01:24:07.840
right?

01:24:07.840 --> 01:24:11.080
So from environmental perspective, at least it's, I would go against it.

01:24:11.080 --> 01:24:17.800
So I mean, I think there's a species to reference there.

01:24:17.800 --> 01:24:24.120
I mean, if you look, actually look at chimpanzee DNA, most genes are identical to human genes.

01:24:24.120 --> 01:24:25.120
They're not human DNA.

01:24:25.120 --> 01:24:29.880
I think it's still 1% or what's a hunch?

01:24:29.880 --> 01:24:30.880
Chimpanzees.

01:24:30.880 --> 01:24:36.480
In the genes, less than 0.1% difference.

01:24:36.480 --> 01:24:38.480
And the conclusion, the philosophical...

01:24:38.480 --> 01:24:43.280
Well, I mean, what you said was what you're saying is we're going to take information.

01:24:43.280 --> 01:24:47.560
I mean, every organ is, every living species is connected to other species.

01:24:47.560 --> 01:24:52.720
So, so these I already showed that there's something wrong with a completely geneticist

01:24:52.720 --> 01:24:58.680
bottom-up approach because we're not, we're significantly different than chimpanzees.

01:24:58.680 --> 01:25:01.360
Ah, 0.1% just 3,000,000.

01:25:01.360 --> 01:25:09.160
But you know, I mean, if you go back to the new Atlantis, which is the definitive initial

01:25:09.160 --> 01:25:15.680
statement of the biological revolution, I mean, Bacon argued not only for fountains of

01:25:15.680 --> 01:25:19.920
youth, the waters of youth, but he argued for chimeras.

01:25:19.920 --> 01:25:20.920
And it's all there.

01:25:20.920 --> 01:25:22.600
It's all part of the vision.

01:25:22.600 --> 01:25:25.560
And I don't know that you can get...

01:25:25.560 --> 01:25:28.760
I think it's inevitable that we'll move in that direction personally.

01:25:28.760 --> 01:25:34.760
I think we're already incrementally halfway there, don't you?

01:25:34.760 --> 01:25:37.760
Oh, it's already happened.

01:25:37.760 --> 01:25:38.760
No.

01:25:38.760 --> 01:25:46.760
And if it can benefit, if it seems to be benefiting human beings, then mothers will do it.

01:25:46.760 --> 01:25:49.520
Oh, yeah, we'll do it.

01:25:49.520 --> 01:25:50.520
Thank you.

01:25:50.520 --> 01:25:53.040
Thank you very much.

01:25:53.040 --> 01:25:54.040
And it's a great conversation.

01:25:54.040 --> 01:25:56.240
Thank you very much for the one of the best here.

01:25:56.240 --> 01:26:03.800
And I just would like to point, I recently read an article about edging.

01:26:03.800 --> 01:26:09.880
Physics makes edging inevitable, not biology.

01:26:09.880 --> 01:26:15.520
It's in now tillus, a candidate that was published in, in, in, uh, laboratory.

01:26:15.520 --> 01:26:23.680
And on a scale, thermal physics guarantees our decline, no matter how many diseases we

01:26:23.680 --> 01:26:24.680
cure.

01:26:24.680 --> 01:26:26.680
It's not my opinion.

01:26:26.680 --> 01:26:37.840
So that are the, are the article in St. David's, you're a comedian about fake shortages.

01:26:37.840 --> 01:26:46.560
They, they, um, um, um, make people see and tell them they put, for example, a stand in

01:26:46.560 --> 01:26:47.960
the heart.

01:26:47.960 --> 01:26:55.200
And these actually make them as good as people with stance with real state stance.

01:26:55.200 --> 01:27:03.000
So even surgeries could help like, see, but if so, and, and so our integrations with machines

01:27:03.000 --> 01:27:06.280
also very questionable in this matter.

01:27:06.280 --> 01:27:12.880
And even, um, hip replacement also exercises and weight loss.

01:27:12.880 --> 01:27:20.880
In many experiments had much more effect and much, but effect there's hip replacement.

01:27:20.880 --> 01:27:29.560
And, and yes, and about AI and about our integration, migration about it.

01:27:29.560 --> 01:27:37.480
We don't discuss problem with, uh, phantom pain, for example, when people get those

01:27:37.480 --> 01:27:40.280
legs, then still have pain.

01:27:40.280 --> 01:27:47.840
Or do you, uh, understand when we, uh, we, uh, it's a question about embodiment, uh, when

01:27:47.840 --> 01:27:53.880
we get out our voice, uploaded some machine and we want to eat, we want to have sex or

01:27:53.880 --> 01:27:56.680
we want not just something to see.

01:27:56.680 --> 01:28:02.720
Don't you think that it will be the perfect reason when we can upload someone there and

01:28:02.720 --> 01:28:04.720
force leave them forever?

01:28:04.720 --> 01:28:05.720
Okay.

01:28:05.720 --> 01:28:07.720
Uh, the honest is on you.

01:28:07.720 --> 01:28:14.160
There are some cognitive, uh, capabilities that we don't have.

01:28:14.160 --> 01:28:20.560
So if you surround somebody with an incredible amount of data, our brain cannot handle that.

01:28:20.560 --> 01:28:25.960
Our brain, as we said, is very efficient, you know, but cannot handle that.

01:28:25.960 --> 01:28:31.720
So if you have to make a decision and you have a lot of data available to you, but you cannot

01:28:31.720 --> 01:28:37.200
handle it, you're going to make a decision with just a very small subset of, of information.

01:28:37.200 --> 01:28:42.400
And so that you're going to make a decision, but probably will not be as good or whatever

01:28:42.400 --> 01:28:48.680
good means in that context as if you could find patterns and information and knowledge

01:28:48.680 --> 01:28:50.600
from all that information.

01:28:50.600 --> 01:28:56.120
So that's something that is not placebo, I think, is something that cannot be replaced

01:28:56.120 --> 01:29:02.040
by something that we can do by ourselves.

01:29:02.040 --> 01:29:04.880
We have time now.

01:29:04.880 --> 01:29:05.880
Okay.

01:29:05.880 --> 01:29:09.760
I'm Giro, neuroscientist by training.

01:29:09.760 --> 01:29:17.680
Um, so in the face of the inevitability of the evolution of technology and the evolution

01:29:17.680 --> 01:29:26.680
of our species, which I think is a fact, um, I do see three major problem with the transhumanist

01:29:26.680 --> 01:29:36.240
movement, which is that overall it is rooted in low complexity thinking.

01:29:36.240 --> 01:29:38.800
And by low complexity thinking, I'm saying two things.

01:29:38.800 --> 01:29:46.240
First, it is reducing human reality to its biological component and its cognitive component,

01:29:46.240 --> 01:29:47.640
ignoring everything else.

01:29:47.640 --> 01:29:51.440
Which is really considering very little about what we are.

01:29:51.440 --> 01:30:00.400
The second also is like it's really classically, uh, offering simple, uh, causalities.

01:30:00.400 --> 01:30:05.440
And it's having a really bad time to understand every, every single action in a much more

01:30:05.440 --> 01:30:12.000
complex environment, which means that in the end, we have always this like, uh, conversation

01:30:12.000 --> 01:30:16.720
about what are the consequences of the thing you, you think is so great.

01:30:16.720 --> 01:30:23.560
And the step thing is that it's completely ignoring the own emotional development of

01:30:23.560 --> 01:30:25.200
human beings.

01:30:25.200 --> 01:30:32.200
And if you have, you know, average human beings, all they want is to be stronger to have a,

01:30:32.200 --> 01:30:37.560
uh, a body that's going to attract more mates, to have a bigger penis, bigger, bigger boobs.

01:30:37.560 --> 01:30:39.000
So we know human beings, right?

01:30:39.000 --> 01:30:42.640
We know what the kind of things we like when we want to be enhanced.

01:30:42.640 --> 01:30:45.800
So my question, my question is this.

01:30:45.800 --> 01:30:55.880
Is that is there any hope of, of sort of like, uh, a neo trans, you managed movement that

01:30:55.880 --> 01:31:04.760
would, yeah, yeah, whatever I said, um, that would actually do the things that it took

01:31:04.760 --> 01:31:10.880
you 15 years to understand, which is that you cannot, and I'm really not taking it personally,

01:31:10.880 --> 01:31:11.880
right?

01:31:11.880 --> 01:31:20.640
It's not, um, which is that we know that any, uh, technological improvement at that scale

01:31:20.640 --> 01:31:23.160
is going to have negative impact.

01:31:23.160 --> 01:31:28.680
So let's put together a transdisciplinary approach where we have like philosophers and

01:31:28.680 --> 01:31:31.480
psychologists and everything, but is that okay?

01:31:31.480 --> 01:31:32.480
Okay.

01:31:32.480 --> 01:31:34.560
So maybe you're going to say, yeah, it's a good idea, but do you think there is a real

01:31:34.560 --> 01:31:36.720
commitment in the world for that?

01:31:36.720 --> 01:31:38.640
Not just like a good intention in this room.

01:31:38.640 --> 01:31:44.560
But, I mean, there is commitment because there are initiatives like the one, like the

01:31:44.560 --> 01:31:48.440
partnership where, yeah, it's one, but there are many initiatives when there in the last

01:31:48.440 --> 01:31:55.440
two years, I think I've seen, I don't know, at least 10 or 20 Institute centers that are

01:31:55.440 --> 01:32:02.840
multidisciplinary and they study exactly this, you know, um, the fact that AI can, uh, has

01:32:02.840 --> 01:32:11.000
this goal you say of enhancing just our cognitive abilities and our, um, um, physical abilities.

01:32:11.000 --> 01:32:19.800
I mean, that's, that's, I mean, you are, you are speaking as if AI is what has been done

01:32:19.800 --> 01:32:21.920
for the last 50 years.

01:32:21.920 --> 01:32:23.880
And now it's done.

01:32:23.880 --> 01:32:24.880
It's not done.

01:32:24.880 --> 01:32:27.560
It's a, it continues revolving technology.

01:32:27.560 --> 01:32:36.040
Uh, after all is only 50 years old and it's really evolving and trying to understand how

01:32:36.040 --> 01:32:43.120
better and better and we're considering the consequences as well can help us.

01:32:43.120 --> 01:32:50.960
So I'm not, uh, it could be that the emphasis right until now was into announcing, uh, physical

01:32:50.960 --> 01:32:58.400
and cognitive capabilities, but I don't think that that's, uh, that the AI people would

01:32:58.400 --> 01:33:06.520
say, oh, we only want to help that, you know, that that's more and the more you are multidisciplinary,

01:33:06.520 --> 01:33:09.640
the more you understand these other dimensions.

01:33:09.640 --> 01:33:14.800
And then the AI people can also understand how to, you know, relate to those other dimensions,

01:33:14.800 --> 01:33:15.800
I think.

01:33:15.800 --> 01:33:21.280
So I think somebody like you engage Ben Gertzel, one of the major transhumanist AI who works

01:33:21.280 --> 01:33:22.280
in.

01:33:22.280 --> 01:33:23.280
I will engage with anybody.

01:33:23.280 --> 01:33:24.280
Yes.

01:33:24.280 --> 01:33:25.280
Yeah.

01:33:25.280 --> 01:33:26.280
But not the preclude.

01:33:26.280 --> 01:33:27.280
Yeah.

01:33:27.280 --> 01:33:28.280
But I want this into this.

01:33:28.280 --> 01:33:33.400
I saw with Nick bus from to, to events all the time and they speak with him all the time.

01:33:33.400 --> 01:33:35.360
So, so get him.

01:33:35.360 --> 01:33:38.920
Yeah, the move in your direction and we'll be in better shape.

01:33:38.920 --> 01:33:45.160
So I would just have a real quick, um, response in terms of actual transhumanism in terms

01:33:45.160 --> 01:33:48.400
of our species, um, evolving.

01:33:48.400 --> 01:33:54.240
Uh, it's, it seems to me that we can't, we can't stop it or let it go.

01:33:54.240 --> 01:33:56.160
We have no idea where it's going to go.

01:33:56.160 --> 01:33:59.880
I mean, do I disagree with this notion of directed evolution?

01:33:59.880 --> 01:34:02.960
No, but nobody's going to direct the evolution of the human species.

01:34:02.960 --> 01:34:07.040
It's just going to happen on our hands.

01:34:07.040 --> 01:34:12.120
I mean, I'm into that, but just tell this to Julia Pritzko, who has a whole conversation

01:34:12.120 --> 01:34:13.280
against what you do.

01:34:13.280 --> 01:34:17.760
They have no power to, they're, they're, they're like utilitarian somewhere.

01:34:17.760 --> 01:34:18.760
Correct.

01:34:18.760 --> 01:34:24.160
It's, you know, creating some sort of idea that does not apply to reality in a democratic

01:34:24.160 --> 01:34:25.160
society.

01:34:25.160 --> 01:34:26.160
I'm sorry.

01:34:26.160 --> 01:34:28.440
If you want to minimize their impact, go ahead and write against them.

01:34:28.440 --> 01:34:30.160
That's exactly what I've been doing.

01:34:30.160 --> 01:34:31.360
So we're running out of time.

01:34:31.360 --> 01:34:37.960
I'm going to ask that the last show of hands, one, two, three, four, that's not going to

01:34:37.960 --> 01:34:38.960
get it right there.

01:34:38.960 --> 01:34:39.960
Sorry.

01:34:39.960 --> 01:34:44.360
I just want to ask you each to make it really quick and you guys hold your, hold yours so

01:34:44.360 --> 01:34:46.000
you can write notes or whatever.

01:34:46.000 --> 01:34:49.360
Put it out there and then we'll give you one last chance to respond.

01:34:49.360 --> 01:34:53.640
So very quickly, please.

01:34:53.640 --> 01:34:55.920
My name is Bernard Starr.

01:34:55.920 --> 01:34:58.360
I'm a recovering academic.

01:34:58.360 --> 01:35:05.720
Several, several months ago, I published an article titled on the verge of immortality

01:35:05.720 --> 01:35:13.040
or are we stuck with death based on a, I'd say a four hour interview with a cellular

01:35:13.040 --> 01:35:14.840
biologist, Leonard Haeflick.

01:35:14.840 --> 01:35:15.840
Oh yeah.

01:35:15.840 --> 01:35:16.840
I'm sure many of you know.

01:35:16.840 --> 01:35:17.840
Yeah.

01:35:17.840 --> 01:35:23.240
Known for the Haeflick limit who set lifespan at about 120 years.

01:35:23.240 --> 01:35:25.840
And he has a radically different position.

01:35:25.840 --> 01:35:35.080
He's more or less a naysayer on the prospects of extensive life, life span.

01:35:35.080 --> 01:35:41.720
And it's a position that I haven't heard discussed here today.

01:35:41.720 --> 01:35:50.520
He believes that more than believes, he asserts that we know virtually zero about cellular

01:35:50.520 --> 01:35:57.480
aging and that most of the gains in longevity aside from public health measures that were

01:35:57.480 --> 01:36:04.280
introduced in the turn of the last century are based primarily on the cure of diseases.

01:36:04.280 --> 01:36:09.000
And he says, well, that's welcome and most of the funding is in that direction because

01:36:09.000 --> 01:36:16.880
that's what people demand and that's what the NIH and other funding agencies tend to

01:36:16.880 --> 01:36:23.160
fund that virtually nothing is spent on cellular aging at all.

01:36:23.160 --> 01:36:28.800
And that although we talk about diseases of old age, nobody really seriously addresses

01:36:28.800 --> 01:36:33.640
the question of why do cells age?

01:36:33.640 --> 01:36:40.760
That he believes that there's a common factor at the cellular level that would lead to a

01:36:40.760 --> 01:36:43.240
cure of all the diseases.

01:36:43.240 --> 01:36:50.760
All your comments, we're going to get all the questions out there.

01:36:50.760 --> 01:36:51.760
Please.

01:36:51.760 --> 01:36:52.920
My name is Moshe.

01:36:52.920 --> 01:36:54.400
I'm a psychologist.

01:36:54.400 --> 01:37:02.960
I'm trying to build on the point that Jill mentioned about the idea of artificial intelligence,

01:37:02.960 --> 01:37:07.600
technology and basically happiness.

01:37:07.600 --> 01:37:12.200
It's hard to imagine, we're just the beginning of that technology and expansion if it's

01:37:12.200 --> 01:37:19.920
cyber technology inside, outside and realizing how much of the people around us because of

01:37:19.920 --> 01:37:27.920
technology become addicted, anxiety exposed to that and so on and so forth, all the problems.

01:37:27.920 --> 01:37:32.920
And it has a spirit in itself, by the way, that's so hard to control and to imagine what

01:37:32.920 --> 01:37:36.640
is going to be, especially with the old stewards of singularity.

01:37:36.640 --> 01:37:41.560
I'm just curious to know about your perspective about that future that it almost has a spirit

01:37:41.560 --> 01:37:48.160
in itself that how is that going to impact us if it's at all impossible to predict about

01:37:48.160 --> 01:37:50.920
our own happiness or maybe spirituality?

01:37:50.920 --> 01:37:51.920
Thank you.

01:37:51.920 --> 01:37:54.920
And there was one over two.

01:37:54.920 --> 01:38:03.040
Very Paul, physics teacher, I was wondering, there was a artificial intelligence program

01:38:03.040 --> 01:38:13.040
that detected an exoplanet and the way it worked was that this exoplanet had been missed

01:38:13.040 --> 01:38:14.440
by humans.

01:38:14.440 --> 01:38:22.640
But we had lots of examples of planets that had been detected by humans and so the program

01:38:22.640 --> 01:38:28.200
was taught to recognize the planet and it recognized the planet better than humans could.

01:38:28.200 --> 01:38:30.360
So obviously that's a good thing.

01:38:30.360 --> 01:38:37.600
But I was wondering how can we defend against artificial intelligence when it might encroach

01:38:37.600 --> 01:38:46.600
on our privacy and be able to find, let's say it's some kind of disability, say somebody

01:38:46.600 --> 01:38:54.400
has a disability that isn't really something that is, you know, we should hold against

01:38:54.400 --> 01:38:55.400
them.

01:38:55.400 --> 01:38:59.400
And yet this artificial intelligence would be able to detect that.

01:38:59.400 --> 01:39:06.960
How do we defend against that?

01:39:06.960 --> 01:39:08.600
My name is Domisio Coutinho.

01:39:08.600 --> 01:39:10.880
I'm from Brazil.

01:39:10.880 --> 01:39:20.440
And I'd like to have a few questions, so if it's more common to what I heard, I really

01:39:20.440 --> 01:39:33.360
understand that it's possible and really admirable in there that it's able to have, you know,

01:39:33.360 --> 01:39:40.240
looking for artificial intelligence and no question about it, something desirable.

01:39:40.240 --> 01:39:49.760
But the question is, is there anything other than these that we should look for?

01:39:49.760 --> 01:39:58.440
We have from Plato a very important saying, nor should you yourself get to know yourself,

01:39:58.440 --> 01:40:00.080
nor is it an option.

01:40:00.080 --> 01:40:04.880
Do you know ourself enough?

01:40:04.880 --> 01:40:09.960
We know you're made from about 70 billions atoms in our body.

01:40:09.960 --> 01:40:18.120
Everything he has said about the importance of this extraordinary element in our life.

01:40:18.120 --> 01:40:23.400
Everything that moves, everything that's visible, everything that's around here is made and

01:40:23.400 --> 01:40:27.000
composed by atoms.

01:40:27.000 --> 01:40:31.480
What are these little ants, little ants in ourself?

01:40:31.480 --> 01:40:45.800
They see the potential opportunities made us here to be what we are.

01:40:45.800 --> 01:40:57.600
They taught us everything we are, how to walk, how to talk, how to speak, how to speak.

01:40:57.600 --> 01:41:03.720
When we try to improve ourself, improve humanity, do we have to look for somebody else, any

01:41:03.720 --> 01:41:10.560
place else, other than those who are responsible, or are existence youngers.

01:41:10.560 --> 01:41:17.400
And then if you do that, sister of the great-to-rate in rooms, you say, who is going to benefit

01:41:17.400 --> 01:41:19.120
our audience first?

01:41:19.120 --> 01:41:24.680
The regular human being, the street staff is going to benefit from artificial intelligence

01:41:24.680 --> 01:41:29.200
primarily, it's up to us to answer that question.

01:41:29.200 --> 01:41:34.040
However, I will say that these...

01:41:34.040 --> 01:41:40.000
There's a lot of things that we...

01:41:40.000 --> 01:41:42.120
I'm an ex-marathon runner.

01:41:42.120 --> 01:41:47.120
Before I run 1,000 miles, I learned how to run 100 miles.

01:41:47.120 --> 01:41:53.600
If you had to try to get artificial intelligence, I would ask you, do you know how to deal

01:41:53.600 --> 01:41:55.360
with the cancer?

01:41:55.360 --> 01:41:59.680
Do you know how to extinguish your fire that every year starts, thousands of thousands

01:41:59.680 --> 01:42:01.800
of miles, billions of billions of miles, are out of the United States?

01:42:01.800 --> 01:42:04.640
And do you cross the army and do not think about it?

01:42:04.640 --> 01:42:05.640
How about this storm?

01:42:05.640 --> 01:42:07.640
Thank you very much.

01:42:07.640 --> 01:42:08.640
Thank you.

01:42:08.640 --> 01:42:14.080
So, last round for everybody here.

01:42:14.080 --> 01:42:17.760
You can pick up on any of those pieces.

01:42:17.760 --> 01:42:25.640
So, I'd like to respond to the hay flick effect.

01:42:25.640 --> 01:42:32.280
And that idea that there was a limit to the number of cell divisions, and so there was

01:42:32.280 --> 01:42:42.560
this limit and cells couldn't go beyond the limit and they died and that consequence

01:42:42.560 --> 01:42:45.840
is that it would mean that there's a limit to human life.

01:42:45.840 --> 01:42:46.840
Sorry?

01:42:46.840 --> 01:42:47.840
Right.

01:42:47.840 --> 01:42:55.440
Now, the several problems with that, that was all done before the realization of what

01:42:55.440 --> 01:42:58.360
genetic engineering could do.

01:42:58.360 --> 01:43:03.000
And genetic engineering can make cells, I mean, you don't even need genetic engineering,

01:43:03.000 --> 01:43:04.240
just cancer.

01:43:04.240 --> 01:43:08.960
Cancer cells, they go dividing and dividing and dividing forever.

01:43:08.960 --> 01:43:12.040
We understand how telomeres are lengthened.

01:43:12.040 --> 01:43:18.920
And the important thing to think about is that if you look at my cells, it goes back

01:43:18.920 --> 01:43:21.960
in a continuous line back for 4 billion years.

01:43:21.960 --> 01:43:32.560
I mean, so, you know, it's, the organisms have figured out how to maintain continuity

01:43:32.560 --> 01:43:36.360
of life, even if the individual organism dies.

01:43:36.360 --> 01:43:43.120
And I think, I mean, I think hay flick is just, you know, he's of the generation prior to

01:43:43.120 --> 01:43:44.120
genetic engineering.

01:43:44.120 --> 01:43:50.360
I think you didn't know the power that was actually possible with that.

01:43:50.360 --> 01:43:56.720
I tend to agree with that assessment, but I think that's part of the way it is.

01:43:56.720 --> 01:44:06.320
Well, the second law of thermodynamics doesn't work because, I mean, there's a, there's a

01:44:06.320 --> 01:44:07.320
energy input, right?

01:44:07.320 --> 01:44:13.840
I mean, a single thermodynamics, everything's going to degrade if there's no energy input.

01:44:13.840 --> 01:44:18.440
For next 2 billion years, there's plenty of energy input and, and...

01:44:18.440 --> 01:44:21.440
At least on this planet.

01:44:21.440 --> 01:44:22.440
Yeah.

01:44:22.440 --> 01:44:23.440
Yeah.

01:44:23.440 --> 01:44:24.440
Okay.

01:44:24.440 --> 01:44:26.800
So, since there were a lot of questions about the AI, so maybe I'll try to respond to some

01:44:26.800 --> 01:44:27.800
of them.

01:44:27.800 --> 01:44:34.000
So, I remember, okay, the last one was about, you know, who is going to benefit for AI,

01:44:34.000 --> 01:44:37.920
you know, if the people in the street are going to benefit and so on.

01:44:37.920 --> 01:44:45.200
And I think that actually, you know, the, we can make, for example, healthcare is one

01:44:45.200 --> 01:44:53.200
obvious and current sector where AI is being used or trying to be used to, and to help

01:44:53.200 --> 01:44:58.080
doctors to find better cures, diagnosis, therapies and so on.

01:44:58.080 --> 01:45:05.080
And of course, this will improve the quality of healthcare in our first world, the country,

01:45:05.080 --> 01:45:06.080
world.

01:45:06.080 --> 01:45:12.040
But, you know, the main impact will be in developing countries where doctors do not see

01:45:12.040 --> 01:45:17.920
as many cases, they do not have, you know, the same education and kind of experience.

01:45:17.920 --> 01:45:22.080
So that's really what the impact is being done, actually.

01:45:22.080 --> 01:45:28.000
And more generally from that, I mean, again, the multidisciplinary is important because

01:45:28.000 --> 01:45:36.880
AI people and people in that know the problems that our planet has, like for example, the

01:45:36.880 --> 01:45:44.000
UN are regularly getting together and studying our AI can help towards the 17 sustainable

01:45:44.000 --> 01:45:45.000
development goals.

01:45:45.000 --> 01:45:48.080
For each one of them, what are the issues?

01:45:48.080 --> 01:45:49.080
What are the problems?

01:45:49.080 --> 01:45:54.160
How they can be framed and solved by AI in part or totally or whatever.

01:45:54.160 --> 01:46:01.600
So really, there is an emphasis on the represent the communities developing countries, you know,

01:46:01.600 --> 01:46:09.040
and what AI can do for that and not just for our first world world.

01:46:09.040 --> 01:46:12.160
The second one is about happiness and well-being.

01:46:12.160 --> 01:46:18.440
Again, I think that there are efforts in that direction, you know, to understand how to use

01:46:18.440 --> 01:46:25.400
AI not just to make us more efficient, but also to improve our well-being.

01:46:25.400 --> 01:46:33.280
So one example is that IEEE, which is the World Wide Association for Engineers, has

01:46:33.280 --> 01:46:42.600
put together a very interesting document, more than 200 pages, is called EthicaliAligned

01:46:42.600 --> 01:46:51.320
Design, which is about all the issues, ethics, issues that can come up when you are developing

01:46:51.320 --> 01:46:58.320
and deploying AI into the real world and for each one of them possible solutions.

01:46:58.320 --> 01:47:03.800
And one chapter of this big book is about well-being.

01:47:03.800 --> 01:47:06.120
So that is something to go.

01:47:06.120 --> 01:47:12.400
Another thing is that I, for example, am discussing with people that know about well-being and

01:47:12.400 --> 01:47:20.680
maybe they don't know about it, yeah, to understand really what it means for AI to help improve

01:47:20.680 --> 01:47:25.320
our well-being, whether it's an individual or a collective well-being and so on.

01:47:25.320 --> 01:47:33.000
So for example, there is a person, the venerable Tenzin Priyadarshi that I work with.

01:47:33.000 --> 01:47:39.320
He is an MIT, he is the Director of the Ethics Institute at MIT, and he is an expert of well-being,

01:47:39.320 --> 01:47:42.960
empathy, and these topics.

01:47:42.960 --> 01:47:48.200
And he is interested in really working to understand how technology and AI specifically

01:47:48.200 --> 01:47:58.960
can make us not less profitable, less efficient, but more, you know, and enhancing our well-being

01:47:58.960 --> 01:48:06.480
and empathy and these traits that are typical than we might want to maintain and even enhance.

01:48:06.480 --> 01:48:12.800
So I see a lot of things going on, but you have to understand that things are not, I mean,

01:48:12.800 --> 01:48:17.600
we are not at the point that, you know, everything is concluded and then you can judge AI right

01:48:17.600 --> 01:48:18.600
now.

01:48:18.600 --> 01:48:19.760
You know, things are evolving.

01:48:19.760 --> 01:48:25.560
People are understanding more and more and by talking to each other, especially in a place

01:48:25.560 --> 01:48:30.840
like this with people that have different point of view, different experience, you know.

01:48:30.840 --> 01:48:37.840
And then this brings along, you know, a better understanding of what to do.

01:48:37.840 --> 01:48:39.840
I'm doing it.

01:48:39.840 --> 01:48:42.840
Well, did you want to?

01:48:42.840 --> 01:48:44.840
I just want to take a moment.

01:48:44.840 --> 01:48:48.840
I'm smacked by how much things have changed in our lifetime.

01:48:48.840 --> 01:48:49.840
We should just be blown.

01:48:49.840 --> 01:48:54.480
I mean, we live day to day and we get used to this stuff very quickly, but these little

01:48:54.480 --> 01:48:59.840
devices here and the way we live today, the flying around the world, and we should just

01:48:59.840 --> 01:49:05.640
sort of take a moment and take a deep breath and say, my, how things have changed in a

01:49:05.640 --> 01:49:07.680
very short period of time.

01:49:07.680 --> 01:49:14.280
And remember that they could stop changing or could plateau or it could continue.

01:49:14.280 --> 01:49:20.960
And if it continues at the pace that it has been, then I think we will be a much transformed

01:49:20.960 --> 01:49:25.360
species on a very transformed planet for better and for ill.

01:49:25.360 --> 01:49:26.600
That's how I would frame it.

01:49:26.600 --> 01:49:28.960
So I think we could stop on that, Douglas.

01:49:28.960 --> 01:49:29.960
Yeah, that sounds good.

01:49:29.960 --> 01:49:35.680
I would also say on that little device in front of you, a certain point 20 years ago,

01:49:35.680 --> 01:49:37.800
very few people had them.

01:49:37.800 --> 01:49:39.360
Now everybody does.

01:49:39.360 --> 01:49:44.000
And so every technological development begins with a certain kind of, if you will, an

01:49:44.000 --> 01:49:46.240
imbalancer and injustice.

01:49:46.240 --> 01:49:53.080
And if we took that as our sole determinant, then we would have no technological development

01:49:53.080 --> 01:49:54.080
whatsoever.

01:49:54.080 --> 01:50:02.240
So I will try to have the last word here by saying that as long as we keep the critical

01:50:02.240 --> 01:50:08.400
perspective on those developments and not take them as inevitable and as necessary and

01:50:08.400 --> 01:50:11.960
as determined, then very good shape.

01:50:11.960 --> 01:50:18.200
But if we buy into the transhumanist myth that it has to happen the way, let's say, tells

01:50:18.200 --> 01:50:22.320
us it's going to happen, then we have a problem.

01:50:22.320 --> 01:50:23.320
Thank you.

01:50:23.320 --> 01:50:24.320
Thanks, Douglas.

01:50:24.320 --> 01:50:25.320
Thank you.

01:50:25.320 --> 01:50:26.320
Thanks.

01:50:26.320 --> 01:50:27.320
Thanks, darling.

01:50:27.320 --> 01:50:28.320
Bye.

01:50:28.320 --> 01:50:29.320
Bye.

01:50:29.320 --> 01:50:30.320
Bye.

01:50:30.320 --> 01:50:31.320
Bye.

01:50:31.320 --> 01:50:32.320
Bye.

01:50:32.320 --> 01:50:57.320
Bye.

